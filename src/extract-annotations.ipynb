{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"max_colwidth\", 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read annotation files from JSON\n",
    "# Dropping rows where summary sentence could not be matched\n",
    "annotation_files = glob.glob(\"../data/annotation/*.json\")\n",
    "annotations_raw = [pd.read_json(f) for f in annotation_files]\n",
    "papers_df = pd.read_pickle(\"../data/papers.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pairwise comparison lists, grouped by summary\n",
    "data = []\n",
    "for file in annotations_raw:\n",
    "    summary_id = file[\"summary_id\"][0]\n",
    "    paper_id = file[\"paper_id\"][0]\n",
    "    summary_doc = file[\"source_text\"].to_list()\n",
    "    paper_doc = list(papers_df[\"paper_text\"][papers_df[\"paper_id\"] == paper_id].item().values())\n",
    "    sids = [(source_sid, target_sid + 1) for source_sid, target_sid in zip(file[\"source_sid\"], file[\"target_sid\"])] # Make title sid = 1\n",
    "\n",
    "    example = {\n",
    "        \"summary_id\": summary_id,\n",
    "        \"source_text\": summary_doc,\n",
    "        \"target_text\": paper_doc,\n",
    "        \"sids\": sids,\n",
    "\n",
    "    }\n",
    "\n",
    "    data.append(example)\n",
    "\n",
    "annotations_docs = pd.DataFrame(data)\n",
    "annotations_docs.to_pickle(\"../data/docs-dataset.pkl\")\n",
    "annotations_docs.to_json(\"../data/docs-dataset.jsonl\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>source_text</th>\n",
       "      <th>target_text</th>\n",
       "      <th>sids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>[The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)., From a DP-based solution to the traveling salesman problem, they present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm., A beam search concept is applied as in speech recognition., There is no global pruning., An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account., In order to handle the necessary word reordering as an optimization problem within the dynamic programming approach, they describe a solution to the traveling salesman problem (TSP) which is based on dy...</td>\n",
       "      <td>[Word Re-ordering and DP-based Search in Statistical Machine Translation, In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)., Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm., A search restriction especially useful for the translation direction from German to English is presented., The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task., The goal of machine translation is the translation of a text given in some source language into a target language., We...</td>\n",
       "      <td>[(1, 2), (2, 3), (3, 166), (4, 167), (5, 36), (6, 40), (7, 194), (8, 195), (9, 5)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02-1025</td>\n",
       "      <td>[This paper presents a maximum entropy-based named entity recognizer (NER)., NER is useful in many NLP applications such as information extraction, question answering, etc .Chieu and Ng have shown that the maximum entropy framework is able to use global information directly from various sources., They believe that global context is useful in most languages, as it is a natural tendency for authors to use abbreviations on entities already mentioned previously., They have made use of local and global features to deal with the instances of same token in a document., They have made use of local and global features to deal with the instances of same token in a document., Their results show that their high performance NER use less training data than other systems., The use of global features ...</td>\n",
       "      <td>[Named Entity Recognition: A Maximum Entropy Approach Using Global Information, This paper presents a maximum entropy-based named entity recognizer (NER)., It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier., Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier., In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data., Considerable amount of work has been done in recent years on the na...</td>\n",
       "      <td>[(1, 2), (2, 7), (3, 205), (4, 63), (4, 64), (5, 15), (6, 11), (7, 199)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C10-1045</td>\n",
       "      <td>[This paper offers a broad insight into of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design., It is probably the first analysis of Arabic parsing of this kind., It is well-known that English constituency parsing models do not generalize to other languages and treebanks., Explanations for this phenomenon are relative informativeness of lexicalization, insensitivity to morphology and the effect of variable word order and these factors lead to syntactic disambiguation., The authors use linguistic and annotation insights to develop a manually annotated grammar and evaluate it and finally provide a realistic evaluation in which segmentation is performed in a pipeline jointly with parsing., The authors use linguistic and ann...</td>\n",
       "      <td>[Better Arabic Parsing: Baselines, Evaluations, and Analysis, In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design., First, we identify sources of syntactic ambiguity understudied in the existing parsing literature., Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic., Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG., Fourth, we show how to build better models for three different parsers., Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% ...</td>\n",
       "      <td>[(1, 2), (2, 27), (3, 8), (4, 8), (5, 23), (5, 25), (6, 22)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D10-1044_swastika</td>\n",
       "      <td>[Foster et all describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not., They extended previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and used simpler training procedure., They incorporated instance-weighting into a mixture-model framework, and found that it yielded consistent improvements over a wide range of baselines., In this paper, the authors proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance., Each out-of-domain phrase pair was ch...</td>\n",
       "      <td>[Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation, We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not., This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure., We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines., Domain adaptation is a common concern when optimizing empirical NLP applications., Even when there is training dat...</td>\n",
       "      <td>[(1, 2), (2, 3), (3, 4), (4, 145), (5, 146), (6, 147), (7, 151), (8, 152)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D10-1083</td>\n",
       "      <td>[In this paper, the authors are of the opinion that the sequence models-based approaches usually treat token-level tag assignment as the primary latent variable., However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity., In this work, they take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model., Their work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process., There are clustering approaches that assign a single POS tag to each word type., These clusters are computed using an SVD variant without relying on transitional structure., The departure from the traditional token-based tagging approach allow them to explicitly capture ...</td>\n",
       "      <td>[Simple Type-Level Unsupervised POS Tagging, Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus., Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy., However, in existing systems, this expansion come with a steep increase in model complexity., This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments., In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training., Our experiments consistently demonstrate that this model architecture yields substantial performance gains over mor...</td>\n",
       "      <td>[(1, 15), (2, 17), (3, 20), (4, 35), (5, 38), (6, 39), (7, 238), (8, 239), (9, 240), (10, 242)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          summary_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      source_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      target_text                                                                                             sids\n",
       "0           C00-2123  [The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)., From a DP-based solution to the traveling salesman problem, they present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm., A beam search concept is applied as in speech recognition., There is no global pruning., An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account., In order to handle the necessary word reordering as an optimization problem within the dynamic programming approach, they describe a solution to the traveling salesman problem (TSP) which is based on dy...  [Word Re-ordering and DP-based Search in Statistical Machine Translation, In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)., Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm., A search restriction especially useful for the translation direction from German to English is presented., The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task., The goal of machine translation is the translation of a text given in some source language into a target language., We...               [(1, 2), (2, 3), (3, 166), (4, 167), (5, 36), (6, 40), (7, 194), (8, 195), (9, 5)]\n",
       "1           C02-1025  [This paper presents a maximum entropy-based named entity recognizer (NER)., NER is useful in many NLP applications such as information extraction, question answering, etc .Chieu and Ng have shown that the maximum entropy framework is able to use global information directly from various sources., They believe that global context is useful in most languages, as it is a natural tendency for authors to use abbreviations on entities already mentioned previously., They have made use of local and global features to deal with the instances of same token in a document., They have made use of local and global features to deal with the instances of same token in a document., Their results show that their high performance NER use less training data than other systems., The use of global features ...  [Named Entity Recognition: A Maximum Entropy Approach Using Global Information, This paper presents a maximum entropy-based named entity recognizer (NER)., It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier., Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier., In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data., Considerable amount of work has been done in recent years on the na...                         [(1, 2), (2, 7), (3, 205), (4, 63), (4, 64), (5, 15), (6, 11), (7, 199)]\n",
       "2           C10-1045  [This paper offers a broad insight into of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design., It is probably the first analysis of Arabic parsing of this kind., It is well-known that English constituency parsing models do not generalize to other languages and treebanks., Explanations for this phenomenon are relative informativeness of lexicalization, insensitivity to morphology and the effect of variable word order and these factors lead to syntactic disambiguation., The authors use linguistic and annotation insights to develop a manually annotated grammar and evaluate it and finally provide a realistic evaluation in which segmentation is performed in a pipeline jointly with parsing., The authors use linguistic and ann...  [Better Arabic Parsing: Baselines, Evaluations, and Analysis, In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design., First, we identify sources of syntactic ambiguity understudied in the existing parsing literature., Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic., Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG., Fourth, we show how to build better models for three different parsers., Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% ...                                     [(1, 2), (2, 27), (3, 8), (4, 8), (5, 23), (5, 25), (6, 22)]\n",
       "3  D10-1044_swastika  [Foster et all describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not., They extended previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and used simpler training procedure., They incorporated instance-weighting into a mixture-model framework, and found that it yielded consistent improvements over a wide range of baselines., In this paper, the authors proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance., Each out-of-domain phrase pair was ch...  [Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation, We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not., This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure., We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines., Domain adaptation is a common concern when optimizing empirical NLP applications., Even when there is training dat...                       [(1, 2), (2, 3), (3, 4), (4, 145), (5, 146), (6, 147), (7, 151), (8, 152)]\n",
       "4           D10-1083  [In this paper, the authors are of the opinion that the sequence models-based approaches usually treat token-level tag assignment as the primary latent variable., However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity., In this work, they take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model., Their work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process., There are clustering approaches that assign a single POS tag to each word type., These clusters are computed using an SVD variant without relying on transitional structure., The departure from the traditional token-based tagging approach allow them to explicitly capture ...  [Simple Type-Level Unsupervised POS Tagging, Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus., Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy., However, in existing systems, this expansion come with a steep increase in model complexity., This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments., In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training., Our experiments consistently demonstrate that this model architecture yields substantial performance gains over mor...  [(1, 15), (2, 17), (3, 20), (4, 35), (5, 38), (6, 39), (7, 238), (8, 239), (9, 240), (10, 242)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_docs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29667 entries, 0 to 29666\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   summary_id   29667 non-null  object\n",
      " 1   paper_id     29667 non-null  object\n",
      " 2   source_sid   29667 non-null  string\n",
      " 3   source_text  29667 non-null  object\n",
      " 4   label        29667 non-null  object\n",
      " 5   target_text  29667 non-null  object\n",
      " 6   target_sid   29667 non-null  object\n",
      "dtypes: object(6), string(1)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_id</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>source_sid</th>\n",
       "      <th>source_text</th>\n",
       "      <th>label</th>\n",
       "      <th>target_text</th>\n",
       "      <th>target_sid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>0</td>\n",
       "      <td>Word Re-ordering and DP-based Search in Statistical Machine Translation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>1</td>\n",
       "      <td>In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>0</td>\n",
       "      <td>Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>0</td>\n",
       "      <td>A search restriction especially useful for the translation direction from German to English is presented.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>0</td>\n",
       "      <td>The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>0</td>\n",
       "      <td>The goal of machine translation is the translation of a text given in some source language into a target language.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>0</td>\n",
       "      <td>We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>0</td>\n",
       "      <td>Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model.</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>0</td>\n",
       "      <td>Our approach uses word-to-word dependencies between source and target words.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>C00-2123</td>\n",
       "      <td>1</td>\n",
       "      <td>The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</td>\n",
       "      <td>0</td>\n",
       "      <td>The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary_id  paper_id source_sid                                                                                                                        source_text label                                                                                                                                                                                                                                                                                                                                                                                                                                         target_text target_sid\n",
       "0   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     0                                                                                                                                                                                                                                                                                                                                                                             Word Re-ordering and DP-based Search in Statistical Machine Translation          0\n",
       "1   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     1                                                                                                                                                                                                                                                                                                                           In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).          1\n",
       "2   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     0                                                                                                                                                                                                                     Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.          2\n",
       "3   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     0                                                                                                                                                                                                                                                                                                                                           A search restriction especially useful for the translation direction from German to English is presented.          3\n",
       "4   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     0                                                                                                                                                                                                                                                                                                 The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.          4\n",
       "5   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     0                                                                                                                                                                                                                                                                                                                                  The goal of machine translation is the translation of a text given in some source language into a target language.          5\n",
       "6   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     0  We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.          6\n",
       "7   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     0                                                                                                                                                                                                                                                                                                                                             Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model.          7\n",
       "8   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     0                                                                                                                                                                                                                                                                                                                                                                        Our approach uses word-to-word dependencies between source and target words.          8\n",
       "9   C00-2123  C00-2123          1  The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).     0                                                                                                                                                                                                                                                                                                       The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).          9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating sentence pairs with binary labels (matching, non-matching)\n",
    "def create_labels(row):\n",
    "    sid = row[\"target_sid\"]\n",
    "    sentences = list(row[\"paper_text\"].keys())\n",
    "    labels = [1 if sid == sentence else 0 for sentence in sentences]\n",
    "    return labels\n",
    "\n",
    "summaries_df = pd.concat(annotations_raw).dropna(subset=\"target_sid\")\n",
    "summaries_df[\"source_sid\"] = summaries_df[\"source_sid\"].astype(\"int32\").astype(\"string\")\n",
    "summaries_df[\"target_sid\"] = summaries_df[\"target_sid\"].astype(\"int32\").astype(\"string\")\n",
    "summaries_df[\"strategy\"] = summaries_df[\"strategy\"].astype(\"category\")\n",
    "\n",
    "# Merge dataframes and get target sentences by id\n",
    "merged_df = summaries_df.merge(papers_df, on=\"paper_id\", how=\"left\").dropna(subset=\"target_sid\")\n",
    "\n",
    "annotations_df = merged_df[[\"summary_id\", \"paper_id\", \"source_sid\", \"target_sid\", \"source_text\"]].copy()\n",
    "annotations_df[\"label\"] = merged_df.apply(create_labels, axis=1)\n",
    "annotations_df[\"target_text\"] = merged_df.apply(lambda row: list(row[\"paper_text\"].values()), axis=1)\n",
    "\n",
    "annotations_df[\"target_sids\"] = merged_df.apply(lambda row: list(row[\"paper_text\"].keys()), axis=1)\n",
    "annotations_binary = annotations_df.explode([\"label\", \"target_text\", \"target_sids\"]).reset_index(drop=True)\n",
    "annotations_binary = annotations_binary.drop(columns=[\"target_sid\"]).rename(columns={\"target_sids\": \"target_sid\"})\n",
    "\n",
    "annotations_binary.to_pickle(\"../data/binary-dataset.pkl\")\n",
    "\n",
    "print(annotations_binary.info())\n",
    "display(annotations_binary.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
