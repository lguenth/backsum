{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import pickle as pk\n",
    "import re\n",
    "import codecs\n",
    "import nltk\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dirs():\n",
    "    Path(\"../data\").mkdir(exist_ok=True)\n",
    "    Path(\"../data/papers\").mkdir(exist_ok=True)\n",
    "    Path(\"../data/papers/xml\").mkdir(exist_ok=True)\n",
    "    Path(\"../data/papers/txt\").mkdir(exist_ok=True)\n",
    "    Path(\"../data/summaries\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files():\n",
    "    uids = []\n",
    "    data = Path(\"../original/data/\")\n",
    "    papers = data.glob(\"**/Reference_XML/*.xml\")\n",
    "    summaries = data.glob(\"**/**/*.human.txt\")\n",
    "\n",
    "    for summary in summaries:\n",
    "        uid = re.sub(\"([A-Z]\\d{2}-\\d{4})(.*)\", \"\\g<1>\", summary.stem)\n",
    "        uids.append(uid)\n",
    "\n",
    "        if bool(re.search(\"(?:[A-Z]\\d{2}-\\d{4})_(.*).human\", summary.stem)):\n",
    "            annotator = re.sub(\"(?:[A-Z]\\d{2}-\\d{4})_(.*).human\", \"\\g<1>\", summary.stem)\n",
    "            Path(f\"../data/summaries/{annotator}\").mkdir(exist_ok=True)\n",
    "            summary_path = f\"data/summaries/{annotator}/{summary.stem.replace(f'_{annotator}.human', '')}.txt\"\n",
    "        else:\n",
    "            summary_path = f\"data/summaries/{summary.stem.replace('.human', '')}.txt\"\n",
    "\n",
    "        shutil.copyfile(summary, summary_path)\n",
    "\n",
    "    for paper in papers:\n",
    "        if paper.stem in uids:\n",
    "            shutil.copyfile(paper, f\"data/papers/xml/{paper.stem}.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(path):\n",
    "    with codecs.open(path, \"r\", encoding=\"latin-1\") as file:\n",
    "        try:\n",
    "            xml = file.read()\n",
    "        except Exception as e:\n",
    "            print(\"Could not parse: \", paper)\n",
    "            print(e)\n",
    "\n",
    "        root = etree.fromstring(xml)\n",
    "        title = root.find(\"./S[@sid='0']\").text if not None else \"\"\n",
    "        text = root.xpath(\".//S[not(@sid = '0')]\")\n",
    "        \n",
    "        sentences = [s.text for s in text if s.text is not None]\n",
    "        sids = [s.attrib.get(\"sid\") for s in text if s.text is not None]\n",
    "\n",
    "        return title, text, sentences, sids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata():\n",
    "    papers = Path(\"data/papers/xml\").glob(\"*.xml\")\n",
    "    summaries = list(Path(\"data/summaries\").glob(\"**/*.txt\"))\n",
    "    data = []\n",
    "\n",
    "    for path in papers:\n",
    "        title, text, sentences, sids = parse_xml(path)\n",
    "        uid = path.stem\n",
    "\n",
    "        path_plain = f\"data/papers/txt/{uid}.txt\"\n",
    "        with open(path_plain, \"w\") as plain:\n",
    "            plain.writelines([s + \"\\n\" for s in sentences])\n",
    "            \n",
    "        path_summaries = [str(summary) for summary in summaries if uid in summary.stem]\n",
    "\n",
    "        paper = {\n",
    "            \"uid\": uid,\n",
    "            \"title\": title,\n",
    "            \"raw_paper\": sentences,\n",
    "            \"ids_paper\": sids,\n",
    "            \"path_xml\": path,\n",
    "            \"path_txt\": path_plain,\n",
    "            \"path_summaries\": path_summaries,\n",
    "        }\n",
    "\n",
    "        data.append(paper)\n",
    "\n",
    "    return sorted(data, key=lambda x: x[\"uid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitlines_summaries():\n",
    "    summaries = Path(\"data/summaries\").glob(\"**/*.txt\")\n",
    "\n",
    "    for summary in summaries:\n",
    "        with codecs.open(summary, \"r+\", encoding=\"latin-1\") as file:\n",
    "            text = file.read()\n",
    "            split = nltk.tokenize.sent_tokenize(text)\n",
    "            file.seek(0)\n",
    "            file.writelines([line + \"\\n\" for line in split])\n",
    "            file.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>raw_paper</th>\n",
       "      <th>ids_paper</th>\n",
       "      <th>path_xml</th>\n",
       "      <th>path_txt</th>\n",
       "      <th>path_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00-2018</td>\n",
       "      <td>A Maximum-Entropy-Inspired Parser *</td>\n",
       "      <td>[We present a new parser for parsing down to P...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/A00-2018.xml</td>\n",
       "      <td>data/papers/txt/A00-2018.txt</td>\n",
       "      <td>[data/summaries/vardha/A00-2018.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00-2030</td>\n",
       "      <td>A Novel Use of Statistical Parsing to Extract ...</td>\n",
       "      <td>[Since 1995, a few statistical parsing algorit...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/A00-2030.xml</td>\n",
       "      <td>data/papers/txt/A00-2030.txt</td>\n",
       "      <td>[data/summaries/vardha/A00-2030.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A97-1014</td>\n",
       "      <td>An Annotation Scheme for Free Word Order Langu...</td>\n",
       "      <td>[We describe an annotation scheme and a tool d...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/A97-1014.xml</td>\n",
       "      <td>data/papers/txt/A97-1014.txt</td>\n",
       "      <td>[data/summaries/vardha/A97-1014.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>Word Re-ordering and DP-based Search in Statis...</td>\n",
       "      <td>[In this paper, we describe a search procedure...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/C00-2123.xml</td>\n",
       "      <td>data/papers/txt/C00-2123.txt</td>\n",
       "      <td>[data/summaries/C00-2123.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02-1025</td>\n",
       "      <td>Named Entity Recognition: A Maximum Entropy Ap...</td>\n",
       "      <td>[This paper presents a maximum entropy-based n...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/C02-1025.xml</td>\n",
       "      <td>data/papers/txt/C02-1025.txt</td>\n",
       "      <td>[data/summaries/C02-1025.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C08-1098</td>\n",
       "      <td>Estimation of Conditional ProbabilitiesWith De...</td>\n",
       "      <td>[We present a HMM part-of-speech tagging metho...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/C08-1098.xml</td>\n",
       "      <td>data/papers/txt/C08-1098.txt</td>\n",
       "      <td>[data/summaries/C08-1098.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C10-1045</td>\n",
       "      <td>Better Arabic Parsing: Baselines, Evaluations,...</td>\n",
       "      <td>[In this paper, we offer broad insight into th...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/C10-1045.xml</td>\n",
       "      <td>data/papers/txt/C10-1045.txt</td>\n",
       "      <td>[data/summaries/C10-1045.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C90-2039</td>\n",
       "      <td>Strategic Lazy Incremental Copy Graph Unification</td>\n",
       "      <td>[The strategic lazy incremental copy graph uni...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/C90-2039.xml</td>\n",
       "      <td>data/papers/txt/C90-2039.txt</td>\n",
       "      <td>[data/summaries/C90-2039.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C94-2154</td>\n",
       "      <td>THE CORRECT AND EFFICIENT IMPLEMENTATION OF AP...</td>\n",
       "      <td>[in this pa,per, we argue tha, t type inferenc...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/C94-2154.xml</td>\n",
       "      <td>data/papers/txt/C94-2154.txt</td>\n",
       "      <td>[data/summaries/C94-2154.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D09-1092</td>\n",
       "      <td>Polylingual Topic Models</td>\n",
       "      <td>[Topic models are a useful tool for analyzing ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/D09-1092.xml</td>\n",
       "      <td>data/papers/txt/D09-1092.txt</td>\n",
       "      <td>[data/summaries/vardha/D09-1092.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D10-1044</td>\n",
       "      <td>Discriminative Instance Weighting for Domain A...</td>\n",
       "      <td>[We describe a new approach to SMT adaptation ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/D10-1044.xml</td>\n",
       "      <td>data/papers/txt/D10-1044.txt</td>\n",
       "      <td>[data/summaries/aakansha/D10-1044.txt, data/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D10-1083</td>\n",
       "      <td>Simple Type-Level Unsupervised POS Tagging</td>\n",
       "      <td>[Part-of-speech (POS) tag distributions are kn...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/D10-1083.xml</td>\n",
       "      <td>data/papers/txt/D10-1083.txt</td>\n",
       "      <td>[data/summaries/D10-1083.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>E03-1005</td>\n",
       "      <td>An Efficient Implementation of a New DOP Model</td>\n",
       "      <td>[Two apparently opposing DOP models exist in t...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/E03-1005.xml</td>\n",
       "      <td>data/papers/txt/E03-1005.txt</td>\n",
       "      <td>[data/summaries/aakansha/E03-1005.txt, data/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E03-1020</td>\n",
       "      <td>Discovering Corpus-Specific Word Senses</td>\n",
       "      <td>[This paper presents an unsupervised algorithm...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/E03-1020.xml</td>\n",
       "      <td>data/papers/txt/E03-1020.txt</td>\n",
       "      <td>[data/summaries/E03-1020.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>E09-2008</td>\n",
       "      <td>Foma: a finite-state compiler and library</td>\n",
       "      <td>[Foma is a compiler, programming language, and...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/E09-2008.xml</td>\n",
       "      <td>data/papers/txt/E09-2008.txt</td>\n",
       "      <td>[data/summaries/E09-2008.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H05-1115</td>\n",
       "      <td>Using Random Walks for Question-focused Senten...</td>\n",
       "      <td>[We consider the problem of question-focused s...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/H05-1115.xml</td>\n",
       "      <td>data/papers/txt/H05-1115.txt</td>\n",
       "      <td>[data/summaries/H05-1115.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H89-2014</td>\n",
       "      <td>Augmenting a Hidden Markov Model for Phrase-De...</td>\n",
       "      <td>[The paper describes refinements that are curr...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/H89-2014.xml</td>\n",
       "      <td>data/papers/txt/H89-2014.txt</td>\n",
       "      <td>[data/summaries/H89-2014.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I05-5011</td>\n",
       "      <td>Automatic Paraphrase Discovery based on Contex...</td>\n",
       "      <td>[Automatic paraphrase discovery is an importan...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/I05-5011.xml</td>\n",
       "      <td>data/papers/txt/I05-5011.txt</td>\n",
       "      <td>[data/summaries/I05-5011.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>J01-2004</td>\n",
       "      <td>Probabilistic Top-Down Parsing and Language Mo...</td>\n",
       "      <td>[This paper describes the functioning of a bro...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/J01-2004.xml</td>\n",
       "      <td>data/papers/txt/J01-2004.txt</td>\n",
       "      <td>[data/summaries/aakansha/J01-2004.txt, data/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>J96-3004</td>\n",
       "      <td>A Stochastic Finite-State Word-Segmentation Al...</td>\n",
       "      <td>[The initial stage of text analysis for any NL...</td>\n",
       "      <td>[, , , , , , 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...</td>\n",
       "      <td>data/papers/xml/J96-3004.xml</td>\n",
       "      <td>data/papers/txt/J96-3004.txt</td>\n",
       "      <td>[data/summaries/J96-3004.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>N01-1011</td>\n",
       "      <td>A Decision Tree of Bigrams is an Accurate Pred...</td>\n",
       "      <td>[This paper presents a corpus-based approach t...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/N01-1011.xml</td>\n",
       "      <td>data/papers/txt/N01-1011.txt</td>\n",
       "      <td>[data/summaries/N01-1011.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>N04-1038</td>\n",
       "      <td>Unsupervised Learning of Contextual Role Knowl...</td>\n",
       "      <td>[We present a coreference resolver called BABA...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/N04-1038.xml</td>\n",
       "      <td>data/papers/txt/N04-1038.txt</td>\n",
       "      <td>[data/summaries/N04-1038.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>N06-2049</td>\n",
       "      <td>Subword-based Tagging by Conditional Random Fi...</td>\n",
       "      <td>[We proposed two approaches to improve Chinese...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/N06-2049.xml</td>\n",
       "      <td>data/papers/txt/N06-2049.txt</td>\n",
       "      <td>[data/summaries/N06-2049.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P04-1036</td>\n",
       "      <td>Finding Predominant Word Senses in Untagged Text</td>\n",
       "      <td>[word sense disambiguation the heuristic of ch...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P04-1036.xml</td>\n",
       "      <td>data/papers/txt/P04-1036.txt</td>\n",
       "      <td>[data/summaries/vardha/P04-1036.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>P05-1004</td>\n",
       "      <td>Supersense Tagging of Unknown Nouns using Sema...</td>\n",
       "      <td>[The limited coverage of lexical-semantic reso...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P05-1004.xml</td>\n",
       "      <td>data/papers/txt/P05-1004.txt</td>\n",
       "      <td>[data/summaries/P05-1004.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>P05-1013</td>\n",
       "      <td>Pseudo-Projective Dependency Parsing</td>\n",
       "      <td>[In order to realize the full potential of dep...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P05-1013.xml</td>\n",
       "      <td>data/papers/txt/P05-1013.txt</td>\n",
       "      <td>[data/summaries/vardha/P05-1013.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>P06-2124</td>\n",
       "      <td>BiTAM: Bilingual Topic AdMixture Models forWor...</td>\n",
       "      <td>[We propose a novel bilingual topical admixtur...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P06-2124.xml</td>\n",
       "      <td>data/papers/txt/P06-2124.txt</td>\n",
       "      <td>[data/summaries/P06-2124.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P08-1028</td>\n",
       "      <td>Vector-based Models of Semantic Composition</td>\n",
       "      <td>[This paper proposes a framework for represent...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P08-1028.xml</td>\n",
       "      <td>data/papers/txt/P08-1028.txt</td>\n",
       "      <td>[data/summaries/aakansha/P08-1028.txt, data/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P08-1043</td>\n",
       "      <td>A Single Generative Model for Joint Morphologi...</td>\n",
       "      <td>[Morphological processes in Semitic languages ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P08-1043.xml</td>\n",
       "      <td>data/papers/txt/P08-1043.txt</td>\n",
       "      <td>[data/summaries/aakansha/P08-1043.txt, data/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>P08-1102</td>\n",
       "      <td>A Cascaded Linear Model for Joint Chinese Word...</td>\n",
       "      <td>[We propose a cascaded linear model for joint ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P08-1102.xml</td>\n",
       "      <td>data/papers/txt/P08-1102.txt</td>\n",
       "      <td>[data/summaries/aakansha/P08-1102.txt, data/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P11-1060</td>\n",
       "      <td>Learning Dependency-Based Compositional Semantics</td>\n",
       "      <td>[Compositional question answering begins by ma...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P11-1060.xml</td>\n",
       "      <td>data/papers/txt/P11-1060.txt</td>\n",
       "      <td>[data/summaries/aakansha/P11-1060.txt, data/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>P11-1061</td>\n",
       "      <td>Unsupervised Part-of-Speech Tagging with Bilin...</td>\n",
       "      <td>[We describe a novel approach for inducing uns...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P11-1061.xml</td>\n",
       "      <td>data/papers/txt/P11-1061.txt</td>\n",
       "      <td>[data/summaries/aakansha/P11-1061.txt, data/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>P87-1015</td>\n",
       "      <td>CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCE...</td>\n",
       "      <td>[We consider the structural descriptions produ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P87-1015.xml</td>\n",
       "      <td>data/papers/txt/P87-1015.txt</td>\n",
       "      <td>[data/summaries/vardha/P87-1015.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>P98-1081</td>\n",
       "      <td>Improving Data Driven Wordclass Tagging by Sys...</td>\n",
       "      <td>[In this paper we examine how the differences ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P98-1081.xml</td>\n",
       "      <td>data/papers/txt/P98-1081.txt</td>\n",
       "      <td>[data/summaries/P98-1081.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>P98-2143</td>\n",
       "      <td>Robust pronoun resolution with limited knowledge</td>\n",
       "      <td>[Most traditional approaches to anaphora resol...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/P98-2143.xml</td>\n",
       "      <td>data/papers/txt/P98-2143.txt</td>\n",
       "      <td>[data/summaries/P98-2143.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>W03-0410</td>\n",
       "      <td>Semi-supervised Verb Class Discovery Using Noi...</td>\n",
       "      <td>[We cluster verbs into lexical semantic classe...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/W03-0410.xml</td>\n",
       "      <td>data/papers/txt/W03-0410.txt</td>\n",
       "      <td>[data/summaries/W03-0410.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>W04-0213</td>\n",
       "      <td>The Potsdam Commentary Corpus</td>\n",
       "      <td>[A corpus of German newspaper commentaries has...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/W04-0213.xml</td>\n",
       "      <td>data/papers/txt/W04-0213.txt</td>\n",
       "      <td>[data/summaries/W04-0213.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>W06-2932</td>\n",
       "      <td>Multilingual Dependency Analysis with a Two-St...</td>\n",
       "      <td>[present a two-stage multilingual pendency par...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/W06-2932.xml</td>\n",
       "      <td>data/papers/txt/W06-2932.txt</td>\n",
       "      <td>[data/summaries/vardha/W06-2932.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>W06-3114</td>\n",
       "      <td>Manual and Automatic Evaluation of Machine Tra...</td>\n",
       "      <td>[Adequacy (rank) Fluency (rank) BLEU (rank) up...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/W06-3114.xml</td>\n",
       "      <td>data/papers/txt/W06-3114.txt</td>\n",
       "      <td>[data/summaries/aakansha/W06-3114.txt, data/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>W08-2222</td>\n",
       "      <td>Wide-Coverage Semantic Analysis with Boxer</td>\n",
       "      <td>[Boxer is an open-domain software component fo...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/W08-2222.xml</td>\n",
       "      <td>data/papers/txt/W08-2222.txt</td>\n",
       "      <td>[data/summaries/W08-2222.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>W11-2123</td>\n",
       "      <td>KenLM: Faster and Smaller Language Model Queries</td>\n",
       "      <td>[We present KenLM, a library that implements t...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/W11-2123.xml</td>\n",
       "      <td>data/papers/txt/W11-2123.txt</td>\n",
       "      <td>[data/summaries/vardha/W11-2123.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>W95-0104</td>\n",
       "      <td>A Bayesian hybrid method for context-sensitive...</td>\n",
       "      <td>[Two classes of methods have been shown to be ...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/W95-0104.xml</td>\n",
       "      <td>data/papers/txt/W95-0104.txt</td>\n",
       "      <td>[data/summaries/W95-0104.txt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>W99-0613</td>\n",
       "      <td>Unsupervised Models for Named Entity Classific...</td>\n",
       "      <td>[This paper discusses the use of unlabeled exa...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/W99-0613.xml</td>\n",
       "      <td>data/papers/txt/W99-0613.txt</td>\n",
       "      <td>[data/summaries/vardha/W99-0613.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>W99-0623</td>\n",
       "      <td>Exploiting Diversity in Natural Language Proce...</td>\n",
       "      <td>[Three state-of-the-art statistical parsers ar...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/W99-0623.xml</td>\n",
       "      <td>data/papers/txt/W99-0623.txt</td>\n",
       "      <td>[data/summaries/vardha/W99-0623.txt, data/summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>X96-1048</td>\n",
       "      <td>OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION</td>\n",
       "      <td>[Test abstract, The latest in a series of natu...</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>data/papers/xml/X96-1048.xml</td>\n",
       "      <td>data/papers/txt/X96-1048.txt</td>\n",
       "      <td>[data/summaries/X96-1048.txt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid                                              title  \\\n",
       "0   A00-2018                A Maximum-Entropy-Inspired Parser *   \n",
       "1   A00-2030  A Novel Use of Statistical Parsing to Extract ...   \n",
       "2   A97-1014  An Annotation Scheme for Free Word Order Langu...   \n",
       "3   C00-2123  Word Re-ordering and DP-based Search in Statis...   \n",
       "4   C02-1025  Named Entity Recognition: A Maximum Entropy Ap...   \n",
       "5   C08-1098  Estimation of Conditional ProbabilitiesWith De...   \n",
       "6   C10-1045  Better Arabic Parsing: Baselines, Evaluations,...   \n",
       "7   C90-2039  Strategic Lazy Incremental Copy Graph Unification   \n",
       "8   C94-2154  THE CORRECT AND EFFICIENT IMPLEMENTATION OF AP...   \n",
       "9   D09-1092                           Polylingual Topic Models   \n",
       "10  D10-1044  Discriminative Instance Weighting for Domain A...   \n",
       "11  D10-1083         Simple Type-Level Unsupervised POS Tagging   \n",
       "12  E03-1005     An Efficient Implementation of a New DOP Model   \n",
       "13  E03-1020            Discovering Corpus-Specific Word Senses   \n",
       "14  E09-2008          Foma: a finite-state compiler and library   \n",
       "15  H05-1115  Using Random Walks for Question-focused Senten...   \n",
       "16  H89-2014  Augmenting a Hidden Markov Model for Phrase-De...   \n",
       "17  I05-5011  Automatic Paraphrase Discovery based on Contex...   \n",
       "18  J01-2004  Probabilistic Top-Down Parsing and Language Mo...   \n",
       "19  J96-3004  A Stochastic Finite-State Word-Segmentation Al...   \n",
       "20  N01-1011  A Decision Tree of Bigrams is an Accurate Pred...   \n",
       "21  N04-1038  Unsupervised Learning of Contextual Role Knowl...   \n",
       "22  N06-2049  Subword-based Tagging by Conditional Random Fi...   \n",
       "23  P04-1036   Finding Predominant Word Senses in Untagged Text   \n",
       "24  P05-1004  Supersense Tagging of Unknown Nouns using Sema...   \n",
       "25  P05-1013               Pseudo-Projective Dependency Parsing   \n",
       "26  P06-2124  BiTAM: Bilingual Topic AdMixture Models forWor...   \n",
       "27  P08-1028        Vector-based Models of Semantic Composition   \n",
       "28  P08-1043  A Single Generative Model for Joint Morphologi...   \n",
       "29  P08-1102  A Cascaded Linear Model for Joint Chinese Word...   \n",
       "30  P11-1060  Learning Dependency-Based Compositional Semantics   \n",
       "31  P11-1061  Unsupervised Part-of-Speech Tagging with Bilin...   \n",
       "32  P87-1015  CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCE...   \n",
       "33  P98-1081  Improving Data Driven Wordclass Tagging by Sys...   \n",
       "34  P98-2143   Robust pronoun resolution with limited knowledge   \n",
       "35  W03-0410  Semi-supervised Verb Class Discovery Using Noi...   \n",
       "36  W04-0213                      The Potsdam Commentary Corpus   \n",
       "37  W06-2932  Multilingual Dependency Analysis with a Two-St...   \n",
       "38  W06-3114  Manual and Automatic Evaluation of Machine Tra...   \n",
       "39  W08-2222         Wide-Coverage Semantic Analysis with Boxer   \n",
       "40  W11-2123   KenLM: Faster and Smaller Language Model Queries   \n",
       "41  W95-0104  A Bayesian hybrid method for context-sensitive...   \n",
       "42  W99-0613  Unsupervised Models for Named Entity Classific...   \n",
       "43  W99-0623  Exploiting Diversity in Natural Language Proce...   \n",
       "44  X96-1048        OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION   \n",
       "\n",
       "                                            raw_paper  \\\n",
       "0   [We present a new parser for parsing down to P...   \n",
       "1   [Since 1995, a few statistical parsing algorit...   \n",
       "2   [We describe an annotation scheme and a tool d...   \n",
       "3   [In this paper, we describe a search procedure...   \n",
       "4   [This paper presents a maximum entropy-based n...   \n",
       "5   [We present a HMM part-of-speech tagging metho...   \n",
       "6   [In this paper, we offer broad insight into th...   \n",
       "7   [The strategic lazy incremental copy graph uni...   \n",
       "8   [in this pa,per, we argue tha, t type inferenc...   \n",
       "9   [Topic models are a useful tool for analyzing ...   \n",
       "10  [We describe a new approach to SMT adaptation ...   \n",
       "11  [Part-of-speech (POS) tag distributions are kn...   \n",
       "12  [Two apparently opposing DOP models exist in t...   \n",
       "13  [This paper presents an unsupervised algorithm...   \n",
       "14  [Foma is a compiler, programming language, and...   \n",
       "15  [We consider the problem of question-focused s...   \n",
       "16  [The paper describes refinements that are curr...   \n",
       "17  [Automatic paraphrase discovery is an importan...   \n",
       "18  [This paper describes the functioning of a bro...   \n",
       "19  [The initial stage of text analysis for any NL...   \n",
       "20  [This paper presents a corpus-based approach t...   \n",
       "21  [We present a coreference resolver called BABA...   \n",
       "22  [We proposed two approaches to improve Chinese...   \n",
       "23  [word sense disambiguation the heuristic of ch...   \n",
       "24  [The limited coverage of lexical-semantic reso...   \n",
       "25  [In order to realize the full potential of dep...   \n",
       "26  [We propose a novel bilingual topical admixtur...   \n",
       "27  [This paper proposes a framework for represent...   \n",
       "28  [Morphological processes in Semitic languages ...   \n",
       "29  [We propose a cascaded linear model for joint ...   \n",
       "30  [Compositional question answering begins by ma...   \n",
       "31  [We describe a novel approach for inducing uns...   \n",
       "32  [We consider the structural descriptions produ...   \n",
       "33  [In this paper we examine how the differences ...   \n",
       "34  [Most traditional approaches to anaphora resol...   \n",
       "35  [We cluster verbs into lexical semantic classe...   \n",
       "36  [A corpus of German newspaper commentaries has...   \n",
       "37  [present a two-stage multilingual pendency par...   \n",
       "38  [Adequacy (rank) Fluency (rank) BLEU (rank) up...   \n",
       "39  [Boxer is an open-domain software component fo...   \n",
       "40  [We present KenLM, a library that implements t...   \n",
       "41  [Two classes of methods have been shown to be ...   \n",
       "42  [This paper discusses the use of unlabeled exa...   \n",
       "43  [Three state-of-the-art statistical parsers ar...   \n",
       "44  [Test abstract, The latest in a series of natu...   \n",
       "\n",
       "                                            ids_paper  \\\n",
       "0   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "1   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "2   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "3   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "4   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "5   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "6   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "7   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "8   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "9   [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "10  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "11  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "12  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "13  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "14  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "15  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "16  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "17  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "18  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "19  [, , , , , , 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...   \n",
       "20  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "21  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "22  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "23  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "24  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "25  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "26  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "27  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "28  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "29  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "30  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "31  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "32  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "33  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "34  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "35  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "36  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "37  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "38  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "39  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "40  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "41  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "42  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "43  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "44  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "\n",
       "                        path_xml                      path_txt  \\\n",
       "0   data/papers/xml/A00-2018.xml  data/papers/txt/A00-2018.txt   \n",
       "1   data/papers/xml/A00-2030.xml  data/papers/txt/A00-2030.txt   \n",
       "2   data/papers/xml/A97-1014.xml  data/papers/txt/A97-1014.txt   \n",
       "3   data/papers/xml/C00-2123.xml  data/papers/txt/C00-2123.txt   \n",
       "4   data/papers/xml/C02-1025.xml  data/papers/txt/C02-1025.txt   \n",
       "5   data/papers/xml/C08-1098.xml  data/papers/txt/C08-1098.txt   \n",
       "6   data/papers/xml/C10-1045.xml  data/papers/txt/C10-1045.txt   \n",
       "7   data/papers/xml/C90-2039.xml  data/papers/txt/C90-2039.txt   \n",
       "8   data/papers/xml/C94-2154.xml  data/papers/txt/C94-2154.txt   \n",
       "9   data/papers/xml/D09-1092.xml  data/papers/txt/D09-1092.txt   \n",
       "10  data/papers/xml/D10-1044.xml  data/papers/txt/D10-1044.txt   \n",
       "11  data/papers/xml/D10-1083.xml  data/papers/txt/D10-1083.txt   \n",
       "12  data/papers/xml/E03-1005.xml  data/papers/txt/E03-1005.txt   \n",
       "13  data/papers/xml/E03-1020.xml  data/papers/txt/E03-1020.txt   \n",
       "14  data/papers/xml/E09-2008.xml  data/papers/txt/E09-2008.txt   \n",
       "15  data/papers/xml/H05-1115.xml  data/papers/txt/H05-1115.txt   \n",
       "16  data/papers/xml/H89-2014.xml  data/papers/txt/H89-2014.txt   \n",
       "17  data/papers/xml/I05-5011.xml  data/papers/txt/I05-5011.txt   \n",
       "18  data/papers/xml/J01-2004.xml  data/papers/txt/J01-2004.txt   \n",
       "19  data/papers/xml/J96-3004.xml  data/papers/txt/J96-3004.txt   \n",
       "20  data/papers/xml/N01-1011.xml  data/papers/txt/N01-1011.txt   \n",
       "21  data/papers/xml/N04-1038.xml  data/papers/txt/N04-1038.txt   \n",
       "22  data/papers/xml/N06-2049.xml  data/papers/txt/N06-2049.txt   \n",
       "23  data/papers/xml/P04-1036.xml  data/papers/txt/P04-1036.txt   \n",
       "24  data/papers/xml/P05-1004.xml  data/papers/txt/P05-1004.txt   \n",
       "25  data/papers/xml/P05-1013.xml  data/papers/txt/P05-1013.txt   \n",
       "26  data/papers/xml/P06-2124.xml  data/papers/txt/P06-2124.txt   \n",
       "27  data/papers/xml/P08-1028.xml  data/papers/txt/P08-1028.txt   \n",
       "28  data/papers/xml/P08-1043.xml  data/papers/txt/P08-1043.txt   \n",
       "29  data/papers/xml/P08-1102.xml  data/papers/txt/P08-1102.txt   \n",
       "30  data/papers/xml/P11-1060.xml  data/papers/txt/P11-1060.txt   \n",
       "31  data/papers/xml/P11-1061.xml  data/papers/txt/P11-1061.txt   \n",
       "32  data/papers/xml/P87-1015.xml  data/papers/txt/P87-1015.txt   \n",
       "33  data/papers/xml/P98-1081.xml  data/papers/txt/P98-1081.txt   \n",
       "34  data/papers/xml/P98-2143.xml  data/papers/txt/P98-2143.txt   \n",
       "35  data/papers/xml/W03-0410.xml  data/papers/txt/W03-0410.txt   \n",
       "36  data/papers/xml/W04-0213.xml  data/papers/txt/W04-0213.txt   \n",
       "37  data/papers/xml/W06-2932.xml  data/papers/txt/W06-2932.txt   \n",
       "38  data/papers/xml/W06-3114.xml  data/papers/txt/W06-3114.txt   \n",
       "39  data/papers/xml/W08-2222.xml  data/papers/txt/W08-2222.txt   \n",
       "40  data/papers/xml/W11-2123.xml  data/papers/txt/W11-2123.txt   \n",
       "41  data/papers/xml/W95-0104.xml  data/papers/txt/W95-0104.txt   \n",
       "42  data/papers/xml/W99-0613.xml  data/papers/txt/W99-0613.txt   \n",
       "43  data/papers/xml/W99-0623.xml  data/papers/txt/W99-0623.txt   \n",
       "44  data/papers/xml/X96-1048.xml  data/papers/txt/X96-1048.txt   \n",
       "\n",
       "                                       path_summaries  \n",
       "0   [data/summaries/vardha/A00-2018.txt, data/summ...  \n",
       "1   [data/summaries/vardha/A00-2030.txt, data/summ...  \n",
       "2   [data/summaries/vardha/A97-1014.txt, data/summ...  \n",
       "3                       [data/summaries/C00-2123.txt]  \n",
       "4                       [data/summaries/C02-1025.txt]  \n",
       "5                       [data/summaries/C08-1098.txt]  \n",
       "6                       [data/summaries/C10-1045.txt]  \n",
       "7                       [data/summaries/C90-2039.txt]  \n",
       "8                       [data/summaries/C94-2154.txt]  \n",
       "9   [data/summaries/vardha/D09-1092.txt, data/summ...  \n",
       "10  [data/summaries/aakansha/D10-1044.txt, data/su...  \n",
       "11                      [data/summaries/D10-1083.txt]  \n",
       "12  [data/summaries/aakansha/E03-1005.txt, data/su...  \n",
       "13                      [data/summaries/E03-1020.txt]  \n",
       "14                      [data/summaries/E09-2008.txt]  \n",
       "15                      [data/summaries/H05-1115.txt]  \n",
       "16                      [data/summaries/H89-2014.txt]  \n",
       "17                      [data/summaries/I05-5011.txt]  \n",
       "18  [data/summaries/aakansha/J01-2004.txt, data/su...  \n",
       "19                      [data/summaries/J96-3004.txt]  \n",
       "20                      [data/summaries/N01-1011.txt]  \n",
       "21                      [data/summaries/N04-1038.txt]  \n",
       "22                      [data/summaries/N06-2049.txt]  \n",
       "23  [data/summaries/vardha/P04-1036.txt, data/summ...  \n",
       "24                      [data/summaries/P05-1004.txt]  \n",
       "25  [data/summaries/vardha/P05-1013.txt, data/summ...  \n",
       "26                      [data/summaries/P06-2124.txt]  \n",
       "27  [data/summaries/aakansha/P08-1028.txt, data/su...  \n",
       "28  [data/summaries/aakansha/P08-1043.txt, data/su...  \n",
       "29  [data/summaries/aakansha/P08-1102.txt, data/su...  \n",
       "30  [data/summaries/aakansha/P11-1060.txt, data/su...  \n",
       "31  [data/summaries/aakansha/P11-1061.txt, data/su...  \n",
       "32  [data/summaries/vardha/P87-1015.txt, data/summ...  \n",
       "33                      [data/summaries/P98-1081.txt]  \n",
       "34                      [data/summaries/P98-2143.txt]  \n",
       "35                      [data/summaries/W03-0410.txt]  \n",
       "36                      [data/summaries/W04-0213.txt]  \n",
       "37  [data/summaries/vardha/W06-2932.txt, data/summ...  \n",
       "38  [data/summaries/aakansha/W06-3114.txt, data/su...  \n",
       "39                      [data/summaries/W08-2222.txt]  \n",
       "40  [data/summaries/vardha/W11-2123.txt, data/summ...  \n",
       "41                      [data/summaries/W95-0104.txt]  \n",
       "42  [data/summaries/vardha/W99-0613.txt, data/summ...  \n",
       "43  [data/summaries/vardha/W99-0623.txt, data/summ...  \n",
       "44                      [data/summaries/X96-1048.txt]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    create_dirs()\n",
    "    copy_files()\n",
    "    splitlines_summaries()\n",
    "\n",
    "    data = extract_metadata()\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_pickle(\"data.pkl\")\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank., This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]., The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events., We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's p...\n",
       "1     [Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard., In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations., Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard., Yet, relatively few have embedded one of these algorithms in a task., Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved s...\n",
       "2     [We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages., Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme., The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata., The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction., In particular, we focus on several methodological issues concerning the annotation of non-configurational languages., In section 2, we examine the appropriateness of existing annot...\n",
       "3     [In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)., Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm., A search restriction especially useful for the translation direction from German to English is presented., The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task., The goal of machine translation is the translation of a text given in some source language into a target language., We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to ...\n",
       "4     [This paper presents a maximum entropy-based named entity recognizer (NER)., It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier., Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier., In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data., Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conference...\n",
       "5     [We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags., It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs., In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers., A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags., Tagsets of this size c...\n",
       "6     [In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design., First, we identify sources of syntactic ambiguity understudied in the existing parsing literature., Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic., Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG., Fourth, we show how to build better models for three different parsers., Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% F1., It is well-known that constituency parsing models design...\n",
       "7     [The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures., One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory., The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation., The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapping occurrences, thus increasing the total efficiency of natural language processing ...\n",
       "8     [in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch., !['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation., Unification lbrmMisms ma.y be either un-typed (DCC~s, PATRII, 1,F(;) or typed (npsG)., A m~L,ior reason for adding types to ~ forma,lism is to express restrictions on fea.ture cooccurences a.s in (;l's(:: [5] in order to rule out nonexista.nt tyl)es of objects., For example, there a.re no verbs which have the [km.ture +R. The simplest way to express such restrictions is by mea.ns of a.n...\n",
       "9     [Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts., Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages., We introduce a polylingual topic model that discovers topics aligned across multiple languages., We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages., Statistical topic models have emerged as an increasingly useful analysis tool for large text collections., Topic models have been used ...\n",
       "10    [We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not., This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure., We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines., Domain adaptation is a common concern when optimizing empirical NLP applications., Even when there is training data available in the domain of interest, there is often additional data from other domains tha...\n",
       "11    [Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus., Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy., However, in existing systems, this expansion come with a steep increase in model complexity., This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments., In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training., Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts., On several ...\n",
       "12    [Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank., This paper proposes an integration of the two models which outperforms each of them separately., Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence., The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments, without imposing a...\n",
       "13    [This paper presents an unsupervised algorithm which automatically discovers word senses from text., The algorithm is based on a graph model representing words and relationships between them., Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word., Discrimination against previously extracted sense clusters enables us to discover new senses., We use the same data for both recognising and resolving ambiguity., This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies., Automatic word sense discovery has applications of many kinds., It can greatly facilitate a lexicographer's work and can be used to automatically constru...\n",
       "14    [Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses., It has specific support for many natural language processing applications such as producing morphological and phonological analyzers., Foma is largely compatible with the Xerox/PARC finite-state toolkit., It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the âMathematical Operatorsâ Unicode block., Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyze...\n",
       "15    [We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event stories published over time., Annotators generated a list of questions central to understanding each story in our corpus., Because of the dynamic nature of the stories,many questions are time-sensitive (e.g.How many victims have been found?)Judges found sentences providing an answer to each question., To address thesentence retrieval problem, we apply astochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization., Currently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentence...\n",
       "16    [The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text., The model has the advantage that a pre-tagged training corpus is not required., Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model., State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models., The structure of the state chains is based on both an analysis of errors and linguistic knowledge., Examples show how word dependency across phrases can be modeled., The determination of part-of-speech categories for words is an important problem in la...\n",
       "17    [Automatic paraphrase discovery is an important but challenging task., We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue., We focus on phrases which connect two Named Entities (NEs), and proceed in two stages., The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets., The second stage links sets which involve the same pairs of individual NEs., A total of 13,976 phrases were grouped., The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the links for two evaluated domains was 73% and 86%., One of the difficulties in Natural Language Processing is the fact that there are many...\n",
       "18    [This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition., The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling., A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers., A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity., Interpolation with a trig...\n",
       "19    [The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.,  For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.,  In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to \"reconstruct\" the word-boundary information.,  In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.,  The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunc...\n",
       "20    [This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby., This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise., It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words., Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs., For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined., For example, suppose bill has the following set of possible meanings: a piece of currency, pending legisla...\n",
       "21    [We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor., BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning., These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible., BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources., Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns., The problem of coreference resolution has received considerable attention, including theoretical dis...\n",
       "22    [We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach., We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation., In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates., By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005., The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005)., Under the scheme, each character of a word is labeled as âBâ if it is...\n",
       "23    [word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed., The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data., Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration., We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically., The acquired predominant senses give a o...\n",
       "24    [The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words., Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET., Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples., We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger., We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity., Lexical-semantic resources have been applied successful to a wide range of Natural Langu...\n",
       "25    [In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures., We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures., Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy., This leads to the best reported performance for robust non-projective parsing of Czech., It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate t...\n",
       "26    [We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation., Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model., Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels)., These models enable word- alignment process to leverage topical contents of document-pairs., Efficient variational approximation algorithms are designed for inference and parameter estimation., With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspect...\n",
       "27    [This paper proposes a framework for representing the meaning of phrases and sentences in vector space., Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions., Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task., Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments., Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science., The appeal of these models lies in their ability to represent meaning simply by using distributional informat...\n",
       "28    [Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence., These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance., Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity., Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so ...\n",
       "29    [We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging., With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly., Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging., On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline., Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages., Several ...\n",
       "30    [Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms., In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs., In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms., On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms., What is the total population of the ten largest capitals in the US?, Answering these types of complex questions compositiona...\n",
       "31    [We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language., Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages., We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010)., Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm., Supervised learning appr...\n",
       "32    [We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate., In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in p...\n",
       "33    [In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system., We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging., Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data., After comparison, their outputs are combined using several voting strategies and second stage classifiers., All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger., In all Natural Language Processing (NLP) systems, we find one or more language models wh...\n",
       "34    [Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge., One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task., This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger., Input is checked against agreement and for a number of antecedent indicators., Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent., Evaluation reports a success rate of 89.7% which is better than the sucÂ­ cess rates of the approaches selected for comparison and tested on the same data., In addition, preliminary e...\n",
       "35    [We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs., The feature set was previously shown to work well in a supervised learning setting, using known English verb classes., In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task., We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties., We find that the unsupervised method we tried cannot be consistently applied to our data., However, the semi- supervised approach (using a seed set of ...\n",
       "36    [A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure., The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation., A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees., Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization)., This paper, however, provides a compre...\n",
       "37    [present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages., The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages., The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph., We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis., Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing., With the availability of resources such as the Penn WSJ Treebank, much of the f...\n",
       "38    [Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) u...\n",
       "39    [Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT)., Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts., The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles., The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic., Boxerâs performance on the shared task for comparing semantic represtations was promising., It was able to produce complete DRSs for all seven texts., Manually inspecting the output reve...\n",
       "40    [We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs., The structure uses linear probing hash tables and is designed for speed., Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline., Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems., This paper describes the several performance techniques used and presents benchmarks against alternative implementations., Language models are widely applied in n...\n",
       "41    [Two classes of methods have been shown to be useful for resolving lexical ambiguity., The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word., These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax., Yarowsky has exploited this complementarity by combining the two methods using decision lists., The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be., This paper takes Yarowsky's work as a starting point, applying decision li...\n",
       "42    [This paper discusses the use of unlabeled examples for the problem of named entity classification., A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules., The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type., We present two algorithms., The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98)., The second algorithm extends ideas from boosting algorithm...\n",
       "43    [Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy., Two general approaches are presented and two combination techniques are described for each approach., Both parametric and non-parametric models are explored., The resulting parsers surpass the best previously published performance results for the Penn Treebank., The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems., The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992; Heath et al., 1996)., Their theoretical finding is simply stated: classification error ra...\n",
       "44    [Test abstract, The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November., Participants were invited to enter their systems in as many as four different task-oriented evaluations., The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time., The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years., The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume., All except the Scenario Template task are de...\n",
       "Name: raw_paper, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)\n",
    "df.raw_paper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
