{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lukel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import pickle as pk\n",
    "import re\n",
    "import codecs\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_original_data():\n",
    "    uids = []\n",
    "    data = Path(\"../original/data/\")\n",
    "    papers = data.glob(\"**/Reference_XML/*.xml\")\n",
    "    summaries = data.glob(\"**/**/*.human.txt\")\n",
    "\n",
    "    for summary in summaries:\n",
    "        uid = re.sub(\"([A-Z]\\d{2}-\\d{4})(.*)\", \"\\g<1>\", summary.stem)\n",
    "        uids.append(uid)\n",
    "        path = f\"../data/summaries/{summary.stem.replace('.human', '')}.txt\"\n",
    "        shutil.copyfile(summary, path)\n",
    "\n",
    "    for paper in papers:\n",
    "        if paper.stem in uids:\n",
    "            shutil.copyfile(paper, f\"../data/papers/{paper.stem}.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(path):\n",
    "    with codecs.open(path, \"r\", encoding=\"latin-1\") as file:\n",
    "        try:\n",
    "            xml = file.read()\n",
    "        except Exception as e:\n",
    "            print(\"Could not parse: \", paper)\n",
    "            print(e)\n",
    "\n",
    "        root = etree.fromstring(xml)\n",
    "        title = root.find(\"./S[@sid='0']\").text if not None else \"\"\n",
    "        text = root.xpath(\".//S[not(@sid = '0')]\")\n",
    "        sentences = [s.text for s in text if s.text is not None]\n",
    "        # sids = [s.attrib.get(\"sid\") for s in text if s.text is not None]\n",
    "\n",
    "        return title, text, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitlines_summaries(summaries):\n",
    "    sentences = []\n",
    "\n",
    "    for summary in summaries:\n",
    "        with codecs.open(summary, \"r+\", encoding=\"latin-1\") as s:\n",
    "            text = s.read().strip()\n",
    "            text = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2', text)\n",
    "\n",
    "            split = nltk.tokenize.sent_tokenize(text)\n",
    "            sentences.append(split)\n",
    "\n",
    "            s.seek(0)\n",
    "            s.writelines([line + \"\\n\" for line in split])\n",
    "            s.truncate()\n",
    "            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers():\n",
    "    papers = Path(\"../data/papers\").glob(\"*.xml\")\n",
    "    data = []\n",
    "\n",
    "    for path in papers:\n",
    "        title, text, paper_sentences = parse_xml(path)\n",
    "        uid = path.stem.replace(\"\", \"\")\n",
    "        summaries = Path(\"../data/summaries\").glob(f\"*{uid}*.txt\")\n",
    "        path_summaries = [str(s) for s in summaries]\n",
    "\n",
    "        paper = {\n",
    "            \"uid\": uid,\n",
    "            \"title\": title,\n",
    "            \"raw_paper\": paper_sentences,\n",
    "            \"path_paper\": path,\n",
    "            \"path_summaries\": path_summaries,\n",
    "        }\n",
    "\n",
    "        data.append(paper)\n",
    "\n",
    "    return sorted(data, key=lambda x: x[\"uid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_annotation(summaries):\n",
    "    paths = []\n",
    "    for summary in summaries:\n",
    "        annotations = []\n",
    "\n",
    "        with codecs.open(summary, \"r\", encoding=\"latin-1\") as file:\n",
    "            for index, sentence in enumerate(file, start=1):\n",
    "                annotation = {\n",
    "                    \"source_id\": index,\n",
    "                    \"target_id\": None,\n",
    "                    \"summary_text\": sentence.strip(),\n",
    "                    \"paper_text\": \"\",\n",
    "                    \"strategy\": \"\",\n",
    "                }\n",
    "                annotations.append(annotation)\n",
    "\n",
    "        path = f\"../data/annotation/{summary.split('/')[-1].replace('.txt', '')}.json\"\n",
    "        paths.append(path)\n",
    "\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(annotations, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>raw_paper</th>\n",
       "      <th>path_paper</th>\n",
       "      <th>path_summaries</th>\n",
       "      <th>raw_summaries</th>\n",
       "      <th>path_annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00-2018</td>\n",
       "      <td>A Maximum-Entropy-Inspired Parser *</td>\n",
       "      <td>[We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank., This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]., The major technical innovation is the use of a &amp;quot;maximum-entropy-inspired&amp;quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events., We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's p...</td>\n",
       "      <td>../data/papers/A00-2018.xml</td>\n",
       "      <td>[../data/summaries/A00-2018_sweta.txt, ../data/summaries/A00-2018_vardha.txt, ../data/summaries/A00-2018_akanksha.txt]</td>\n",
       "      <td>[[In this paper the author aims at the major technical innovation that is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let successfully to test and combines many different conditioning events., They also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head., They also talk about the new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision or recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established  \"standard\" sections of the Wall Street Journal treebank., This represe...</td>\n",
       "      <td>[../data/annotation/A00-2018_sweta.json, ../data/annotation/A00-2018_vardha.json, ../data/annotation/A00-2018_akanksha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00-2030</td>\n",
       "      <td>A Novel Use of Statistical Parsing to Extract Information from Text</td>\n",
       "      <td>[Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard., In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations., Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard., Yet, relatively few have embedded one of these algorithms in a task., Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved s...</td>\n",
       "      <td>../data/papers/A00-2030.xml</td>\n",
       "      <td>[../data/summaries/A00-2030_sweta.txt, ../data/summaries/A00-2030_vardha.txt, ../data/summaries/A00-2030_aakansha.txt]</td>\n",
       "      <td>[[In this paper the author aimed at reporting, adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate the new technique on MUC-7 template elements and template relations., The author is able to integrate both syntactic and semantic information into the parsing process, thus avoiding potential errors of syntax first followed by semantics., Their parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation., They were able to specify relatively simple guidelines that students with no training in computational linguistics could annotate., The semantic training corpus was produced by students according to a simple set ...</td>\n",
       "      <td>[../data/annotation/A00-2030_sweta.json, ../data/annotation/A00-2030_vardha.json, ../data/annotation/A00-2030_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A97-1014</td>\n",
       "      <td>An Annotation Scheme for Free Word Order Languages</td>\n",
       "      <td>[We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages., Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme., The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata., The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction., In particular, we focus on several methodological issues concerning the annotation of non-configurational languages., In section 2, we examine the appropriateness of existing annot...</td>\n",
       "      <td>../data/papers/A97-1014.xml</td>\n",
       "      <td>[../data/summaries/A97-1014_vardha.txt, ../data/summaries/A97-1014_swastika.txt, ../data/summaries/A97-1014_sweta.txt]</td>\n",
       "      <td>[[This paper talks about an annotation scheme for free word order languages., The main key words annotated in this paper are tree bank, corpus, and free word order., It aims at providing syntactically annotated corpora ('tree banks') for stochastic grammar induction., The requirements for such formalism differ from those posited for configurational languages; several features have been added, influencing the architecture of the scheme., In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and refinements., This paper focuses on annotating argument structure rather than constituent trees; it differs from existing tree banks in several aspects., These differences are illustrated by a comparison with the Penn Treebank annotation scheme., Our annotatio...</td>\n",
       "      <td>[../data/annotation/A97-1014_vardha.json, ../data/annotation/A97-1014_swastika.json, ../data/annotation/A97-1014_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>Word Re-ordering and DP-based Search in Statistical Machine Translation</td>\n",
       "      <td>[In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)., Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm., A search restriction especially useful for the translation direction from German to English is presented., The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task., The goal of machine translation is the translation of a text given in some source language into a target language., We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to ...</td>\n",
       "      <td>../data/papers/C00-2123.xml</td>\n",
       "      <td>[../data/summaries/C00-2123.txt]</td>\n",
       "      <td>[[The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)., From a DP-based solution to the traveling salesman problem, they present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm., A beam search concept is applied as in speech recognition., There is no global pruning., An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account., In order to handle the necessary word reordering as an optimization problem within the dynamic programming approach, they describe a solution to the traveling salesman problem (TSP) which is based on d...</td>\n",
       "      <td>[../data/annotation/C00-2123.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02-1025</td>\n",
       "      <td>Named Entity Recognition: A Maximum Entropy Approach Using Global Information</td>\n",
       "      <td>[This paper presents a maximum entropy-based named entity recognizer (NER)., It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier., Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier., In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data., Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conference...</td>\n",
       "      <td>../data/papers/C02-1025.xml</td>\n",
       "      <td>[../data/summaries/C02-1025.txt]</td>\n",
       "      <td>[[This paper presents a maximum entropy-based named entity recognizer (NER)., NER is useful in many NLP applications such as information extraction, question answering, etc .Chieu and Ng have shown that the maximum entropy framework is able to use global information directly from various sources., They believe that global context is useful in most languages, as it is a natural tendency for authors to use abbreviations on entities already mentioned previously., They have made use of local and global features to deal with the instances of same token in a document., Their results show that their high performance NER use less training data than other systems., The use of global features has shown excellent result in the performance on MUC-6 and MUC-7 test data., Using less training data th...</td>\n",
       "      <td>[../data/annotation/C02-1025.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C08-1098</td>\n",
       "      <td>Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging</td>\n",
       "      <td>[We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags., It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs., In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers., A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags., Tagsets of this size c...</td>\n",
       "      <td>../data/papers/C08-1098.xml</td>\n",
       "      <td>[../data/summaries/C08-1098.txt]</td>\n",
       "      <td>[[In this paper, Schmid and Laws present a  RFTagger or Hidden-Markov-Model (HMM) part-of-speech tagger  using German and Czech corpora., HMM tagging decomposes the POS tags into a set of simple attributes, and uses decision tree to estimate the probability of each attribute., Decision tree assigns classes to objects which are represented as attribute vectors., Their tagger applies a beam-search strategy to increase the speed and uses dot to separate the attributes., It also applies pre-pruning citeria., Their tagger is fast and can be successfully applied to a wide range of languages and training corpora., Their tagger is highly accurate in comparison to TnTagger and SVMTool.]]</td>\n",
       "      <td>[../data/annotation/C08-1098.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C10-1045</td>\n",
       "      <td>Better Arabic Parsing: Baselines, Evaluations, and Analysis</td>\n",
       "      <td>[In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design., First, we identify sources of syntactic ambiguity understudied in the existing parsing literature., Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic., Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG., Fourth, we show how to build better models for three different parsers., Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% F1., It is well-known that constituency parsing models design...</td>\n",
       "      <td>../data/papers/C10-1045.xml</td>\n",
       "      <td>[../data/summaries/C10-1045.txt]</td>\n",
       "      <td>[[This paper offers a broad insight into of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design., It is probably the first analysis of Arabic parsing of this kind., It is well-known that English constituency parsing models do not generalize to other languages and treebanks., Explanations for this phenomenon are relative informativeness of lexicalization, insensitivity to morphology and the effect of variable word order and these factors lead to syntactic disambiguation., The authors use linguistic and annotation insights to develop a manually annotated grammar and evaluate it and finally provide a realistic evaluation in which segmentation is performed in a pipeline jointly with parsing., The authors show that PATB is sim...</td>\n",
       "      <td>[../data/annotation/C10-1045.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C90-2039</td>\n",
       "      <td>Strategic Lazy Incremental Copy Graph Unification</td>\n",
       "      <td>[The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures., One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory., The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation., The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapping occurrences, thus increasing the total efficiency of natural language processing ...</td>\n",
       "      <td>../data/papers/C90-2039.xml</td>\n",
       "      <td>[../data/summaries/C90-2039.txt]</td>\n",
       "      <td>[[The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification method - the lazy incremental copy graph (LING) unification and the strategic incremental copy graph (SING) unification method., The LING unification method achieves structure sharing which avoids memory wastage and increases the portion of token identical substructures of FSs., The SING unification method introduces the feature unification strategy and lists the factors on which itâs efficiency depends., The combined method increases the total efficiency of FS unification-based natural language processing systems.]]</td>\n",
       "      <td>[../data/annotation/C90-2039.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C94-2154</td>\n",
       "      <td>THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES</td>\n",
       "      <td>[in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch., !['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation., Unification lbrmMisms ma.y be either un-typed (DCC~s, PATRII, 1,F(;) or typed (npsG)., A m~L,ior reason for adding types to ~ forma,lism is to express restrictions on fea.ture cooccurences a.s in (;l's(:: [5] in order to rule out nonexista.nt tyl)es of objects., For example, there a.re no verbs which have the [km.ture +R. The simplest way to express such restrictions is by mea.ns of a.n...</td>\n",
       "      <td>../data/papers/C94-2154.xml</td>\n",
       "      <td>[../data/summaries/C94-2154.txt]</td>\n",
       "      <td>[[In this paper, the authors try to show the kind of constraints expressible by appropriateness conditions cane be implemented in a practical system employing typed features structures and unification as the primary operation on feature structures., Being able to express the class of constraints by appropriateness conditions corresponding closely to the class of constraints that can be efficiently pre-compiled is taken as a justification for appropriateness formalisms.]]</td>\n",
       "      <td>[../data/annotation/C94-2154.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D09-1092</td>\n",
       "      <td>Polylingual Topic Models</td>\n",
       "      <td>[Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts., Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages., We introduce a polylingual topic model that discovers topics aligned across multiple languages., We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages., Statistical topic models have emerged as an increasingly useful analysis tool for large text collections., Topic models have been used ...</td>\n",
       "      <td>../data/papers/D09-1092.xml</td>\n",
       "      <td>[../data/summaries/D09-1092_sweta.txt, ../data/summaries/D09-1092_swastika.txt, ../data/summaries/D09-1092_vardha.txt]</td>\n",
       "      <td>[[In this paper the author aims at introducing a polylingual topic model that discovers topics aligned across multiple languages., They explore the modelâs characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages., Statistical topic models have emerged as an increasingly useful analysis tool for large text collections., Topic models have been used for analyzing topic trends in research literature, inferring captions for images, social network analysis in email, and expanding queries with topically related words in information retrieval., The author argues that topic modelling is both a useful and appropriate tool for leveraging correspondences between se...</td>\n",
       "      <td>[../data/annotation/D09-1092_sweta.json, ../data/annotation/D09-1092_swastika.json, ../data/annotation/D09-1092_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D10-1044</td>\n",
       "      <td>Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</td>\n",
       "      <td>[We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not., This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure., We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines., Domain adaptation is a common concern when optimizing empirical NLP applications., Even when there is training data available in the domain of interest, there is often additional data from other domains tha...</td>\n",
       "      <td>../data/papers/D10-1044.xml</td>\n",
       "      <td>[../data/summaries/D10-1044_swastika.txt, ../data/summaries/D10-1044_sweta.txt, ../data/summaries/D10-1044_aakansha.txt]</td>\n",
       "      <td>[[Foster et all describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not., They extended previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and used simpler training procedure., They incorporated instance-weighting into a mixture-model framework, and found that it yielded consistent improvements over a wide range of baselines., In this paper, the authors proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance., Each out-of-domain phrase pair was c...</td>\n",
       "      <td>[../data/annotation/D10-1044_swastika.json, ../data/annotation/D10-1044_sweta.json, ../data/annotation/D10-1044_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D10-1083</td>\n",
       "      <td>Simple Type-Level Unsupervised POS Tagging</td>\n",
       "      <td>[Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus., Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy., However, in existing systems, this expansion come with a steep increase in model complexity., This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments., In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training., Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts., On several ...</td>\n",
       "      <td>../data/papers/D10-1083.xml</td>\n",
       "      <td>[../data/summaries/D10-1083.txt]</td>\n",
       "      <td>[[In this paper, the authors are of the opinion that the sequence models-based approaches usually treat token-level tag assignment as the primary latent variable., However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity., In this work, they take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model., Their work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process., There are clustering approaches that assign a single POS tag to each word type., These clusters are computed using an SVD variant without relying on transitional structure., The departure from the traditional token-based tagging approach allow them to explicitly capture...</td>\n",
       "      <td>[../data/annotation/D10-1083.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>E03-1005</td>\n",
       "      <td>An Efficient Implementation of a New DOP Model</td>\n",
       "      <td>[Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank., This paper proposes an integration of the two models which outperforms each of them separately., Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence., The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments, without imposing a...</td>\n",
       "      <td>../data/papers/E03-1005.xml</td>\n",
       "      <td>[../data/summaries/E03-1005_swastika.txt, ../data/summaries/E03-1005_sweta.txt, ../data/summaries/E03-1005_aakansha.txt]</td>\n",
       "      <td>[[In this paper, Ren Bods proposes an integration of two existing DOP models (one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank)which outperforms each of them separately., They showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree resulted in fast processing times and very competitive accuracy on the Wall Street Journal treebank., They also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic dependencies., The results showed an 11% relative reduction in error rate over previous models, and an average processing time of 3...</td>\n",
       "      <td>[../data/annotation/E03-1005_swastika.json, ../data/annotation/E03-1005_sweta.json, ../data/annotation/E03-1005_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E03-1020</td>\n",
       "      <td>Discovering Corpus-Specific Word Senses</td>\n",
       "      <td>[This paper presents an unsupervised algorithm which automatically discovers word senses from text., The algorithm is based on a graph model representing words and relationships between them., Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word., Discrimination against previously extracted sense clusters enables us to discover new senses., We use the same data for both recognising and resolving ambiguity., This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies., Automatic word sense discovery has applications of many kinds., It can greatly facilitate a lexicographer's work and can be used to automatically constru...</td>\n",
       "      <td>../data/papers/E03-1020.xml</td>\n",
       "      <td>[../data/summaries/E03-1020.txt]</td>\n",
       "      <td>[[This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionary or, taxonomies., It is based on a graph model representing words and relationships between them., A word sense algorithm is outlined which bypasses the problem of parameter turning., The authors describe the experiment that examines the performance of their algorithm on a set of words with varying degree of ambiguity., They also present a sample of the results., The algorithm recognises and resolves ambiguity., It gives rise to an automatic, unsupervised word sense disambiguation algorithm.]]</td>\n",
       "      <td>[../data/annotation/E03-1020.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>E09-2008</td>\n",
       "      <td>Foma: a finite-state compiler and library</td>\n",
       "      <td>[Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses., It has specific support for many natural language processing applications such as producing morphological and phonological analyzers., Foma is largely compatible with the Xerox/PARC finite-state toolkit., It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the âMathematical Operatorsâ Unicode block., Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyze...</td>\n",
       "      <td>../data/papers/E09-2008.xml</td>\n",
       "      <td>[../data/summaries/E09-2008.txt]</td>\n",
       "      <td>[[This paper discusses Foma, a finite state compiler, programming language, and regular expression or finite-state library designed for multi-purpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological or phonological analyzers and spellchecking applications., The aim of Foma is to provide specific support for many natural language processing applications such as producing morphological and phonological analyzers., Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples., Though the main concern with Foma has not been that of efficiency, but of compatibili...</td>\n",
       "      <td>[../data/annotation/E09-2008.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H05-1115</td>\n",
       "      <td>Using Random Walks for Question-focused Sentence Retrieval</td>\n",
       "      <td>[We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event stories published over time., Annotators generated a list of questions central to understanding each story in our corpus., Because of the dynamic nature of the stories,many questions are time-sensitive (e.g.How many victims have been found?)Judges found sentences providing an answer to each question., To address thesentence retrieval problem, we apply astochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization., Currently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentence...</td>\n",
       "      <td>../data/papers/H05-1115.xml</td>\n",
       "      <td>[../data/summaries/H05-1115.txt]</td>\n",
       "      <td>[[The authorsâ aim is to consider the problem of question focused sentence retrieval from complex news\\narticles describing multi-event stories published over time., Annotators generated a list of questions\\ncentral to understanding each story in our corpus., Recent work has motivated the need for systems\\nthat support âInformation Synthesisâ tasks, in which a user seeks a global understanding of a topic or\\nstory., The specific problem they consider differs from the classic task of PR for a Q&amp;A system in\\ninteresting ways, due to the time-sensitive nature of the stories in our corpus., They aim to develop a\\nmethod for sentence retrieval that goes beyond finding sentences that are similar to a single query., To\\nthis end, they propose to use a stochastic, graph-based method., To...</td>\n",
       "      <td>[../data/annotation/H05-1115.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H89-2014</td>\n",
       "      <td>Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging</td>\n",
       "      <td>[The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text., The model has the advantage that a pre-tagged training corpus is not required., Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model., State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models., The structure of the state chains is based on both an analysis of errors and linguistic knowledge., Examples show how word dependency across phrases can be modeled., The determination of part-of-speech categories for words is an important problem in la...</td>\n",
       "      <td>../data/papers/H89-2014.xml</td>\n",
       "      <td>[../data/summaries/H89-2014.txt]</td>\n",
       "      <td>[[The paper aims at describing refinements that are currently being investigated in a model for part-ofspeech\\nassignment to words in unrestricted text., The model has the advantage that a pre-tagged\\ntraining corpus is not required., The determination of part-of-speech categories for words is an\\nimportant problem in language modelling, because both the syntactic and semantic roles of words\\ndepend on their part-of-speech category., The statistical methods can be described in terms of Markov\\nmodels., States in a model represent categories., In Hidden Markov model, the Baum-Welch algorithm\\ncan be used to estimate the model parameters., This has the great advantage of eliminating the pretagged\\ncorpus., It minimizes the resources required, facilitates experimentation with different wo...</td>\n",
       "      <td>[../data/annotation/H89-2014.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I05-5011</td>\n",
       "      <td>Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs</td>\n",
       "      <td>[Automatic paraphrase discovery is an important but challenging task., We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue., We focus on phrases which connect two Named Entities (NEs), and proceed in two stages., The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets., The second stage links sets which involve the same pairs of individual NEs., A total of 13,976 phrases were grouped., The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the links for two evaluated domains was 73% and 86%., One of the difficulties in Natural Language Processing is the fact that there are many...</td>\n",
       "      <td>../data/papers/I05-5011.xml</td>\n",
       "      <td>[../data/summaries/I05-5011.txt]</td>\n",
       "      <td>[[This paper conducted research in the area of automatic paraphrase discovery., The authors believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system., They proposed an unsupervised method to discover paraphrases from a large untagged corpus., They focused on phrases which two Named Entities, and proceed in two stages., This topic has been getting more attention, driven by the needs of various NLP applications., Most IE researchers have been creating paraphrase knowledge by hand and specific tasks., The authors cluster NE instance pairs based on the words in the context using bag-of-words methods., In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold.]]</td>\n",
       "      <td>[../data/annotation/I05-5011.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>J01-2004</td>\n",
       "      <td>Probabilistic Top-Down Parsing and Language Modeling</td>\n",
       "      <td>[This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition., The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling., A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers., A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity., Interpolation with a trig...</td>\n",
       "      <td>../data/papers/J01-2004.xml</td>\n",
       "      <td>[../data/summaries/J01-2004_swastika.txt, ../data/summaries/J01-2004_sweta.txt, ../data/summaries/J01-2004_aakansha.txt]</td>\n",
       "      <td>[[Roark, in this paper, described the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition., They varied the base beam factor in trials on the Chelba and Jelinek corpora, keeping the level of conditioning information constant., With a simple conditional probability model, and simple statistical search heuristics, they were able to find very accurate parses efficiently, and, assign word probabilities that yielded a perplexity improvement over previous results., Interpolation with a trigram model yielded an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by their parsing model was orthogonal to that captured by a...</td>\n",
       "      <td>[../data/annotation/J01-2004_swastika.json, ../data/annotation/J01-2004_sweta.json, ../data/annotation/J01-2004_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>J96-3004</td>\n",
       "      <td>A Stochastic Finite-State Word-Segmentation Algorithm for Chinese</td>\n",
       "      <td>[The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.,  For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.,  In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to \"reconstruct\" the word-boundary information.,  In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.,  The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunc...</td>\n",
       "      <td>../data/papers/J96-3004.xml</td>\n",
       "      <td>[../data/summaries/J96-3004.txt]</td>\n",
       "      <td>[[In this paper the authors present a stochastic finite-state model for segmenting Chinese text into words., The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers., It also incorporates the Good-Turing methodin estimating the likelihoods of previously unseen constructions, including morphological derivatives and personal names., they evaluate various specific aspects of the segmentation, as well as the overall segmentation performance., The evaluation compares the performance of the system with that of several human judges and inter-human agreement on a single correct way to segment a text.they showed that the average agreement among the human judges is .76, and the average agreement between ST(system) an...</td>\n",
       "      <td>[../data/annotation/J96-3004.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>N01-1011</td>\n",
       "      <td>A Decision Tree of Bigrams is an Accurate Predictor of Word Sense</td>\n",
       "      <td>[This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby., This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise., It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words., Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs., For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined., For example, suppose bill has the following set of possible meanings: a piece of currency, pending legisla...</td>\n",
       "      <td>../data/papers/N01-1011.xml</td>\n",
       "      <td>[../data/summaries/N01-1011.txt]</td>\n",
       "      <td>[[This paper presents a corpus-based approach to\\nword sense disambiguation where a decision tree assigns\\na sense to an ambiguous word based on the\\nbigrams that occur nearby.for this purpose the\\nsense inventory of word sense has already been determined., This paper describes an approach where a deci-\\nsion tree is learned from some number of sentences\\nwhere each instance of an ambiguous word has been\\nmanually annotated with a sense-tag that denotes\\nthe most appropriate sense for that context and for Building a Feature Set of Bigrams;\\ntwo alternatives, the power divergence family\\nand the Dice Coefficient were explored., this study utilizes the training and test\\ndata from the 1998 SENSEVAL evaluation of word\\nsense disambiguation systems., The results of this approach\\nare compa...</td>\n",
       "      <td>[../data/annotation/N01-1011.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>N04-1038</td>\n",
       "      <td>Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution</td>\n",
       "      <td>[We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor., BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning., These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible., BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources., Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns., The problem of coreference resolution has received considerable attention, including theoretical dis...</td>\n",
       "      <td>../data/papers/N04-1038.xml</td>\n",
       "      <td>[../data/summaries/N04-1038.txt]</td>\n",
       "      <td>[[In this paper, Ben and Riloff present a coreference resolver called BABAR that focuses on the use of contextual-role knowledge for coreference resolution., The problem of coreference resolution has received considerable attention, including theoretical discourse models and supervised machine learning systems., BABARâs performance in both domains of terrorism and natural disaster, and the contextual-role knowledge in pronouns have shown successful results., However, using the top-level semantic classes of WordNet proved to be problematic as the class distinctions are too coarse., Bean and Riloff also used bootstrapping to extend their semantic compatibility model, proposed using caseframe network for anaphora resolution, information extraction patterns to identify contextual clues f...</td>\n",
       "      <td>[../data/annotation/N04-1038.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>N06-2049</td>\n",
       "      <td>Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation</td>\n",
       "      <td>[We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach., We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation., In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates., By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005., The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005)., Under the scheme, each character of a word is labeled as âBâ if it is...</td>\n",
       "      <td>../data/papers/N06-2049.xml</td>\n",
       "      <td>[../data/summaries/N06-2049.txt]</td>\n",
       "      <td>[[In this paper, the authors proposed a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters., Their word segmentation process is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subwordbased tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging., They used the data from the Sighan Bakeoff 2005 to test their approaches., By these techniques, they achieved higher F-scores in City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR) corpora than the best results from Si...</td>\n",
       "      <td>[../data/annotation/N06-2049.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P04-1036</td>\n",
       "      <td>Finding Predominant Word Senses in Untagged Text</td>\n",
       "      <td>[word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed., The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data., Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration., We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically., The acquired predominant senses give a o...</td>\n",
       "      <td>../data/papers/P04-1036.xml</td>\n",
       "      <td>[../data/summaries/P04-1036_aakansha.txt, ../data/summaries/P04-1036_swastika.txt, ../data/summaries/P04-1036_vardha.txt, ../data/summaries/P04-1036_sweta.txt]</td>\n",
       "      <td>[[Diana McCarthy &amp; Rob Koeling &amp; Julie Weeds &amp; John Carroll in their paper 'Finding Predominant Word Senses in Untagged Text'present a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.ey chose documents from the SPORTS domain and a the FINANCE domain.they use an automatically acquired thesaurus and a WordNet Similarity measure.we use the SENSEVAL -2 all-words data (Palmer et al.,2001).This is a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II., The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL -2 English all-words task giving us a WSD precision of 64% on an all-nouns task.], [McCarthy et all presented work on the use of a...</td>\n",
       "      <td>[../data/annotation/P04-1036_aakansha.json, ../data/annotation/P04-1036_swastika.json, ../data/annotation/P04-1036_vardha.json, ../data/annotation/P04-1036_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>P05-1004</td>\n",
       "      <td>Supersense Tagging of Unknown Nouns using Semantic Similarity</td>\n",
       "      <td>[The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words., Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET., Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples., We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger., We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity., Lexical-semantic resources have been applied successful to a wide range of Natural Langu...</td>\n",
       "      <td>../data/papers/P05-1004.xml</td>\n",
       "      <td>[../data/summaries/P05-1004.txt]</td>\n",
       "      <td>[[The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words., Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET., Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction and class-based smoothing, to text classification and question answering., Some specialist topics are better covered in WORDNET than others., A considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORDNET and the construction of new wordnet using the concept structure from E...</td>\n",
       "      <td>[../data/annotation/P05-1004.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>P05-1013</td>\n",
       "      <td>Pseudo-Projective Dependency Parsing</td>\n",
       "      <td>[In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures., We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures., Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy., This leads to the best reported performance for robust non-projective parsing of Czech., It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate t...</td>\n",
       "      <td>../data/papers/P05-1013.xml</td>\n",
       "      <td>[../data/summaries/P05-1013_swastika.txt, ../data/summaries/P05-1013_aakansha.txt, ../data/summaries/P05-1013_vardha.txt]</td>\n",
       "      <td>[[Authors Nilsson and Nivre showed how a data-driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures., Experiments using data from the Prague Dependency Treebank showed that the combined system can handle non-projective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy., The combined system could recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion., This led to the best reported performance for robust non-projective parsing of Czech.], [Joakim Nivre and Jens Nilsson in their paper'Pseud...</td>\n",
       "      <td>[../data/annotation/P05-1013_swastika.json, ../data/annotation/P05-1013_aakansha.json, ../data/annotation/P05-1013_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>P06-2124</td>\n",
       "      <td>BiTAM: Bilingual Topic AdMixture Models forWord Alignment</td>\n",
       "      <td>[We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation., Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model., Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels)., These models enable word- alignment process to leverage topical contents of document-pairs., Efficient variational approximation algorithms are designed for inference and parameter estimation., With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspect...</td>\n",
       "      <td>../data/papers/P06-2124.xml</td>\n",
       "      <td>[../data/summaries/P06-2124.txt]</td>\n",
       "      <td>[[In this paper, the authors proposed a probabilistic admixture model to capture latent topics underlying the context of document- pairs., They proposed a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT., They used IBM Model I as the baseline., They investigated three instances of the BiTAM model, They were data-driven and did not need handcrafted knowledge engineering., The proposed models signiï¬cantly improved the alignment accuracy and lead to better translation qualities., Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.]]</td>\n",
       "      <td>[../data/annotation/P06-2124.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P08-1028</td>\n",
       "      <td>Vector-based Models of Semantic Composition</td>\n",
       "      <td>[This paper proposes a framework for representing the meaning of phrases and sentences in vector space., Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions., Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task., Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments., Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science., The appeal of these models lies in their ability to represent meaning simply by using distributional informat...</td>\n",
       "      <td>../data/papers/P08-1028.xml</td>\n",
       "      <td>[../data/summaries/P08-1028_swastika.txt, ../data/summaries/P08-1028_sweta.txt, ../data/summaries/P08-1028_aakansha.txt]</td>\n",
       "      <td>[[In this paper, Mitchell and Lapata proposed a framework for vector-based semantic composition., Central to their approach was vector composition which they operationalize in terms of additive and multiplicative functions., Under this framework, they introduced a wide range of composition models which they evaluated empirically on a sentence similarity task., Experimental results demonstrated that the multiplicative models were superior- at least, for the sentence similarity task attempted- to the additive alternatives when compared against human judgments., They conjectured that the additive models are not sensitive to the fine-grained meaning distinctions involved in their materials., Multiplicative models considered a subset, namely non-zero components whereas additive models captu...</td>\n",
       "      <td>[../data/annotation/P08-1028_swastika.json, ../data/annotation/P08-1028_sweta.json, ../data/annotation/P08-1028_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P08-1043</td>\n",
       "      <td>A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</td>\n",
       "      <td>[Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence., These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance., Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity., Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so ...</td>\n",
       "      <td>../data/papers/P08-1043.xml</td>\n",
       "      <td>[../data/summaries/P08-1043_sweta.txt, ../data/summaries/P08-1043_swastika.txt, ../data/summaries/P08-1043_aakansha.txt]</td>\n",
       "      <td>[[In this paper they propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity., Using a Treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique their model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far., One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects., The implication of this ambiguity for...</td>\n",
       "      <td>[../data/annotation/P08-1043_sweta.json, ../data/annotation/P08-1043_swastika.json, ../data/annotation/P08-1043_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>P08-1102</td>\n",
       "      <td>A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</td>\n",
       "      <td>[We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging., With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly., Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging., On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline., Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages., Several ...</td>\n",
       "      <td>../data/papers/P08-1102.xml</td>\n",
       "      <td>[../data/summaries/P08-1102_sweta.txt, ../data/summaries/P08-1102_swastika.txt, ../data/summaries/P08-1102_aakansha.txt]</td>\n",
       "      <td>[[This paper aims at proposing a cascaded linear model for joint Chinese word segmentation and part-of- speech tagging., With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly., Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages., However, as such features are generated dynamically during the decoding procedure, two limitations arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the curr...</td>\n",
       "      <td>[../data/annotation/P08-1102_sweta.json, ../data/annotation/P08-1102_swastika.json, ../data/annotation/P08-1102_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P11-1060</td>\n",
       "      <td>Learning Dependency-Based Compositional Semantics</td>\n",
       "      <td>[Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms., In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs., In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms., On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms., What is the total population of the ten largest capitals in the US?, Answering these types of complex questions compositiona...</td>\n",
       "      <td>../data/papers/P11-1060.xml</td>\n",
       "      <td>[../data/summaries/P11-1060_aakansha.txt, ../data/summaries/P11-1060_swastika.txt, ../data/summaries/P11-1060_sweta.txt]</td>\n",
       "      <td>[[The paper 'Learning Dependency-Based Compositional Semantics' by Percy Liang,Michael I. Jordan and Dan Klein propose a new way to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs., The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive., The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees,which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient., The logical forms in DCS are called DCS trees,where nodes are labeled with predicates, and edges are labeled with relations., DCS trees are le...</td>\n",
       "      <td>[../data/annotation/P11-1060_aakansha.json, ../data/annotation/P11-1060_swastika.json, ../data/annotation/P11-1060_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>P11-1061</td>\n",
       "      <td>Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</td>\n",
       "      <td>[We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language., Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages., We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010)., Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm., Supervised learning appr...</td>\n",
       "      <td>../data/papers/P11-1061.xml</td>\n",
       "      <td>[../data/summaries/P11-1061_sweta.txt, ../data/summaries/P11-1061_swastika.txt, ../data/summaries/P11-1061_aakansha.txt]</td>\n",
       "      <td>[[In this paper the author aims at describing a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labelled training data, but have translated text in a resource-rich language., They use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model., Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems., Unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for training models., To bridge this gap, the author considers a practically motivated scenario, in which he wants to leverage existing resources from a re...</td>\n",
       "      <td>[../data/annotation/P11-1061_sweta.json, ../data/annotation/P11-1061_swastika.json, ../data/annotation/P11-1061_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>P87-1015</td>\n",
       "      <td>CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</td>\n",
       "      <td>[We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate., In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in p...</td>\n",
       "      <td>../data/papers/P87-1015.xml</td>\n",
       "      <td>[../data/summaries/P87-1015_swastika.txt, ../data/summaries/P87-1015_sweta.txt, ../data/summaries/P87-1015_vardha.txt]</td>\n",
       "      <td>[[Vijay-Shankar et all considered the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate., They showed that it was useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees, find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars., On the basis of that observation, they described a class of formalisms which they called Linear Context- Free Rewriting Systems (LCFRs), and showed they were recognizable in polynom...</td>\n",
       "      <td>[../data/annotation/P87-1015_swastika.json, ../data/annotation/P87-1015_sweta.json, ../data/annotation/P87-1015_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>P98-1081</td>\n",
       "      <td>Improving Data Driven Wordclass Tagging by System Combination</td>\n",
       "      <td>[In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system., We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging., Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data., After comparison, their outputs are combined using several voting strategies and second stage classifiers., All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger., In all Natural Language Processing (NLP) systems, we find one or more language models wh...</td>\n",
       "      <td>../data/papers/P98-1081.xml</td>\n",
       "      <td>[../data/summaries/P98-1081.txt]</td>\n",
       "      <td>[[This paper examines how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system., This paper is concerned with the question whether the differences between models can indeed be exploited to yield a data driven model with superior performance.the approach is known as ensemble, stacked, or combined classifiers.the approach is applied mopho-syntactic wordclass tagging.for this purpose a traditional trig-ram model,the Transformation Based Learning system,Memory-Based Learning and the MXPOST system are used.the tagged LOB corpus has been used for data., The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error ra...</td>\n",
       "      <td>[../data/annotation/P98-1081.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>P98-2143</td>\n",
       "      <td>Robust pronoun resolution with limited knowledge</td>\n",
       "      <td>[Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge., One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task., This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger., Input is checked against agreement and for a number of antecedent indicators., Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent., Evaluation reports a success rate of 89.7% which is better than the sucÂ­ cess rates of the approaches selected for comparison and tested on the same data., In addition, preliminary e...</td>\n",
       "      <td>../data/papers/P98-2143.xml</td>\n",
       "      <td>[../data/summaries/P98-2143.txt]</td>\n",
       "      <td>[[This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger in 1998.It makes use of only a part-of-speech tagger, plus simple noun phrase rules and operates on the basis of antecedent-tracking preferences or antecedent indicators.these indicators depend upon a number of salience factors like definiteness,immediate reference etc.this system does not require domain specific knowledge.it shows success rate of 89.7%.it can be applied to different languages other than english.]]</td>\n",
       "      <td>[../data/annotation/P98-2143.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>W03-0410</td>\n",
       "      <td>Semi-supervised Verb Class Discovery Using Noisy Features</td>\n",
       "      <td>[We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs., The feature set was previously shown to work well in a supervised learning setting, using known English verb classes., In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task., We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties., We find that the unsupervised method we tried cannot be consistently applied to our data., However, the semi- supervised approach (using a seed set of ...</td>\n",
       "      <td>../data/papers/W03-0410.xml</td>\n",
       "      <td>[../data/summaries/W03-0410.txt]</td>\n",
       "      <td>[[In this paper, the authors used a semi-supervised method for verb class discovery using Noisy Features., The feature set was previously shown to work well in a supervised learning setting, using known English verb classes., They face the problem of having a large number of irrelevant features for a particular clustering task., They used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all the unsupervised experiments., The evaluation was based on the accuracy of assigning members to the correct clusters., The authors discuss their clustering approach to identify that the task becomes difficult if the learning method has to distinguish multiple classes, rather than focus on the important properties of a single class.]]</td>\n",
       "      <td>[../data/annotation/W03-0410.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>W04-0213</td>\n",
       "      <td>The Potsdam Commentary Corpus</td>\n",
       "      <td>[A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure., The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation., A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees., Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization)., This paper, however, provides a compre...</td>\n",
       "      <td>../data/papers/W04-0213.xml</td>\n",
       "      <td>[../data/summaries/W04-0213.txt]</td>\n",
       "      <td>[[This paper discusses the Potsdam Commentary Corpus, a corpus of german assembeled by potsdam university., The corpus was annoted with different linguitic information.the Potsdam Commentary Corpus or PCC consists of\\n170 commentaries from M¨arkische Allgemeine Zeitung, a German regional daily., This corpus includes 173 texts on politics from the on-line newspaper\\nMärkische Allgemeine Zeitung., It contains 32,962 words and 2,195 sentences., It is annotated with several data: morphology, syntax, rhetorical\\nstructure, connectors, correference and informative structure., Nevertheless, only a part of this corpus (10 texts), which the authors name \"core corpus\",\\nis annotated with all this information., The texts were annotated with the RSTtool., This corpus has several advantages: it i...</td>\n",
       "      <td>[../data/annotation/W04-0213.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>W06-2932</td>\n",
       "      <td>Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</td>\n",
       "      <td>[present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages., The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages., The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph., We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis., Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing., With the availability of resources such as the Penn WSJ Treebank, much of the f...</td>\n",
       "      <td>../data/papers/W06-2932.xml</td>\n",
       "      <td>[../data/summaries/W06-2932_vardha.txt, ../data/summaries/W06-2932_sweta.txt, ../data/summaries/W06-2932_swastika.txt]</td>\n",
       "      <td>[[This paper talks about Multilingual Dependency Analysis with a Two-Stage Discriminative Parser., Here we present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages., The first stage of our system creates an unlabeled parse y for an input sentence x., The second stage takes the output parse y for sentence x and classifies each edge (i, j) E y with a particular label l (i,j)., Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph., The current system simply includes all morphological bi-gram features., It is our hope that a better morphological fe...</td>\n",
       "      <td>[../data/annotation/W06-2932_vardha.json, ../data/annotation/W06-2932_sweta.json, ../data/annotation/W06-2932_swastika.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>W06-3114</td>\n",
       "      <td>Manual and Automatic Evaluation of Machine Translation between European Languages</td>\n",
       "      <td>[Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) u...</td>\n",
       "      <td>../data/papers/W06-3114.xml</td>\n",
       "      <td>[../data/summaries/W06-3114_swastika.txt, ../data/summaries/W06-3114_aakansha.txt, ../data/summaries/W06-3114_sweta.txt]</td>\n",
       "      <td>[[Koehn and Monz carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs., While many systems had similar performance, the results offered interesting insights, especially, about the relative performance of statistical and rule-based systems., Due to many similarly performing systems, they are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics., The bias of automatic methods in favour of statistical systems seemed to be less pronounced on out-of-domain test data., The manual evaluation of scoring translation on a graded scale from 1&amp;#8211;5 seemed to be very hard to perform., Human judges also pointed out difficulties with the evaluation of long sentences., They fou...</td>\n",
       "      <td>[../data/annotation/W06-3114_swastika.json, ../data/annotation/W06-3114_aakansha.json, ../data/annotation/W06-3114_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>W08-2222</td>\n",
       "      <td>Wide-Coverage Semantic Analysis with Boxer</td>\n",
       "      <td>[Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT)., Used together with the C&amp;C tools, Boxer reaches more than 95% coverage on newswire texts., The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles., The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic., Boxerâs performance on the shared task for comparing semantic represtations was promising., It was able to produce complete DRSs for all seven texts., Manually inspecting the output reve...</td>\n",
       "      <td>../data/papers/W08-2222.xml</td>\n",
       "      <td>[../data/summaries/W08-2222.txt]</td>\n",
       "      <td>[[In this paper, the authors describe Boxer, their open-domain software component for semantic analysis of text, based on Combinatory Categorical Grammar (CCG) and Discourse Representation Theory (DRT)., Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorical Grammar (CCG) and Discourse Representation Theory (DRT)., Based on Discourse Representation Theory Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called âboxesâ because of the way they are graphically displayed) for English sentences and texts., Boxer implements a syntax-semantics interface based on Combinatory Categorical Grammar, CCG., Because the syntax-semantics is clearly defined, the choice of logical form can be independent of...</td>\n",
       "      <td>[../data/annotation/W08-2222.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>W11-2123</td>\n",
       "      <td>KenLM: Faster and Smaller Language Model Queries</td>\n",
       "      <td>[We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs., The structure uses linear probing hash tables and is designed for speed., Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline., Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems., This paper describes the several performance techniques used and presents benchmarks against alternative implementations., Language models are widely applied in n...</td>\n",
       "      <td>../data/papers/W11-2123.xml</td>\n",
       "      <td>[../data/summaries/W11-2123_vardha.txt, ../data/summaries/W11-2123_swastika.txt, ../data/summaries/W11-2123_aakansha.txt]</td>\n",
       "      <td>[[This paper talks about KenLM: Faster and Smaller Language Model Queries., The PROBING data structure uses linear probing hash tables and is designed for speed., This paper presents methods to query N-gram language models, minimizing time and space costs., Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders., For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed., The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM., The code is open source, has minimal depen...</td>\n",
       "      <td>[../data/annotation/W11-2123_vardha.json, ../data/annotation/W11-2123_swastika.json, ../data/annotation/W11-2123_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>W95-0104</td>\n",
       "      <td>A Bayesian hybrid method for context-sensitive spelling correction</td>\n",
       "      <td>[Two classes of methods have been shown to be useful for resolving lexical ambiguity., The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word., These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax., Yarowsky has exploited this complementarity by combining the two methods using decision lists., The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be., This paper takes Yarowsky's work as a starting point, applying decision li...</td>\n",
       "      <td>../data/papers/W95-0104.xml</td>\n",
       "      <td>[../data/summaries/W95-0104.txt]</td>\n",
       "      <td>[[This paper presents a hybrid method where two classes of methods have been used for resolving lexical ambiguity., The authors investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence., Their method combines context-word and collocation methods where the former captures the lexical \"atmosphere\", while the latter captures local syntax., The authors have used five methods for spelling correction: baseline, context words, collocations, decision list and Bayesian classifier., Decision list and Bayesian classifiers are their hybrid methods., For testing this method, confusion sets have been used with ambiguous words like desert and dessert, there and their etc., In their ev...</td>\n",
       "      <td>[../data/annotation/W95-0104.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>W99-0613</td>\n",
       "      <td>Unsupervised Models for Named Entity Classification Collins</td>\n",
       "      <td>[This paper discusses the use of unlabeled examples for the problem of named entity classification., A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &amp;quot;seed&amp;quot; rules., The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type., We present two algorithms., The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98)., The second algorithm extends ideas from boosting algorithm...</td>\n",
       "      <td>../data/papers/W99-0613.xml</td>\n",
       "      <td>[../data/summaries/W99-0613_swastika.txt, ../data/summaries/W99-0613_aakansha.txt, ../data/summaries/W99-0613_sweta.txt, ../data/summaries/W99-0613_vardha.txt]</td>\n",
       "      <td>[[Collins and Singer, in this paper, discussed the use of unlabeled examples for the problem of named entity classification., They showed that the use of data could reduce the requirements for supervision to just 7 simple rules., The approach gained leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appeared were sufficient to determine its type., They presented two algorithms., The first method used a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98)., The second algorithm extended ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98) and boosted the objective functions.], [This pape...</td>\n",
       "      <td>[../data/annotation/W99-0613_swastika.json, ../data/annotation/W99-0613_aakansha.json, ../data/annotation/W99-0613_sweta.json, ../data/annotation/W99-0613_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>W99-0623</td>\n",
       "      <td>Exploiting Diversity in Natural Language Processing: Combining Parsers</td>\n",
       "      <td>[Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy., Two general approaches are presented and two combination techniques are described for each approach., Both parametric and non-parametric models are explored., The resulting parsers surpass the best previously published performance results for the Penn Treebank., The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems., The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992; Heath et al., 1996)., Their theoretical finding is simply stated: classification error ra...</td>\n",
       "      <td>../data/papers/W99-0623.xml</td>\n",
       "      <td>[../data/summaries/W99-0623_vardha.txt, ../data/summaries/W99-0623_swastika.txt, ../data/summaries/W99-0623_sweta.txt]</td>\n",
       "      <td>[[This paper talks about Exploiting Diversity in Natural Language Processing: Combining Parsers., Two general approaches are presented and two combination techniques are described for each approach., Here both parametric and non-parametric models are explored., One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure., The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank., Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result., Combining multiple highly-accurate independent parsers yields promisin...</td>\n",
       "      <td>[../data/annotation/W99-0623_vardha.json, ../data/annotation/W99-0623_swastika.json, ../data/annotation/W99-0623_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>X96-1048</td>\n",
       "      <td>OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION</td>\n",
       "      <td>[Test abstract, The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November., Participants were invited to enter their systems in as many as four different task-oriented evaluations., The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time., The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years., The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume., All except the Scenario Template task are de...</td>\n",
       "      <td>../data/papers/X96-1048.xml</td>\n",
       "      <td>[../data/summaries/X96-1048.txt]</td>\n",
       "      <td>[[This paper is an overview of results of the Sixth Message Understanding Conference (MUC-6) in November., This paper surveys the results of the natural language processing system evaluation on tasks like named entity,coreference,template element and scenario template., Discussion of the results for each task is organized generally under the following topics: Results on task as whole, Results on some aspects of task, Performance on \"walkthrough article.\", The walkthrough article is an article selected from the test set., Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers., Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium., The articles used in the evaluation we...</td>\n",
       "      <td>[../data/annotation/X96-1048.json]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid                                                                                                      title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        raw_paper                   path_paper                                                                                                                                                   path_summaries                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    raw_summaries                                                                                                                                                         path_annotations\n",
       "0   A00-2018                                                                        A Maximum-Entropy-Inspired Parser *  [We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank., This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]., The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events., We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's p...  ../data/papers/A00-2018.xml                                           [../data/summaries/A00-2018_sweta.txt, ../data/summaries/A00-2018_vardha.txt, ../data/summaries/A00-2018_akanksha.txt]  [[In this paper the author aims at the major technical innovation that is the use of a \"maximum-entropy-inspired\" model for conditioning and smoothing that let successfully to test and combines many different conditioning events., They also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head., They also talk about the new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision or recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established  \"standard\" sections of the Wall Street Journal treebank., This represe...                                             [../data/annotation/A00-2018_sweta.json, ../data/annotation/A00-2018_vardha.json, ../data/annotation/A00-2018_akanksha.json]\n",
       "1   A00-2030                                        A Novel Use of Statistical Parsing to Extract Information from Text  [Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard., In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations., Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard., Yet, relatively few have embedded one of these algorithms in a task., Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved s...  ../data/papers/A00-2030.xml                                           [../data/summaries/A00-2030_sweta.txt, ../data/summaries/A00-2030_vardha.txt, ../data/summaries/A00-2030_aakansha.txt]  [[In this paper the author aimed at reporting, adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate the new technique on MUC-7 template elements and template relations., The author is able to integrate both syntactic and semantic information into the parsing process, thus avoiding potential errors of syntax first followed by semantics., Their parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation., They were able to specify relatively simple guidelines that students with no training in computational linguistics could annotate., The semantic training corpus was produced by students according to a simple set ...                                             [../data/annotation/A00-2030_sweta.json, ../data/annotation/A00-2030_vardha.json, ../data/annotation/A00-2030_aakansha.json]\n",
       "2   A97-1014                                                         An Annotation Scheme for Free Word Order Languages  [We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages., Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme., The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata., The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction., In particular, we focus on several methodological issues concerning the annotation of non-configurational languages., In section 2, we examine the appropriateness of existing annot...  ../data/papers/A97-1014.xml                                           [../data/summaries/A97-1014_vardha.txt, ../data/summaries/A97-1014_swastika.txt, ../data/summaries/A97-1014_sweta.txt]  [[This paper talks about an annotation scheme for free word order languages., The main key words annotated in this paper are tree bank, corpus, and free word order., It aims at providing syntactically annotated corpora ('tree banks') for stochastic grammar induction., The requirements for such formalism differ from those posited for configurational languages; several features have been added, influencing the architecture of the scheme., In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and refinements., This paper focuses on annotating argument structure rather than constituent trees; it differs from existing tree banks in several aspects., These differences are illustrated by a comparison with the Penn Treebank annotation scheme., Our annotatio...                                             [../data/annotation/A97-1014_vardha.json, ../data/annotation/A97-1014_swastika.json, ../data/annotation/A97-1014_sweta.json]\n",
       "3   C00-2123                                    Word Re-ordering and DP-based Search in Statistical Machine Translation  [In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)., Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm., A search restriction especially useful for the translation direction from German to English is presented., The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task., The goal of machine translation is the translation of a text given in some source language into a target language., We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to ...  ../data/papers/C00-2123.xml                                                                                                                                 [../data/summaries/C00-2123.txt]  [[The authors in this paper describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP)., From a DP-based solution to the traveling salesman problem, they present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm., A beam search concept is applied as in speech recognition., There is no global pruning., An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account., In order to handle the necessary word reordering as an optimization problem within the dynamic programming approach, they describe a solution to the traveling salesman problem (TSP) which is based on d...                                                                                                                                       [../data/annotation/C00-2123.json]\n",
       "4   C02-1025                              Named Entity Recognition: A Maximum Entropy Approach Using Global Information  [This paper presents a maximum entropy-based named entity recognizer (NER)., It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier., Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier., In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data., Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conference...  ../data/papers/C02-1025.xml                                                                                                                                 [../data/summaries/C02-1025.txt]  [[This paper presents a maximum entropy-based named entity recognizer (NER)., NER is useful in many NLP applications such as information extraction, question answering, etc .Chieu and Ng have shown that the maximum entropy framework is able to use global information directly from various sources., They believe that global context is useful in most languages, as it is a natural tendency for authors to use abbreviations on entities already mentioned previously., They have made use of local and global features to deal with the instances of same token in a document., Their results show that their high performance NER use less training data than other systems., The use of global features has shown excellent result in the performance on MUC-6 and MUC-7 test data., Using less training data th...                                                                                                                                       [../data/annotation/C02-1025.json]\n",
       "5   C08-1098  Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging  [We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags., It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs., In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers., A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags., Tagsets of this size c...  ../data/papers/C08-1098.xml                                                                                                                                 [../data/summaries/C08-1098.txt]                                                                                                                  [[In this paper, Schmid and Laws present a  RFTagger or Hidden-Markov-Model (HMM) part-of-speech tagger  using German and Czech corpora., HMM tagging decomposes the POS tags into a set of simple attributes, and uses decision tree to estimate the probability of each attribute., Decision tree assigns classes to objects which are represented as attribute vectors., Their tagger applies a beam-search strategy to increase the speed and uses dot to separate the attributes., It also applies pre-pruning citeria., Their tagger is fast and can be successfully applied to a wide range of languages and training corpora., Their tagger is highly accurate in comparison to TnTagger and SVMTool.]]                                                                                                                                       [../data/annotation/C08-1098.json]\n",
       "6   C10-1045                                                Better Arabic Parsing: Baselines, Evaluations, and Analysis  [In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design., First, we identify sources of syntactic ambiguity understudied in the existing parsing literature., Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic., Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG., Fourth, we show how to build better models for three different parsers., Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â5% F1., It is well-known that constituency parsing models design...  ../data/papers/C10-1045.xml                                                                                                                                 [../data/summaries/C10-1045.txt]  [[This paper offers a broad insight into of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design., It is probably the first analysis of Arabic parsing of this kind., It is well-known that English constituency parsing models do not generalize to other languages and treebanks., Explanations for this phenomenon are relative informativeness of lexicalization, insensitivity to morphology and the effect of variable word order and these factors lead to syntactic disambiguation., The authors use linguistic and annotation insights to develop a manually annotated grammar and evaluate it and finally provide a realistic evaluation in which segmentation is performed in a pipeline jointly with parsing., The authors show that PATB is sim...                                                                                                                                       [../data/annotation/C10-1045.json]\n",
       "7   C90-2039                                                          Strategic Lazy Incremental Copy Graph Unification  [The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures., One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory., The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation., The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapping occurrences, thus increasing the total efficiency of natural language processing ...  ../data/papers/C90-2039.xml                                                                                                                                 [../data/summaries/C90-2039.txt]                                                                                                                                                              [[The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification method - the lazy incremental copy graph (LING) unification and the strategic incremental copy graph (SING) unification method., The LING unification method achieves structure sharing which avoids memory wastage and increases the portion of token identical substructures of FSs., The SING unification method introduces the feature unification strategy and lists the factors on which itâs efficiency depends., The combined method increases the total efficiency of FS unification-based natural language processing systems.]]                                                                                                                                       [../data/annotation/C90-2039.json]\n",
       "8   C94-2154    THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES  [in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch., !['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation., Unification lbrmMisms ma.y be either un-typed (DCC~s, PATRII, 1,F(;) or typed (npsG)., A m~L,ior reason for adding types to ~ forma,lism is to express restrictions on fea.ture cooccurences a.s in (;l's(:: [5] in order to rule out nonexista.nt tyl)es of objects., For example, there a.re no verbs which have the [km.ture +R. The simplest way to express such restrictions is by mea.ns of a.n...  ../data/papers/C94-2154.xml                                                                                                                                 [../data/summaries/C94-2154.txt]                                                                                                                                                                                                                                                                                                                                      [[In this paper, the authors try to show the kind of constraints expressible by appropriateness conditions cane be implemented in a practical system employing typed features structures and unification as the primary operation on feature structures., Being able to express the class of constraints by appropriateness conditions corresponding closely to the class of constraints that can be efficiently pre-compiled is taken as a justification for appropriateness formalisms.]]                                                                                                                                       [../data/annotation/C94-2154.json]\n",
       "9   D09-1092                                                                                   Polylingual Topic Models  [Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts., Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages., We introduce a polylingual topic model that discovers topics aligned across multiple languages., We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages., Statistical topic models have emerged as an increasingly useful analysis tool for large text collections., Topic models have been used ...  ../data/papers/D09-1092.xml                                           [../data/summaries/D09-1092_sweta.txt, ../data/summaries/D09-1092_swastika.txt, ../data/summaries/D09-1092_vardha.txt]  [[In this paper the author aims at introducing a polylingual topic model that discovers topics aligned across multiple languages., They explore the modelâs characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages., Statistical topic models have emerged as an increasingly useful analysis tool for large text collections., Topic models have been used for analyzing topic trends in research literature, inferring captions for images, social network analysis in email, and expanding queries with topically related words in information retrieval., The author argues that topic modelling is both a useful and appropriate tool for leveraging correspondences between se...                                             [../data/annotation/D09-1092_sweta.json, ../data/annotation/D09-1092_swastika.json, ../data/annotation/D09-1092_vardha.json]\n",
       "10  D10-1044                 Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation  [We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not., This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure., We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines., Domain adaptation is a common concern when optimizing empirical NLP applications., Even when there is training data available in the domain of interest, there is often additional data from other domains tha...  ../data/papers/D10-1044.xml                                         [../data/summaries/D10-1044_swastika.txt, ../data/summaries/D10-1044_sweta.txt, ../data/summaries/D10-1044_aakansha.txt]  [[Foster et all describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not., They extended previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and used simpler training procedure., They incorporated instance-weighting into a mixture-model framework, and found that it yielded consistent improvements over a wide range of baselines., In this paper, the authors proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance., Each out-of-domain phrase pair was c...                                           [../data/annotation/D10-1044_swastika.json, ../data/annotation/D10-1044_sweta.json, ../data/annotation/D10-1044_aakansha.json]\n",
       "11  D10-1083                                                                 Simple Type-Level Unsupervised POS Tagging  [Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus., Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy., However, in existing systems, this expansion come with a steep increase in model complexity., This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments., In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training., Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts., On several ...  ../data/papers/D10-1083.xml                                                                                                                                 [../data/summaries/D10-1083.txt]  [[In this paper, the authors are of the opinion that the sequence models-based approaches usually treat token-level tag assignment as the primary latent variable., However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity., In this work, they take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model., Their work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process., There are clustering approaches that assign a single POS tag to each word type., These clusters are computed using an SVD variant without relying on transitional structure., The departure from the traditional token-based tagging approach allow them to explicitly capture...                                                                                                                                       [../data/annotation/D10-1083.json]\n",
       "12  E03-1005                                                             An Efficient Implementation of a New DOP Model  [Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank., This paper proposes an integration of the two models which outperforms each of them separately., Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence., The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments, without imposing a...  ../data/papers/E03-1005.xml                                         [../data/summaries/E03-1005_swastika.txt, ../data/summaries/E03-1005_sweta.txt, ../data/summaries/E03-1005_aakansha.txt]  [[In this paper, Ren Bods proposes an integration of two existing DOP models (one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank)which outperforms each of them separately., They showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree resulted in fast processing times and very competitive accuracy on the Wall Street Journal treebank., They also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic dependencies., The results showed an 11% relative reduction in error rate over previous models, and an average processing time of 3...                                           [../data/annotation/E03-1005_swastika.json, ../data/annotation/E03-1005_sweta.json, ../data/annotation/E03-1005_aakansha.json]\n",
       "13  E03-1020                                                                    Discovering Corpus-Specific Word Senses  [This paper presents an unsupervised algorithm which automatically discovers word senses from text., The algorithm is based on a graph model representing words and relationships between them., Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word., Discrimination against previously extracted sense clusters enables us to discover new senses., We use the same data for both recognising and resolving ambiguity., This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies., Automatic word sense discovery has applications of many kinds., It can greatly facilitate a lexicographer's work and can be used to automatically constru...  ../data/papers/E03-1020.xml                                                                                                                                 [../data/summaries/E03-1020.txt]                                                                                                                                                   [[This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionary or, taxonomies., It is based on a graph model representing words and relationships between them., A word sense algorithm is outlined which bypasses the problem of parameter turning., The authors describe the experiment that examines the performance of their algorithm on a set of words with varying degree of ambiguity., They also present a sample of the results., The algorithm recognises and resolves ambiguity., It gives rise to an automatic, unsupervised word sense disambiguation algorithm.]]                                                                                                                                       [../data/annotation/E03-1020.json]\n",
       "14  E09-2008                                                                  Foma: a finite-state compiler and library  [Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses., It has specific support for many natural language processing applications such as producing morphological and phonological analyzers., Foma is largely compatible with the Xerox/PARC finite-state toolkit., It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the âMathematical Operatorsâ Unicode block., Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyze...  ../data/papers/E09-2008.xml                                                                                                                                 [../data/summaries/E09-2008.txt]  [[This paper discusses Foma, a finite state compiler, programming language, and regular expression or finite-state library designed for multi-purpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological or phonological analyzers and spellchecking applications., The aim of Foma is to provide specific support for many natural language processing applications such as producing morphological and phonological analyzers., Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples., Though the main concern with Foma has not been that of efficiency, but of compatibili...                                                                                                                                       [../data/annotation/E09-2008.json]\n",
       "15  H05-1115                                                 Using Random Walks for Question-focused Sentence Retrieval  [We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event stories published over time., Annotators generated a list of questions central to understanding each story in our corpus., Because of the dynamic nature of the stories,many questions are time-sensitive (e.g.How many victims have been found?)Judges found sentences providing an answer to each question., To address thesentence retrieval problem, we apply astochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization., Currently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentence...  ../data/papers/H05-1115.xml                                                                                                                                 [../data/summaries/H05-1115.txt]  [[The authorsâ aim is to consider the problem of question focused sentence retrieval from complex news\\narticles describing multi-event stories published over time., Annotators generated a list of questions\\ncentral to understanding each story in our corpus., Recent work has motivated the need for systems\\nthat support âInformation Synthesisâ tasks, in which a user seeks a global understanding of a topic or\\nstory., The specific problem they consider differs from the classic task of PR for a Q&A system in\\ninteresting ways, due to the time-sensitive nature of the stories in our corpus., They aim to develop a\\nmethod for sentence retrieval that goes beyond finding sentences that are similar to a single query., To\\nthis end, they propose to use a stochastic, graph-based method., To...                                                                                                                                       [../data/annotation/H05-1115.json]\n",
       "16  H89-2014                                         Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging  [The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text., The model has the advantage that a pre-tagged training corpus is not required., Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model., State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models., The structure of the state chains is based on both an analysis of errors and linguistic knowledge., Examples show how word dependency across phrases can be modeled., The determination of part-of-speech categories for words is an important problem in la...  ../data/papers/H89-2014.xml                                                                                                                                 [../data/summaries/H89-2014.txt]  [[The paper aims at describing refinements that are currently being investigated in a model for part-ofspeech\\nassignment to words in unrestricted text., The model has the advantage that a pre-tagged\\ntraining corpus is not required., The determination of part-of-speech categories for words is an\\nimportant problem in language modelling, because both the syntactic and semantic roles of words\\ndepend on their part-of-speech category., The statistical methods can be described in terms of Markov\\nmodels., States in a model represent categories., In Hidden Markov model, the Baum-Welch algorithm\\ncan be used to estimate the model parameters., This has the great advantage of eliminating the pretagged\\ncorpus., It minimizes the resources required, facilitates experimentation with different wo...                                                                                                                                       [../data/annotation/H89-2014.json]\n",
       "17  I05-5011                              Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs  [Automatic paraphrase discovery is an important but challenging task., We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue., We focus on phrases which connect two Named Entities (NEs), and proceed in two stages., The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets., The second stage links sets which involve the same pairs of individual NEs., A total of 13,976 phrases were grouped., The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the links for two evaluated domains was 73% and 86%., One of the difficulties in Natural Language Processing is the fact that there are many...  ../data/papers/I05-5011.xml                                                                                                                                 [../data/summaries/I05-5011.txt]                [[This paper conducted research in the area of automatic paraphrase discovery., The authors believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system., They proposed an unsupervised method to discover paraphrases from a large untagged corpus., They focused on phrases which two Named Entities, and proceed in two stages., This topic has been getting more attention, driven by the needs of various NLP applications., Most IE researchers have been creating paraphrase knowledge by hand and specific tasks., The authors cluster NE instance pairs based on the words in the context using bag-of-words methods., In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold.]]                                                                                                                                       [../data/annotation/I05-5011.json]\n",
       "18  J01-2004                                                       Probabilistic Top-Down Parsing and Language Modeling  [This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition., The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling., A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers., A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity., Interpolation with a trig...  ../data/papers/J01-2004.xml                                         [../data/summaries/J01-2004_swastika.txt, ../data/summaries/J01-2004_sweta.txt, ../data/summaries/J01-2004_aakansha.txt]  [[Roark, in this paper, described the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition., They varied the base beam factor in trials on the Chelba and Jelinek corpora, keeping the level of conditioning information constant., With a simple conditional probability model, and simple statistical search heuristics, they were able to find very accurate parses efficiently, and, assign word probabilities that yielded a perplexity improvement over previous results., Interpolation with a trigram model yielded an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by their parsing model was orthogonal to that captured by a...                                           [../data/annotation/J01-2004_swastika.json, ../data/annotation/J01-2004_sweta.json, ../data/annotation/J01-2004_aakansha.json]\n",
       "19  J96-3004                                          A Stochastic Finite-State Word-Segmentation Algorithm for Chinese  [The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.,  For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.,  In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to \"reconstruct\" the word-boundary information.,  In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.,  The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunc...  ../data/papers/J96-3004.xml                                                                                                                                 [../data/summaries/J96-3004.txt]  [[In this paper the authors present a stochastic finite-state model for segmenting Chinese text into words., The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers., It also incorporates the Good-Turing methodin estimating the likelihoods of previously unseen constructions, including morphological derivatives and personal names., they evaluate various specific aspects of the segmentation, as well as the overall segmentation performance., The evaluation compares the performance of the system with that of several human judges and inter-human agreement on a single correct way to segment a text.they showed that the average agreement among the human judges is .76, and the average agreement between ST(system) an...                                                                                                                                       [../data/annotation/J96-3004.json]\n",
       "20  N01-1011                                          A Decision Tree of Bigrams is an Accurate Predictor of Word Sense  [This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby., This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise., It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words., Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs., For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined., For example, suppose bill has the following set of possible meanings: a piece of currency, pending legisla...  ../data/papers/N01-1011.xml                                                                                                                                 [../data/summaries/N01-1011.txt]  [[This paper presents a corpus-based approach to\\nword sense disambiguation where a decision tree assigns\\na sense to an ambiguous word based on the\\nbigrams that occur nearby.for this purpose the\\nsense inventory of word sense has already been determined., This paper describes an approach where a deci-\\nsion tree is learned from some number of sentences\\nwhere each instance of an ambiguous word has been\\nmanually annotated with a sense-tag that denotes\\nthe most appropriate sense for that context and for Building a Feature Set of Bigrams;\\ntwo alternatives, the power divergence family\\nand the Dice Coefficient were explored., this study utilizes the training and test\\ndata from the 1998 SENSEVAL evaluation of word\\nsense disambiguation systems., The results of this approach\\nare compa...                                                                                                                                       [../data/annotation/N01-1011.json]\n",
       "21  N04-1038                              Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution  [We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor., BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning., These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible., BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources., Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns., The problem of coreference resolution has received considerable attention, including theoretical dis...  ../data/papers/N04-1038.xml                                                                                                                                 [../data/summaries/N04-1038.txt]  [[In this paper, Ben and Riloff present a coreference resolver called BABAR that focuses on the use of contextual-role knowledge for coreference resolution., The problem of coreference resolution has received considerable attention, including theoretical discourse models and supervised machine learning systems., BABARâs performance in both domains of terrorism and natural disaster, and the contextual-role knowledge in pronouns have shown successful results., However, using the top-level semantic classes of WordNet proved to be problematic as the class distinctions are too coarse., Bean and Riloff also used bootstrapping to extend their semantic compatibility model, proposed using caseframe network for anaphora resolution, information extraction patterns to identify contextual clues f...                                                                                                                                       [../data/annotation/N04-1038.json]\n",
       "22  N06-2049                           Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation  [We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach., We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation., In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates., By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005., The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005)., Under the scheme, each character of a word is labeled as âBâ if it is...  ../data/papers/N06-2049.xml                                                                                                                                 [../data/summaries/N06-2049.txt]  [[In this paper, the authors proposed a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters., Their word segmentation process is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subwordbased tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging., They used the data from the Sighan Bakeoff 2005 to test their approaches., By these techniques, they achieved higher F-scores in City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR) corpora than the best results from Si...                                                                                                                                       [../data/annotation/N06-2049.json]\n",
       "23  P04-1036                                                           Finding Predominant Word Senses in Untagged Text  [word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed., The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data., Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration., We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically., The acquired predominant senses give a o...  ../data/papers/P04-1036.xml  [../data/summaries/P04-1036_aakansha.txt, ../data/summaries/P04-1036_swastika.txt, ../data/summaries/P04-1036_vardha.txt, ../data/summaries/P04-1036_sweta.txt]  [[Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll in their paper 'Finding Predominant Word Senses in Untagged Text'present a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.ey chose documents from the SPORTS domain and a the FINANCE domain.they use an automatically acquired thesaurus and a WordNet Similarity measure.we use the SENSEVAL -2 all-words data (Palmer et al.,2001).This is a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II., The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL -2 English all-words task giving us a WSD precision of 64% on an all-nouns task.], [McCarthy et all presented work on the use of a...  [../data/annotation/P04-1036_aakansha.json, ../data/annotation/P04-1036_swastika.json, ../data/annotation/P04-1036_vardha.json, ../data/annotation/P04-1036_sweta.json]\n",
       "24  P05-1004                                              Supersense Tagging of Unknown Nouns using Semantic Similarity  [The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words., Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET., Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples., We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger., We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity., Lexical-semantic resources have been applied successful to a wide range of Natural Langu...  ../data/papers/P05-1004.xml                                                                                                                                 [../data/summaries/P05-1004.txt]  [[The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words., Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET., Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction and class-based smoothing, to text classification and question answering., Some specialist topics are better covered in WORDNET than others., A considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORDNET and the construction of new wordnet using the concept structure from E...                                                                                                                                       [../data/annotation/P05-1004.json]\n",
       "25  P05-1013                                                                       Pseudo-Projective Dependency Parsing  [In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures., We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures., Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy., This leads to the best reported performance for robust non-projective parsing of Czech., It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate t...  ../data/papers/P05-1013.xml                                        [../data/summaries/P05-1013_swastika.txt, ../data/summaries/P05-1013_aakansha.txt, ../data/summaries/P05-1013_vardha.txt]  [[Authors Nilsson and Nivre showed how a data-driven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures., Experiments using data from the Prague Dependency Treebank showed that the combined system can handle non-projective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy., The combined system could recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion., This led to the best reported performance for robust non-projective parsing of Czech.], [Joakim Nivre and Jens Nilsson in their paper'Pseud...                                          [../data/annotation/P05-1013_swastika.json, ../data/annotation/P05-1013_aakansha.json, ../data/annotation/P05-1013_vardha.json]\n",
       "26  P06-2124                                                  BiTAM: Bilingual Topic AdMixture Models forWord Alignment  [We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation., Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model., Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels)., These models enable word- alignment process to leverage topical contents of document-pairs., Efficient variational approximation algorithms are designed for inference and parameter estimation., With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspect...  ../data/papers/P06-2124.xml                                                                                                                                 [../data/summaries/P06-2124.txt]                                                                      [[In this paper, the authors proposed a probabilistic admixture model to capture latent topics underlying the context of document- pairs., They proposed a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT., They used IBM Model I as the baseline., They investigated three instances of the BiTAM model, They were data-driven and did not need handcrafted knowledge engineering., The proposed models signiï¬cantly improved the alignment accuracy and lead to better translation qualities., Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.]]                                                                                                                                       [../data/annotation/P06-2124.json]\n",
       "27  P08-1028                                                                Vector-based Models of Semantic Composition  [This paper proposes a framework for representing the meaning of phrases and sentences in vector space., Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions., Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task., Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments., Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science., The appeal of these models lies in their ability to represent meaning simply by using distributional informat...  ../data/papers/P08-1028.xml                                         [../data/summaries/P08-1028_swastika.txt, ../data/summaries/P08-1028_sweta.txt, ../data/summaries/P08-1028_aakansha.txt]  [[In this paper, Mitchell and Lapata proposed a framework for vector-based semantic composition., Central to their approach was vector composition which they operationalize in terms of additive and multiplicative functions., Under this framework, they introduced a wide range of composition models which they evaluated empirically on a sentence similarity task., Experimental results demonstrated that the multiplicative models were superior- at least, for the sentence similarity task attempted- to the additive alternatives when compared against human judgments., They conjectured that the additive models are not sensitive to the fine-grained meaning distinctions involved in their materials., Multiplicative models considered a subset, namely non-zero components whereas additive models captu...                                           [../data/annotation/P08-1028_swastika.json, ../data/annotation/P08-1028_sweta.json, ../data/annotation/P08-1028_aakansha.json]\n",
       "28  P08-1043                       A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing  [Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence., These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance., Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity., Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so ...  ../data/papers/P08-1043.xml                                         [../data/summaries/P08-1043_sweta.txt, ../data/summaries/P08-1043_swastika.txt, ../data/summaries/P08-1043_aakansha.txt]  [[In this paper they propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity., Using a Treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique their model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far., One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects., The implication of this ambiguity for...                                           [../data/annotation/P08-1043_sweta.json, ../data/annotation/P08-1043_swastika.json, ../data/annotation/P08-1043_aakansha.json]\n",
       "29  P08-1102                     A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging  [We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging., With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly., Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging., On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline., Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages., Several ...  ../data/papers/P08-1102.xml                                         [../data/summaries/P08-1102_sweta.txt, ../data/summaries/P08-1102_swastika.txt, ../data/summaries/P08-1102_aakansha.txt]  [[This paper aims at proposing a cascaded linear model for joint Chinese word segmentation and part-of- speech tagging., With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly., Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages., However, as such features are generated dynamically during the decoding procedure, two limitations arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the curr...                                           [../data/annotation/P08-1102_sweta.json, ../data/annotation/P08-1102_swastika.json, ../data/annotation/P08-1102_aakansha.json]\n",
       "30  P11-1060                                                          Learning Dependency-Based Compositional Semantics  [Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms., In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs., In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms., On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms., What is the total population of the ten largest capitals in the US?, Answering these types of complex questions compositiona...  ../data/papers/P11-1060.xml                                         [../data/summaries/P11-1060_aakansha.txt, ../data/summaries/P11-1060_swastika.txt, ../data/summaries/P11-1060_sweta.txt]  [[The paper 'Learning Dependency-Based Compositional Semantics' by Percy Liang,Michael I. Jordan and Dan Klein propose a new way to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs., The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive., The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees,which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient., The logical forms in DCS are called DCS trees,where nodes are labeled with predicates, and edges are labeled with relations., DCS trees are le...                                           [../data/annotation/P11-1060_aakansha.json, ../data/annotation/P11-1060_swastika.json, ../data/annotation/P11-1060_sweta.json]\n",
       "31  P11-1061                                 Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections  [We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language., Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages., We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010)., Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm., Supervised learning appr...  ../data/papers/P11-1061.xml                                         [../data/summaries/P11-1061_sweta.txt, ../data/summaries/P11-1061_swastika.txt, ../data/summaries/P11-1061_aakansha.txt]  [[In this paper the author aims at describing a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labelled training data, but have translated text in a resource-rich language., They use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model., Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems., Unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for training models., To bridge this gap, the author considers a practically motivated scenario, in which he wants to leverage existing resources from a re...                                           [../data/annotation/P11-1061_sweta.json, ../data/annotation/P11-1061_swastika.json, ../data/annotation/P11-1061_aakansha.json]\n",
       "32  P87-1015                         CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*  [We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate., In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in p...  ../data/papers/P87-1015.xml                                           [../data/summaries/P87-1015_swastika.txt, ../data/summaries/P87-1015_sweta.txt, ../data/summaries/P87-1015_vardha.txt]  [[Vijay-Shankar et all considered the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate., They showed that it was useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees, find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars., On the basis of that observation, they described a class of formalisms which they called Linear Context- Free Rewriting Systems (LCFRs), and showed they were recognizable in polynom...                                             [../data/annotation/P87-1015_swastika.json, ../data/annotation/P87-1015_sweta.json, ../data/annotation/P87-1015_vardha.json]\n",
       "33  P98-1081                                              Improving Data Driven Wordclass Tagging by System Combination  [In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system., We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging., Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data., After comparison, their outputs are combined using several voting strategies and second stage classifiers., All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger., In all Natural Language Processing (NLP) systems, we find one or more language models wh...  ../data/papers/P98-1081.xml                                                                                                                                 [../data/summaries/P98-1081.txt]  [[This paper examines how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system., This paper is concerned with the question whether the differences between models can indeed be exploited to yield a data driven model with superior performance.the approach is known as ensemble, stacked, or combined classifiers.the approach is applied mopho-syntactic wordclass tagging.for this purpose a traditional trig-ram model,the Transformation Based Learning system,Memory-Based Learning and the MXPOST system are used.the tagged LOB corpus has been used for data., The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error ra...                                                                                                                                       [../data/annotation/P98-1081.json]\n",
       "34  P98-2143                                                           Robust pronoun resolution with limited knowledge  [Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge., One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task., This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger., Input is checked against agreement and for a number of antecedent indicators., Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent., Evaluation reports a success rate of 89.7% which is better than the sucÂ­ cess rates of the approaches selected for comparison and tested on the same data., In addition, preliminary e...  ../data/papers/P98-2143.xml                                                                                                                                 [../data/summaries/P98-2143.txt]                                                                                                                                                                                                                          [[This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger in 1998.It makes use of only a part-of-speech tagger, plus simple noun phrase rules and operates on the basis of antecedent-tracking preferences or antecedent indicators.these indicators depend upon a number of salience factors like definiteness,immediate reference etc.this system does not require domain specific knowledge.it shows success rate of 89.7%.it can be applied to different languages other than english.]]                                                                                                                                       [../data/annotation/P98-2143.json]\n",
       "35  W03-0410                                                  Semi-supervised Verb Class Discovery Using Noisy Features  [We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs., The feature set was previously shown to work well in a supervised learning setting, using known English verb classes., In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task., We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties., We find that the unsupervised method we tried cannot be consistently applied to our data., However, the semi- supervised approach (using a seed set of ...  ../data/papers/W03-0410.xml                                                                                                                                 [../data/summaries/W03-0410.txt]                 [[In this paper, the authors used a semi-supervised method for verb class discovery using Noisy Features., The feature set was previously shown to work well in a supervised learning setting, using known English verb classes., They face the problem of having a large number of irrelevant features for a particular clustering task., They used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all the unsupervised experiments., The evaluation was based on the accuracy of assigning members to the correct clusters., The authors discuss their clustering approach to identify that the task becomes difficult if the learning method has to distinguish multiple classes, rather than focus on the important properties of a single class.]]                                                                                                                                       [../data/annotation/W03-0410.json]\n",
       "36  W04-0213                                                                              The Potsdam Commentary Corpus  [A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure., The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation., A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees., Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization)., This paper, however, provides a compre...  ../data/papers/W04-0213.xml                                                                                                                                 [../data/summaries/W04-0213.txt]  [[This paper discusses the Potsdam Commentary Corpus, a corpus of german assembeled by potsdam university., The corpus was annoted with different linguitic information.the Potsdam Commentary Corpus or PCC consists of\\n170 commentaries from M¨arkische Allgemeine Zeitung, a German regional daily., This corpus includes 173 texts on politics from the on-line newspaper\\nMärkische Allgemeine Zeitung., It contains 32,962 words and 2,195 sentences., It is annotated with several data: morphology, syntax, rhetorical\\nstructure, connectors, correference and informative structure., Nevertheless, only a part of this corpus (10 texts), which the authors name \"core corpus\",\\nis annotated with all this information., The texts were annotated with the RSTtool., This corpus has several advantages: it i...                                                                                                                                       [../data/annotation/W04-0213.json]\n",
       "37  W06-2932                                    Multilingual Dependency Analysis with a Two-Stage Discriminative Parser  [present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages., The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages., The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph., We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis., Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing., With the availability of resources such as the Penn WSJ Treebank, much of the f...  ../data/papers/W06-2932.xml                                           [../data/summaries/W06-2932_vardha.txt, ../data/summaries/W06-2932_sweta.txt, ../data/summaries/W06-2932_swastika.txt]  [[This paper talks about Multilingual Dependency Analysis with a Two-Stage Discriminative Parser., Here we present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages., The first stage of our system creates an unlabeled parse y for an input sentence x., The second stage takes the output parse y for sentence x and classifies each edge (i, j) E y with a particular label l (i,j)., Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph., The current system simply includes all morphological bi-gram features., It is our hope that a better morphological fe...                                             [../data/annotation/W06-2932_vardha.json, ../data/annotation/W06-2932_sweta.json, ../data/annotation/W06-2932_swastika.json]\n",
       "38  W06-3114                          Manual and Automatic Evaluation of Machine Translation between European Languages  [Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) u...  ../data/papers/W06-3114.xml                                         [../data/summaries/W06-3114_swastika.txt, ../data/summaries/W06-3114_aakansha.txt, ../data/summaries/W06-3114_sweta.txt]  [[Koehn and Monz carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs., While many systems had similar performance, the results offered interesting insights, especially, about the relative performance of statistical and rule-based systems., Due to many similarly performing systems, they are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics., The bias of automatic methods in favour of statistical systems seemed to be less pronounced on out-of-domain test data., The manual evaluation of scoring translation on a graded scale from 1&#8211;5 seemed to be very hard to perform., Human judges also pointed out difficulties with the evaluation of long sentences., They fou...                                           [../data/annotation/W06-3114_swastika.json, ../data/annotation/W06-3114_aakansha.json, ../data/annotation/W06-3114_sweta.json]\n",
       "39  W08-2222                                                                 Wide-Coverage Semantic Analysis with Boxer  [Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT)., Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts., The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles., The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic., Boxerâs performance on the shared task for comparing semantic represtations was promising., It was able to produce complete DRSs for all seven texts., Manually inspecting the output reve...  ../data/papers/W08-2222.xml                                                                                                                                 [../data/summaries/W08-2222.txt]  [[In this paper, the authors describe Boxer, their open-domain software component for semantic analysis of text, based on Combinatory Categorical Grammar (CCG) and Discourse Representation Theory (DRT)., Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorical Grammar (CCG) and Discourse Representation Theory (DRT)., Based on Discourse Representation Theory Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called âboxesâ because of the way they are graphically displayed) for English sentences and texts., Boxer implements a syntax-semantics interface based on Combinatory Categorical Grammar, CCG., Because the syntax-semantics is clearly defined, the choice of logical form can be independent of...                                                                                                                                       [../data/annotation/W08-2222.json]\n",
       "40  W11-2123                                                           KenLM: Faster and Smaller Language Model Queries  [We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs., The structure uses linear probing hash tables and is designed for speed., Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline., Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems., This paper describes the several performance techniques used and presents benchmarks against alternative implementations., Language models are widely applied in n...  ../data/papers/W11-2123.xml                                        [../data/summaries/W11-2123_vardha.txt, ../data/summaries/W11-2123_swastika.txt, ../data/summaries/W11-2123_aakansha.txt]  [[This paper talks about KenLM: Faster and Smaller Language Model Queries., The PROBING data structure uses linear probing hash tables and is designed for speed., This paper presents methods to query N-gram language models, minimizing time and space costs., Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders., For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed., The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM., The code is open source, has minimal depen...                                          [../data/annotation/W11-2123_vardha.json, ../data/annotation/W11-2123_swastika.json, ../data/annotation/W11-2123_aakansha.json]\n",
       "41  W95-0104                                         A Bayesian hybrid method for context-sensitive spelling correction  [Two classes of methods have been shown to be useful for resolving lexical ambiguity., The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word., These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax., Yarowsky has exploited this complementarity by combining the two methods using decision lists., The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be., This paper takes Yarowsky's work as a starting point, applying decision li...  ../data/papers/W95-0104.xml                                                                                                                                 [../data/summaries/W95-0104.txt]  [[This paper presents a hybrid method where two classes of methods have been used for resolving lexical ambiguity., The authors investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence., Their method combines context-word and collocation methods where the former captures the lexical \"atmosphere\", while the latter captures local syntax., The authors have used five methods for spelling correction: baseline, context words, collocations, decision list and Bayesian classifier., Decision list and Bayesian classifiers are their hybrid methods., For testing this method, confusion sets have been used with ambiguous words like desert and dessert, there and their etc., In their ev...                                                                                                                                       [../data/annotation/W95-0104.json]\n",
       "42  W99-0613                                                Unsupervised Models for Named Entity Classification Collins  [This paper discusses the use of unlabeled examples for the problem of named entity classification., A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules., The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type., We present two algorithms., The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98)., The second algorithm extends ideas from boosting algorithm...  ../data/papers/W99-0613.xml  [../data/summaries/W99-0613_swastika.txt, ../data/summaries/W99-0613_aakansha.txt, ../data/summaries/W99-0613_sweta.txt, ../data/summaries/W99-0613_vardha.txt]  [[Collins and Singer, in this paper, discussed the use of unlabeled examples for the problem of named entity classification., They showed that the use of data could reduce the requirements for supervision to just 7 simple rules., The approach gained leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appeared were sufficient to determine its type., They presented two algorithms., The first method used a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98)., The second algorithm extended ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98) and boosted the objective functions.], [This pape...  [../data/annotation/W99-0613_swastika.json, ../data/annotation/W99-0613_aakansha.json, ../data/annotation/W99-0613_sweta.json, ../data/annotation/W99-0613_vardha.json]\n",
       "43  W99-0623                                     Exploiting Diversity in Natural Language Processing: Combining Parsers  [Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy., Two general approaches are presented and two combination techniques are described for each approach., Both parametric and non-parametric models are explored., The resulting parsers surpass the best previously published performance results for the Penn Treebank., The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems., The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992; Heath et al., 1996)., Their theoretical finding is simply stated: classification error ra...  ../data/papers/W99-0623.xml                                           [../data/summaries/W99-0623_vardha.txt, ../data/summaries/W99-0623_swastika.txt, ../data/summaries/W99-0623_sweta.txt]  [[This paper talks about Exploiting Diversity in Natural Language Processing: Combining Parsers., Two general approaches are presented and two combination techniques are described for each approach., Here both parametric and non-parametric models are explored., One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure., The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank., Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result., Combining multiple highly-accurate independent parsers yields promisin...                                             [../data/annotation/W99-0623_vardha.json, ../data/annotation/W99-0623_swastika.json, ../data/annotation/W99-0623_sweta.json]\n",
       "44  X96-1048                                                                OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION  [Test abstract, The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November., Participants were invited to enter their systems in as many as four different task-oriented evaluations., The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time., The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years., The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume., All except the Scenario Template task are de...  ../data/papers/X96-1048.xml                                                                                                                                 [../data/summaries/X96-1048.txt]  [[This paper is an overview of results of the Sixth Message Understanding Conference (MUC-6) in November., This paper surveys the results of the natural language processing system evaluation on tasks like named entity,coreference,template element and scenario template., Discussion of the results for each task is organized generally under the following topics: Results on task as whole, Results on some aspects of task, Performance on \"walkthrough article.\", The walkthrough article is an article selected from the test set., Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers., Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium., The articles used in the evaluation we...                                                                                                                                       [../data/annotation/X96-1048.json]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Path(\"../data/papers\").mkdir(exist_ok=True, parents=True)\n",
    "    Path(\"../data/summaries\").mkdir(exist_ok=True, parents=True)\n",
    "    Path(\"../data/annotation\").mkdir(exist_ok=True, parents=True)   \n",
    "\n",
    "    copy_original_data()\n",
    "    data = get_papers()\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"raw_summaries\"] = df[\"path_summaries\"].apply(lambda s: splitlines_summaries(s))\n",
    "    df[\"path_annotations\"] = df[\"path_summaries\"].apply(lambda s: prepare_for_annotation(s))\n",
    "    \n",
    "    df.to_pickle(\"../data/data.pkl\")\n",
    "    df.to_csv(\"../data/data.csv\")\n",
    "\n",
    "    display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
