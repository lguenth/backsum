{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lukel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import pickle as pk\n",
    "import re\n",
    "import codecs\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"max_colwidth\", 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_original_data():\n",
    "    uids = []\n",
    "    data = Path(\"../original/data/\")\n",
    "    papers = data.glob(\"**/Reference_XML/*.xml\")\n",
    "    summaries = data.glob(\"**/**/*.human.txt\")\n",
    "\n",
    "    for summary in summaries:\n",
    "        uid = re.sub(\"([A-Z]\\d{2}-\\d{4})(.*)\", \"\\g<1>\", summary.stem)\n",
    "        uids.append(uid)\n",
    "        path = f\"../data/summaries/{summary.stem.replace('.human', '')}.txt\"\n",
    "        shutil.copyfile(summary, path)\n",
    "\n",
    "    for paper in papers:\n",
    "        if paper.stem in uids:\n",
    "            shutil.copyfile(paper, f\"../data/papers/{paper.stem}.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(path):\n",
    "    with codecs.open(path, \"r\", encoding=\"latin-1\") as file:\n",
    "        try:\n",
    "            xml = file.read()\n",
    "        except Exception as e:\n",
    "            print(\"Could not parse: \", paper)\n",
    "            print(e)\n",
    "\n",
    "        root = etree.fromstring(xml)\n",
    "        title = root.find(\"./S[@sid='0']\").text if not None else \"\"\n",
    "        s_elems = root.xpath(\".//S\")\n",
    "        sentences = [s.text for s in s_elems if s.text is not None]\n",
    "        sids = [s.attrib.get(\"sid\") for s in s_elems if s.text is not None]\n",
    "\n",
    "        return title, dict(zip(sids, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers():\n",
    "    papers = Path(\"../data/papers\").glob(\"*.xml\")\n",
    "    data = []\n",
    "\n",
    "    for path in papers:\n",
    "        # Parse paper XML\n",
    "        title, text = parse_xml(path)\n",
    "        paper_id = path.stem\n",
    "\n",
    "        # Get related summaries based on paper id\n",
    "        summaries = Path(\"../data/summaries\").glob(f\"*{paper_id}*.txt\")\n",
    "        summary_paths = [str(s) for s in summaries]\n",
    "        summary_ids = [s.split(\"/\")[-1].replace(\".txt\", \"\") for s in summary_paths]\n",
    "\n",
    "        paper = {\n",
    "            \"paper_id\": paper_id,\n",
    "            \"paper_title\": title,\n",
    "            \"paper_path\": path,\n",
    "            \"paper_text\": text,\n",
    "            \"summary_ids\": summary_ids,\n",
    "            \"summary_paths\": summary_paths,\n",
    "        }\n",
    "\n",
    "        data.append(paper)\n",
    "\n",
    "    return sorted(data, key=lambda p: p[\"paper_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitlines_summaries(summaries):\n",
    "    for summary in summaries:\n",
    "        with codecs.open(summary, \"r+\", encoding=\"latin-1\") as s:\n",
    "            text = s.read().strip()\n",
    "            text = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2', text)\n",
    "            split = nltk.tokenize.sent_tokenize(text)\n",
    "            s.seek(0)\n",
    "            s.writelines([line + \"\\n\" for line in split])\n",
    "            s.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_annotation(row):\n",
    "    summary_paths = row[\"summary_paths\"]\n",
    "    summary_ids = row[\"summary_ids\"]\n",
    "    paper_id = row[\"paper_id\"]\n",
    "\n",
    "    paths = []\n",
    "    for summary, summary_id in zip(summary_paths, summary_ids):\n",
    "        annotations = []\n",
    "\n",
    "        with codecs.open(summary, \"r\", encoding=\"latin-1\") as file:\n",
    "            for index, sentence in enumerate(file, start=1):\n",
    "                annotation = {\n",
    "                    \"summary_id\": summary_id,\n",
    "                    \"paper_id\": paper_id,\n",
    "                    \"source_sid\": index,\n",
    "                    \"target_sid\": None,\n",
    "                    \"source_text\": sentence.strip(),\n",
    "                    \"strategy\": None,\n",
    "                }\n",
    "                annotations.append(annotation)\n",
    "\n",
    "        path = f\"../data/tba/{summary_id}.json\"\n",
    "        paths.append(path)\n",
    "\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(annotations, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>summary_ids</th>\n",
       "      <th>summary_paths</th>\n",
       "      <th>annotation_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00-2018</td>\n",
       "      <td>A Maximum-Entropy-Inspired Parser *</td>\n",
       "      <td>../data/papers/A00-2018.xml</td>\n",
       "      <td>{'0': 'A Maximum-Entropy-Inspired Parser *', '1': 'We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.', '2': 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', '3': 'The major technical innovation is the use of a &amp;quot;maximum-entropy-inspired&amp;quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', '4': 'We also present some partial results showing the effects of different conditioning information, inc...</td>\n",
       "      <td>[A00-2018_sweta, A00-2018_akanksha, A00-2018_vardha]</td>\n",
       "      <td>[../data/summaries/A00-2018_sweta.txt, ../data/summaries/A00-2018_akanksha.txt, ../data/summaries/A00-2018_vardha.txt]</td>\n",
       "      <td>[../data/tba/A00-2018_sweta.json, ../data/tba/A00-2018_akanksha.json, ../data/tba/A00-2018_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00-2030</td>\n",
       "      <td>A Novel Use of Statistical Parsing to Extract Information from Text</td>\n",
       "      <td>../data/papers/A00-2030.xml</td>\n",
       "      <td>{'0': 'A Novel Use of Statistical Parsing to Extract Information from Text', '1': 'Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.', '2': 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', '3': 'Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.', '4': 'Yet, relatively few have embedded one of these algorithms in a task.', '5': 'Chi...</td>\n",
       "      <td>[A00-2030_sweta, A00-2030_aakansha, A00-2030_vardha]</td>\n",
       "      <td>[../data/summaries/A00-2030_sweta.txt, ../data/summaries/A00-2030_aakansha.txt, ../data/summaries/A00-2030_vardha.txt]</td>\n",
       "      <td>[../data/tba/A00-2030_sweta.json, ../data/tba/A00-2030_aakansha.json, ../data/tba/A00-2030_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A97-1014</td>\n",
       "      <td>An Annotation Scheme for Free Word Order Languages</td>\n",
       "      <td>../data/papers/A97-1014.xml</td>\n",
       "      <td>{'0': 'An Annotation Scheme for Free Word Order Languages', '1': 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', '2': 'Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme.', '3': 'The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.', '4': 'The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.', '5': 'In particular, we focus on several methodological issues concerning the annotation of n...</td>\n",
       "      <td>[A97-1014_vardha, A97-1014_sweta, A97-1014_swastika]</td>\n",
       "      <td>[../data/summaries/A97-1014_vardha.txt, ../data/summaries/A97-1014_sweta.txt, ../data/summaries/A97-1014_swastika.txt]</td>\n",
       "      <td>[../data/tba/A97-1014_vardha.json, ../data/tba/A97-1014_sweta.json, ../data/tba/A97-1014_swastika.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C00-2123</td>\n",
       "      <td>Word Re-ordering and DP-based Search in Statistical Machine Translation</td>\n",
       "      <td>../data/papers/C00-2123.xml</td>\n",
       "      <td>{'0': 'Word Re-ordering and DP-based Search in Statistical Machine Translation', '1': 'In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).', '2': 'Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.', '3': 'A search restriction especially useful for the translation direction from German to English is presented.', '4': 'The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.', '5': 'The goal of machine translation is the translation of a text given in some so...</td>\n",
       "      <td>[C00-2123]</td>\n",
       "      <td>[../data/summaries/C00-2123.txt]</td>\n",
       "      <td>[../data/tba/C00-2123.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C02-1025</td>\n",
       "      <td>Named Entity Recognition: A Maximum Entropy Approach Using Global Information</td>\n",
       "      <td>../data/papers/C02-1025.xml</td>\n",
       "      <td>{'0': 'Named Entity Recognition: A Maximum Entropy Approach Using Global Information', '1': 'This paper presents a maximum entropy-based named entity recognizer (NER).', '2': 'It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.', '3': 'Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.', '4': 'In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.', '5': 'Considerable amount of wor...</td>\n",
       "      <td>[C02-1025]</td>\n",
       "      <td>[../data/summaries/C02-1025.txt]</td>\n",
       "      <td>[../data/tba/C02-1025.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C08-1098</td>\n",
       "      <td>Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging</td>\n",
       "      <td>../data/papers/C08-1098.xml</td>\n",
       "      <td>{'0': 'Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging', '1': 'We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.', '2': 'It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.', '3': 'In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.', '4': 'A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN ...</td>\n",
       "      <td>[C08-1098]</td>\n",
       "      <td>[../data/summaries/C08-1098.txt]</td>\n",
       "      <td>[../data/tba/C08-1098.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C10-1045</td>\n",
       "      <td>Better Arabic Parsing: Baselines, Evaluations, and Analysis</td>\n",
       "      <td>../data/papers/C10-1045.xml</td>\n",
       "      <td>{'0': 'Better Arabic Parsing: Baselines, Evaluations, and Analysis', '1': 'In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design.', '2': 'First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.', '3': 'Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.', '4': 'Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG.', '5': 'Fourth, we show how to build better models for three different parsers.', '6': 'Finally, we show that in application settings, the absence of gold se...</td>\n",
       "      <td>[C10-1045]</td>\n",
       "      <td>[../data/summaries/C10-1045.txt]</td>\n",
       "      <td>[../data/tba/C10-1045.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C90-2039</td>\n",
       "      <td>Strategic Lazy Incremental Copy Graph Unification</td>\n",
       "      <td>../data/papers/C90-2039.xml</td>\n",
       "      <td>{'0': 'Strategic Lazy Incremental Copy Graph Unification', '1': 'The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures.', '2': 'One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.', '3': 'The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.', '4': 'The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapp...</td>\n",
       "      <td>[C90-2039]</td>\n",
       "      <td>[../data/summaries/C90-2039.txt]</td>\n",
       "      <td>[../data/tba/C90-2039.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C94-2154</td>\n",
       "      <td>THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES</td>\n",
       "      <td>../data/papers/C94-2154.xml</td>\n",
       "      <td>{'0': 'THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES', '1': 'in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch.', '2': '!['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation.', '3': 'Unification lbrmMisms ma.y be either un-typed (DCC~s, PATRII, 1,F(;) or typed (npsG).', '4': 'A m~L,ior reason for adding types to ~ forma,lism is to express restrictions on fea.ture cooccurences a.s in (;l's(:: [5] in order to rule out nonexista.nt tyl)es ...</td>\n",
       "      <td>[C94-2154]</td>\n",
       "      <td>[../data/summaries/C94-2154.txt]</td>\n",
       "      <td>[../data/tba/C94-2154.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D09-1092</td>\n",
       "      <td>Polylingual Topic Models</td>\n",
       "      <td>../data/papers/D09-1092.xml</td>\n",
       "      <td>{'0': 'Polylingual Topic Models', '1': 'Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.', '2': 'Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.', '3': 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', '4': 'We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', '5': 'Statistical topic models have emerged as an increasingly useful anal...</td>\n",
       "      <td>[D09-1092_swastika, D09-1092_vardha, D09-1092_sweta]</td>\n",
       "      <td>[../data/summaries/D09-1092_swastika.txt, ../data/summaries/D09-1092_vardha.txt, ../data/summaries/D09-1092_sweta.txt]</td>\n",
       "      <td>[../data/tba/D09-1092_swastika.json, ../data/tba/D09-1092_vardha.json, ../data/tba/D09-1092_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>D10-1044</td>\n",
       "      <td>Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</td>\n",
       "      <td>../data/papers/D10-1044.xml</td>\n",
       "      <td>{'0': 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', '1': 'We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', '2': 'This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure.', '3': 'We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.', '4': 'Domain adaptation is a common concern when optimizing empirical NLP applications...</td>\n",
       "      <td>[D10-1044_aakansha, D10-1044_sweta, D10-1044_swastika]</td>\n",
       "      <td>[../data/summaries/D10-1044_aakansha.txt, ../data/summaries/D10-1044_sweta.txt, ../data/summaries/D10-1044_swastika.txt]</td>\n",
       "      <td>[../data/tba/D10-1044_aakansha.json, ../data/tba/D10-1044_sweta.json, ../data/tba/D10-1044_swastika.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>D10-1083</td>\n",
       "      <td>Simple Type-Level Unsupervised POS Tagging</td>\n",
       "      <td>../data/papers/D10-1083.xml</td>\n",
       "      <td>{'0': 'Simple Type-Level Unsupervised POS Tagging', '1': 'Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus.', '2': 'Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy.', '3': 'However, in existing systems, this expansion come with a steep increase in model complexity.', '4': 'This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments.', '5': 'In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training.', '6': 'Our experiments consistently demonstrate that this model architectu...</td>\n",
       "      <td>[D10-1083]</td>\n",
       "      <td>[../data/summaries/D10-1083.txt]</td>\n",
       "      <td>[../data/tba/D10-1083.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>E03-1005</td>\n",
       "      <td>An Efficient Implementation of a New DOP Model</td>\n",
       "      <td>../data/papers/E03-1005.xml</td>\n",
       "      <td>{'0': 'An Efficient Implementation of a New DOP Model', '1': 'Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.', '2': 'This paper proposes an integration of the two models which outperforms each of them separately.', '3': 'Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.', '4': 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of p...</td>\n",
       "      <td>[E03-1005_swastika, E03-1005_sweta, E03-1005_aakansha]</td>\n",
       "      <td>[../data/summaries/E03-1005_swastika.txt, ../data/summaries/E03-1005_sweta.txt, ../data/summaries/E03-1005_aakansha.txt]</td>\n",
       "      <td>[../data/tba/E03-1005_swastika.json, ../data/tba/E03-1005_sweta.json, ../data/tba/E03-1005_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E03-1020</td>\n",
       "      <td>Discovering Corpus-Specific Word Senses</td>\n",
       "      <td>../data/papers/E03-1020.xml</td>\n",
       "      <td>{'0': 'Discovering Corpus-Specific Word Senses', '1': 'This paper presents an unsupervised algorithm which automatically discovers word senses from text.', '2': 'The algorithm is based on a graph model representing words and relationships between them.', '3': 'Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.', '4': 'Discrimination against previously extracted sense clusters enables us to discover new senses.', '5': 'We use the same data for both recognising and resolving ambiguity.', '6': 'This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies.', '7': 'Automatic word sense discovery has applications of many k...</td>\n",
       "      <td>[E03-1020]</td>\n",
       "      <td>[../data/summaries/E03-1020.txt]</td>\n",
       "      <td>[../data/tba/E03-1020.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>E09-2008</td>\n",
       "      <td>Foma: a finite-state compiler and library</td>\n",
       "      <td>../data/papers/E09-2008.xml</td>\n",
       "      <td>{'0': 'Foma: a finite-state compiler and library', '1': 'Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.', '2': 'It has specific support for many natural language processing applications such as producing morphological and phonological analyzers.', '3': 'Foma is largely compatible with the Xerox/PARC finite-state toolkit.', '4': 'It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the âMathematical Operatorsâ Unicode block.', '5': 'Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use...</td>\n",
       "      <td>[E09-2008]</td>\n",
       "      <td>[../data/summaries/E09-2008.txt]</td>\n",
       "      <td>[../data/tba/E09-2008.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H05-1115</td>\n",
       "      <td>Using Random Walks for Question-focused Sentence Retrieval</td>\n",
       "      <td>../data/papers/H05-1115.xml</td>\n",
       "      <td>{'0': 'Using Random Walks for Question-focused Sentence Retrieval', '1': 'We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event stories published over time.', '2': 'Annotators generated a list of questions central to understanding each story in our corpus.', '3': 'Because of the dynamic nature of the stories,many questions are time-sensitive (e.g.How many victims have been found?)Judges found sentences providing an answer to each question.', '4': 'To address thesentence retrieval problem, we apply astochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization.', '5': 'Currently, we present a topic-sensitive versionof our method and hy...</td>\n",
       "      <td>[H05-1115]</td>\n",
       "      <td>[../data/summaries/H05-1115.txt]</td>\n",
       "      <td>[../data/tba/H05-1115.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H89-2014</td>\n",
       "      <td>Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging</td>\n",
       "      <td>../data/papers/H89-2014.xml</td>\n",
       "      <td>{'0': 'Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging', '1': 'The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.', '2': 'The model has the advantage that a pre-tagged training corpus is not required.', '3': 'Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.', '4': 'State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.', '5': 'The structure of the state chains is based on both an analysis of errors and linguistic knowledge.', '6': 'Examples show how word dependency ac...</td>\n",
       "      <td>[H89-2014]</td>\n",
       "      <td>[../data/summaries/H89-2014.txt]</td>\n",
       "      <td>[../data/tba/H89-2014.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I05-5011</td>\n",
       "      <td>Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs</td>\n",
       "      <td>../data/papers/I05-5011.xml</td>\n",
       "      <td>{'0': 'Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs', '1': 'Automatic paraphrase discovery is an important but challenging task.', '2': 'We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.', '3': 'We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.', '4': 'The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets.', '5': 'The second stage links sets which involve the same pairs of individual NEs.', '6': 'A total of 13,976 phrases were grouped.', '7': 'The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the li...</td>\n",
       "      <td>[I05-5011]</td>\n",
       "      <td>[../data/summaries/I05-5011.txt]</td>\n",
       "      <td>[../data/tba/I05-5011.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>J01-2004</td>\n",
       "      <td>Probabilistic Top-Down Parsing and Language Modeling</td>\n",
       "      <td>../data/papers/J01-2004.xml</td>\n",
       "      <td>{'0': 'Probabilistic Top-Down Parsing and Language Modeling', '1': 'This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition.', '2': 'The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.', '3': 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', '4': 'A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show t...</td>\n",
       "      <td>[J01-2004_aakansha, J01-2004_swastika, J01-2004_sweta]</td>\n",
       "      <td>[../data/summaries/J01-2004_aakansha.txt, ../data/summaries/J01-2004_swastika.txt, ../data/summaries/J01-2004_sweta.txt]</td>\n",
       "      <td>[../data/tba/J01-2004_aakansha.json, ../data/tba/J01-2004_swastika.json, ../data/tba/J01-2004_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>J96-3004</td>\n",
       "      <td>A Stochastic Finite-State Word-Segmentation Algorithm for Chinese</td>\n",
       "      <td>../data/papers/J96-3004.xml</td>\n",
       "      <td>{'0': 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', '': ' We evaluate the system's performance by comparing its segmentation 'Tudgments\" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.', '1': 'Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TIS).', '2': 'An initial step of any textÂ­ analysis task is the tokenization of the input into words.', '3': 'For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.', '4': 'Thus in an English sentence ...</td>\n",
       "      <td>[J96-3004]</td>\n",
       "      <td>[../data/summaries/J96-3004.txt]</td>\n",
       "      <td>[../data/tba/J96-3004.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>N01-1011</td>\n",
       "      <td>A Decision Tree of Bigrams is an Accurate Predictor of Word Sense</td>\n",
       "      <td>../data/papers/N01-1011.xml</td>\n",
       "      <td>{'0': 'A Decision Tree of Bigrams is an Accurate Predictor of Word Sense', '1': 'This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.', '2': 'This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.', '3': 'It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.', '4': 'Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs.', '5': 'For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined....</td>\n",
       "      <td>[N01-1011]</td>\n",
       "      <td>[../data/summaries/N01-1011.txt]</td>\n",
       "      <td>[../data/tba/N01-1011.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>N04-1038</td>\n",
       "      <td>Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution</td>\n",
       "      <td>../data/papers/N04-1038.xml</td>\n",
       "      <td>{'0': 'Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution', '1': 'We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.', '2': 'BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.', '3': 'These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.', '4': 'BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.', '5': 'Experiments in two domains showed that the contextual role knowledge improved coreference performance, espec...</td>\n",
       "      <td>[N04-1038]</td>\n",
       "      <td>[../data/summaries/N04-1038.txt]</td>\n",
       "      <td>[../data/tba/N04-1038.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>N06-2049</td>\n",
       "      <td>Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation</td>\n",
       "      <td>../data/papers/N06-2049.xml</td>\n",
       "      <td>{'0': 'Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation', '1': 'We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.', '2': 'We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.', '3': 'In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.', '4': 'By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.', '5': 'The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 200...</td>\n",
       "      <td>[N06-2049]</td>\n",
       "      <td>[../data/summaries/N06-2049.txt]</td>\n",
       "      <td>[../data/tba/N06-2049.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P04-1036</td>\n",
       "      <td>Finding Predominant Word Senses in Untagged Text</td>\n",
       "      <td>../data/papers/P04-1036.xml</td>\n",
       "      <td>{'0': 'Finding Predominant Word Senses in Untagged Text', '1': 'word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', '2': 'The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data.', '3': 'Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration.', '4': 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to f...</td>\n",
       "      <td>[P04-1036_sweta, P04-1036_vardha, P04-1036_swastika, P04-1036_aakansha]</td>\n",
       "      <td>[../data/summaries/P04-1036_sweta.txt, ../data/summaries/P04-1036_vardha.txt, ../data/summaries/P04-1036_swastika.txt, ../data/summaries/P04-1036_aakansha.txt]</td>\n",
       "      <td>[../data/tba/P04-1036_sweta.json, ../data/tba/P04-1036_vardha.json, ../data/tba/P04-1036_swastika.json, ../data/tba/P04-1036_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>P05-1004</td>\n",
       "      <td>Supersense Tagging of Unknown Nouns using Semantic Similarity</td>\n",
       "      <td>../data/papers/P05-1004.xml</td>\n",
       "      <td>{'0': 'Supersense Tagging of Unknown Nouns using Semantic Similarity', '1': 'The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.', '2': 'Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.', '3': 'Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.', '4': 'We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.', '5': 'We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semant...</td>\n",
       "      <td>[P05-1004]</td>\n",
       "      <td>[../data/summaries/P05-1004.txt]</td>\n",
       "      <td>[../data/tba/P05-1004.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>P05-1013</td>\n",
       "      <td>Pseudo-Projective Dependency Parsing</td>\n",
       "      <td>../data/papers/P05-1013.xml</td>\n",
       "      <td>{'0': 'Pseudo-Projective Dependency Parsing', '1': 'In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures.', '2': 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', '3': 'Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.', '4': 'This leads to the best reported performance for robust non-projective parsing of Czech.', '5': 'It is sometimes claimed that one of the advantages of dependency gr...</td>\n",
       "      <td>[P05-1013_swastika, P05-1013_aakansha, P05-1013_vardha]</td>\n",
       "      <td>[../data/summaries/P05-1013_swastika.txt, ../data/summaries/P05-1013_aakansha.txt, ../data/summaries/P05-1013_vardha.txt]</td>\n",
       "      <td>[../data/tba/P05-1013_swastika.json, ../data/tba/P05-1013_aakansha.json, ../data/tba/P05-1013_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>P06-2124</td>\n",
       "      <td>BiTAM: Bilingual Topic AdMixture Models forWord Alignment</td>\n",
       "      <td>../data/papers/P06-2124.xml</td>\n",
       "      <td>{'0': 'BiTAM: Bilingual Topic AdMixture Models forWord Alignment', '1': 'We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.', '2': 'Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.', '3': 'Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).', '4': 'These models enable word- alignment process to leverage topical contents of document-pairs.', '5': 'Efficient variational approximation algorithms are designed for inference and parameter estimation.', '6': 'With the inferred latent topics,...</td>\n",
       "      <td>[P06-2124]</td>\n",
       "      <td>[../data/summaries/P06-2124.txt]</td>\n",
       "      <td>[../data/tba/P06-2124.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P08-1028</td>\n",
       "      <td>Vector-based Models of Semantic Composition</td>\n",
       "      <td>../data/papers/P08-1028.xml</td>\n",
       "      <td>{'0': 'Vector-based Models of Semantic Composition', '1': 'This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', '2': 'Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.', '3': 'Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.', '4': 'Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.', '5': 'Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science.', '6': 'The appeal of th...</td>\n",
       "      <td>[P08-1028_swastika, P08-1028_aakansha, P08-1028_sweta]</td>\n",
       "      <td>[../data/summaries/P08-1028_swastika.txt, ../data/summaries/P08-1028_aakansha.txt, ../data/summaries/P08-1028_sweta.txt]</td>\n",
       "      <td>[../data/tba/P08-1028_swastika.json, ../data/tba/P08-1028_aakansha.json, ../data/tba/P08-1028_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P08-1043</td>\n",
       "      <td>A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</td>\n",
       "      <td>../data/papers/P08-1043.xml</td>\n",
       "      <td>{'0': 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', '1': 'Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence.', '2': 'These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.', '3': 'Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.', '4': 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems f...</td>\n",
       "      <td>[P08-1043_sweta, P08-1043_aakansha, P08-1043_swastika]</td>\n",
       "      <td>[../data/summaries/P08-1043_sweta.txt, ../data/summaries/P08-1043_aakansha.txt, ../data/summaries/P08-1043_swastika.txt]</td>\n",
       "      <td>[../data/tba/P08-1043_sweta.json, ../data/tba/P08-1043_aakansha.json, ../data/tba/P08-1043_swastika.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>P08-1102</td>\n",
       "      <td>A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</td>\n",
       "      <td>../data/papers/P08-1102.xml</td>\n",
       "      <td>{'0': 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging', '1': 'We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.', '2': 'With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.', '3': 'Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.', '4': 'On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.', '5': 'Word segment...</td>\n",
       "      <td>[P08-1102_aakansha, P08-1102_sweta, P08-1102_swastika]</td>\n",
       "      <td>[../data/summaries/P08-1102_aakansha.txt, ../data/summaries/P08-1102_sweta.txt, ../data/summaries/P08-1102_swastika.txt]</td>\n",
       "      <td>[../data/tba/P08-1102_aakansha.json, ../data/tba/P08-1102_sweta.json, ../data/tba/P08-1102_swastika.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P11-1060</td>\n",
       "      <td>Learning Dependency-Based Compositional Semantics</td>\n",
       "      <td>../data/papers/P11-1060.xml</td>\n",
       "      <td>{'0': 'Learning Dependency-Based Compositional Semantics', '1': 'Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', '2': 'In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.', '3': 'In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', '4': 'On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms.', '5': 'What is the total population of ...</td>\n",
       "      <td>[P11-1060_swastika, P11-1060_aakansha, P11-1060_sweta]</td>\n",
       "      <td>[../data/summaries/P11-1060_swastika.txt, ../data/summaries/P11-1060_aakansha.txt, ../data/summaries/P11-1060_sweta.txt]</td>\n",
       "      <td>[../data/tba/P11-1060_swastika.json, ../data/tba/P11-1060_aakansha.json, ../data/tba/P11-1060_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>P11-1061</td>\n",
       "      <td>Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</td>\n",
       "      <td>../data/papers/P11-1061.xml</td>\n",
       "      <td>{'0': 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', '1': 'We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.', '2': 'Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.', '3': 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).', '4': 'Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% ov...</td>\n",
       "      <td>[P11-1061_swastika, P11-1061_sweta, P11-1061_aakansha]</td>\n",
       "      <td>[../data/summaries/P11-1061_swastika.txt, ../data/summaries/P11-1061_sweta.txt, ../data/summaries/P11-1061_aakansha.txt]</td>\n",
       "      <td>[../data/tba/P11-1061_swastika.json, ../data/tba/P11-1061_sweta.json, ../data/tba/P11-1061_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>P87-1015</td>\n",
       "      <td>CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</td>\n",
       "      <td>../data/papers/P87-1015.xml</td>\n",
       "      <td>{'0': 'CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*', '1': 'We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.', '2': 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class...</td>\n",
       "      <td>[P87-1015_swastika, P87-1015_sweta, P87-1015_vardha]</td>\n",
       "      <td>[../data/summaries/P87-1015_swastika.txt, ../data/summaries/P87-1015_sweta.txt, ../data/summaries/P87-1015_vardha.txt]</td>\n",
       "      <td>[../data/tba/P87-1015_swastika.json, ../data/tba/P87-1015_sweta.json, ../data/tba/P87-1015_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>P98-1081</td>\n",
       "      <td>Improving Data Driven Wordclass Tagging by System Combination</td>\n",
       "      <td>../data/papers/P98-1081.xml</td>\n",
       "      <td>{'0': 'Improving Data Driven Wordclass Tagging by System Combination', '1': 'In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.', '2': 'We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging.', '3': 'Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.', '4': 'After comparison, their outputs are combined using several voting strategies and second stage classifiers.', '5': 'All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best indi...</td>\n",
       "      <td>[P98-1081]</td>\n",
       "      <td>[../data/summaries/P98-1081.txt]</td>\n",
       "      <td>[../data/tba/P98-1081.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>P98-2143</td>\n",
       "      <td>Robust pronoun resolution with limited knowledge</td>\n",
       "      <td>../data/papers/P98-2143.xml</td>\n",
       "      <td>{'0': 'Robust pronoun resolution with limited knowledge', '1': 'Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.', '2': 'One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.', '3': 'This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.', '4': 'Input is checked against agreement and for a number of antecedent indicators.', '5': 'Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.', '6': 'Evaluation reports a success rate of 89.7% which is better than the sucÂ­ cess rates ...</td>\n",
       "      <td>[P98-2143]</td>\n",
       "      <td>[../data/summaries/P98-2143.txt]</td>\n",
       "      <td>[../data/tba/P98-2143.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>W03-0410</td>\n",
       "      <td>Semi-supervised Verb Class Discovery Using Noisy Features</td>\n",
       "      <td>../data/papers/W03-0410.xml</td>\n",
       "      <td>{'0': 'Semi-supervised Verb Class Discovery Using Noisy Features', '1': 'We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.', '2': 'The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.', '3': 'In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task.', '4': 'We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.', '5': 'We find that the unsupervised method we tried canno...</td>\n",
       "      <td>[W03-0410]</td>\n",
       "      <td>[../data/summaries/W03-0410.txt]</td>\n",
       "      <td>[../data/tba/W03-0410.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>W04-0213</td>\n",
       "      <td>The Potsdam Commentary Corpus</td>\n",
       "      <td>../data/papers/W04-0213.xml</td>\n",
       "      <td>{'0': 'The Potsdam Commentary Corpus', '1': 'A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.', '2': 'The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.', '3': 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', '4': 'Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowle...</td>\n",
       "      <td>[W04-0213]</td>\n",
       "      <td>[../data/summaries/W04-0213.txt]</td>\n",
       "      <td>[../data/tba/W04-0213.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>W06-2932</td>\n",
       "      <td>Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</td>\n",
       "      <td>../data/papers/W06-2932.xml</td>\n",
       "      <td>{'0': 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', '1': 'present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', '2': 'The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.', '3': 'The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.', '4': 'We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.', '5': 'Often in language processing we require a deep syntactic representation of a sentence in or...</td>\n",
       "      <td>[W06-2932_swastika, W06-2932_sweta, W06-2932_vardha]</td>\n",
       "      <td>[../data/summaries/W06-2932_swastika.txt, ../data/summaries/W06-2932_sweta.txt, ../data/summaries/W06-2932_vardha.txt]</td>\n",
       "      <td>[../data/tba/W06-2932_swastika.json, ../data/tba/W06-2932_sweta.json, ../data/tba/W06-2932_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>W06-3114</td>\n",
       "      <td>Manual and Automatic Evaluation of Machine Translation between European Languages</td>\n",
       "      <td>../data/papers/W06-3114.xml</td>\n",
       "      <td>{'0': 'Manual and Automatic Evaluation of Machine Translation between European Languages', '1': 'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (...</td>\n",
       "      <td>[W06-3114_swastika, W06-3114_sweta, W06-3114_aakansha]</td>\n",
       "      <td>[../data/summaries/W06-3114_swastika.txt, ../data/summaries/W06-3114_sweta.txt, ../data/summaries/W06-3114_aakansha.txt]</td>\n",
       "      <td>[../data/tba/W06-3114_swastika.json, ../data/tba/W06-3114_sweta.json, ../data/tba/W06-3114_aakansha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>W08-2222</td>\n",
       "      <td>Wide-Coverage Semantic Analysis with Boxer</td>\n",
       "      <td>../data/papers/W08-2222.xml</td>\n",
       "      <td>{'0': 'Wide-Coverage Semantic Analysis with Boxer', '1': 'Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).', '2': 'Used together with the C&amp;C tools, Boxer reaches more than 95% coverage on newswire texts.', '3': 'The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.', '4': 'The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic.', '5': 'Boxerâs performance on the shared task for comparing semantic represtations was promising.', '6': 'It...</td>\n",
       "      <td>[W08-2222]</td>\n",
       "      <td>[../data/summaries/W08-2222.txt]</td>\n",
       "      <td>[../data/tba/W08-2222.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>W11-2123</td>\n",
       "      <td>KenLM: Faster and Smaller Language Model Queries</td>\n",
       "      <td>../data/papers/W11-2123.xml</td>\n",
       "      <td>{'0': 'KenLM: Faster and Smaller Language Model Queries', '1': 'We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', '2': 'The structure uses linear probing hash tables and is designed for speed.', '3': 'Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', '4': 'Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.', '5': 'This paper describes the several performance techniques used and presen...</td>\n",
       "      <td>[W11-2123_swastika, W11-2123_aakansha, W11-2123_vardha]</td>\n",
       "      <td>[../data/summaries/W11-2123_swastika.txt, ../data/summaries/W11-2123_aakansha.txt, ../data/summaries/W11-2123_vardha.txt]</td>\n",
       "      <td>[../data/tba/W11-2123_swastika.json, ../data/tba/W11-2123_aakansha.json, ../data/tba/W11-2123_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>W95-0104</td>\n",
       "      <td>A Bayesian hybrid method for context-sensitive spelling correction</td>\n",
       "      <td>../data/papers/W95-0104.xml</td>\n",
       "      <td>{'0': 'A Bayesian hybrid method for context-sensitive spelling correction', '1': 'Two classes of methods have been shown to be useful for resolving lexical ambiguity.', '2': 'The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.', '3': 'These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax.', '4': 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.', '5': 'The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence...</td>\n",
       "      <td>[W95-0104]</td>\n",
       "      <td>[../data/summaries/W95-0104.txt]</td>\n",
       "      <td>[../data/tba/W95-0104.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>W99-0613</td>\n",
       "      <td>Unsupervised Models for Named Entity Classification Collins</td>\n",
       "      <td>../data/papers/W99-0613.xml</td>\n",
       "      <td>{'0': 'Unsupervised Models for Named Entity Classification Collins', '1': 'This paper discusses the use of unlabeled examples for the problem of named entity classification.', '2': 'A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &amp;quot;seed&amp;quot; rules.', '3': 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', '4': 'We present two algorithms.', '5': 'The first method uses a similar algorithm to that of (Yarowsky 95), with modifica...</td>\n",
       "      <td>[W99-0613_vardha, W99-0613_aakansha, W99-0613_swastika, W99-0613_sweta]</td>\n",
       "      <td>[../data/summaries/W99-0613_vardha.txt, ../data/summaries/W99-0613_aakansha.txt, ../data/summaries/W99-0613_swastika.txt, ../data/summaries/W99-0613_sweta.txt]</td>\n",
       "      <td>[../data/tba/W99-0613_vardha.json, ../data/tba/W99-0613_aakansha.json, ../data/tba/W99-0613_swastika.json, ../data/tba/W99-0613_sweta.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>W99-0623</td>\n",
       "      <td>Exploiting Diversity in Natural Language Processing: Combining Parsers</td>\n",
       "      <td>../data/papers/W99-0623.xml</td>\n",
       "      <td>{'0': 'Exploiting Diversity in Natural Language Processing: Combining Parsers', '1': 'Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', '2': 'Two general approaches are presented and two combination techniques are described for each approach.', '3': 'Both parametric and non-parametric models are explored.', '4': 'The resulting parsers surpass the best previously published performance results for the Penn Treebank.', '5': 'The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.', '6': 'The machine learning community has been in a similar situation and has studied the combination of multip...</td>\n",
       "      <td>[W99-0623_sweta, W99-0623_swastika, W99-0623_vardha]</td>\n",
       "      <td>[../data/summaries/W99-0623_sweta.txt, ../data/summaries/W99-0623_swastika.txt, ../data/summaries/W99-0623_vardha.txt]</td>\n",
       "      <td>[../data/tba/W99-0623_sweta.json, ../data/tba/W99-0623_swastika.json, ../data/tba/W99-0623_vardha.json]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>X96-1048</td>\n",
       "      <td>OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION</td>\n",
       "      <td>../data/papers/X96-1048.xml</td>\n",
       "      <td>{'0': 'OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION', '1': 'Test abstract', '2': 'The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.', '3': 'Participants were invited to enter their systems in as many as four different task-oriented evaluations.', '4': 'The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.', '5': 'The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years.', '6': 'The evolution and design of the MUC6 evaluation are discussed in the...</td>\n",
       "      <td>[X96-1048]</td>\n",
       "      <td>[../data/summaries/X96-1048.txt]</td>\n",
       "      <td>[../data/tba/X96-1048.json]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paper_id                                                                                                paper_title                   paper_path                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       paper_text                                                              summary_ids                                                                                                                                                    summary_paths                                                                                                                             annotation_paths\n",
       "0   A00-2018                                                                        A Maximum-Entropy-Inspired Parser *  ../data/papers/A00-2018.xml  {'0': 'A Maximum-Entropy-Inspired Parser *', '1': 'We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.', '2': 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', '3': 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', '4': 'We also present some partial results showing the effects of different conditioning information, inc...                     [A00-2018_sweta, A00-2018_akanksha, A00-2018_vardha]                                           [../data/summaries/A00-2018_sweta.txt, ../data/summaries/A00-2018_akanksha.txt, ../data/summaries/A00-2018_vardha.txt]                                      [../data/tba/A00-2018_sweta.json, ../data/tba/A00-2018_akanksha.json, ../data/tba/A00-2018_vardha.json]\n",
       "1   A00-2030                                        A Novel Use of Statistical Parsing to Extract Information from Text  ../data/papers/A00-2030.xml  {'0': 'A Novel Use of Statistical Parsing to Extract Information from Text', '1': 'Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.', '2': 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', '3': 'Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.', '4': 'Yet, relatively few have embedded one of these algorithms in a task.', '5': 'Chi...                     [A00-2030_sweta, A00-2030_aakansha, A00-2030_vardha]                                           [../data/summaries/A00-2030_sweta.txt, ../data/summaries/A00-2030_aakansha.txt, ../data/summaries/A00-2030_vardha.txt]                                      [../data/tba/A00-2030_sweta.json, ../data/tba/A00-2030_aakansha.json, ../data/tba/A00-2030_vardha.json]\n",
       "2   A97-1014                                                         An Annotation Scheme for Free Word Order Languages  ../data/papers/A97-1014.xml  {'0': 'An Annotation Scheme for Free Word Order Languages', '1': 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', '2': 'Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme.', '3': 'The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.', '4': 'The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.', '5': 'In particular, we focus on several methodological issues concerning the annotation of n...                     [A97-1014_vardha, A97-1014_sweta, A97-1014_swastika]                                           [../data/summaries/A97-1014_vardha.txt, ../data/summaries/A97-1014_sweta.txt, ../data/summaries/A97-1014_swastika.txt]                                      [../data/tba/A97-1014_vardha.json, ../data/tba/A97-1014_sweta.json, ../data/tba/A97-1014_swastika.json]\n",
       "3   C00-2123                                    Word Re-ordering and DP-based Search in Statistical Machine Translation  ../data/papers/C00-2123.xml  {'0': 'Word Re-ordering and DP-based Search in Statistical Machine Translation', '1': 'In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).', '2': 'Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃcient search algorithm.', '3': 'A search restriction especially useful for the translation direction from German to English is presented.', '4': 'The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.', '5': 'The goal of machine translation is the translation of a text given in some so...                                                               [C00-2123]                                                                                                                                 [../data/summaries/C00-2123.txt]                                                                                                                  [../data/tba/C00-2123.json]\n",
       "4   C02-1025                              Named Entity Recognition: A Maximum Entropy Approach Using Global Information  ../data/papers/C02-1025.xml  {'0': 'Named Entity Recognition: A Maximum Entropy Approach Using Global Information', '1': 'This paper presents a maximum entropy-based named entity recognizer (NER).', '2': 'It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.', '3': 'Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.', '4': 'In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.', '5': 'Considerable amount of wor...                                                               [C02-1025]                                                                                                                                 [../data/summaries/C02-1025.txt]                                                                                                                  [../data/tba/C02-1025.json]\n",
       "5   C08-1098  Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging  ../data/papers/C08-1098.xml  {'0': 'Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging', '1': 'We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.', '2': 'It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.', '3': 'In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.', '4': 'A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tËN = tË1, ..., tËN ...                                                               [C08-1098]                                                                                                                                 [../data/summaries/C08-1098.txt]                                                                                                                  [../data/tba/C08-1098.json]\n",
       "6   C10-1045                                                Better Arabic Parsing: Baselines, Evaluations, and Analysis  ../data/papers/C10-1045.xml  {'0': 'Better Arabic Parsing: Baselines, Evaluations, and Analysis', '1': 'In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design.', '2': 'First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.', '3': 'Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.', '4': 'Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG.', '5': 'Fourth, we show how to build better models for three different parsers.', '6': 'Finally, we show that in application settings, the absence of gold se...                                                               [C10-1045]                                                                                                                                 [../data/summaries/C10-1045.txt]                                                                                                                  [../data/tba/C10-1045.json]\n",
       "7   C90-2039                                                          Strategic Lazy Incremental Copy Graph Unification  ../data/papers/C90-2039.xml  {'0': 'Strategic Lazy Incremental Copy Graph Unification', '1': 'The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures.', '2': 'One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.', '3': 'The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.', '4': 'The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapp...                                                               [C90-2039]                                                                                                                                 [../data/summaries/C90-2039.txt]                                                                                                                  [../data/tba/C90-2039.json]\n",
       "8   C94-2154    THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES  ../data/papers/C94-2154.xml  {'0': 'THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES', '1': 'in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch.', '2': '!['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation.', '3': 'Unification lbrmMisms ma.y be either un-typed (DCC~s, PATRII, 1,F(;) or typed (npsG).', '4': 'A m~L,ior reason for adding types to ~ forma,lism is to express restrictions on fea.ture cooccurences a.s in (;l's(:: [5] in order to rule out nonexista.nt tyl)es ...                                                               [C94-2154]                                                                                                                                 [../data/summaries/C94-2154.txt]                                                                                                                  [../data/tba/C94-2154.json]\n",
       "9   D09-1092                                                                                   Polylingual Topic Models  ../data/papers/D09-1092.xml  {'0': 'Polylingual Topic Models', '1': 'Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.', '2': 'Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.', '3': 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', '4': 'We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', '5': 'Statistical topic models have emerged as an increasingly useful anal...                     [D09-1092_swastika, D09-1092_vardha, D09-1092_sweta]                                           [../data/summaries/D09-1092_swastika.txt, ../data/summaries/D09-1092_vardha.txt, ../data/summaries/D09-1092_sweta.txt]                                      [../data/tba/D09-1092_swastika.json, ../data/tba/D09-1092_vardha.json, ../data/tba/D09-1092_sweta.json]\n",
       "10  D10-1044                 Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation  ../data/papers/D10-1044.xml  {'0': 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', '1': 'We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', '2': 'This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure.', '3': 'We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.', '4': 'Domain adaptation is a common concern when optimizing empirical NLP applications...                   [D10-1044_aakansha, D10-1044_sweta, D10-1044_swastika]                                         [../data/summaries/D10-1044_aakansha.txt, ../data/summaries/D10-1044_sweta.txt, ../data/summaries/D10-1044_swastika.txt]                                    [../data/tba/D10-1044_aakansha.json, ../data/tba/D10-1044_sweta.json, ../data/tba/D10-1044_swastika.json]\n",
       "11  D10-1083                                                                 Simple Type-Level Unsupervised POS Tagging  ../data/papers/D10-1083.xml  {'0': 'Simple Type-Level Unsupervised POS Tagging', '1': 'Part-of-speech (POS) tag distributions are known to exhibit sparsity â a word is likely to take a single predominant tag in a corpus.', '2': 'Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy.', '3': 'However, in existing systems, this expansion come with a steep increase in model complexity.', '4': 'This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments.', '5': 'In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training.', '6': 'Our experiments consistently demonstrate that this model architectu...                                                               [D10-1083]                                                                                                                                 [../data/summaries/D10-1083.txt]                                                                                                                  [../data/tba/D10-1083.json]\n",
       "12  E03-1005                                                             An Efficient Implementation of a New DOP Model  ../data/papers/E03-1005.xml  {'0': 'An Efficient Implementation of a New DOP Model', '1': 'Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.', '2': 'This paper proposes an integration of the two models which outperforms each of them separately.', '3': 'Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.', '4': 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of p...                   [E03-1005_swastika, E03-1005_sweta, E03-1005_aakansha]                                         [../data/summaries/E03-1005_swastika.txt, ../data/summaries/E03-1005_sweta.txt, ../data/summaries/E03-1005_aakansha.txt]                                    [../data/tba/E03-1005_swastika.json, ../data/tba/E03-1005_sweta.json, ../data/tba/E03-1005_aakansha.json]\n",
       "13  E03-1020                                                                    Discovering Corpus-Specific Word Senses  ../data/papers/E03-1020.xml  {'0': 'Discovering Corpus-Specific Word Senses', '1': 'This paper presents an unsupervised algorithm which automatically discovers word senses from text.', '2': 'The algorithm is based on a graph model representing words and relationships between them.', '3': 'Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.', '4': 'Discrimination against previously extracted sense clusters enables us to discover new senses.', '5': 'We use the same data for both recognising and resolving ambiguity.', '6': 'This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies.', '7': 'Automatic word sense discovery has applications of many k...                                                               [E03-1020]                                                                                                                                 [../data/summaries/E03-1020.txt]                                                                                                                  [../data/tba/E03-1020.json]\n",
       "14  E09-2008                                                                  Foma: a finite-state compiler and library  ../data/papers/E09-2008.xml  {'0': 'Foma: a finite-state compiler and library', '1': 'Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.', '2': 'It has specific support for many natural language processing applications such as producing morphological and phonological analyzers.', '3': 'Foma is largely compatible with the Xerox/PARC finite-state toolkit.', '4': 'It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the âMathematical Operatorsâ Unicode block.', '5': 'Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use...                                                               [E09-2008]                                                                                                                                 [../data/summaries/E09-2008.txt]                                                                                                                  [../data/tba/E09-2008.json]\n",
       "15  H05-1115                                                 Using Random Walks for Question-focused Sentence Retrieval  ../data/papers/H05-1115.xml  {'0': 'Using Random Walks for Question-focused Sentence Retrieval', '1': 'We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event stories published over time.', '2': 'Annotators generated a list of questions central to understanding each story in our corpus.', '3': 'Because of the dynamic nature of the stories,many questions are time-sensitive (e.g.How many victims have been found?)Judges found sentences providing an answer to each question.', '4': 'To address thesentence retrieval problem, we apply astochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization.', '5': 'Currently, we present a topic-sensitive versionof our method and hy...                                                               [H05-1115]                                                                                                                                 [../data/summaries/H05-1115.txt]                                                                                                                  [../data/tba/H05-1115.json]\n",
       "16  H89-2014                                         Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging  ../data/papers/H89-2014.xml  {'0': 'Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging', '1': 'The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.', '2': 'The model has the advantage that a pre-tagged training corpus is not required.', '3': 'Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.', '4': 'State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.', '5': 'The structure of the state chains is based on both an analysis of errors and linguistic knowledge.', '6': 'Examples show how word dependency ac...                                                               [H89-2014]                                                                                                                                 [../data/summaries/H89-2014.txt]                                                                                                                  [../data/tba/H89-2014.json]\n",
       "17  I05-5011                              Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs  ../data/papers/I05-5011.xml  {'0': 'Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs', '1': 'Automatic paraphrase discovery is an important but challenging task.', '2': 'We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.', '3': 'We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.', '4': 'The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets.', '5': 'The second stage links sets which involve the same pairs of individual NEs.', '6': 'A total of 13,976 phrases were grouped.', '7': 'The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the li...                                                               [I05-5011]                                                                                                                                 [../data/summaries/I05-5011.txt]                                                                                                                  [../data/tba/I05-5011.json]\n",
       "18  J01-2004                                                       Probabilistic Top-Down Parsing and Language Modeling  ../data/papers/J01-2004.xml  {'0': 'Probabilistic Top-Down Parsing and Language Modeling', '1': 'This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition.', '2': 'The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.', '3': 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', '4': 'A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show t...                   [J01-2004_aakansha, J01-2004_swastika, J01-2004_sweta]                                         [../data/summaries/J01-2004_aakansha.txt, ../data/summaries/J01-2004_swastika.txt, ../data/summaries/J01-2004_sweta.txt]                                    [../data/tba/J01-2004_aakansha.json, ../data/tba/J01-2004_swastika.json, ../data/tba/J01-2004_sweta.json]\n",
       "19  J96-3004                                          A Stochastic Finite-State Word-Segmentation Algorithm for Chinese  ../data/papers/J96-3004.xml  {'0': 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', '': ' We evaluate the system's performance by comparing its segmentation 'Tudgments\" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.', '1': 'Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TIS).', '2': 'An initial step of any textÂ­ analysis task is the tokenization of the input into words.', '3': 'For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.', '4': 'Thus in an English sentence ...                                                               [J96-3004]                                                                                                                                 [../data/summaries/J96-3004.txt]                                                                                                                  [../data/tba/J96-3004.json]\n",
       "20  N01-1011                                          A Decision Tree of Bigrams is an Accurate Predictor of Word Sense  ../data/papers/N01-1011.xml  {'0': 'A Decision Tree of Bigrams is an Accurate Predictor of Word Sense', '1': 'This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.', '2': 'This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.', '3': 'It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.', '4': 'Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs.', '5': 'For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined....                                                               [N01-1011]                                                                                                                                 [../data/summaries/N01-1011.txt]                                                                                                                  [../data/tba/N01-1011.json]\n",
       "21  N04-1038                              Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution  ../data/papers/N04-1038.xml  {'0': 'Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution', '1': 'We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.', '2': 'BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.', '3': 'These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.', '4': 'BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.', '5': 'Experiments in two domains showed that the contextual role knowledge improved coreference performance, espec...                                                               [N04-1038]                                                                                                                                 [../data/summaries/N04-1038.txt]                                                                                                                  [../data/tba/N04-1038.json]\n",
       "22  N06-2049                           Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation  ../data/papers/N06-2049.xml  {'0': 'Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation', '1': 'We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.', '2': 'We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.', '3': 'In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.', '4': 'By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.', '5': 'The character-based âIOBâ tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 200...                                                               [N06-2049]                                                                                                                                 [../data/summaries/N06-2049.txt]                                                                                                                  [../data/tba/N06-2049.json]\n",
       "23  P04-1036                                                           Finding Predominant Word Senses in Untagged Text  ../data/papers/P04-1036.xml  {'0': 'Finding Predominant Word Senses in Untagged Text', '1': 'word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', '2': 'The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data.', '3': 'Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration.', '4': 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to f...  [P04-1036_sweta, P04-1036_vardha, P04-1036_swastika, P04-1036_aakansha]  [../data/summaries/P04-1036_sweta.txt, ../data/summaries/P04-1036_vardha.txt, ../data/summaries/P04-1036_swastika.txt, ../data/summaries/P04-1036_aakansha.txt]  [../data/tba/P04-1036_sweta.json, ../data/tba/P04-1036_vardha.json, ../data/tba/P04-1036_swastika.json, ../data/tba/P04-1036_aakansha.json]\n",
       "24  P05-1004                                              Supersense Tagging of Unknown Nouns using Semantic Similarity  ../data/papers/P05-1004.xml  {'0': 'Supersense Tagging of Unknown Nouns using Semantic Similarity', '1': 'The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.', '2': 'Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.', '3': 'Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.', '4': 'We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.', '5': 'We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semant...                                                               [P05-1004]                                                                                                                                 [../data/summaries/P05-1004.txt]                                                                                                                  [../data/tba/P05-1004.json]\n",
       "25  P05-1013                                                                       Pseudo-Projective Dependency Parsing  ../data/papers/P05-1013.xml  {'0': 'Pseudo-Projective Dependency Parsing', '1': 'In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures.', '2': 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', '3': 'Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.', '4': 'This leads to the best reported performance for robust non-projective parsing of Czech.', '5': 'It is sometimes claimed that one of the advantages of dependency gr...                  [P05-1013_swastika, P05-1013_aakansha, P05-1013_vardha]                                        [../data/summaries/P05-1013_swastika.txt, ../data/summaries/P05-1013_aakansha.txt, ../data/summaries/P05-1013_vardha.txt]                                   [../data/tba/P05-1013_swastika.json, ../data/tba/P05-1013_aakansha.json, ../data/tba/P05-1013_vardha.json]\n",
       "26  P06-2124                                                  BiTAM: Bilingual Topic AdMixture Models forWord Alignment  ../data/papers/P06-2124.xml  {'0': 'BiTAM: Bilingual Topic AdMixture Models forWord Alignment', '1': 'We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.', '2': 'Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.', '3': 'Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).', '4': 'These models enable word- alignment process to leverage topical contents of document-pairs.', '5': 'Efficient variational approximation algorithms are designed for inference and parameter estimation.', '6': 'With the inferred latent topics,...                                                               [P06-2124]                                                                                                                                 [../data/summaries/P06-2124.txt]                                                                                                                  [../data/tba/P06-2124.json]\n",
       "27  P08-1028                                                                Vector-based Models of Semantic Composition  ../data/papers/P08-1028.xml  {'0': 'Vector-based Models of Semantic Composition', '1': 'This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', '2': 'Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.', '3': 'Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.', '4': 'Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.', '5': 'Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science.', '6': 'The appeal of th...                   [P08-1028_swastika, P08-1028_aakansha, P08-1028_sweta]                                         [../data/summaries/P08-1028_swastika.txt, ../data/summaries/P08-1028_aakansha.txt, ../data/summaries/P08-1028_sweta.txt]                                    [../data/tba/P08-1028_swastika.json, ../data/tba/P08-1028_aakansha.json, ../data/tba/P08-1028_sweta.json]\n",
       "28  P08-1043                       A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing  ../data/papers/P08-1043.xml  {'0': 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', '1': 'Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence.', '2': 'These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.', '3': 'Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.', '4': 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems f...                   [P08-1043_sweta, P08-1043_aakansha, P08-1043_swastika]                                         [../data/summaries/P08-1043_sweta.txt, ../data/summaries/P08-1043_aakansha.txt, ../data/summaries/P08-1043_swastika.txt]                                    [../data/tba/P08-1043_sweta.json, ../data/tba/P08-1043_aakansha.json, ../data/tba/P08-1043_swastika.json]\n",
       "29  P08-1102                     A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging  ../data/papers/P08-1102.xml  {'0': 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging', '1': 'We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.', '2': 'With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.', '3': 'Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.', '4': 'On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.', '5': 'Word segment...                   [P08-1102_aakansha, P08-1102_sweta, P08-1102_swastika]                                         [../data/summaries/P08-1102_aakansha.txt, ../data/summaries/P08-1102_sweta.txt, ../data/summaries/P08-1102_swastika.txt]                                    [../data/tba/P08-1102_aakansha.json, ../data/tba/P08-1102_sweta.json, ../data/tba/P08-1102_swastika.json]\n",
       "30  P11-1060                                                          Learning Dependency-Based Compositional Semantics  ../data/papers/P11-1060.xml  {'0': 'Learning Dependency-Based Compositional Semantics', '1': 'Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', '2': 'In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.', '3': 'In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', '4': 'On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms.', '5': 'What is the total population of ...                   [P11-1060_swastika, P11-1060_aakansha, P11-1060_sweta]                                         [../data/summaries/P11-1060_swastika.txt, ../data/summaries/P11-1060_aakansha.txt, ../data/summaries/P11-1060_sweta.txt]                                    [../data/tba/P11-1060_swastika.json, ../data/tba/P11-1060_aakansha.json, ../data/tba/P11-1060_sweta.json]\n",
       "31  P11-1061                                 Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections  ../data/papers/P11-1061.xml  {'0': 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', '1': 'We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.', '2': 'Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.', '3': 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).', '4': 'Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% ov...                   [P11-1061_swastika, P11-1061_sweta, P11-1061_aakansha]                                         [../data/summaries/P11-1061_swastika.txt, ../data/summaries/P11-1061_sweta.txt, ../data/summaries/P11-1061_aakansha.txt]                                    [../data/tba/P11-1061_swastika.json, ../data/tba/P11-1061_sweta.json, ../data/tba/P11-1061_aakansha.json]\n",
       "32  P87-1015                         CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*  ../data/papers/P87-1015.xml  {'0': 'CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*', '1': 'We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.', '2': 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class...                     [P87-1015_swastika, P87-1015_sweta, P87-1015_vardha]                                           [../data/summaries/P87-1015_swastika.txt, ../data/summaries/P87-1015_sweta.txt, ../data/summaries/P87-1015_vardha.txt]                                      [../data/tba/P87-1015_swastika.json, ../data/tba/P87-1015_sweta.json, ../data/tba/P87-1015_vardha.json]\n",
       "33  P98-1081                                              Improving Data Driven Wordclass Tagging by System Combination  ../data/papers/P98-1081.xml  {'0': 'Improving Data Driven Wordclass Tagging by System Combination', '1': 'In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.', '2': 'We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging.', '3': 'Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.', '4': 'After comparison, their outputs are combined using several voting strategies and second stage classifiers.', '5': 'All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best indi...                                                               [P98-1081]                                                                                                                                 [../data/summaries/P98-1081.txt]                                                                                                                  [../data/tba/P98-1081.json]\n",
       "34  P98-2143                                                           Robust pronoun resolution with limited knowledge  ../data/papers/P98-2143.xml  {'0': 'Robust pronoun resolution with limited knowledge', '1': 'Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.', '2': 'One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.', '3': 'This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.', '4': 'Input is checked against agreement and for a number of antecedent indicators.', '5': 'Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.', '6': 'Evaluation reports a success rate of 89.7% which is better than the sucÂ­ cess rates ...                                                               [P98-2143]                                                                                                                                 [../data/summaries/P98-2143.txt]                                                                                                                  [../data/tba/P98-2143.json]\n",
       "35  W03-0410                                                  Semi-supervised Verb Class Discovery Using Noisy Features  ../data/papers/W03-0410.xml  {'0': 'Semi-supervised Verb Class Discovery Using Noisy Features', '1': 'We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.', '2': 'The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.', '3': 'In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task.', '4': 'We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.', '5': 'We find that the unsupervised method we tried canno...                                                               [W03-0410]                                                                                                                                 [../data/summaries/W03-0410.txt]                                                                                                                  [../data/tba/W03-0410.json]\n",
       "36  W04-0213                                                                              The Potsdam Commentary Corpus  ../data/papers/W04-0213.xml  {'0': 'The Potsdam Commentary Corpus', '1': 'A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.', '2': 'The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.', '3': 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', '4': 'Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowle...                                                               [W04-0213]                                                                                                                                 [../data/summaries/W04-0213.txt]                                                                                                                  [../data/tba/W04-0213.json]\n",
       "37  W06-2932                                    Multilingual Dependency Analysis with a Two-Stage Discriminative Parser  ../data/papers/W06-2932.xml  {'0': 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', '1': 'present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', '2': 'The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.', '3': 'The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.', '4': 'We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.', '5': 'Often in language processing we require a deep syntactic representation of a sentence in or...                     [W06-2932_swastika, W06-2932_sweta, W06-2932_vardha]                                           [../data/summaries/W06-2932_swastika.txt, ../data/summaries/W06-2932_sweta.txt, ../data/summaries/W06-2932_vardha.txt]                                      [../data/tba/W06-2932_swastika.json, ../data/tba/W06-2932_sweta.json, ../data/tba/W06-2932_vardha.json]\n",
       "38  W06-3114                          Manual and Automatic Evaluation of Machine Translation between European Languages  ../data/papers/W06-3114.xml  {'0': 'Manual and Automatic Evaluation of Machine Translation between European Languages', '1': 'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (...                   [W06-3114_swastika, W06-3114_sweta, W06-3114_aakansha]                                         [../data/summaries/W06-3114_swastika.txt, ../data/summaries/W06-3114_sweta.txt, ../data/summaries/W06-3114_aakansha.txt]                                    [../data/tba/W06-3114_swastika.json, ../data/tba/W06-3114_sweta.json, ../data/tba/W06-3114_aakansha.json]\n",
       "39  W08-2222                                                                 Wide-Coverage Semantic Analysis with Boxer  ../data/papers/W08-2222.xml  {'0': 'Wide-Coverage Semantic Analysis with Boxer', '1': 'Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).', '2': 'Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.', '3': 'The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.', '4': 'The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic.', '5': 'Boxerâs performance on the shared task for comparing semantic represtations was promising.', '6': 'It...                                                               [W08-2222]                                                                                                                                 [../data/summaries/W08-2222.txt]                                                                                                                  [../data/tba/W08-2222.json]\n",
       "40  W11-2123                                                           KenLM: Faster and Smaller Language Model Queries  ../data/papers/W11-2123.xml  {'0': 'KenLM: Faster and Smaller Language Model Queries', '1': 'We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', '2': 'The structure uses linear probing hash tables and is designed for speed.', '3': 'Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', '4': 'Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.', '5': 'This paper describes the several performance techniques used and presen...                  [W11-2123_swastika, W11-2123_aakansha, W11-2123_vardha]                                        [../data/summaries/W11-2123_swastika.txt, ../data/summaries/W11-2123_aakansha.txt, ../data/summaries/W11-2123_vardha.txt]                                   [../data/tba/W11-2123_swastika.json, ../data/tba/W11-2123_aakansha.json, ../data/tba/W11-2123_vardha.json]\n",
       "41  W95-0104                                         A Bayesian hybrid method for context-sensitive spelling correction  ../data/papers/W95-0104.xml  {'0': 'A Bayesian hybrid method for context-sensitive spelling correction', '1': 'Two classes of methods have been shown to be useful for resolving lexical ambiguity.', '2': 'The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.', '3': 'These methods have complementary coverage: the former captures the lexical \"atmosphere\" (discourse topic, tense, etc.), while the latter captures local syntax.', '4': 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.', '5': 'The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence...                                                               [W95-0104]                                                                                                                                 [../data/summaries/W95-0104.txt]                                                                                                                  [../data/tba/W95-0104.json]\n",
       "42  W99-0613                                                Unsupervised Models for Named Entity Classification Collins  ../data/papers/W99-0613.xml  {'0': 'Unsupervised Models for Named Entity Classification Collins', '1': 'This paper discusses the use of unlabeled examples for the problem of named entity classification.', '2': 'A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', '3': 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', '4': 'We present two algorithms.', '5': 'The first method uses a similar algorithm to that of (Yarowsky 95), with modifica...  [W99-0613_vardha, W99-0613_aakansha, W99-0613_swastika, W99-0613_sweta]  [../data/summaries/W99-0613_vardha.txt, ../data/summaries/W99-0613_aakansha.txt, ../data/summaries/W99-0613_swastika.txt, ../data/summaries/W99-0613_sweta.txt]  [../data/tba/W99-0613_vardha.json, ../data/tba/W99-0613_aakansha.json, ../data/tba/W99-0613_swastika.json, ../data/tba/W99-0613_sweta.json]\n",
       "43  W99-0623                                     Exploiting Diversity in Natural Language Processing: Combining Parsers  ../data/papers/W99-0623.xml  {'0': 'Exploiting Diversity in Natural Language Processing: Combining Parsers', '1': 'Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', '2': 'Two general approaches are presented and two combination techniques are described for each approach.', '3': 'Both parametric and non-parametric models are explored.', '4': 'The resulting parsers surpass the best previously published performance results for the Penn Treebank.', '5': 'The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.', '6': 'The machine learning community has been in a similar situation and has studied the combination of multip...                     [W99-0623_sweta, W99-0623_swastika, W99-0623_vardha]                                           [../data/summaries/W99-0623_sweta.txt, ../data/summaries/W99-0623_swastika.txt, ../data/summaries/W99-0623_vardha.txt]                                      [../data/tba/W99-0623_sweta.json, ../data/tba/W99-0623_swastika.json, ../data/tba/W99-0623_vardha.json]\n",
       "44  X96-1048                                                                OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION  ../data/papers/X96-1048.xml  {'0': 'OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION', '1': 'Test abstract', '2': 'The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.', '3': 'Participants were invited to enter their systems in as many as four different task-oriented evaluations.', '4': 'The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.', '5': 'The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years.', '6': 'The evolution and design of the MUC6 evaluation are discussed in the...                                                               [X96-1048]                                                                                                                                 [../data/summaries/X96-1048.txt]                                                                                                                  [../data/tba/X96-1048.json]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create necessary data folders\n",
    "    Path(\"../data/papers\").mkdir(exist_ok=True, parents=True)\n",
    "    Path(\"../data/summaries\").mkdir(exist_ok=True, parents=True)\n",
    "    Path(\"../data/tba\").mkdir(exist_ok=True, parents=True) # To-be-annotated...\n",
    "    Path(\"../data/annotation\").mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Copy original data, only getting papers with summaries\n",
    "    copy_original_data()\n",
    "\n",
    "    # Extract metadata about papers\n",
    "    data = get_papers()\n",
    "    papers_df = pd.DataFrame(data)\n",
    "\n",
    "    # Tokenize summaries (sentence-level)\n",
    "    papers_df[\"summary_paths\"].apply(lambda s: splitlines_summaries(s))\n",
    "\n",
    "    # Prepare annotation files from summaries\n",
    "    papers_df[\"annotation_paths\"] = papers_df.apply(lambda row: prepare_for_annotation(row), axis=1)\n",
    "    \n",
    "    # Save to file\n",
    "    papers_df.to_pickle(\"../data/papers.pkl\")\n",
    "    papers_df.to_csv(\"../data/papers.csv\", index=False)\n",
    "    \n",
    "    display(papers_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
