[
    {
        "summary_sid": 1,
        "paper_sid": null,
        "summary_text": "This paper is an overview of results of the Sixth Message Understanding Conference (MUC-6) in November.This paper surveys the results of the natural language processing system evaluation on tasks like named entity,coreference,template element and scenario template.\n",
        "match": ""
    },
    {
        "summary_sid": 2,
        "paper_sid": null,
        "summary_text": "Discussion of the results for each task is organized generally under the following topics: Results on task as whole, Results on some aspects of task, Performance on \"walkthrough article.\"\n",
        "match": ""
    },
    {
        "summary_sid": 3,
        "paper_sid": null,
        "summary_text": "The walkthrough article is an article selected from the test set.\n",
        "match": ""
    },
    {
        "summary_sid": 4,
        "paper_sid": null,
        "summary_text": "Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers.Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.\n",
        "match": ""
    },
    {
        "summary_sid": 5,
        "paper_sid": null,
        "summary_text": "The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes.From the 100 test articles, a subset of 30 articles was selected for use as the test set for the Named Entity and Coreference tasks.although Named Entity, Coreference and Template Element are defined as domain-independent tasks,the articles that were used for MUC-6 testing were selected using domain-dependent criteria pertinent to the Scenario Template task.The results of the evaluation give clear evidence of the challenges that have been overcome and the ones that remain along dimensions of both breadth and depth in automated text analysis.a number of named entity systems performed very well for MUC6, well enough to compete with human performance.Performance on TE overall is as high as 80% on the F-measure, with performance on organization objects significantly lower than on person objects.Systems scored approximately 15-25 points lower (F-measure) on ST than on TE.\n",
        "match": ""
    },
    {
        "summary_sid": 6,
        "paper_sid": null,
        "summary_text": "As defined for MUC-6.The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations.\n",
        "match": ""
    }
]