[
    {
        "summary_sid": 1,
        "paper_sid": null,
        "summary_text": "In this paper the author evaluates machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back.\n",
        "match": ""
    },
    {
        "summary_sid": 2,
        "paper_sid": null,
        "summary_text": "Evaluation was done automatically using the BLEU score and manually on fluency and adequacy.\n",
        "match": ""
    },
    {
        "summary_sid": 3,
        "paper_sid": null,
        "summary_text": "Due to many similarly performing systems, the author was not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.\n",
        "match": ""
    },
    {
        "summary_sid": 4,
        "paper_sid": null,
        "summary_text": "The bias of automatic methods in favour of statistical systems seems to be less pronounced on out-of-domain test data.\n",
        "match": ""
    },
    {
        "summary_sid": 5,
        "paper_sid": null,
        "summary_text": "The manual evaluation of scoring translation on a graded scale from 1â5 seems to be very hard to perform.\n",
        "match": ""
    },
    {
        "summary_sid": 6,
        "paper_sid": null,
        "summary_text": "Replacing this with a ranked evaluation seems to be more suitable.\n",
        "match": ""
    },
    {
        "summary_sid": 7,
        "paper_sid": null,
        "summary_text": "Human judges also pointed out difficulties with the evaluation of long sentences.\n",
        "match": ""
    }
]