[
    {
        "source_id": 1,
        "target_id": 170,
        "summary_text": "In this paper the author evaluates machine translation performance for six European language pairs that participated in a shared task: translating French, German, Spanish texts to English and back.",
        "paper_text": "",
        "strategy": "abstracted"
    },
    {
        "source_id": 2,
        "target_id": [35, 68],
        "summary_text": "Evaluation was done automatically using the BLEU score and manually on fluency and adequacy.",
        "paper_text": "",
        "strategy": "abstracted"
    },
    {
        "source_id": 3,
        "target_id": 172,
        "summary_text": "Due to many similarly performing systems, the author was not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.",
        "paper_text": "",
        "strategy": "abstracted"
    },
    {
        "source_id": 4,
        "target_id": 173,
        "summary_text": "The bias of automatic methods in favour of statistical systems seems to be less pronounced on out-of-domain test data.",
        "paper_text": "",
        "strategy": "extracted"
    },
    {
        "source_id": 5,
        "target_id": 174,
        "summary_text": "The manual evaluation of scoring translation on a graded scale from 1â5 seems to be very hard to perform.",
        "paper_text": "",
        "strategy": "extracted"
    },
    {
        "source_id": 6,
        "target_id": 175,
        "summary_text": "Replacing this with a ranked evaluation seems to be more suitable.",
        "paper_text": "",
        "strategy": "extracted"
    },
    {
        "source_id": 7,
        "target_id": 175,
        "summary_text": "Human judges also pointed out difficulties with the evaluation of long sentences.",
        "paper_text": "",
        "strategy": "extracted"
    }
]