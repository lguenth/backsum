[
    {
        "summary_id": "W06-3114_aakansha",
        "paper_id": "W06-3114",
        "source_sid": 1,
        "target_sid": null,
        "source_text": "The paper'Manual and Automatic Evaluation of Machine Translation between European Languages'by Philipp Koehn and Christof Monz talks about a shared task to evaluate machine translation performance.",
        "strategy": null
    },
    {
        "summary_id": "W06-3114_aakansha",
        "paper_id": "W06-3114",
        "source_sid": 2,
        "target_sid": null,
        "source_text": "They assembled various forms of data and resources: a baseline MT system,language models, prepared training and test sets,resulting in actual machine translation output from several state-of-the-art systems and manual evaluations.",
        "strategy": null
    },
    {
        "summary_id": "W06-3114_aakansha",
        "paper_id": "W06-3114",
        "source_sid": 3,
        "target_sid": null,
        "source_text": "Training and testing is based on the Europarl corpus.",
        "strategy": null
    },
    {
        "summary_id": "W06-3114_aakansha",
        "paper_id": "W06-3114",
        "source_sid": 4,
        "target_sid": null,
        "source_text": "For translation they used automatic evaluation and manual evaluation.",
        "strategy": null
    },
    {
        "summary_id": "W06-3114_aakansha",
        "paper_id": "W06-3114",
        "source_sid": 5,
        "target_sid": null,
        "source_text": "For automatic evaluation they used BLEU.They were not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.",
        "strategy": null
    }
]