In our approach elementary structures are associated with their lexical heads.
These structures specify extended domains of locality (as compared to a context-free gram- mar) over which constraints can be stated.
These con- straints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure.
We state the conditions under which context-free based grammars can be lexicalized without changing the lin- guistic structures originally produced.
We argue that even if one extends the domain of locality of CFGs to trees, us- ing only substitution does not givo the freedom to choose the head of each structure.
We show how adjunction al- lows us to lexicalize a CFG freely.
We then show how a lexicalized grammar naturally follows from the extended omain of locality of TAGs and present some of the linguistic advantages ofour approach.
A novel general parsing strategy for lexicalized gram- mars is discussed.
In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect o this set.
The strategy is independent of the linguistic theory adopted and of the underlying rammar formalism.
How- ever, we focus our attention on TAGs.
Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy.
Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated.
The parser is also able to use non-local information to guide the search.
We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach.
*This work is partially supported by ARO grant DAA29-84-9- 007, DARPA grant N0014-85-K0018, NSF grants MCS-82-191169 and DGR-84-10413.
The second author is also partially supported by J.W.
Zellldja grant.
The authors would llke to thank Mitch Marcus for his helpful conunents about this work.
Thanks are also due to Ellen Hays.
**Visiting from University of Paris VII.
1 Lexicalization of grammar for- malisms Most of the current linguistics theories tend to give lexical accounts of several phenomena that used to be consid- ered purely syntactic.
The information put in the lexi- con is therefore increased and complexified (e.g.
lexical rules in LFG, used also by HPSG, or Gross 1984s lexicon- grammar).
But the question of what it means to lexical- ize a grammar is seldom addressed.
The possible conse- quences of this question for parsing are not fully investi- gated.
We present how to lexicalize grammars uch as CFGs in a radical way, while possibly keeping the rules in their full generality.
If one assumes that the input sentence is finite and that it cannot be syntactically infinitely am- biguous, the lexicalization simplifies the task of a parser.
We say that a grammar formalism is lexicalized if it consists of: ?
a finite set of structures to be associated with lexical items, which usually will be heads of these structures, ?
an operation or operations for composing the structures.
1 The finite set of structures define the domain of locality over which constraints are speci- fied and these are local with respect o their lexical heads.
Not every grammar formalism in a given form is in a lexicalized form.
For example, a CFG, in general, will not be in a lexicalized form.
However, by extending its domain of locality, it can be lexicalized.
We require that the lexicalized grammar produces not only the same lan- guage as the original grammar, but also the same struc- tures (or tree set)?
We propose to study the conditions under which such a lexicalization is possible for CFGs and TAGs.
The domain of locality of a CFG can be extended by using a tree rewriting system that only uses substitution.
We state the conditions under which CFGs can be lexlcalized without changing the structures originally produced.
We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to  choose the head of each structure.
We then 1 By lexicalization we mean tlmt in each structure there is a lex- ical item that is realized.
We do not mean just adding features (such as head) and unification equations to the rules of the formalism.
Categorlal grammars are lexicaUzed according to our definition, However, they do not correspond in a simple way to a rtde-based system that could be used for top-down recognition.
57,~ show how adjunction enables one to freely lexicalize a CFG.
2 Lexical izat ion  of  CFGs The domain of locality of CFGs can be easily extended by using a tree rewriting grammar.
This tree rewriting grammar consists of a set of trees that are not restricted to be of depth one (as in CFGs).
It uses only substitution as a combining operation.
Substitution can take place only on non-terminal nodes of the frontier of each tree.
The language is defined to be the set of strings on the frontiers of trees whose roots are labeled by a distinguished symbol S. It is easy to see that the set of languages generated by this tree rewriting grammar is exactly the same set as context-free languages.
If no recarsive chain rules exist, it is formally possible to lexicalize a CFG with this tree rewriting grammar, a Recursive chain rules are disallowed since they introduce unbounded structures with no lexical items attached to them.
Although a CFG can be lexicalized by using trees, it is not possible to choose freely the lexical item that plays the role of the head for each structure.
Consider the following example: S ~ NP  VP VP  "-~ adv VP VP  --+ v NP  --+ n The grammar can be qexicalized as follows: S NP VP / adv VP VP VP NP A I I adv VP v n However, in this lexiealization one is forced to choose adv as the head of the structure given in the first tree.
It is not possible to choose the verb v as the head of this struc- ture.
If one tried to do so, recursion on the substitution of the VP node would be inhibited.
2"his example shows that although it is possible to lexi- calize CFGs, substitution alone does not allow us to freely choose the lexical heads.
Substitution alone forces us to make choices that might not be syntactically and seman- tically justified.
Tree adjoining grammars (TAGs) are also a tree-based system, ltowever, the major composition operation in TAGs is adjoining or adjunct ion.
I t  builds a new tree from an auxiliary tree # and a tree c~ (a is any tree, initial, auxiliary or derived by adjunction).
The resulting tree is called a derived tree.
Let t~ be atree containing a node n labeled by X and let # be an auxiliary tree whose root node is also labeled by X.
Then the adjunction of fl to a at node n results a tree 7 as shown in Figure 1.
Adjunetion enables to factor recursion from local dependencies.
aNote that  a CFG in Greibach normal  form can be lexicallzed trivially.
But since G~eihach normal  form of a given CFG might not generate the same tree set as  the  original grammar,  it cannot be used as a 8.eneral method for lexicaUzation.
(a) (g) A Figure 1: The mechanism of adjunction The previous CFG can be lexicalized by using adjunc- tion as follows: 4 s NP VP NP VP I I A v n adv VP The auxiliary tree rooted by VP can be inserted in the S tree on tile VP node by adjunction.
Using adjunction one is thus able to choose the appropriate l xical item as head.
This example illustrates the fact that a CFG with no recursive chain rules can be lexicalized in TAGs, and that if that is done the head can be freely chosen.
3 TAGs  and  lexical izat ion TAGs are naturally lexicalized because they used an ex- tended domain of locality.
TAGs were first introduced by Joshi, Levy and Takabashi (1975) and Joshi (1985).
For more details on the original definition of TAGs, we refer the reader to Joshi (1985), Kroch and Joshi (1985) or Vijay-Shanker (1987).
It is known that Tree Adjoin- ing Languages (TALs) are mildly context-sensitive.
TALs properly contain context-free languages.
It is also possi- ble to encode a context-free grammar with auxiliary trees using adjunction only.
However, although the languages correspond, the possible ncoding does not directly reflect the original context-free grammar since this encoding uses adjunction.
Although adjunction is more powerful than substitution and could be used to simulate it, in recent linguistic work in TAG (Abeill~ 1988) substitution has been used in ad- dition to adjunction in order to obtain appropriate struc- tural descriptions in certain cases, such as verbs taking two sentential arguments (e.g.
"John equates olving this problem with doing the impossible").
Adding substitution does not change the mathematical properties of TAGs.
We describe very briefly the Tree Adjoining Grammar formalism with adjunction and substitution.
A Tree Adjoining Grammar  is a tree-based system that consists of.three finite sets of trees: I, A and L. The trees in I O A tJ L are called e lementary trees.
The trees in I are called initial trees.
Initial trees rep- resent basic sententiai structures.
They are usually con- sidered as projections of the verb and they take nominal 4We chose v as lexical head of the S tree but  we could have chosen n instead (although it is not motivated).
579 complements.
Initial trees (see the left tree in Figure 2) are rooted in S and their frontier consists of terminal sym- bols (including the empty string) and non-terminal nodes to be substituted for.
The trees in A are called auxi l iary trees.
They can represent constituents which ar e adjuncts to basic struc- tures (adverbial).
They can also represent basic senten- tial structures corresponding to verbs or predicates taking sentential complements.
Auxiliary trees (see right tree in Figure 2) are characterized as follows: ?
internal nodes are labeled by non-terminals; ?
leaf nodes are labeled by terminals or by non-terminal nodes to be substituted except for exactly one node (called the foot node) labeled by a non-terminal on which only adjunction can apply; furthermore the la- bel of the foot node is the same as the label of the root node.
Initial ~:  Auxiliary ~c~: $ ; ; substitution nodes Figure 2: Schematic initial and auxiliaxy trees The trees in L are called lexical trees.
They repre- sent basic categories or constituents which serve as argu- ments, to initial or auxiliary trees.
They are reduced to a pre-terminal node in the case of simple categories or are expanded into tree structures in the case 0f compounds.
Structurally they are characterized the same way as initial trees except hat they are not necessary rooted by S. As noted in Section 2, the major composition operation in TAGs is adjunct ion.
We define subst i tut ion in TAGs to take place on spec- ified nodes on the frontiers of elementary trees.
When a node is marked to be substituted, no adjunction can take place on that node.
Furthermore , substitution is always mandatory.
In case of substitution on a node labeled by S (sentential complement), only trees derived from initial trees (therefore rooted by S) can be substituted.
In all other cases, any tree derived from a lexlcal tree rooted by the same label as the given node can be substituted.
The resulting tree is obtained by replacing the node by the derived tree.
Substitution is illustrated in Figure 3.
We conventionally mark substitution odes by a down ar row (~).
We define the tree set  of a TAG G, T(G) to be the set of all derived trees starting from initial trees in I. Further- more, the str ing language generated by a TAG, ?
:(G), is defined to be the set of all terminal strings of the trees in T(G).
Grammar ules defined by the linguistic theory are not the same as the rules used by the parser--let us refer to them as parser  rules.
A parser rule is defined to be a structure encoding a rule of the grammar (or a set of rules) instantiated bythe parser when it comes to alex- 580 / Figure 3: Mechanism of substitution ical item (considered to yield the rule(s)).
It is thus a unique object.
It is individualized by the lexical item, which is itself individualized by its position in the input string.
The lexical item is directly inserted into the struc- ture corresponding to the parser ule, and such a rule can only occur once.
Lexleal i tems are differentiated by their realization in the input sentence and also their position in the sentence.
Therefore a given rule corresponds to ex- actly one lexical item in the input sentence.
The structures are produced by lexical items which serve as heads.
If a structure has only one terminal, the terminal is the head of the structure; if there are several terminals, the choice of the head is linguistically motivated, e.g.
by the principles of X theory.
S also has to be considered as the projection of a lexical head, usually V. Each lexical item corresponds to as many entries as there are possible category or argument structures.
The category structure is a lexical tree that is not necessarily reduced to a single category.
It corresponds to the maximal projection of a category in the case of simple phrases, to the entire compound, in the case of compound categories.
Category structures can be of two different kinds: ?
lexical trees reduced to a single category: ~ DET NP PP DET N of N$ DET , I I t hell) JR bunch(i ) ?
lexical trees that consist of a phrase: NP NP A I D~ N N I I boy (I) Ma r y (I) The argument  st ructure is not reduced to a list of arguments as the usual subcategorization frames.
It is the syntactic structure constructed with the lexlcal value of the predicate and with all the nodes for its arguments.
The argument structure for a predicate is its maximal struc- ture.
An argument is present in the argumefit structure even if it is optional and its optionality is stated in the structure.
SThe index in "parentheses on a lexical item that produces the structure ncodes the position of the lexical item in the string.
A simple case of a argument structure is a verb with its subcategorized arguments.
For example, the verb saw (at position i) generates the following structures (among others): 6 S 8 NPo$ VP V NPI$ V $ I I saw(l) eaw(i) The left structure corresponds to: 0 Jolt.
1 aaw 2 llary a ( i=  2) and the other to: 0 John  1 saw 2 that  3 Mary  4 le f t  5.
( i - - - -2 ) An argument structure can correspond to either one or a set of syntactic surface structures.
The lexical head will then produce a set of possible trees, one for NP0 saw NP1 and another for whol did  NP 0 see e i ?, for exam- ple.
If one defines principles for building such sets of trees, these principles will correspond to syntactic rules in a derivation-based theory of grammar.
Category and argument structures thus instantiated as the parser scans the input string are combined together in a sentence t~tructure by adjoining or substituting.
As Gross (1984), we consider verbs, nouns, and adjec- tives as predicates yielding sentences.
They can take nomi- nal or sentential arguments.
If the predicate takes nominal arguments it produces an initial tree.
If it takes a senten- tial argument then it produces an auxiliary tree.
Putting arguments into predicates i done by substituting nomi- nal arguments or by adjoining a predicate structure to its sentential argument.
Adjuncts are represented as auxiliary trees rooted by the category of the node they are adjoined to.
They are also produced by a head.
They can be reduced to a basic category or take nominal or sentential arguments intro- duced by substitution.
Example,~ of Adjuncts: S vp A A S VP PP S S A A A S ADV P NP$ SC S~ l I I probablYll) during(i) while(i) 4 Parsing lexicalized grammars If we have a lexicalized grammar, the grammar of the parser can be reduced to a set of structures whose nature depends on the input string and whose size is proportional to the length of the sentence (if we suppose that the num- ber of structures associated with a lexical item is finite).
Since each structure ( rule) corresponds to a token in the ?We put indices on categories to express syntactic roles (0 for subject, 1 for object).
sentence, it can be used only once.
Rules are now differen- tiated by their realization in the sentence.
The number of rules that can be used for a given sentence is bounded and is proportional to the length of the sentence.
Since each rule can be used once, recursion does not lead to the usual non-termination problem.
Once a structure has been cho~ sen for a given token, the other possible structures for the same token do not participate in the parse.
Of course, if the sentence is ambiguous, there may be more than one choice.
If one adopts an off-line parsing algorithm, the parsing problem is reduced to the following two steps: t First produce the set of structures corresponding to each word in the sentence.
This step performs the role of an expanded morphological nalysis (or tagging).
Then put the argument structures into the predicate structures.
This step performs a modified syntactic analysis.
In principle any parsing strategy can be applied to execute the second step, since the number of structures produced js finite and since each of them corresponds to a token in the input string, the search space is finite and termination is guaranteed.
In principle, one can proceed inside out, left to right or in any other way.
Of course, standard parsing algorithm can be used too.
In particular, we can use the top-down parsing strategy without encountering the usual problems due to recursion.
Problems in the prediction step of the Earley parser used for unification-based formalisms no longer exist.
The use of restrictors as proposed by Shieber (1985) is no longer necessary and the difficulties caused by treating subcategorization as a feature is no longer a problem.
By assuming that the number of structures associated with a lexical item is finite, since each structure has a lexi- cal item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous.
Since the trees are produced by the input string, the parser can use information that might be non-local to guide the search.
For example, consider the language gen- erated by the following CFG (example due to Mitch Mar- cus): S ~ A IB A ~ aAlax B -* aBlay This grammar generates the language:{a*x} U {ay}.
In a standard CFG parsing algorithm, As and Bs will be built until the last token in the input (x or y) is recog- nized.
It would require unbounded look-ahead to decide which rule (3 -+ A or S ~ B) to choose.
One can encode the grammar in TAG as follows: S S A B A A A A a x a y Suppose that the heads of the initial trees are respec- tively x and y and that a is the head of both auxiliary 581 trees.
Then, if the elementary trees are built according to the input string, and if a top-down strategy is used, only A or B trees will be built.
An application concerns the parsing of discontinuous constituents.
They are recognized even if there are un- bounded insertions between their components and even if their head is the last element of the string.
In the two-step strategy described here, before the first step is taken, there is no grammar.
After the first step, we have a grammar whose size is proportional to the length of the input string.
The size of the grammar to be taken into consideration i  the analysis of the parsing complexity of grammar formalisms has been reduced to an amount proportional to the length of the input.
Although we have not yet investigated the implication of this approach on some complexity results, we feel that some of them might be improved.
It is possible to express the parsing problem in a de- cidable deduction system on trees (similar to Lambeks deduction system on categories (1958 and 1961)).
The grammar can be thought as a five-tuple (VN, ~, O, S, Lex) where: ?
VN is a finite set of non-terminal symbols, ?
~ is a finite set of alphabet symbols, ?
O is the set of trees constructed with P,* and VN (the elements of Z* having ranked 0).
Lex is the lexicon , i.e.
a function from lexical items to finite subsets of O: P?
--+ 2?(finite).
A sequent is defined to be of the form: Vl,.. -, rn ~ A, where ri E O and A E VN Two inference rules combine two trees of the left hand side to form a new one.
One inference rule corresponds to adjunction of two trees, and the other to substitution of a node in one tree by the other tree.
Once two trees are combined, they are replaced by the resulting tree in the left hand side of the seouent.
This facts takes into account that each tree corresponds to a single lexical item in the input string.
Therefore each tree can be used only once.
Axioms of the system are of the form: v ---+ A where r is a completed tree rooted by A.
The sequent T1," ? "
,Tn "----+ A is said to be provable if the sequent can be reduced (by the inference rules) to an axiom; we write: ~- r l , .
Since there are finitely many ways to combine a finite num- ber of trees with each other, the system is decidable.
The language generated by such system is defined to be: = {a i ,  .
,anl3rl e Lex(al) s. t. ~- r l ,  " , rn  ----+ S} Also, one can state a necessary condition on the correct- ness of a sentence similar to the category count theorem of van Benthem (1985 and 1986).
5 Extending the Earley-type parser for TAGs An Earley-type parser for TAGs has been proposed by Schabes and Joshi (1988a).
It takes as input a TAG and a sentence to be parsed.
It places no restrictions on the grammar.
The algorithm is a bottom-up arser that uses top-down filtering.
It is able to parse constraints on ad- junction, substitution and feature structures for TAGs as defined by Vijay-Shanker (1987) and Vijay-Shanker and Joshi (1988).
It is able to parse directly CFGs and TAGs.
Thus it embeds the essential aspects of PATR-II as defined by Shieber (1984 and 1986).
Its correctness was proven in Sehabes and Joshi (1988b).
The concepts of dotted rule and states have been extended to TAG trees.
The algo- rithm as described by Schabes and Joshi (1988a) manip- ulates states of the form: s = [a, dot, side, pos, l, fl, fi, star, t[, b[, snbst?]
where a is a tree, dot is the address of the dot in the tree, side is the side of the symbol the dot is on (left or right), pos is the position of the dot (above or below), star is an address in a and l, f~, fr, star, t~, b~ are indices of positions in the input string.
The variable subst?
is a boolean that indicates whether the tree has been predicted for substi- tution.
The algorithm uses nine processes: ?
The Scanner  allows lexical items to be recognized.
Move dot  down and Move dot  up perform a tree traversal that allow the parser to scan the input from left to right.
The Left P red ic tor  predicts an adjunetion if it is possible.
Suppose that the auxiliary tree that we left-predicted has been recognized as far as its foot, then the Left Completor  tries to recognize what was pushed under the foot.
Once the subtree pushed under the foot has been rec- ognized, the R ight  P red ic tor  tries to recognize the other half of the auxiliary tree.
If the auxiliary tree has been totally recognized, the Right  Completor  tries to recognize the rest of the tree in which the auxiliary tree has been adjoined.
The Subst i tu t ion  Pred ic tor  performs the same op- erations as Earleys original predictor.
It predicts for substitution (when appropriate) all lexical trees or ini- tial trees that could be substituted.
If the tree that we predicted for substitution has been totally recognized, the Subst i tu t ion  Comple- tor  tries to recognize the rest of the tree in which we predicted a substitution.
The Earley-type parser can be extended to take advan- tage of  the lexicon-based strategy proposed earlier.
Once the input string has been scanned and the corresponding elementary trees have been built, the parser will proceed bottom-up using the top-down filtering from the initial trees that have been built.
In order to take into account that each tree is unique and therefore can be used only once, a new component r is added to the states.
A state is now defined to be: s = [a, dot, side, pos, l, fl, fr, star, t~, b~, subst?, r] r encodes the trees corresponding to the input string that have not yet been used: r ~--- {{"/11, """ , ~lk},"" : ,  { ~ml,"""  , ~mk}} where {7i l ,  " ,7~j} is the set of trees generated by the lexical item a~.
582 The left predictor must be modified so that it predicts only trees that are in the set F of the given state.
As soon as one tree (say 7in) is used, the entire set of trees cor- responding to the same token ({711,"  ,7i j})  cannot be used later on.
Of course, all competitive paths are taken in parallel as in the usual Earley parser.
The way that F is modified by the Left Predictor is illustrated in the following figure: A addedtoSi r=((~ ,...a } ..... {~ ..... ~ ), ..., (v ..... v 1} r=({~ ,...,~ } ........ (~ ..... r }1 n lr il is ml mt 11 lr ml rat Figure 4: Update of F in the Left Predictor The tree 71u is predicted and therefore the trees corre- sponding to the token ai ({ Y / l , -  " ,  "/is}) are removed from 1 a" The scanner must also be slightly modified since the head of the structure is differentiated not only by its lexical value but al,~o by its position in the string.
6 Conclusion In this paper we presented a general parsing strategy based on lexicalized grammar.
We defined the notion of lexi- calization of a grammar.
We showed how a CFG can be lexicalized by using only substitution.
But the use of adjunction permits lexicalization with linguistically mo- tivated structures.
TAGs have been shown to be naturally lexicalized.
Then we gave an overview of the specific lex- icon of TAGs.
The %xicalization ~ of grammar lead us to introduce a two step parsing strategy.
The first step picks up the set of structures corresponding to each word in the sentence.
The second step puts the argument structures into predicate structures.
Therefore, the relationship be- tween the morphological nd syntactic analyses has been modified.
In the first step, structures instead of categories are associated with lexical items.
The strategy has been shown to be able to use non-local information in the in- put string.
Also problems due to recursion are eliminated.
The grammar of the parser has been reduced to a set of structures whose size is proportional to the length Of the input sentence.
Furthermore, the parsing strategy applies to any parsing algorithm; in particular top-down.
It can be formalized into a decidable deduction system that has finite search space for a sentence of finite length.
The Earley-type parser for TAGs has been easily extended to take advantage of this strategy.
R e f e r e n c e s Abeilld, Anne, 1988.
Parsing French with Tree Adjoining Grammar: some Linguistic Accounts.
In Proceeding of the 12 ?
International Conference on Computational Linguistics.
van Benthem, Johns, 1985.
Lambek Calculus.
Manuscript, Filosofisch Instituut, Rijks Universiteit, Groningen.
van Benthem, Johan, 1986.
Essays on Logical Semantics, Chapter 7, pages 123-150.
D. Reidel Publishing Company.
Gross, Manriee, 1984.
Lexicon-Grammar nd the Syntactic Analysis of French.
In Proceeding of the 10 th International Conference on Computational Linguistics.
Joshi, Aravind K., 1985.
How Much Context-Sensitivlty is Nec- essary for Characterizing Structural Descriptions--Tree Ad- joining Grammars.
In Dowry, D.; Karttunen, L.; and Zwicky, A.
(editors), Natural Language Processing--Theoretical, Com- putational and Psychological Perspectives.
Cambridge Univer- sity Press, New York.
Originally presented in 1983.
Joshi, A. K.; Levy, L. S.; and Ta~ahashi, M, 1975.
Tree Ad- junct Grammars.
Kroch, A. and Joshi, A. K., 1985.
Linguistic Relevance of Tree Adjoining Grammars.
Technical Report MS-CIS-85-18, Department ofComputer and Information Science, University of Pennsylvania.
Lambek, Joachim, 1958.
The Mathematics of Sentence Struc- ture.
American Mathematical Monthly 65:154-170.
Lambek, Joachim, 1961.
On the Calculus of Syntactic Types.
In Proceedings of the Symposium on Applied Mathematics, pages 166-178.
Schabes, Yves and Joshi, Aravind K., 1988 (a).
An Earley- Type Parsing Algorithm for Tree Adjoining Grammars.
In 26 th Meeting of the Association for Computational Linguistics.
Schabes, Yves and Joshi, Aravind K., ]988 (b).
An Earley- type Parser for Tree Adjoining Grammars.
Technical Report, Department of Computer and Information Science, University of Pennsylvania.
Shieber, Stuart M., 1984.
The Design of a Computer Language for Linguistic Information.
In 22 ~d Meeting of the Association for Computational Linguistics, pages 362-366.
Shieber, Stuart M., 1985.
Using Restriction to Extend Pars- ing Algorithms for Complex-feature-based Formalisms.
In 23 rd Meeting of the Association for Computational Linguistics, pages 82-93.
Shieber, Stuart M., 1986.
An Introduction to Unification-Based Approaches to Grammar.
Center for the Study of Language and Information, Stanford, CA.
Vijay-Shanker, K., 1987.
A Study of Tree Adjoining Gram- mars.
PhD thesis, Department of Computer and Information Science, University of Pennsylvania.
Vijay-Shanker, K. and Joshi, A.K., 1988.
Feature Structure Baaed Tree Adjoining Grammars.
In Proceedings of the 12 th International Conference on Computational Linguistics.
