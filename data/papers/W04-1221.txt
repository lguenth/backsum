As the wealth of biomedical knowledge in the form of literature increases, there is a rising need for effective natural language processing tools to assist in organizing, curating, and retrieving this information.
To that end, named entity recognition (the task of identifying words and phrases in free text that belong to certain classes of interest) is an important first step for many of these larger information management goals.
In recent years, much attention has been focused on the problem of recognizing gene and protein mentions in biomedical abstracts.
This paper presents a framework for simultaneously recognizing occurrences of PROTEIN, DNA, RNA, CELL-LINE, and CELL-TYPE entity classes using Conditional Random Fields with a variety of traditional and novel features.
I show that this approach can achieve an overall F1 measure around 70, which seems to be the current state of the art.
The system described here was developed as part of the BioNLP/NLPBA 2004 shared task.
Experiments were conducted on a training and evaluation set provided by the task organizers.
Biomedical named entity recognition can be thought of as a sequence segmentation problem: each word is a token in a sequence to be assigned a label (e.g.
PROTEIN, DNA, RNA, CELL-LINE, CELL-TYPE, or OTHER1).
Conditional Random Fields (CRFs) are undirected statistical graphical models, a special case of which is a linear chain that corresponds to a conditionally trained finite-state machine.
Such models are well suited to sequence analysis, and CRFs in 'More accurately, the data is in IOB format.
B-DNA labels the first word of a DNA mention, I-DNA labels all subsequent words (likewise for other entities), and O labels non-entities.
For simplicity, this paper only refers to the entities, not all the IOB label variants. particular have been shown to be useful in partof-speech tagging (Lafferty et al., 2001), shallow parsing (Sha and Pereira, 2003), and named entity recognition for newswire data (McCallum and Li, 2003).
They have also just recently been applied to the more limited task of finding gene and protein mentions (McDonald and Pereira, 2004), with promising early results.
Let o = (o1, o2, ... , on) be an sequence of observed words of length n. Let 5 be a set of states in a finite state machine, each corresponding to a label l E L (e.g.
PROTEIN, DNA, etc.).
Let s = (s1, s2, ... , sn) be the sequence of states in 5 that correspond to the labels assigned to words in the input sequence o. Linearchain CRFs define the conditional probability of a state sequence given an input sequence to be: where Zo is a normalization factor of all state sequences, fj(si_1, si, o, i) is one of m functions that describes a feature, and λj is a learned weight for each such feature function.
This paper considers the case of CRFs that use a firstorder Markov independence assumption with binary feature functions.
For example, a feature may have a value of 0 in most cases, but given the text “the ATPase” it has the value 1 along the transition where si_1 corresponds to a state with the label OTHER, si corresponds to a state with the label PROTEIN, and fj is the feature function WORD=ATPase E o at position i in the sequence.
Other feature functions that could have the value 1 along this transition are CAPITALIZED, MIXEDCASE, and SUFFIX=ase.
Intuitively, the learned feature weight λj for each feature fj should be positive for features that are correlated with the target label, negative for features that are anti-correlated with the label, and near zero for relatively uninformative features.
These weights are set to maximize the conditional log likelihood of labeled sequences in a training set D = f(o, l)(1), ... , (o, l)(n)�: When the training state sequences are fully labeled and unambiguous, the objective function is convex, thus the model is guaranteed to find the optimal weight settings in terms of LL(D).
Once these settings are found, the labeling for an new, unlabeled sequence can be done using a modified Viterbi algorithm.
CRFs are presented in more complete detail by Lafferty et al. (2001).
These experiments use the MALLET implementation of CRFs (McCallum, 2002), which uses a quasi-Newton method called L-BFGS to find these feature weights efficiently.
One property that makes feature based statistical models like CRFs so attractive is that they reduce the problem to finding an appropriate feature set.
This section outlines the two main types of features used in these experiments.
The simplest and most obvious feature set is the vocabulary from the training data.
Generalizations over how these words appear (e.g. capitalization, affixes, etc.) are also important.
The present model includes training vocabulary, 17 orthographic features based on regular expressions (e.g.
ALPHANUMERIC, HASDASH, ROMANNUMERAL) as well as prefixes and suffixes in the character length range [3,5].
Words are also assigned a generalized “word class” similar to Collins (2002), which replaces capital letters with ‘A’, lowercase letters with ‘a’, digits with ‘0’, and all other characters with ‘ ’.
There is a similar “brief word class” feature which collapses consecutive identical characters into one.
Thus the words “IL5” and “SH3” would both be given the features WC=AA0 and BWC=A0, while “F-actin” and “T-cells” would both be assigned WC=A aaaaa and BWC=A a.
To model local context simply, neighboring words in the window [-1,1] are also added as features.
For instance, the middle token in the sequence “human UDG promoter” would have features WORD=UDG, NEIGHBOR=human and NEIGHBOR=promoter.
In addition to orthography, the model could also benefit from generalized semantic word groups.
If training sequences contain “PML/RAR alpha,” “beta 2-M,” and “kappa B-specific DNA binding protein” all labeled with PROTEIN, the model might learn that the words “alpha,” “beta,” and “kappa” are indicative of proteins, but cannot capture the fact that they are all semantically related because they are Greek letters.
Similarly, words with the feature WC=Aaa are often part of protein names, such as “Rab,” “Alu,” and “Gag.” But the model may have a difficult time setting the weights for this feature when confronted with words like “Phe,” “Arg,” and “Cys,” which are amino acid abbreviations and not often labeled as part of a protein name.
This sort of semantic domain knowledge can be provided in the form of lexicons.
I prepared a total of 17 such lexicons, which include 7 that were entered by hand (Greek letters, amino acids, chemical elements, known viruses, plus abbreviations of all these), and 4 corresponding to genes, chromosome locations, proteins, and cell lines, drawn from online public databases (Cancer GeneticsWeb,2 BBID,3 SwissProt,4 and the Cell Line Database5).
Feature functions for the lexicons are set to 1 if they match words in the input sequence exactly.
For lexicon entries that are multi-word, all words are required to match in the input sequence.
Since no suitable database of terms for the CELL-TYPE class was found online, a lexicon was constructed by utilizing Google Sets,6 an online tool which takes a few seed examples and leverages Google’s web index to return other terms that appear in similar formatting and context as the seeds on web pages across the Internet.
Several examples from the training data (e.g.
“lymphocyte” and “neutrophil”) were used as seeds and new cell types (e.g.
“chondroblast,” which doesn’t even occur in the training data), were returned.
The process was repeated until the lexicon grew to roughly 50 entries, though it could probably be more complete.
With all this information at the model’s disposal, it can still be difficult to properly disambiguate between these entities.
For example, the acronym “EPC” appears in these static lexicons both as a protein (“eosinophil cationic protein” [sic]) and as a cell line (“epithelioma papulosum cyprini”).
Furthermore, a single word like “transcript” is sometimes all that disambiguates between RNA and DNA mentions (e.g.
“BMLF1 transcript”).
The CRF can learn weights for these individual words, but it may help to build general, dynamic keyword lexicons that are associated with each label to assist in disambiguating between similar classes (and perhaps boost performance on low-frequency labels, such as RNA and CELL-LINE, for which training data are sparse).
These keyword lexicons are generated automatically as follows.
All of the labeled terms are extracted from the training set and separated into five lists (one for each entity class).
Stop words, Greek letters, and digits are filtered, and remaining words are tallied for raw frequency counts under each entity class label.
These frequencies are then subjected to a x2 test, where the null hypothesis is that a word’s frequency is the same for a given entity as it is for any other entity of interest (i.e.
PROTEIN vs. DNA + RNA + CELL-LINE + CELL-TYPE, such that there is only one degree of freedom).
All words for which the null hypothesis is rejected with a p-value < 0.005 are added to the keyword lexicon for its majority class.
Some example keywords are listed in table 1.
Two experiments were completed in the time allotted: one CRF model using only the orthographic features described in section 3.1, and a second system using all the semantic lexicons from 3.2 as well.
Detailed results are presented in table 2.
The orthographic model achieves an overall F1 measure of 69.8 on the evaluation set (88.9 on the training set), converging after 230 training iterations and approximately 18 hours of computation.
The complete model, however, only reached an overall F1 of 69.5 on the evaluation set (86.7 on the training set), converging after 152 iterations in approximately 9 hours.
The deleterious effect of the semantic lexicons is surprising and puzzling.7 However, even though semantic lexicons slightly decrease overall performance, it is worthwhile to note that adding lexicons actually improves both recall and precision for the RNA and CELL-LINE entities.
These happen to be the two lowest frequency class labels in the data, together comprising less than 10% of the mentions in either the training or evaluation set.
Error analysis shows that several of the orthographic model’s false negatives for these entities are of the form “messenger accumulation” (RNA) or “nonadherent culture” (CELL-LINE).
It may be that keyword lexicons contributed to the model identifying these low frequency terms more accurately.
Also of note is that, in both experiments, the CRF framework achieves somewhat comparable performance across all entities.
In a previous attempt to use a Hidden Markov Model to simultaneously recognize multiple biomedical entities (Collier et al., 2000), HMM performance for a particular entity seemed more or less proportional to its frequency in the data.
The advantage of the CRF here may be due to the fact that HMMs are generative models trained to learn the joint probability P(o, l) — where data for l may be sparse — and use Bayes rule to predict the best label.
CRFs are discriminative models trained to maximize P(l|o) directly.
In short, I have presented in detail a framework for recognizing multiple entity classes in biomedical abstracts with Conditional Random Fields.
I have shown that a CRF-based model with only simple orthographic features can achieve performance near the current state of the art, while using semantic lexicons (as presented here) do not positively affect performance.$ While the system presented here shows promise, there is still much to be explored.
Richer syntactic information such as shallow parsing may be useful.
The method introduced in section 3.2 to generate semantic keywords can also be adapted to generate features for entityspecific morphology (e.g. affixes) and context, both linearly (e.g. neighboring words) and hierarchically (e.g. from a parse).
Most interesting, though, might be to investigate why the lexicons do not generally help.
One explanation is simply an issue of tokenization.
While one abstract refers to “IL12,” others may write “IL-12” or “IL 12.” Similarly, the generalization of entities to groups (e.g.
“x antibody” vs. “x antibodies”) can cause problems for these rigid lexicons that require exact matching.
Enumerating all such variants for every entry in a lexicon is absurd.
Perhaps relaxing the matching criteria and standardizing tokenization for both the input and lexicons will improve their utility.
I would like to thank my advisor Mark Craven for his advice and guidance, as well as Andrew McCallum and Aron Culotta for answering my questions about the MALLET system.
This work is supported by NLM training grant 5T15LM007359-02 and NIH grant R01 LM07050-01.
