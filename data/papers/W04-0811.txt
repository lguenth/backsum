merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &quot;Without U&quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively.
System Name Email Contact autoPS dianam©sussex.ac.uk autoPSNVs dianam©sussex.ac.uk clr04-aw ken©clres.com DFA-Unsup-AW david©lsi.uned.es DLSI-UA-Nosu montoyo©dlsi.ua.es GAMBL-AW bart.decadt©ua.ac.be IR ST-DDD-00 strappaKfitc.it IRST-DDD-09 strappaKfitc.it IRST-DDD-LSI strappaKfitc.it Koc University dyuret©ku.edu.tr KUNLP-Eng-All hcseoKM1p.korea.ac.kr LCCaw parker©languagecomputer.com Meaning lluism©lsi.upc.es Meaning simple lluism©lsi.upc.es merl.systeml bhiksha©merl.com merl.system2 bhiksha©merl.com merl.system3 bhiksha©merl.com R2D2: EAW montoyo©dlsi.ua.es SenseLearner rada©cs.unt.edu UJAEN mgarcia©ujaen.es USussex-Prob3 Judita.Preiss©cl.cam.ac.uk USussex-Prob4 Judita.Preiss©cl.cam.ac.uk USussex-Prob5 Judita.Preiss©cl.cam.ac.uk upv-shmm-eaw amolina©dsic.upv.es upv-CIAOSENSO amolina©dsic.upv.es upv-CIAOSENS02 amolina©dsic.upv.es Table 3: email contact for each system; sorted alphabetically. the unsupervised systems (Palmer et al., 2001).
In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline.
While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range.
This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses — resources more suitable for the task at hand.
the glosses.
The glosses do not themselves make the sense distinctions explicit.
In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
26 systems were submitted by a total of 16 teams.
The system names, along with email contacts are listed in table 3.
Two sets of scores were computed for each system.
For the first set of scores (&quot;With U&quot;), we assumed an answer of U (untaggable) whenever the system failed to provide a sense.
Thus the instance would be scored as correct if the answer key also marked it as U, and incorrect otherwise.
For the second set of scores (&quot;Without U&quot;), we simply skipped every instance where the system did not provide a sense.
Thus precision was not affected by those instances, but recall was lowered.
Even though any given team may have intended their results to be interpreted one way or the other, we have included both sets of scores for comparative purposes.
Table 1 shows the system performance under the first interpretation of the results (&quot;With U&quot;).
The average precision and recall is 52.2%.
Table 2 shows the system performance under the second interpretation of the results (&quot;Without U&quot;).
The average precision is 57.4% and 51.9% is the average recall.
Since comprehensive groupings of the WordNet senses do not yet exist, all results given are the result of fine-grained scoring.
Although we did not compute a baseline score, we received several baseline figures from our participants.
Deniz Yuret, of Koc University, computed a baseline of 60.9% precision and recall by using the first WordNet entry for the given word and part-of-speech.
Bart Decadt, of the University of Antwerp and submitter of the GAMBL-AW system, provided a baseline of 62.4% using the same method (the 1.5% difference is most likely explained by how well the baseline systems dealt with multi-word constructions and hyphenated words).
As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).
In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
The greatest difference between these results and those of the SENSEVAL-2 English all-words task is that a greater number of systems have now achieved scores at or above the baseline.
While this result is encouraging, it seems that the best systems have a hit a wall in the 6570% range.
This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses — resources more suitable for the task at hand.
