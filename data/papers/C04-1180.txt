This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.
Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.
We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.
The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999),Charniak (2000)) have led to their use in a num ber of NLP applications, such as question-answering(Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplifica tion (Carroll et al, 1999), and a linguist?s search engine (Resnik and Elkiss, 2003).
Such parsers typically return phrase-structure trees in the styleof the Penn Treebank, but without traces and co indexation.
However, the usefulness of this outputis limited, since the underlying meaning (as repre sented in a predicate-argument structure or logical form) is difficult to reconstruct from such skeletal parse trees.In this paper we demonstrate how a widecoverage statistical parser using Combinatory Categorial Grammar (CCG) can be used to generate semantic representations.
There are a number of ad vantages to using CCG for this task.
First, CCG provides ?surface compositional?
analysis of certainsyntactic phenomena such as coordination and ex traction, allowing the logical form to be obtained for such cases in a straightforward way.
Second, CCG isa lexicalised grammar, and only uses a small num ber of semantically transparent combinatory rules tocombine CCG categories.
Hence providing a compositional semantics for CCG simply amounts to assigning semantic representations to the lexical en tries and interpreting the combinatory rules.
Andthird, there exist highly accurate, efficient and ro bust CCG parsers which can be used directly for this task (Clark and Curran, 2004b; Hockenmaier, 2003).The existing CCG parsers deliver predicate argu ment structures, but not semantic representations that can be used for inference.
The present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use invarious NLP applications that require semantic in terpretation.We show how to construct first-order represen tations from CCG derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen WSJ text.
The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).
The use of the ?-calculusis integral to our method.
However, first-order rep resentations are simply used as a proof-of-concept; we could have used DRSs (Kamp and Reyle, 1993)or some other representation more tailored to the ap plication in hand.There is some existing work with a similar motivation to ours.
Briscoe and Carroll (2002) gen erate underspecified semantic representations fromtheir robust parser.
Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).
However, there is a key difference between these approaches and ours.
In our approach the creation of the semantic representations forms a completely It could cost taxpayers 15 million to install and residents 1 million a year to maintain NP 
