In recent years, there is a phenomenal growth in the amount of online text material available from the greatest information repository known as the World Wide Web.
Various traditional information retrieval(IR) techniques combined with natural language processing(NLP) techniques have been re-targeted to enable efficient access of the WWW—search engines, indexing, relevance feedback, query term and keyword weighting, document analysis, document classification, etc.
Most of these techniques aim at efficient online search for information already on the Web.
Meanwhile, the corpus linguistic community regards the WWW as a vast potential of corpus resources.
It is now possible to download a large amount of texts with automatic tools when one needs to compute, for example, a list of synonyms; or download domain-specific monolingual texts by specifying a keyword to the search engine, and then use this text to extract domain-specific terms.
It remains to be seen how we can also make use of the multilingual texts as NLP resources.
In the years since the appearance of the first papers on using statistical models for bilingual lexicon compilation and machine translation(Brown et al., 1993; Brown et al., 1991; Gale and Church, 1993; Church, 1993; Simard et al., 1992), large amount of human effort and time has been invested in collecting parallel corpora of translated texts.
Our goal is to alleviate this effort and enlarge the scope of corpus resources by looking into monolingual, comparable texts.
This type of texts are known as nonparallel corpora.
Such nonparallel, monolingual texts should be much more prevalent than parallel texts.
However, previous attempts at using nonparallel corpora for terminology translation were constrained by the inadequate availability of same-domain, comparable texts in electronic form.
The type of nonparallel texts obtained from the LDC or university libraries were often restricted, and were usually out-of-date as soon as they became available.
For new word translation, the timeliness of corpus resources is a prerequisite, so is the continuous and automatic availability of nonparallel, comparable texts in electronic form.
Data collection effort should not inhibit the actual translation effort.
Fortunately, nowadays the World Wide Web provides us with a daily increase of fresh, up-to-date multilingual material, together with the archived versions, all easily downloadable by software tools running in the background.
It is possible to specify the URL of the online site of a newspaper, and the start and end dates, and automatically download all the daily newspaper materials between those dates.
In this paper, we describe a new method which combines IR and NLP techniques to extract new word translation from automatically downloaded English-Chinese nonparallel newspaper texts.
To improve the performance of a machine translation system, it is often necessary to update its bilingual lexicon, either by human lexicographers or statistical methods using large corpora.
Up until recently, statistical bilingual lexicon compilation relies largely on parallel corpora.
This is an undesirable constraint at times.
In using a broad-coverage English-Chinese MT system to translate some text recently, we discovered that it is unable to translate Mel! liougan which occurs very frequently in the text.
Other words which the system cannot find in its 20,000-entry lexicon include proper names such as the Taiwanese president Lee Teng-Hui, and the Hong Kong Chief Executive Tung CheeHwa.
To our disappointment, we cannot locate any parallel texts which include such words since they only start to appear frequently in recent months.
A quick search on the Web turned up archives of multiple local newspapers in English and Chinese.
Our challenge is to find the translation of 1.,133 I liougan and other words from this online nonparallel, comparable corpus of newspaper materials.
We choose to use issues of the English newspaper Hong Kong Standard and the Chinese newspaper Mingpao, from Dec.12,97 to Dec.31,97, as our corpus.
The English text contains about 3 Mb of text whereas the Chinese text contains 8.8 Mb of 2 byte character texts.
So both texts are comparable in size.
Since they are both local mainstream newspapers, it is reasonable to assume that their contents are comparable as well.
Unlike in parallel texts, the position of a word in a text does not give us information about its translation in the other language.
(Rapp, 1995; Fung and McKeown, 1997) suggest that a content word is closely associated with some words in its context.
As a tutorial example, we postulate that the words which appear in the context ofMet, Iliougan should be similar to the words appearing in the context of its English translation, flu.
We can form a vector space model of a word in terms of its context word indices, similar to the vector space model of a text in terms of its constituent word indices (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korfhage, 1995; Jones, 1979).
The value of the i-th dimension of a word vector W is f if the i-th word in the lexicon appears f times in the same sentences as W. Left columns in Table 1 and Table 2 show the list of content words which appear most frequently in the context of flu and Africa respectively.
The right column shows those which occur most frequently in the context of bv,g.
We can see that the context of At is more similar to that of flu than to that of Africa.
So the first clue to the similarity between a word and its translation number of common words in their contexts.
In a bilingual corpus, the &quot;common word&quot; is actually a bilingual word pair.
We use the lexicon of the MT system to &quot;bridge&quot; all bilingual word pairs in the corpora.
These word pairs are used as seed words.
We found that the contexts of flu and 'MI Iliougan share 233 &quot;common&quot; context words, whereas the contexts of Africa and W„Iff Iliougan share only 121 common words, even though the context of flu has 491 unique words and the context of Africa has 328 words.
In the vector space model, W[flu] and W[liougan] has 233 overlapping dimensions, whereas there are 121 overlapping dimensions between W[flu] and W [Africa].
The flu example illustrates that the actual ranking of the context word frequencies provides a second clue to the similarity between a bilingual word pair.
For example, virus ranks very high for both flu and ME Iliougan and is a strong &quot;bridge&quot; between this bilingual word pair.
This leads us to use the term frequency(TF) measure.
The TF of a context word is defined as the frequency of the word in the context of W. (e.g.
TF of virus in flu is 26, in MM. is 147).
However, the TF of a word is not independent of its general usage frequency.
In an extreme case, the function word the appears most frequently in English texts and would have the highest TF in the context of any W. In our HKStandard/Mingpao corpus, Hong Kong is the most frequent content word which appears everywhere.
So in the flu example, we would like to reduce the significance of Hong Kong's TF while keeping that of virus.
A common way to account for this difference is by using the inverse document frequency(IDF).
Among the variants of IDF, we choose the following representation from (Jones, 1979):
Next, a ranking algorithm is needed to match the unknown word vectors to their counterparts in the other language.
A ranking algorithm selects the best target language candidate for a source language word according to direct comparison of some similarity measures (Frakes and Baeza-Yates, 1992).
We modify the similarity measure proposed by (Salton and Buckley, 1988) into the following SO: where maxn = ni the maximum frequency of any word in the corpus the total number of occurrences of word i in the corpus SO(We, We) = where wic = wie = (wieX wie) VELiwic2 x ELiwie2 TFic TFie The IDF of virus is 1.81 and that of Hong Kong is 1.23 in the English text.
The IDF of WS is 1.92 and that of Hong Kong is 0.83 in Chinese.
So in both cases, virus is a stronger &quot;bridge&quot; for biglliougan than Hong Kong.
Hence, for every context seed word i, we assign a word weighting factor (Salton and Buckley, 1988) w = TFiw x IDFi where TFiw is the TF of word i in the context of word W. The updated vector space model of word W has wi in its i-th dimension.
The ranking of the 20 words in the contexts of W,,,f3 Iliougan is rearranged by this weighting factor as shown in Table3.
Variants of similarity measures such as the above have been used extensively in the IR community (Frakes and Baeza-Yates, 1992).
They are mostly based on the Cosine Measure of two vectors.
For different tasks, the weighting factor might vary.
For example, if we add the IDF into the weighting factor, we get the following measure Si: where wic = TFic x IDF, wie = TFie x IDFi In addition, the Dice and Jaccard coefficients are also suitable similarity measures for document comparison (Frakes and Baeza-Yates, 1992).
We also implement the Dice coefficient into similarity measure S2: where wic = TFie x IDFi Si is often used in comparing a short query with a document text, whereas S2 is used in comparing two document texts.
Reasoning that our objective falls somewhere in between—we are comparing segments of a document, we also multiply the above two measures into a third similarity measure S3.
In using bilingual seed words such asN,Et /virus as &quot;bridges&quot; for terminology translation, the quality of the bilingual seed lexicon naturally affects the system output.
In the case of European language pairs such as French-English, we can envision using words sharing common cognates as these &quot;bridges&quot;.
Most importantly, we can assume that the word boundaries are similar in French and English.
However, the situation is messier with English and Chinese.
First, segmentation of the Chinese text into words already introduces some ambiguity of the seed word identities.
Secondly, English-Chinese translations are complicated by the fact that the two languages share very little stemming properties, or part-of-speech set, or word order.
This property causes every English word to have many Chinese translations and vice versa.
In a source-target language translation scenario, the translated text can be &quot;rearranged&quot; and cleaned up by a monolingual language model in the target language.
However, the lexicon is not very reliable in establishing &quot;bridges&quot; between nonparallel English-Chinese texts.
To compensate for this ambiguity in the seed lexicon, we introduce a confidence weighting to each bilingual word pair used as seed words.
If a word ie is the k—th candidate for word ic, then wite wite /ki.
The similarity scores then become S4 and S5 and S6 = S4 x S5: We also experiment with other combinations of the similarity scores such as S7 = SO x S5.
All similarity measures S3 — S7 are used in the experiment for finding a translation for ME •
In order to apply the above algorithm to find the translation for blgt, I liougan from the HKStandard/Mingpao corpus, we first use a script to select the 118 English content words which are not in the lexicon as possible candidates.
Using similarity measures S3— S7, the highest ranking candidates of MS are shown in Table 6.
S6 and S7 appear to be the best similarity measures.
We then test the algorithm with S7 on more Chinese words which are not found in the lexicon but which occur frequently enough in the Mingpao texts.
A statistical new word extraction tool can be used to find these words.
The unknown Chinese words and their English counterparts, as well as the occurrence frequencies of these words in HKStandard/Mingpao are shown in Table 4.
Frequency numbers with a * indicates that this word does not occur frequent enough to be found.
Chinese words with a * indicates that it is a word with segmentation and translation ambiguities.
For example, 44: (Lam) could be a family name, or part of another word meaning forest.
When it is used as a family name, it could be transliterated into Lam in Cantonese or Lin in Mandarin.
Disregarding all entries with a * in the above table, we apply the algorithm to the rest of the Chinese unknown words and the 118 English unknown words from HKStandard.
The output is ranked by the similarity scores.
The highest ranking translated pairs are shown in Table 5.
The only Chinese unknown words which are not correctly translated in the above list are ik6 fl/ Lunar and MR / Yeltsin 1 .
Tung/CheeHwa is a pair of collocates which is actually the full name of the Chief Executive.
Poultry in Chinese is closely related to flu because the Chinese name for bird flu is poultry flu.
In fact, almost all unambiguous Chinese new words find their translations in the first 100 of the ranked list.
Six of the Chinese words have correct translation as their first candidate.
Using vector space model and similarity measures for ranking is a common approach in IR for query/text and text/text comparisons (Salton and Buckley, 1988; Salton and Yang, 1973; Croft, 1984; Turtle and Croft, 1992; Bookstein, 1983; Korfhage, 1995; Jones, 1979).
This approach has also been used by (Dagan and Itai, 1994; Gale et al., 1992; Shfitze, 1992; Gale et al., 1993; Yarowsky, 1995; Gale and Church, 1994) for sense disambiguation between multiple usages of the same word.
Some of the early statistical terminology translation methods are (Brown et al., 1993; Wu and Xia, 1994; Dagan and Church, 1994; Gale and Church, 1991; Kupiec, 1993; Smadja et al., 1996; Kay and Roscheisen, 1993; Fung and Church, 1994; Fung, 1995b).
These algorithms all require parallel, translated texts as input.
Attempts at exploring nonparallel corpora for terminology translation are very few (Rapp, 1995; Fung, 1995a; Fung and McKeown, 1997).
Among these, (Rapp, 1995) proposes that the association between a word and its close collocate is preserved in any language. and (Fung and McKeown, 1997) suggests that the associations between a word and many seed words are also preserved in another language.
In this paper, we have demonstrated that the associations between a word and its context seed words are well-preserved in nonparallel, comparable texts of different languages.
Our algorithm is the first to have generated a collocation bilingual lexicon, albeit small, from a nonparallel, comparable corpus.
We have shown that the algorithm has good precision, but the recall is low due to the difficulty in extracting unambiguous Chinese and English words.
Better results can be obtained when the following changes are made: We will test the precision and recall of the algorithm on a larger set of unknown words.
We have devised an algorithm using context seed word TF/IDF for extracting bilingual lexicon from nonparallel, comparable corpus in English-Chinese.
This algorithm takes into account the reliability of bilingual seed words and is language independent.
This algorithm can be applied to other language pairs such as English-French or English-German.
In these cases, since the languages are more similar linguistically and the seed word lexicon is more reliable, the algorithm should yield better results.
This algorithm can also be applied in an iterative fashion where high-ranking bilingual word pairs can be added to the seed word list, which in turn can yield more new bilingual word pairs.
