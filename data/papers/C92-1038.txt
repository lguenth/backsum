The result is a straightforward algorithm that is computation- ally tractable, sensitive to the preferences of human users, and reasonably domain-independent.
We pro- vide a specification of the resources a host system must provide in order to make use of the algorithm, and describe an implementation used in the IDAS sys- tem.
In t roduct ion In previous work [Da189,DH91,Rei90a,Rei90b] we have proposed algorithms for determining the con- tent of referring expressions.
Scrutiny of the psy- cholinguistics literature and transcripts of human di- alogues hows that in a number of respects the be- haviour of these algorithms does not correspond to what people do.
In particular, as compared to these algorithms, human speakers pay far less attention to reducing the length of a referring expression, and far more attention to making sure they use attributes and values that human hearers can easily process; in the terms introduced in [Da188,Da189], hearers are more concerned with the principle of sensitivity than with the principle of efficiency.
We have designed a new referring expression generation algorithm that is based on the~ observations, and believe that the new algorithm is more practical for real-world natu- ral language generation systems than the algorithms we have previously proposed.
In particular, the al- gorithm is: ?
fast: its run-time is linear in the number of distrac- tors, and independent of the number of possible modifiers; ?
sensitive to human preferences: it attempts to use easily perceivable attributes and basic-level [Ros78] attribute values; and ?
Supported by SERC grant GR/F/36750.
E-mail ad- dress is E.Reiter@ed.
tAiso of the Centre for Cognitive Science at the Univer- sity of Edinburgh.
E-mail address i  R. DaleQed.
Ehud Re i te r*and  Rober t  Da le f Depar tment  of Art i f ic ia l  Inte l l igence Un ivers i ty  of Ed inburgh Ed inburgh  EH1 1tlN Scot land ?
domain-independent: he core algorithm should work in any domain, once an appropriate knowl- edge base and user model has been set up.
A version of the algorithm has been implemented within the IDAS natural-language neration system [RML92], and it is performing satisfactorily.
The algorithm presented in this paper only gener- ates definite noun phrases that identify an object that is in the current focus of attention.
Algorithms and models that can be used to generate pronominal and one-anaphoric referring expressions have been presented elsewhere, .g., [Sid81,GJW83,Da189].
We have recently begun to look at the problem of gen- erating referring expressions for objects that are not in the current focus of attention; this is discussed in the section on Future Work.
Background D is t ingu ish ing  Descr ip t ions The term referring expression has been used by dif- ferent people to mean different things.
In this paper, we define a referring expression i  intentional terms: a noun phrase is considered to be a referring expres- sion if and only if its only communicative purpose is to identify an object to the hearer, in Kronfelds ter- minology [Kro86], we only use the modal aspect of Donefians distinction between attributive and ref- erential descriptions [Don66]; we consider a noun phrase to be referential if it is intended to identify the object it describes to the hearer, and attributive if it is intended to communicate information about that object to the hearer.
This usage is similar to that adopted by Reiter [Rei90b] and Dale and Had- dock [DH91], but differs from the terminology used by Appelt [App85], who allowed referring expres- sions to satisfy any communicative goal that could be stated in the underlying logical framework.
We here follow Dale and Haddock [DH91] in assum- ing that a referring expression satisfies the referential communicative goal if it is a d is t ingu ish ing  de- acript ion, i.e., if it is an accurate description of the entity being referred to, but not of any other object in the current context  set.
We define the context set to be the set of entities that the hearer is cur- rently assumed to be attending to; this is similar ACRES DE COLING-92, NAtCrES, 23-28 AOt~q" 1992 2 3 2 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992 to the notion of a discour~ focus space [GS86].
We also define the cont ras t  set  to be all elements of the context set except he intended referent.
The role of tile conlponcnts of a referring expression can then be regarded as ruling out  members of the contrast set.
For example, if the speaker wished to identify a small black dog in a situation wlmre tile contrast set consisted of a large white dog and a small black cat, she might choose the adjective black in order to rule out the white dog and the heart noun dog in order to rule out the eat; this results in the referring ex- pression the black dog, which matches the intended referent but no other object in the current context.
The small dog would also be a succ~csful referring expre~-~ion i this context, under the distinguishing description model.
Unnecessary  Mod i f ie rs A referring expression must communicate enough in- fornlation to be able to uniquely identify the in- tended referent in the current discourse context (i.e., it must adhere to the principle of adequacy [Da188,Da189]).
But.this is not the only constraint a good referring expression must obey; it is clear that many referring expressions that meet this con- straint are inappropriate because they couvey incor- rect and unwanted conversat iona l  imp l ieatures [Gri75,Rei90a] to a human hearer.
One source of such false implicatures can he the pre~ ence of redundant or otherwise unnecessary modifiers in a referring expression.
For example, consider two possible referring expressions that  a speaker might use to request ilat a hearer sit by a talfle: (l) a.
Sit by the table.
h. Sit by the brown wooden table.
If the context was such that only one table was vis- ible, and this table was brown raid made of wood, utterances (In) and (lb) would both be distinguish- ing descriptions that unranbiguously identified the intended referent o the hearer; a hearer who heard either utterance would know where he was supposed to sit.
However, a hearer who heard utterance (lb) in such a context might make the additional infer- enee that  it was important to the disc~mrse that the tM)le was brown and made of wood; for, tile hearer might reason, why else would the speaker include in- formation about the tables colour and material that was not necessary for the reference task?
This infer- enc~ is an example of a conversational implicature caused by a violation of Grices maxim of Quantity [Gri75].
Inappropr ia te  Mod i f ie rs Unwanted conversational implicatures can alse be caused by the use of overly specific or otherwise un- expected modifiers.
One example is ms follows: (2) IL l,ook at the doq.
Look at the pil bull.
In a context where there is only one dog present, tile hearer would nommlly expect utterance (2a) to be used, since dog is a basic- level  class [Ros78] for most native speakers of English.
Hence the use of ut- terance (2b) might implicate to the hearer that  the speaker thought it was relevant that  the animal was a pit bull and not some other kind of dog [Cru77], per- haps because the speaker wished to warn the hearer that the animal might be dangerous; if the speaker had no such intention, site should avoid using utter- ance (2b), despite the fact that it fulfills the referen- tial communicative goal.
P rev ious  Work In previous work [Dalgg,D}191,Rei90a,Rei90b] we have noted that  the presence of extra information in a referring expression can lead the hearer to make false implicaturcs, and therefore concluded that a referring-expression generation system should taake a strong attempt to ensure that  generated refer- ring expressions do not include unnecessary infor- mation, either as superfluous NP modifiers or as overly-specific head nouns or attr ibute values.
Dale [DaI88,DaI89,DH91] }ins suggested oing this by re- quiring the generation system to produce mtn imal distinguishing descriptions, i.e., distinguishing de- scriptions that  include as few attributes of the in- tended referent as possible.
Keiter [Keig0a, Rei90b] ha.s pointed out that  this task is in fact NP-Hard, and has proposed instead that referring expressions should obey three rules: No  Unnecessary  Components :  all components of a referring expremion must be necessary to ful- fill the referential goal.
For example, the small black dog is not acceptable if the black dog is a dis- tinguishing description, since this means small is an unnecessary component.
Loca l  Brev i ty :  it should not be po~ible to pro- duce a shorter eferring expression by replacing a set of existing modifiers by a single new modifier.
For exanlple, the sleeping female dog should not he treed if the small dog is a distinguishing descrip- tion, since the two modifiers sleeping and female can bc replaced by the single modifier small.
Lexical  P re ference :  this is an extension of the ba.sicAcvel preference proposed by Cruse [Cru77]; more details are given in [Reigl].
A referring expression that mt.~ts l[eiters con- straints cart be foumt in polynomial time if the lexical preference r lation meets certain conditions [Rei90a]; such a referring expression can not, imwever, always be found in linear time.
Psycho log ica l  and  Transcr ip t  Data Psycho log ica l  Ev idence Subsequent to performing the above research, we have looked in some detail at the psychological lit- erature on human generation of referring expres- sions.
This research (e.g., [FO75,Whi76,Son85, A(XES DE COLING-92, NANrES, 23-28 AOlrl 1992 2 3 3 ISOC.
hi: COLING-92, NANTES, AUG. 23-28, 1992 Pec891; [Lev89, pages 129-134] is a useful summary of much of this work) clearly shows that in many cases human speakem do include unnecessary modi- tiers in referring expressions; this presumably implies that in many cases human hearers do not make impli- catures from the presence of unnecessary modifiers.
For example, if human subjects are shown a picture of a white bird, a black cup, and a white cup, and are asked to identify the white bird, they frequently say the white bird, even though just the bird would have been sufficient in this ease.
A partial explanation for this use of redundancy may be that human speakers generate referring expres- sions incrementally [Pee89].
An incremental gener- ation algorithm cannot always detect unnecessary modifiers; in the above example, for instance, one could imagine the algorithm choosing the adjective white to rule out the black cup, and then the noun bird in order to rule out the white cup, without then erasing white because the black cup is also ruled out by bird.
Another explanation of redundancy might involve the speakers desire to make it easier for the hearer to identify the object; the speaker might believe, for example, that it is easier for the hearer to identify a white bird than a bird, since colour may be more immediately perceptible than shape.
Both of the above explanations primarily justify ad- jectives that have some discriminatory power even if they are redundant in this particular context.
In the above example, for instance, white possesses some discriminatory power since it rules out the black cup, even though it does happen to be redundant in the expression the white bird.
It would be harder for either of the above factors to explain the use of a modifier with no discriminatory power, e.g., the use of white if all objects in the contrast set were white.
2qaere is some psychological research (e.g., [FO75]) that suggests that human speakers do not use mod- ifiers that have no discriminatory power, but this research is probably not conclusive.
Thc argument can be made that  psychological real- ism is not the most important constraint for gener- ation algorithms; the goal of such algorithms hould be to produce referring expressions that human hear- ers will understand, rather than referring expressions that human speakers would utter.
The fact that hu- man speakers include redundant modifiers in refer- ring expressions does not mean that NL generation systems are also required to include such modifiers; there is nothing in principle wrong with building gen- eration systems that perform more optimizatious of their output than human speakers.
On the other hand, if such beyond-human-speaker optimizations 1Another possible explanation is that speakers may in some cases use precompiled reference scripts instead of computing a referring expression from scratch; such refer- enoe scripts pecify a set of attributes that are included as a group in a referring expression, even if some members of the group have no discriminatory power in the current context are computationally expensive and require complex algorithms, they may not be worth performing; they are clearly unnecessary in some sense, after all, since human speakers do not perform them.
Transcr ip t  Ana lys i s In addition to the l~ychological l iterature review, we have also examined a transcript of a dialogue be- tween two humans performing an assembly task.
~ We were particularly interested in questions of mod- ifier choice; if a discriminating description can be formed by adding any one of several modifiers to a head noun, which modifier should be used?
In par- ticular, 1.
Which attribute should be used?
E.g., is it better to generate the small dog, the black dog, or the female dog, if these are discriminating descriptions but jnst the dog is not?
Is it preferable to add a modifier or to use a more specific head noun?
E.g., is it better to say the small dog or the chihuahua?
Should relative or absolute adjectives be used?
E.g., is it better to say the small dog or the one foot high dog?
In our analysis, we observed several phenomena which we believe may generalise to other situations involving spoken, face-tooface language: 1.
Human speakers prefer to use adjectives that com- municate size, shape, or colour in referring expres?
In tile above examples, for instance, a hu- man speaker would probably prefer the black dog and the small dog over the female dog.
Human hearers ometimes have trouble determin- ing if an object belongs to a specialized class.
In the above example, for instance, the chihuahua should only be used if the speaker is certain the hearer is capable of distinguishing chihuahuas from other types of dogs.
If there is any doubt about the heaters ability to do this, adding an explicit modifier (e.g., the small dog) is a better strategy than using a specialized head noun.
Human speakers eem to prefer to use relative ad- jectives, and human hearers eem to have less trou- ble understanding them.
However, human-written instructional texts sometimes use absolute adjec- tives instead of relative ones; this may be a con- sequence of the fact that writers cannot predict the context heir text will be read in, and hence how readers will interpret relative adjectives.
In the above example, therefore, a speaker would be expected to use the small dog, but a writer might use the one foot high dog.
ZThe transcript was made by Phil Agre and John Batali, from a videotape taken by Candy Sidser.
We are very grateful to them for allowing us to use it.
ACTF~ DE COLING-92, NANqT~, 23-28 Aotrr 1992 2 3 4 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992 The A lgor i thm Based on the above considerations, we have created a new algorithm for generating referring expressions.
This algorithm is simpler and faster than the algo- rithms proposed in [Dai89,Rei90a] because i t  per- forms much less length-oriented optimization of its outputi we now believe that  the level of optimiza- tion suggested in [Da189,ReigOa] was unnecessary and psycholinguistically implausible.
The algorithm has been implemented as part  of a larger natural- language generation system, and we are pleased with its performance to date.
Assumpt ions  about  the  Knowledge Base Our algorithm is intended to be reasonably domain- independent.
do, however, make some assump- tions about the structure of the host systems un- derlying knowledge base, and require that certain interface functions be provided.
in particular, we assume that: ?
Every entity is characterised in terms of a col- lection of a t t r ibutes  and their values.
An attrii)ute-value pair is what is sometimes thought of as a property; an example is (colour, red).
Every entity has as one of its attributes ome type .
This is a special attr ibute that  corresponds to the kinds of properties that  are typically real- iT, ed by head nouns; an example is (type, dog).
The knowledge base may organize some attr ibute values in a subsumption taxonomy (e.g., as is done in KI:ONE [BS85] and related KR systems).
Such a taxonomy might record, for example, that  an- imM subsumes dog, and that  red subsumes car- let.
For such taxonomically-organized values, the knowledge-base or an associated user-model should specify which level of the taxonomy is basic-level for the current user.
We require that the following interface functions be provided: value(object,attribute) returns the value (if any) that an attribute has for a particular object.
Value should return the most specific possible value for this attribute, e.g., chihuahua instead of dog, and scarlet instead of red.
taxonomy-children(value) returns the immediate chil- dren of a value in the taxonomy.
For example, taxonomy-children(animal) might be the set {dog, cat, horse, .
basle-level-value(object.attribute) returns the basic~ level value of an attribute of an object.
FOr exam- ple, basic-level-value(Garfield, type) might be cat.
The knowledge-representation system should in principle allow different basic-level classes to be specified for different users [Ros78,Rei91].
user-knows(object, a tribute-value-pair) returns true if the user knows or can easily determine (e.g., by direct visual perception) that  the attribute-valuc pair applies to the object; false if the user knows or can easily determine that  the attr ibute-value pair does not apply to the object; and unknown other- wise.
FOr exmnple, if object x had the attr ibute- value pair (type, chihuahua), and the user was ca- pable of distinguishing dogs from eats, then user- knows(x, (type, dog)) would be true, while user- knows(x, (type, cat)) would be false.
If the user was not, however, capable of distinguishing differ- ent breeds of dogs, and had no prior knowledge of xs breed, then user-knows(x, (type, chihuahua)) and user~knows(x, (type, poodle)) would both re- turn unknown, since the user would not know or be able to easily determine whether x was a chi- huahua, poodle, or some other breed of dog.
Finally, we a~ume that the global variable *p~eferred-attributes* lists the attributes that  human speakers and hearers prefer (e.g., type, size, shape, and colour in the ~.,~embly task transcript mentioned above).
These attributes hould be listed in order of preference, with the most preferable attr ibute flint.
The elements of this list and their order will vary with the domain, a~ld should be determined by em- pirical inv~tigation.
Inputs  to  the  A lgor i thm In order to construct a reference to a particular em tity, tile host system must provide: - a symbol corresponding to the intended referent; and ?
a list of symbols correspondiug to the members of the contrast set (i.e., the other entities in focus, besides the intended referent).
The algorithm returus a list of attr ibute-value pairs that  correspond to tim romantic ontent of the refer- ring expression to be realized.
This list can then be converted into an SPL [K&~9] term, as is done in the II)AS implementation; it can also be converted into a recoverab le  semant ic  s t ructure  of the kind used in Daies EPICOltE system [Da188,Dai89].
The  A lgor i thm In general terms, the algorithm iterates through the attributes in *preferred-attributes*.
For each at- tribute, it checks if specifying a value for it would rule out at least one member of the contrast set that has not already becu ruled out; if so, this attribute is added to the referring ~t ,  with a value that  is known to the User, rules out as many contrast set mem- bers as possible, and, subject to these constraints, is as cl(~e as possible to the basic-level value.
The process of adding attr ibut~value pairs continues mt- til a referring expression has been formed that  rules out every member of the contrast set.
There is no backtracking; once an attr ibute-value pair has been added to the referring expression, it is not removed even if the addition of subsequent attribute-value pairs make it unnecessary.
A head noun (i.e., a value for tim type attribute) is always included, even if it Acres DE COLING-92, NANTES, 23-28 AO~n 1992 2 3 5 PROC.
OV COTING-92, NANTES, AUG. 23-28, 1992 l make-referring-expression(r, C,P) I L*-- {} D, -C for each member A~ of list P do V = flnd-best-value(A~, baslc-level-value(r, A~)) I fV ~ nil A rules-out((A~, V)) ~ nil then L ~ L U {(AI, V)} D ~ D - rules-out((At, V)) endlf If D = {} then if (type, X) (: L for some X then re turn  L else return L U {(type, basic-level-value(r, type))} endif endif next return failure I find-best-value(A, initial-valse) l ff user-knows(r, (A. initial-value)) = true then value ~-- initial-value else value ~ nil vndlf for v~ E taxonomy-children(initial-value) lfv~ subsumes value(r, A) A (new-value ~ find-best-value(A, vi)) ~ nil A (value = nll Y Irules-out((A, new-value)) I > Irules-out((a, valse) l) then value ~ new-value endif next return value [ ,ul;s-out(<A, v>)[ return {x : x E D A user-knows(x, (A, V)) = false} Figure 1: The Algorithm has no discriminatory power (in which ease the basic level value is used); other attribute values are only included if, at the time they were under considera- tion, they had some discriminatory power.
More precisely, the algorithm is as shown in Figure 1.
Here, r is the intended referent, C is the contrast set, P is the list of preferred attributes, D is the set of distractom (contrast set members) that have not yet been ruled out, and L is the list of attr ibute-value pairs returned, a make-referring-expression is the top level function.
This returns a list of attr ibute-value pairs that specify a referring expression for the intended ref- a For simplicity of expo6ition, the algorithm as described here returns failure if it is not pesaible to rule out all the mernbem of the contrast set.
A more robust algorithm might attempt o pur~m other strategies here, e.g, gen- erating a referring expression of the form one of the Xs, or modifying the contrast set by adding navigation i for- mation (navigation is discussed in the section on Future Work).
Note that the attributes are tried in the or- der specified in the *preferred-attributes* li t, and that a value for type is always included, even if type has no discriminatory power.
find-best-value takes an attribute and an initial value; it returns a value for that attribute that is subsumed by the initial value, accurately describes the intended referent (i.e., subsumes the value the intended referent possesses for the attribute), rules out as many distractors as possible, and, subject to these constraints, is as close as possible in the taxonomy to the initial value.
rules-out akes an attribut~.~value pair and returns the elements of the set of remaining distractom that are ruled out by this attr ibute-value pair.
An  Example Assume the task is to create a referring expression for Objectl in a context that  also includes Object2 and Object3: ?
Object1: (type, chihuahua), (size, small), (calour, black) ?
Object2: (type, chihuahua), (size, large), (colour, white) * Object3: (type, siamese-cat), (size, small), (colour, black) In other words, r = 0bject l  and (7 = {Objest2, Object3}.
Assume that P = {type, colour, size, .
When make-referring-expression i  called in this con- text, it initializes L to the empty set and D to C, i.e., to {Object2, Object3}.
Find-best-value is then caUed with A = type, and initial-value set to the basic-level type of Object1, which, let us assume, is dog.
Assume user-knows(Object1, (type, dog)) is true, i.e., the user knows or can easily perceive that  Objectl is a dog.
Find-best-value then sets value to dog, and examines the taxonomic descendants of dog to see if any of them are accurate descriptions of Ob- ject1 (this is the subsumption test) and rule out more distractors than dog does.
In this case, the only accurate child of dog is chihuahua, but (type, chihuahua) does not have more discriminatory power than (type, dog) (both rule out {Object3}), so find- best-value returns dog as the best value for the type attribute.
Make-referring-expression the  verifies that (type.
dog) rules out at least one distraetor, and therefore adds this attribute-value pair to L, while removing rules-out((type, dog)) = {Object3} from D. This means that  the only remaining distraetor in D is Object2.
Make-referring-expression (after cheek- ing that  D is not empty) calls find-best-value again with A = colour (the second member of P).
Find- best-value returns Objectl~s basic-level colour value, which is black, since no more specific colour term has more discriminatory power.
Make-referring- expression then adds (colour, black) to L and removes rules-out((colour, black)) = {Object2} from D. D is then empty, so the generation task is completed, ACTES DE COLING-92, NANTES, 23-28 AO~r 1992 2 3 6 PROC.
OF COLING-92, NArCrEs.
1992 and make-referring-expression returns {(type, dog), (celour, black)} , i.e., a specification for the refer- ring expression the black day.
Note that if P had been {type, size, colour, .
instead of {type, cnlour, size, .
,  make-referring-expeession would have rc~ turned {(type, dog), (size, small)} instead, i.e., the sraall do#.
Imp lementat ion The algorithm is currently being used within the n)AS system [RML92].
ll)hS is a natural m~guage generation system that generates on-line documen- tation and help texts from a domain arid linguistic knowledge base, lining user expertise models, user task models, and discourse models.
IDAS uses a KL-ONE type knowledge repr~entation system, with roles corresponding to attributes and lillem to values.
The type attribute is implicit in the position of an object in the taxonomy, and is not explicitly represented.
The value and taxonomy- children functions are defined in terms of standard knowledge-base access functions.
A knowledg~base author can specify explicit basic- level attr ibute values in IDAS user models, but IDAS is also capable of using heuristics to guess which value is basic-level.
The heuristics are fairly simple (e.g., "nse the most general value that is not in the upper- model [BKMW90] and has a one-word realization"), but they seem (so far) to be at least somewhat effec- tive.
A *preferred-attributes* li t has been crcated for IOASs domain (complex electronic machinery) by visual inspection of the equipment being docu- mented; its first members are type, colour, and la- bel.
The user-knows function simply returns true if the attributc~value pair is accurate and false other- wise; this essentially assumes that the user can visu- ally perceive the value of any attribute in *preferred- attributes*, which may not tie true in general.
The referring expression generation model seems rea- sonably successful in IDAS.
In parLieular, the algo- rithm lure proven to be useful because: 1.
The algorithm runs in linear time in the number of distractors, which is probably im- possible for any algorithm that includes an ex- plicit brevity requirement (e.g., the algorithms of [Da189,Rei90a]).
Of equal importance, its run- time is independent of the number of potential attributes that could be used in the referring ex- preszion.
This ks a consequence of the fact that the algorithm does not attempt to find the at- tribute with the highest discriminatory power, but rather simply takes attributes from the *preferred- attributes* list until it has built a successful refer- ring expression.
It allows human preferences and capabilities to be taken into consideration.
The *preferred- attributes* list, the preference for basic-level val- ues, and the user~knows function are all ways of biasing the algorithm towards generating referring expressions that use attributes and values that hu- taan hearers, with all their perceptual limitations, lind easy to process.
Almost all referring expressions generated by IDAS contain a head noun and zero, one, or perhaps at most two modifiers; longer referring expressions are rare.
The most important task of the algorithm is therefore to quickly generate asy-to-understand re- ferring expre~mions in such simple cases; optimal han- dling of more complex referring expressions i lees important, although the algorithm should be robuat enough to generate something plausible if a long re- ferring expression is needed.
Future  Work Nav igat ion As mentioned in the introduction, the algorithm pre- sented here assumes that  the intended referent is in the context set.
An important question we need to address is what action should be taken if this is not the c~.se, i.e., if the intended referent is not in the current focus of attention.
Unfortunately, we have very little data available ou which to bose a model of the generation of such refer- ring expressions.
Psyclmlinguistic researchers seem to have paid relatively little attention to such eases, and the transcripts we have (to date) examined have contained relatively few instances where the intended referent was not already salient.
ltowever, we take the view that, in the general case, a referring expression contains two kinds of informa- tion: nav igat ion  and d isc r iminat ion .
Each de~ scriptor used in a referring expression plays one of these two roles.
Navigational, or a t tent ion -d i rec t ing  informa- tion, is intended to bring the intended referent into the hearers focus of attention.
Discrimination information is intended to distin- guish the intended referent from other objects in the hearers focus of attention; such information has been the subject of this paper.
Navigational information is not needed if the in- tended referent is already in the focus of attention.
If it is needed, it frequently (although not always) takes the form of loeational information.
The IDAS system, for example, can generate referring expres- sions such as tl~e black power supply in the equipment rack.
In this case, in the equipment rack is navigation information that is intended to bring the equipment rack and its components into the hearers focus of attention, while black power supply is discrimination information that is intended Ix) distinguish the in- tended referent from other members of the context ~t  (e.g., the white power supply that is also present in the equipment rack).
The navigation model currently implemented in If)AS is simplistic and not theoretically well-justified.
We hope to do further research on building a better- ju~stified model of navigation.
AcrEs DE COLING-92, NAbrlns.
23-28 ^ o~r 1992 2 3 7 PRoc, OF COLING-92, NANTES, AtJcl.
23-28, 1992 Relat ive  At t r ibute  Va lues As mentioned previously, the transcript analysis shows that human speakers and hearers often pre- fer relative instead of absolute attr ibute values, e.g., small instead of one inch.
Knowledge bases some- times explicitly encode relative attribute values (e.g., (size.
small)), but this can cause difficulties when re- ferring expressions need to he generated in different contexts; a one-inch screw, for example, might be considered to be small in a context where the other screws were all two-inch screws, but large in a context where the other screws were all half-inch screws.
A better solution is for the knowledge base to record absolute attr ibute values, and then for the genera- tion algorithm to automatically convert absolute val- ues to relative values, depending on the values that other members of the context set pussc~ for this at- tribute.
Thus, the knowledge base might record that a particular screw had (size.
one-inch), and the gen- eration system would choose to call this screw small or/arye depending on the size of the other screws in the context set.
We hope to do further research on determining how exactly this process hould work.
Conc lus ions We have presented an algorithm for the generation of referring expressions that  is substantially simpler and faster than the algorithms we have proposed in previous work [Da189,Rei90a], largely because it performs much less length-oriented optimization of its output.
We have been guided in this simplifica- tion effort by psycholinguistic findings and transcript analyses, and believe that the resulting algorithm is a more practical one for natural  anguage generation systems than the ones we proposed previously.
Re ferences lapp85] Douglas E Appelt.
Planning English Sentences.
Cambridge Univemity Press, New York, 1985.
[BKMWgO] John Bateman, Robert T Kasper, Johanna D Moore and Richard A Whitney.
A General Organization ofKnowledge for Natural Language Processing:.
The Penman Upper Model.
Unpub- lished technical report, Information Sciences Insti- tute/University ofSouthern California, 1990.
[BS85] Ronald Brachman and James Schmolze.
An overview of the KL-ONE knowledge r presentation system.
Cognitive Science 9:171-216, 1985.
[Cru77] D. Cruse.
The pragmatics of lexieal specificity.
Journal of Linguistics , 13:153-164~ 1977.
[Da188] Robert Dale.
Generating Referring Expressions in a Domain of Objects and Processes.
PhD Thesis, Centre for Cognitive Science, University of Edin- burgh, 1988.
IDol89] Robert Dale.
Cooking up referring expre~ens.
In Proceedings of the $7th Annual Meeting of the Association for Computational Linguistics, pages 68-75.
[DH91] Robert Dale and Nicholas Haddock.
Content de- termination i the generation of referring expres- sions.
Computational Intelligence, 7(4), 1991.
[Don66] Kelth Donnellan.
Reference and definite descrip- tion.
Philosophical Review, 75:281-304, 1986.
[17075] William Ford and David Olsea The elaboration of the noun phrase in childrens de~riptton of ob- jects.
Journal of Ezpemmentol Child Psychology, 19:371-382, 1975.
[GJW83] Barbara Gresz, Aravind Jeshi, and Scott Wein- stein.
Providing a unified account of definite noun phrasesin discourse.
In Proceedings ofthe ?1st An- nual Meeting of the A ssoeiation for Computafional Linguistics, pages 44-50.
[Gri75] H. Paul Grice.
Logic and conversation.
In P. Cole and J. Morgun~ editors, Syntax and Semantics: Vat 3, Speech Acts, pages 43-58.
Academic Prese, New York, 1975.
[GS86] Barbara Grenz and Candaen Siduer.
Attentlon~ intention, and the structure of discourse.
Compu- tatioual Linguistic, 12:175-208, 1986.
[Kns89] Robert Kasper.
A flexible interface for linking applications toPenmans sentence generator.
Pro- ceedings of the 1989 DARPA Speech and Natural Language Workshop~ pages 153-158.
[Kro86] Amichai Kronfeid.
Donnsllans distinction and a computational model of reference.
In Proceedings of the ~.~th Annual Meeting of the Association for Computational Linguistics, pages 185-191.
[Lev89] Willera Levelt.
Speaking: bq~om Intention to Ar- ticulation.
MIT Pre~s, 1989.
[Pec89] Thomas Pechmann.
Incremental speech produc- tion and referential overspeeificatlon.
Linguistics, 27:89-110, 1989.
[Rdg0a] Ehud Reiter.
The computational complexity of avoiding conversational implicatures.
In Pea~.2A- ings of the ~8th Annual Meeting of the Association for Computational Linguistics, pages 97-104.
[Relg0b] Ehud Reiter.
Generating Appropriate Natural Language Object Descriptions.
Phi3 thesis, Aiken Computation Lab, Harvard Univeraity, 1990.
Also available as Aiken Computation Lab technical re- port TK-10-go.
[Rei91] Ehud Reiter.
A new model of lexical choice for nouns.
Computational Intelligence, 7(4), 1991.
[RML92] Ehod Relter, Chris Mellish, and John Levine.
Automatic generation ofon-line documentation In tbe IDAS project.
In Proceedings of the Third Cor~ ferenee on Applied Natural Language Pn~.p_.~sing, pages 64-71.
IRes78] Eleanor Rcech.
Principles of categorization.
In E. Resc~ and B. Lloydj editors, Cognition and Cat- egorization, pages 27-48.
Lawrence Erlbaum, Hills- dale, NJ, 1978.
[Sid81] Canda~e Sidner.
Focusing in the comprehension of definite anaphora.
In M. Brady and R. Berwick, editors, Computational Models of Discourse, pages 267-330.
MIT Press, Cambridge, Mass, 1981.
[Son85] Susan Sonnenschein.
The development of rder- ential communication skills: Some situations in which speakers give redundant messages.
Journal of Paycholingnistic Research, 14:489-508, 1985.
[Wh176] Graver Whitehurst.
The development of commu- nicatiea: Changes with age and modeling.
Child Development, 47:473-482, 1876.
ACIds DE COLING-92, NANI~S, 23-28 AOUT 1992 2 3 8 PROC.
OF COL1NG-92, NANIES, AUG. 23-28, 1992
