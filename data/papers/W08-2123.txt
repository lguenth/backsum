To tackle the problem of joint syntactic?semantic anal- ysis, the system relies on a syntactic and a semantic subcomponent.
The syntactic model is a bottom-up projective parser us- ing pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of clas- sifiers.
The complete syntactic?semantic output is selected from a candidate pool generated by the subsystems.
The system achieved the top score in the closed challenge: a labeled syntactic accu- racy of 89.32%, a labeled semantic F1 of 81.65, and a labeled macro F1 of 85.49.
1 Introduction: Syntactic?Semantic Analysis Intuitively, semantic interpretation should help syntactic disambiguation, and joint syntactic?
semantic analysis has a long tradition in linguis- tic theory.
This motivates a statistical modeling of the problem of finding a syntactic tree y?
syn and a semantic graph y?
sem for a sentence x as maximiz- ing a function F that scores the joint syntactic?
semantic structure: ?y?
= arg max y syn ,y sem F (x, y syn , y sem ) The dependencies in the feature representation used to compute F determine the tractability of the search procedure needed to perform the maximiza- tion.
To be able to use complex syntactic features c ?
Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported li- cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
such as paths when predicting semantic structures, exact search is clearly intractable.
This is true even with simpler feature representations ?
the problem is a special case of multi-headed dependency anal- ysis, which is NP-hard even if the number of heads is bounded (Chickering et al., 1994).
This means that we must resort to a simplifica- tion such as an incremental method or a reranking approach.
We chose the latter option and thus cre- ated syntactic and semantic submodels.
The joint syntactic?semantic prediction is selected from a small list of candidates generated by the respective subsystems.
2 Syntactic Submodel We model the process of syntactic parsing of a sentence x as finding the parse tree y?
syn = argmax y F (x, y) that maximizes a scoring func- tion F .
The learning problem consists of fitting this function so that the cost of the predictions is as low as possible according to a cost function ?.
In this work, we consider linear scoring functions of the following form: F (x, y) = w ??
(x, y) is a numeric feature representation of the pair (x, y) andw a vector of feature weights.
We defined the syntactic cost ?
as the sum of link costs, where the link cost was 0 for a correct de- pendency link with a correct label, 0.5 for a correct link with an incorrect label, and 1 for an incorrect link.
A widely used framework for fitting the weight vector is the max-margin model (Taskar et al., 2003), which is a generalization of the well- known support vector machines to general cost- based prediction problems.
Since the large num- ber of training examples and features in our case make an exact solution of the max-margin opti- mization problem impractical, we used the on- line passive?aggressive algorithm (Crammer et al., 183 2006), which approximates the optimization pro- cess in two ways: ?
The weight vector w is updated incremen- tally, one example at a time.
For each example, only the most violated con- straint is considered.
The algorithm is a margin-based variant of the per- ceptron (preliminary experiments show that it out- performs the ordinary perceptron on this task).
Al- gorithm 1 shows pseudocode for the algorithm.
Algorithm 1 The Online PA Algorithm input Training set T = {(x t , y t )} T t=1 Number of iterations N Regularization parameter C Initialize w to zeros repeat N times for (x t , y t ) in T let y?
t = argmax y F (x t , y) + ?
(y t , y) let ?
t )?F (x t ,y t )+?
(x,y t )??(x,y?
t )) returnwaverage We used a C value of 0.01, and the number of iterations was 6.
2.1 Features and Search The feature function ?
is a second-order edge- factored representation (McDonald and Pereira, 2006; Carreras, 2007).
The second-order repre- sentation allows us to express features not only of head?dependent links, but also of siblings and chil- dren of the dependent.
This feature set forces us to adopt the expensive search procedure by Car- reras (2007), which extends Eisner?s span-based dynamic programming algorithm (1996) to allow second-order feature dependencies.
Since the cost function ?
is based on the cost of single links, this procedure can also be used to find the maximizer of F (x i , y ij )+?
(y i , y ij ), which is needed at train- ing time.
The search was constrained to disallow multiple root links.
2.2 Handling Nonprojective Links Although only 0.4% of the links in the training set are nonprojective, 7.6% of the sentences contain at least one nonprojective link.
Many of these links represent long-range dependencies ?
such as wh- movement ?
that are valuable for semantic pro- cessing.
Nonprojectivity cannot be handled by span-based dynamic programming algorithms.
For parsers that consider features of single links only, the Chu-Liu/Edmonds algorithm can be used in- stead.
However, this algorithm cannot be gen- eralized to the second-order setting ?
McDonald and Pereira (2006) proved that this problem is NP- hard, and described an approximate greedy search algorithm.
To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which nonprojective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the nonprojective links at parse time.
The use of trace labels in the pseudo-projective transfor- mation leads to a proliferation of edge label types: from 69 to 234 in the training set, many of which occur only once.
Since the running time of our parser depends on the number of labels, we used only the 20 most frequent trace labels.
3 Semantic Submodel Our semantic model consists of three parts: ?
A SRL classifier pipeline that generates a list of candidate predicate?argument structures.
A constraint system that filters the candidate list to enforce linguistic restrictions on the global configuration of arguments.
A global classifier that rescores the predicate?
argument structures in the filtered candidate list.
Rather than training the models on gold- standard syntactic input, we created an automati- cally parsed training set by 5-fold cross-validation.
Training on automatic syntax makes the semantic classifiers more resilient to parsing errors, in par- ticular adjunct labeling errors.
3.1 SRL Pipeline The SRL pipeline consists of classifiers for predi- cate identification, predicate disambiguation, sup- port identification (for noun predicates), argument identification, and argument classification.
We trained one set of classifiers for verb predicates and another for noun predicates.
For the pred- icate disambiguation classifiers, we trained one subclassifier for each lemma.
All classifiers in the pipeline were L2-regularized linear logistic regres- sion classifiers, implemented using the efficient LIBLINEAR package (Lin et al., 2008).
For multi- class problems, we used the one-vs-all binarization 184 method, which makes it easy to prevent outputs not allowed by the PropBank or NomBank frame.
Since our classifiers were logistic, their output values could be meaningfully interpreted as prob- abilities.
This allowed us to combine the scores from subclassifiers into a score for the complete predicate?argument structure.
To generate the can- didate lists used by the global SRL models, we ap- plied beam search based on these scores using a beam width of 4.
The features used by the classifiers are listed in Tables 1 and 2.
In the tables, the features used by the classifiers for noun and verb predicates are indicated by N and V, respectively.
We selected the feature sets by greedy forward subset selection.
Feature PredId PredDis PREDWORD N,V N,V PREDLEMMA N,V N,V PREDPARENTWORD/POS N,V N,V CHILDDEPSET N,V N,V CHILDWORDSET N,V N,V CHILDWORDDEPSET N,V N,V CHILDPOSSET N,V N,V CHILDPOSDEPSET N,V N,V DEPSUBCAT N,V N,V PREDRELTOPARENT N,V N,V Table 1: Classifier features in predicate identifica- tion and disambiguation.
Feature Supp ArgId ArgCl PREDPARENTWORD/POS N N,V CHILDDEPSET N N,V N,V PREDLEMMASENSE N N,V N,V VOICE V V POSITION N N,V N,V ARGWORD/POS N N,V N,V LEFTWORD/POS N N,V RIGHTWORD/POS N N,V N,V LEFTSIBLINGWORD/POS N,V RIGHTSIBLINGWORD/POS N N PREDPOS N N,V V RELPATH N N,V N,V POSPATH N RELPATHTOSUPPORT N N VERBCHAINHASSUBJ V V CONTROLLERHASOBJ V N PREDRELTOPARENT N N,V N,V FUNCTION N,V Table 2: Classifier features in argument identifica- tion and classification and support detection.
Features Used in Predicate Identification and Disambiguation PREDWORD, PREDLEMMA.
The lexical form and lemma of the predicate.
PREDPARENTWORD and PREDPARENTPOS.
Form and part-of-speech tag of the parent node of the predicate.
CHILDDEPSET, CHILDWORDSET, CHILD- WORDDEPSET, CHILDPOSSET, CHILD- POSDEPSET.
These features represent the set of dependents of the predicate using combinations of dependency labels, words, and parts of speech.
Subcategorization frame: the con- catenation of the dependency labels of the predicate dependents.
PREDRELTOPARENT.
Dependency relation be- tween the predicate and its parent.
Features Used in Argument Identification and Classification PREDLEMMASENSE.
The lemma and sense number of the predicate, e.g.
For verbs, this feature is Active or Pas- sive.
For nouns, it is not defined.
Position of the argument with respect to the predicate: Before, After, or On.
ARGWORD and ARGPOS.
Lexical form and part-of-speech tag of the argument node.
LEFTWORD, LEFTPOS, RIGHTWORD, RIGHT- POS.
Form/part-of-speech tag of the left- most/rightmost dependent of the argument.
LEFTSIBLINGWORD, LEFTSIBLINGPOS, RIGHTSIBLINGWORD, RIGHTSIBLING- POS.
Form/part-of-speech tag of the left/right sibling of the argument.
Part-of-speech tag of the predicate.
A representation of the complex grammatical relation between the predicate and the argument.
It consists of the sequence of dependency relation labels and link direc- tions in the path between predicate and argu- ment, e.g.
An alternative view of the grammat- ical relation, which consists of the POS tags passed when moving from predicate to argu- ment, e.g.
RELPATHTOSUPPORT.
The RELPATH from the argument to a support chain.
VERBCHAINHASSUBJ.
Binary feature that is set to true if the predicate verb chain has a sub- ject.
The purpose of this feature is to resolve verb coordination ambiguity as in Figure 1.
CONTROLLERHASOBJ.
Binary feature that is true if the link between the predicate verb chain and its parent is OPRD, and the parent has an object.
This feature is meant to resolve control ambiguity as in Figure 2.
The grammatical function of the ar- gument node.
For direct dependents of the predicate, this is identical to the RELPATH.
I SBJ eat drinkyouand COORD SBJ CONJROOT SBJ COORD ROOT drinkandeatI CONJ Figure 1: Coordination ambiguity: The subject I is in an ambiguous position with respect to drink.
I to IMSBJ want sleephim OBJ OPRD ROOT IM sleepI SBJ want ROOT to OPRD Figure 2: Subject/object control ambiguity: I is in an ambiguous position with respect to sleep.
3.2 Linguistically Motivated Global Constraints The following three global constraints were used to filter the candidates generated by the pipeline.
CORE ARGUMENT CONSISTENCY.
Core argu- ment labels must not appear more than once.
DISCONTINUITY CONSISTENCY.
If there is a la- bel C-X, it must be preceded by a label X.
REFERENCE CONSISTENCY.
If there is a label R-X and the label is inside a relative clause, it must be preceded by a label X.
3.3 Global SRL Model Toutanova et al.
(2005) have showed that a global model that scores the complete predicate?
argument structure can lead to substantial perfor- mance gains.
We therefore created a global SRL classifier using the following global features in ad- dition to the features from the pipeline: CORE ARGUMENT LABEL SEQUENCE.
The complete sequence of core argument labels.
The sequence also includes the predicate and voice, for instance A0+break.01/Active+A1.
MISSING CORE ARGUMENT LABELS.
The set of core argument labels declared in the Prop- Bank/NomBank frame that are not present in the predicate?argument structure.
Similarly to the syntactic submodel, we trained the global SRL model using the online passive?
aggressive algorithm.
The cost function ?
was defined as the number of incorrect links in the predicate?argument structure.
The number of it- erations was 20 and the regularization parameter C was 0.01.
Interestingly, we noted that the global SRL model outperformed the pipeline even when no global features were added.
This shows that the global learning model can correct label bias prob- lems introduced by the pipeline architecture.
4 Syntactic?Semantic Integration Our baseline joint feature representation contained only three features: the log probability of the syn- tactic tree and the log probability of the semantic structure according to the pipeline and the global model, respectively.
This model was trained on the complete training set using cross-validation.
The probabilities were obtained using the multinomial logistic function (?softmax?).
We carried out an initial experiment with a more complex joint feature representation, but failed to improve over the baseline.
Time prevented us from exploring this direction conclusively.
5 Results The submitted results on the development and test corpora are presented in the upper part of Table 3.
After the submission deadline, we corrected a bug in the predicate identification method.
This re- sulted in improved results shown in the lower part.
Corpus Syn acc Sem F1 Macro F1 Development 88.47 80.80 84.66 Test WSJ 90.13 81.75 85.95 Test Brown 82.81 69.06 75.95 Test WSJ + Brown 89.32 80.37 84.86 Development 88.47 81.86 85.17 Test WSJ 90.13 83.75 86.61 Test Brown 82.84 69.85 76.34 Test WSJ + Brown 89.32 81.65 85.49 Table 3: Results.
5.1 Syntactic Results Table 4 shows the effect of adding second-order features to the parser in terms of accuracy as well as training and parsing time on a Mac Pro, 3.2 GHz.
The training times were measured on the complete training set and the parsing time and ac- curacies on the development set.
Similarly to Car- reras (2007), we see that these features have a very large impact on parsing accuracy, but also that the parser pays dearly in terms of efficiency as the search complexity increases fromO(n3) toO(n4).
186 Since the low efficiency of the second-order parser restricts its use to batch applications, we see an in- teresting research direction to find suitable com- promises between the two approaches, for instance by sacrificing the exact search procedure.
System Training Parse Labeled Unlabeled 1st order 65 min 28 sec 85.78 89.51 2nd order 60 hours 34 min 88.33 91.43 Table 4: Impact of second-order features.
Table 5 shows the dependency types most af- fected by the addition of second-order features to the parser when ordered by the increase in F1.
As can be seen, they are all verb adjunct categories, which demonstrates the effect of grandchild fea- tures on PP attachment and labeling.
Label ?R ?P ?F 1 TMP 14.7 12.9 13.9 DTV 0 19.9 10.5 LOC 7.8 12.3 9.9 PRP 12.4 6.7 9.6 DIR 5.9 7.2 6.5 Table 5: Labels affected by second-order features.
5.2 Semantic Results To assess the effect of the components in the se- mantic submodel, we tested their performance on the top-scoring parses from the syntactic model.
Table 6 shows the results.
The baseline system consists of the SRL pipeline only (P).
Adding lin- guistic constraints (C) results in a more precision- oriented system with slightly lower recall, but sig- nificantly higher F1.
Even higher performance is obtained when adding the global SRL model (G).
System P R F1 P 80.74 77.98 79.33 P+C 82.42 77.66 79.97 P+C+G 83.64 78.14 80.40 Table 6: SRL results on the top-scoring parse trees.
5.3 Syntactic?Semantic Integration The final experiment concerned the integration of syntactic and semantic analysis.
In this setting, the system chooses the output that maximizes the joint syntactic?semantic score, based on the top N syntactic trees.
Table 7 shows the results on the development set.
We see that syntactic?semantic integration improves both syntactic accuracy and semantic F1.
This holds for the constraint-based SRL system as well as for the full system.
Sem model N Syn acc Sem F1 Macro F1 P+C 1 88.33 79.97 84.17 P+C 16 88.42 80.42 84.44 P+C+G 1 88.33 80.40 84.39 P+C+G 16 88.47 80.80 84.66 Table 7: Syntactic?semantic integration.
6 Conclusion We have described a system1 for syntactic and se- mantic dependency analysis based on PropBank and NomBank, and detailed the implementation of its subsystems.
Crucial to our success was the high performance of the syntactic parser, which achieved a high accuracy.
In addition, we recon- firmed the benefits of global inference in semantic analysis: both constraint-based and learning-based methods resulted in improvements over a baseline.
Finally, we showed that integration of syntactic and semantic analysis is beneficial for both sub- tasks.
We hope that this shared task will spur fur- ther research that leads to new feature representa- tions and search procedures to handle the problem of joint syntactic and semantic analysis.
References Carreras, Xavier.
Experiments with a higher-order pro- jective dependency parser.
In Proceedings of CoNLL.
Chickering, David M., Dan Geiger, and David Heckerman.
Learning Bayesian networks: The combination of knowledge and statistical data.
Technical Report MSR- TR-94-09, Microsoft Research.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev- Schwartz, and Yoram Singer.
Online passive- aggressive algorithms.
JMLR, 2006(7):551?585.
Eisner, Jason M. 1996.
Three new probabilistic models for dependency parsing: An exploration.
Lin, Chih-Jen, Ruby C. Weng, and S. Sathiya Keerthi.
Trust region Newton method for large-scale logistic regres- sion.
JMLR, 2008(9):627?650.
McDonald, Ryan and Fernando Pereira.
Online learn- ing of approximate dependency parsing algorithms.
In Proceedings of EACL-2006.
Nivre, Joakim and Jens Nilsson.
Pseudo-projective dependency parsing.
In Proceedings of ACL-2005.
Surdeanu, Mihai, Richard Johansson, Adam Meyers, Llu?s M?rquez, and Joakim Nivre.
The CoNLL?2008 shared task on joint parsing of syntactic and semantic de- pendencies.
In Proceedings of CoNLL?2008.
Taskar, Ben, Carlos Guestrin, and Daphne Koller.
Max-margin Markov networks.
In Proceedings of NIPS.
Toutanova, Kristina, Aria Haghighi, and Christopher D. Man- ning.
Joint learning improves semantic role label- ing.
In Proceedings of ACL-2005.
1Our system is freely available for download at http://nlp.cs.lth.se/lth_srl.
