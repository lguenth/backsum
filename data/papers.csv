paper_id,paper_title,paper_path,paper_text,summary_ids,summary_paths,annotation_paths
A00-2018,A Maximum-Entropy-Inspired Parser *,../data/papers/A00-2018.xml,"{'0': 'A Maximum-Entropy-Inspired Parser *', '1': 'We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.', '2': 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', '3': 'The major technical innovation is the use of a &quot;maximum-entropy-inspired&quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.', '4': ""We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."", '5': 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.', '6': 'This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].', '7': 'Following [5,10], our parser is based upon a probabilistic generative model.', '8': 'That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.', '9': 'The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.', '10': 'That is, the parser implements the function arg maxrp(7r s) = arg maxirp(7r, s) = arg maxrp(w).', '11': 'What fundamentally distinguishes probabilistic generative parsers is how they compute p(r), and it is to that topic we turn next.', '12': 'The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &quot;tag&quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).', '13': 'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g., whether it is a noun phrase (np), verb-phrase, etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.', '14': 'Much of the interesting work is determining what goes into H (c).', '15': 'Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c).', '16': 'In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.', '17': 'In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.', '18': 'The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15].', '19': 'The method we use follows that of [10].', '20': 'In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols.', '21': '(We assume that all terminal symbols are generated by rules of the form &quot;preterm word&quot; and we treat these as a special case.)', '22': 'For us the non-terminal symbols are those of the tree-bank, augmented by the symbols aux and auxg, which have been assigned deterministically to certain auxiliary verbs such as &quot;have&quot; or &quot;having&quot;.', '23': 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).', '24': 'M(c) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children.', '25': 'To the left of M is a sequence of one or more left labels Li (c) including the special termination symbol A, which indicates that there are no more symbols to the left, and similarly for the labels to the right, Ri(c).', '26': 'Thus an expansion e(c) looks like: The expansion is generated by guessing first M, then in order L1 through L,„.+1 (= A), and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.', '27': 'In the simplest of such models, a zeroorder Markov grammar, each label on the righthand side is generated conditioned only on / — that is, according to the distributions p(Li j1), p(M I 1), and p(Ri I 1).', '28': 'More generally, one can condition on the m previously generated labels, thereby obtaining an mth-order Markov grammar.', '29': 'So, for example, in a second-order Markov PCFG, L2 would be conditioned on L1 and M. In our complete model, of course, the probability of each label in the expansions is also conditioned on other material as specified in Equation 1, e.g., p(e t, h, H).', '30': 'Thus we would use p(L2 I L1, M, 1, t, h, H).', '31': 'Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.', '32': 'The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model, and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.', '33': 'For example, in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1, M,1, t, h, H).', '34': 'Also, remember that H is a pla,ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.', '35': 'In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17].', '36': 'A complete review of log-linear models is beyond the scope of this paper.', '37': 'Rather, we concentrate on the aspects of these models that most directly influenced the model presented here.', '38': 'To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.', '39': 'In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.', '40': 'In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.', '41': ""For example, in computing the probability of the head's pre-terminal t we might want a feature schema f (t, 1) that returns 1 if the observed pre-terminal of c = t and the label of c = 1, and zero otherwise."", '42': 'This feature is obviously composed of two sub-features, one recognizing t, the other 1.', '43': 'If both return 1, then the feature returns 1.', '44': 'Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated A.', '45': 'The function Z(H), called the partition function, is a normalizing constant (for fixed H), so the probabilities over all a sum to one.', '46': 'Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a, H) for 0 < i < j: Here go(a, H) = 11Z (H) and gi(a, H) = eAi(a,H) fi(°,11).', '47': 'The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely, one if the feature has no effect, and smaller than one if it makes the probability less likely.', '48': 'Maximum-entropy models have two benefits for a parser builder.', '49': 'First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable — just change the set of features used.', '50': 'This point is emphasized by Ratnaparkhi in discussing his parser [17).', '51': 'Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.', '52': 'This is useful if one is using a loglinear model for smoothing.', '53': 'That is, suppose we want to compute a conditional probability p(a b, c), but we are not sure that we have enough examples of the conditioning event b, c in the training corpus to ensure that the empirically obtained probability P (a I b, c) is accurate.', '54': ""The traditional way to handle this is also to compute P(a b), and perhaps P(a c) as well, and take some combination of these values as one's best estimate for p(a I b, c)."", '55': 'This method is known as &quot;deleted interpolation&quot; smoothing.', '56': 'In max-entropy models one can simply include features for all three events f1 (a, b, c), f2 (a, b), and f3(a, c) and combine them in the model according to Equation 3, or equivalently, Equation 4.', '57': 'The fact that the features are very far from independent is not a concern.', '58': 'Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', '59': 'As it stands, this last equation is pretty much content-free.', '60': 'But let us look at how it works for a particular case in our parsing scheme.', '61': 'Consider the probability distribution for choosing the pre-terminal for the head of a constituent.', '62': 'In Equation 1 we wrote this as p(t I 1, H).', '63': ""As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &quot;before&quot;), and the label of the grandparent of c (la)."", '64': 'That is, we wish to compute p(t 1, lp, tp, lb, lg, hp).', '65': ""We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history."", '66': 'In many cases this is clearly warranted.', '67': 'For example, it does not seem to make much sense to condition on, say, hp without first conditioning on ti,.', '68': 'In other cases, however, we seem to be conditioning on apples and oranges, so to speak.', '69': ""For example, one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling, or the grandparent label."", '70': 'One way to do this is to modify the simple version shown in Equation 6 to allow this: Note the changes to the last three terms in Equation 7.', '71': 'Rather than conditioning each term on the previous ones, they are now conditioned only on those aspects of the history that seem most relevant.', '72': 'The hope is that by doing this we will have less difficulty with the splitting of conditioning events, and thus somewhat less difficulty with sparse data.', '73': 'We make one more point on the connection of Equation 7 to a maximum entropy formulation.', '74': 'Suppose we were, in fact, going to compute a true maximum entropy model based upon the features used in Equation 7, Ii (t,1), f2(t,1,1p), f3(t,1,lp) .', '75': '...', '76': 'This requires finding the appropriate Ais for Equation 3, which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.', '77': 'With no prior knowledge of values for the Ai one traditionally starts with Ai = 0, this being a neutral assumption that the feature has neither a positive nor negative impact on the probability in question.', '78': 'With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.', '79': 'We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.', '80': '(Our experience is that rather than requiring 50 or so iterations, three suffice.)', '81': 'Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7.', '82': 'The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).', '83': 'In the simple (content-free) form (Equation 6), it is clear that Z(H) = 1.', '84': 'In the more interesting version, Equation 7, this is not true in general, but one would not expect it to differ much from one, and we assume that as long as we are not publishing the raw probabilities (as we would be doing, for example, in publishing perplexity results) the difference from one should be unimportant.', '85': 'As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.', '86': 'Naturally, the distributions required by Equation 7 cannot be used without smoothing.', '87': ""In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17]."", '88': 'While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation.', '89': '(Actually, we use a minor variant described in [4].)', '90': 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.', '91': 'As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', '92': 'For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.', '93': 'This allows the second pass to see expansions not present in the training corpus.', '94': 'We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.', '95': 'We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.', '96': 'As noted above, the probability model uses five smoothed probability distributions, one each for Li, M,Ri,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.', '97': ""The other four equations can be found in a longer version of this paper available on the author's website (www.cs.brown.eduhiec)."", '98': 'L and R are conditioned on three previous labels so we are using a third-order Markov grammar.', '99': 'Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.', '100': 'This is due to the importance of this factor in parsing, as noted in, e.g., [14].', '101': 'In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).', '102': 'Performance on the test corpus is measured using the standard measures from [5,9,10,17].', '103': 'In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with < 2 cross brackets (2CB).', '104': 'Again as standard, we take separate measurements for all sentences of length < 40 and all sentences of length < 100.', '105': 'Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work.', '106': 'As noted in [5], these definitions typically give results about 0.4% higher than the more obvious ones.', '107': 'The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.', '108': 'As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.', '109': ""Looking in particular at the precision and recall figures, the new parser's give us a 13% error reduction over the best of the previous work, Co1199 [9]."", '110': 'In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel.', '111': 'However, we do not think this aspect is the sole or even the most important reason for its comparative success.', '112': 'Here we list what we believe to be the most significant contributions and give some experimental results on how well the program behaves without them.', '113': 'We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.', '114': 'That parser, as stated in Figure 1, achieves an average precision/recall of 87.5.', '115': 'As noted in [5], that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.', '116': 'This is as opposed to the &quot;Markovgrammar&quot; approach used in the current parser.', '117': 'Also, the earlier parser uses two techniques not employed in the current parser.', '118': 'First, it uses a clustering scheme on words to give the system a &quot;soft&quot; clustering of heads and sub-heads.', '119': '(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)', '120': 'Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.', '121': 'Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.', '122': 'In this section we evaluate the effects of the various changes we have made by running various versions of our current program.', '123': 'To avoid repeated evaluations based upon the testing corpus, here our evaluation is based upon sentences of length < 40 from the development corpus.', '124': 'We note here that this corpus is somewhat more difficult than the &quot;official&quot; test corpus.', '125': 'For example, the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', '126': 'This is indicated in Figure 2, where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.', '127': 'This is in accord with our experience that developmentcorpus results are from 0.3% to 0.5% lower than those obtained on the test corpus.', '128': 'The model labeled &quot;Old&quot; attempts to recreate the Char97 system using the current program.', '129': 'It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar.', '130': 'This parser achieves an average precision/recall of 86.2%.', '131': 'This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus.', '132': 'Between the Old model and the Best model, Figure 2 gives precision/recall measurements for several different versions of our parser.', '133': 'One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.', '134': 'As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question.', '135': ""In contrast, the current parser first guesses the head's pre-terminal, then the head, and then the expansion."", '136': 'It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].', '137': ""However, Collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader."", '138': 'Indeed, it was lost on the present author until he went back after the fact and found it there.', '139': 'In Figure 2 we show that this one factor improves performance by nearly 2%.', '140': 'It may not be obvious why this should make so great a difference, since most words are effectively unambiguous.', '141': '(For example, part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)', '142': 'We believe that two factors contribute to this performance gain.', '143': 'The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).', '144': 'This quantity is a relatively intuitive one (as, for example, it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probability upon which all smoothing of p(h) is based.', '145': 'This one &quot;fix&quot; makes slightly over a percent difference in the results.', '146': 'The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.', '147': 'For example, when we first guess the lexical head we can move from computing p(r I 1,1p, h) to p(r I /, t, /p, h).', '148': 'So, e.g., even if the word &quot;conflating&quot; does not appear in the training corpus (and it does not), the &quot;ng&quot; ending allows our program to guess with relative security that the word has the vbg pre-terminal, and thus the probability of various rule expansions can be considerable sharpened.', '149': 'For example, the tree-bank PCFG probability of the rule &quot;VP --+ vbg np&quot; is 0.0145, whereas once we condition on the fact that the lexical head is a vbg we get a probability of 0.214.', '150': 'The second modification is the explicit marking of noun and verb-phrase coordination.', '151': 'We have already noted the importance of conditioning on the parent label /p.', '152': 'So, for example, information about an np is conditioned on the parent — e.g., an s, vp, pp, etc.', '153': 'Note that when an np is part of an np coordinate structure the parent will itself be an np, and similarly for a vp.', '154': 'But nps and vps can occur with np and vp parents in non-coordinate structures as well.', '155': 'For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.', '156': 'Note that the subordinate vp has a vp parent.', '157': 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.', '158': 'A vp coordinate structure is defined here as a constituent with two or more vp children, one or more of the constituents comma, cc, conjp (conjunctive phrase), and nothing else; coordinate np phrases are defined similarly.', '159': 'Something very much like this is done in [15].', '160': 'As shown in Figure 2, conditioning on this information gives a 0.6% improvement.', '161': ""We believe that this is mostly due to improvements in guessing the sub-constituent's pre-terminal and head."", '162': 'Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.', '163': 'Next we add the less obvious conditioning events noted in our previous discussion of the final model — grandparent label lg and left sibling label /b.', '164': ""When we do so using our maximum-entropy-inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 on the line labeled &quot;MaxEnt-Inspired'."", '165': 'Note that we also tried including this information using a standard deleted-interpolation model.', '166': 'The results here are shown in the line &quot;Standard Interpolation&quot;.', '167': 'Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model.', '168': 'Indeed, the resulting performance is worse than not using this information at all.', '169': 'Up to this point all the models considered in this section are tree-bank grammar models.', '170': 'That is, the PCFG grammar rules are read directly off the training corpus.', '171': 'As already noted, our best model uses a Markov-grammar approach.', '172': 'As one can see in Figure 2, a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.', '173': 'However, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser.', '174': 'We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.', '175': 'This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].', '176': 'That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.', '177': 'The results reported here disprove this conjecture.', '178': 'The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.', '179': 'Indeed, it may be that adding this new parser to the mix may yield still higher results.', '180': ""From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head."", '181': 'Neither of these results were anticipated at the start of this research.', '182': 'As noted above, the main methodological innovation presented here is our &quot;maximumentropy-inspired&quot; model for conditioning and smoothing.', '183': 'Two aspects of this model deserve some comment.', '184': 'The first is the slight, but important, improvement achieved by using this model over conventional deleted interpolation, as indicated in Figure 2.', '185': 'We expect that as we experiment with other, more semantic conditioning information, the importance of this aspect of the model will increase.', '186': 'More important in our eyes, though, is the flexibility of the maximum-entropy-inspired model.', '187': 'Though in some respects not quite as flexible as true maximum entropy, it is much simpler and, in our estimation, has benefits when it comes to smoothing.', '188': 'Ultimately it is this flexibility that let us try the various conditioning events, to move on to a Markov grammar approach, and to try several Markov grammars of different orders, without significant programming.', '189': 'Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.', '190': 'It is to this project that our future parsing work will be devoted.'}","['A00-2018_sweta', 'A00-2018_akanksha', 'A00-2018_vardha']","['../data/summaries/A00-2018_sweta.txt', '../data/summaries/A00-2018_akanksha.txt', '../data/summaries/A00-2018_vardha.txt']","['../data/tba/A00-2018_sweta.json', '../data/tba/A00-2018_akanksha.json', '../data/tba/A00-2018_vardha.json']"
A00-2030,A Novel Use of Statistical Parsing to Extract Information from Text,../data/papers/A00-2030.xml,"{'0': 'A Novel Use of Statistical Parsing to Extract Information from Text', '1': 'Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.', '2': 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', '3': 'Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.', '4': 'Yet, relatively few have embedded one of these algorithms in a task.', '5': 'Chiba, (1999) was able to use such a parsing algorithm to reduce perplexity with the long term goal of improved speech recognition.', '6': 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.', '7': 'The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.', '8': 'Several technical challenges confronted us and were solved: TREEBANK on Wall Street Journal adequately train the algorithm for New York Times newswire, which includes dozens of newspapers?', '9': 'Manually creating sourcespecific training data for syntax was not required.', '10': 'Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.', '11': 'We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).', '12': 'The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).', '13': 'For each organization in an article, one must identify all of its names as used in the article, its type (corporation, government, or other), and any significant description of it.', '14': ""For each person, one must find all of the person's names within the document, his/her type (civilian or military), and any significant descriptions (e.g., titles)."", '15': 'For each location, one must also give its type (city, province, county, body of water, etc.).', '16': 'For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.', '17': 'For the following example, the template relation in Figure 2 was to be generated: &quot;Donald M. Goldstein, a historian at the University of Pittsburgh who helped write...&quot;', '18': 'Almost all approaches to information extraction — even at the sentence level — are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.', '19': 'Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.', '20': 'However, pipelined architectures suffer from a serious disadvantage: errors accumulate as they propagate through the pipeline.', '21': 'For example, an error made during part-of-speech-tagging may cause a future error in syntactic analysis, which may in turn cause a semantic interpretation failure.', '22': 'There is no opportunity for a later stage, such as parsing, to influence or correct an earlier stage such as part-of-speech tagging.', '23': 'An integrated model can limit the propagation of errors by making all decisions jointly.', '24': 'For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.', '25': 'A second consideration influenced our decision toward an integrated model.', '26': 'We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.', '27': '1997).', '28': 'Finally, our newly constructed parser, like that of (Collins 1997), was based on a generative statistical model.', '29': 'Thus, each component of what would be the first three stages of our pipeline was based on the same general class of statistical model.', '30': 'Although each model differed in its detailed probability structure, we believed that the essential elements of all three models could be generalized in a single probability model.', '31': 'If the single generalized model could then be extended to semantic analysis, all necessary sentence level processing would be contained in that model.', '32': 'Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties — especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs — would also benefit semantic analysis.', '33': 'Our integrated model represents syntax and semantics jointly using augmented parse trees.', '34': 'In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.', '35': 'An example of an augmented parse tree is shown in Figure 3.', '36': 'The five key facts in this example are: Here, each &quot;reportable&quot; name or description is identified by a &quot;-r&quot; suffix attached to its semantic label.', '37': 'For example, &quot;per-r&quot; identifies &quot;Nance&quot; as a named person, and &quot;per-desc-r&quot; identifies &quot;a paid consultant to ABC News&quot; as a person description.', '38': 'Other labels indicate relations among entities.', '39': 'For example, the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case, because the argument does not connect directly to the relation, the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.', '40': 'Further details are discussed in the section Tree Augmentation.', '41': 'To train our integrated model, we required a large corpus of augmented parse trees.', '42': 'Since it was known that the MUC-7 evaluation data would be drawn from a variety of newswire sources, and that the articles would focus on rocket launches, it was important that our training corpus be drawn from similar sources and that it cover similar events.', '43': 'Thus, we did not consider simply adding semantic labels to the existing Penn TREEBANK, which is drawn from a single source — the Wall Street Journal — and is impoverished in articles about rocket launches.', '44': 'Instead, we applied an information retrieval system to select a large number of articles from the desired sources, yielding a corpus rich in the desired types of events.', '45': 'The retrieved articles would then be annotated with augmented tree structures to serve as a training corpus.', '46': 'Initially, we tried to annotate the training corpus by hand marking, for each sentence, the entire augmented tree.', '47': 'It soon became painfully obvious that this task could not be performed in the available time.', '48': 'Our annotation staff found syntactic analysis particularly complex and slow going.', '49': 'By necessity, we adopted the strategy of hand marking only the semantics.', '50': 'Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.', '51': 'To produce a corpus of augmented parse trees, we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus, now annotated with complete augmented trees like that in Figure 3.', '52': 'In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.', '53': 'For each sentence, combining these two sources involved five steps.', '54': 'These steps are given below:', '55': 'syntactic modifier of the other, the inserted node serves to indicate the relation as well as the argument.', '56': 'For example, in the phrase &quot;Lt. Cmdr.', '57': 'David Edwin Lewis,&quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', '58': 'Whenever a relation involves an entity that is not a direct descendant of that relation in the parse tree, semantic pointer labels are attached to all of the intermediate nodes.', '59': 'These labels serve to form a continuous chain between the relation and its argument.', '60': 'In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).', '61': 'The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.', '62': 'For each constituent, the head is generated first, followed by the modifiers, which are generated from the head outward.', '63': 'Head words, along with their part-of-speech tags and features, are generated for each modifier as soon as the modifier is created.', '64': 'Word features are introduced primarily to help with unknown words, as in (Weischedel et al. 1993).', '65': 'We illustrate the generation process by walking through a few of the steps of the parse shown in Figure 3.', '66': 'At each step in the process, a choice is made from a statistical distribution, with the probability of each possible selection dependent on particular features of previously generated elements.', '67': 'We pick up the derivation just after the topmost S and its head word, said, have been produced.', '68': 'The next steps are to generate in order: In this case, there are none.', '69': '8.', '70': 'Post-modifier constituents for the PER/NP.', '71': 'First a comma, then an SBAR structure, and then a second comma are each generated in turn.', '72': 'This generation process is continued until the entire tree has been produced.', '73': 'We now briefly summarize the probability structure of the model.', '74': 'The categories for head constituents, cl„ are predicted based solely on the category of the parent node, cp: Modifier constituent categories, cm, are predicted based on their parent node, cp, the head constituent of their parent node, chp, the previously generated modifier, c„,_1, and the head word of their parent, wp.', '75': 'Separate probabilities are maintained for left (pre) and right (post) modifiers: Part-of-speech tags, t,,, for modifiers are predicted based on the modifier, cm, the partof-speech tag of the head word, th, and the head word itself, wh: Head words, w„„ for modifiers are predicted based on the modifier, cm, the part-of-speech tag of the modifier word , t„„ the part-ofspeech tag of the head word , th, and the head word itself, wh: lAwmicm,tm,th,wh), e.g.', '76': 'Finally, word features, fm, for modifiers are predicted based on the modifier, cm, the partof-speech tag of the modifier word , t„„ the part-of-speech tag of the head word th, the head word itself, wh, and whether or not the modifier head word, w„„ is known or unknown.', '77': 'The probability of a complete tree is the product of the probabilities of generating each element in the tree.', '78': 'If we generalize the tree components (constituent labels, words, tags, etc.) and treat them all as simply elements, e, and treat all the conditioning factors as the history, h, we can write:', '79': 'Maximum likelihood estimates for the model probabilities can be obtained by observing frequencies in the training corpus.', '80': 'However, because these estimates are too sparse to be relied upon, we use interpolated estimates consisting of mixtures of successively lowerorder estimates (as in Placeway et al. 1993).', '81': 'For modifier constituents, the mixture components are: For part-of-speech tags, the mixture components are: Finally, for word features, the mixture components are:', '82': 'Given a sentence to be analyzed, the search program must find the most likely semantic and syntactic interpretation.', '83': 'More precisely, it must find the most likely augmented parse tree.', '84': 'Although mathematically the model predicts tree elements in a top-down fashion, we search the space bottom-up using a chartbased search.', '85': 'The search is kept tractable through a combination of CKY-style dynamic programming and pruning of low probability elements.', '86': 'Whenever two or more constituents are equivalent relative to all possible later parsing decisions, we apply dynamic programming, keeping only the most likely constituent in the chart.', '87': 'Two constituents are considered equivalent if: threshold of the highest scoring constituent are maintained; all others are pruned.', '88': 'For purposes of pruning, and only for purposes of pruning, the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman, 1997).', '89': 'We can think of this prior probability as an estimate of the probability of generating a subtree with the constituent category, starting at the topmost node.', '90': 'Thus, the scores used in pruning can be considered as the product of: 1.', '91': 'The probability of generating a constituent of the specified category, starting at the topmost node.', '92': '2.', '93': 'The probability of generating the structure beneath that constituent, having already generated a constituent of that category.', '94': 'Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.', '95': 'The semantics — that is, the entities and relations — can then be directly extracted from these sentential trees.', '96': 'Our system for MUC-7 consisted of the sentential model described in this paper, coupled with a simple probability model for cross-sentence merging.', '97': 'The evaluation results are summarized in Table 1.', '98': 'In both Template Entity (TE) and Template Relation (TR), our system finished in second place among all entrants.', '99': 'Nearly all of the work was done by the sentential model; disabling the cross-sentence model entirely reduced our overall F-Score by only 2 points.', '100': 'Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.', '101': 'We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97), and evaluated name finding accuracy on the MUC7 named entity test.', '102': 'The results are summarized in Table 2.', '103': 'While performance did not quite match the best previously reported results for any of these three tasks, we were pleased to observe that the scores were at or near state-of-the-art levels for all cases.', '104': 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', '105': 'A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.', '106': 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', '107': 'The semantic training corpus was produced by students according to a simple set of guidelines.', '108': 'This simple semantic annotation was the only source of task knowledge used to configure the model.', '109': 'The work reported here was supported in part by the Defense Advanced Research Projects Agency.', '110': 'Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062, F30602-97-C-0096, and 4132-BBN-001.', '111': 'The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government.', '112': 'We thank Michael Collins of the University of Pennsylvania for his valuable suggestions.'}","['A00-2030_sweta', 'A00-2030_aakansha', 'A00-2030_vardha']","['../data/summaries/A00-2030_sweta.txt', '../data/summaries/A00-2030_aakansha.txt', '../data/summaries/A00-2030_vardha.txt']","['../data/tba/A00-2030_sweta.json', '../data/tba/A00-2030_aakansha.json', '../data/tba/A00-2030_vardha.json']"
A97-1014,An Annotation Scheme for Free Word Order Languages,../data/papers/A97-1014.xml,"{'0': 'An Annotation Scheme for Free Word Order Languages', '1': 'We describe an annotation scheme and a tool developed for creating linguistically annotated corpora for non-configurational languages.', '2': 'Since the requirements for such a formalism differ from those posited for configurational languages, several features have been added, influencing the architecture of the scheme.', '3': 'The resulting scheme reflects a stratificational notion of language, and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.', '4': ""The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."", '5': 'In particular, we focus on several methodological issues concerning the annotation of non-configurational languages.', '6': 'In section 2, we examine the appropriateness of existing annotation schemes.', '7': 'On the basis of these considerations, we formulate several additional requirements.', '8': 'A formalism complying with these requirements is described in section 3.', '9': 'Section 4 deals with the treatment of selected phenomena.', '10': 'For a description of the annotation tool see section 5.', '11': 'Combining raw language data with linguistic information offers a promising basis for the development of new efficient and robust NLP methods.', '12': 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', '13': 'The data-drivenness of this approach presents a clear advantage over the traditional, idealised notion of competence grammar.', '14': 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.', '15': 'Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.', '16': 'Theory-independence: Annotations should not be influenced by theory-specific considerations.', '17': 'Nevertheless, different theory-specific representations shall be recoverable from the annotation, cf.', '18': '(Marcus et al., 1994).', '19': 'Multi-stratal representation: Clear separation of different description levels is desirable.', '20': 'Data-drivenness: The scheme must provide representational means for all phenomena occurring in texts.', '21': 'Disambiguation is based on human processing skills (cf.', '22': '(Marcus et. al., 1994), (Sampson, 1995), (Black et. al.', '23': ', 1996)).', '24': 'The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.', '25': 'The underlying argument SirlteilITC is not represented directly, but can be recovered from the tree and trace-filler annotations.', '26': 'Syntactic category is encoded in node labels.', '27': 'Grammatical functions constitute a complex label system (cf.', '28': '(Bies et al., 1995), (Sampson, 1995)).', '29': 'Part-of-Speech is annotated at word level.', '30': 'Thus the context-free constituent backbone plays a pivotal role in the annotation scheme.', '31': 'Due to the substantial differences between existing models of constituent structure, the question arises of how the theory independencf requirement, can be satisfied.', '32': 'At, this point the importance of the underlying argument structure is emphasised (cf.', '33': '(Lehmann et al., 1996), (Marcus et al., 1994), (Sampson, 1995)).', '34': 'Treebanks of the format, described in the above section have been designed for English.', '35': 'Therefore, the solutions they offer are not always optimal for other language types.', '36': 'As for free word order languages, the following features may cause problems: sition between the two poles.', '37': 'In light of these facts, serious difficulties can be expected arising from the structural component of the existing formalisms.', '38': 'Due to the frequency of discontinuous constituents in non-configurational languages, the filler-trace mechanism would be used very often, yielding syntactic trees fairly different from the underlying predicate-argument structures.', '39': ""Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes."", '40': 'This hybrid representation makes the structure less transparent, and therefore more difficult to annotate.', '41': 'Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.', '42': 'Since most methods of handling discontinuous constituents make the formalism more powerful, the efficiency of processing deteriorates, too.', '43': 'An alternative solution is to make argument structure the main structural component of the formalism.', '44': 'This assumption underlies a growing number of recent syntactic theories which give up the context-free constituent backbone, cf.', '45': '(McCawley, 1987), (Dowty, 1989), (Reape, 1993), (Kathol and Pollard, 1995).', '46': 'These approaches provide an adequate explanation for several issues problematic for phrase-structure grammars (clause union, extraposition, diverse second-position phenomena).', '47': 'Argument structure can be represented in terms of unordered trees (with crossing branches).', '48': ""In order to reduce their ambiguity potential, rather simple, 'flat' trees should be employed, while more information can be expressed by a rich system of function labels."", '49': 'Furthermore, the required theory-independence means that the form of syntactic trees should not reflect theory-specific assumptions, e.g. every syntactic structure has a unique head.', '50': 'Thus, notions such as head should be distinguished at the level of syntactic functions rather than structures.', '51': 'This requirement speaks against the traditional sort of dependency trees, in which heads a,re represented as non-terminal nodes, cf.', '52': '(Hudson, 1984).', '53': ""A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen, dais er 'vein!"", '54': 'Such a word order independent representation has the advantage of all structural information being encoded in a single data structure.', '55': ""A uniform representation of local and non-local dependencies makes the structure more transparent'."", '56': ""We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node."", '57': 'Argument structure, represented in terms of unordered trees.', '58': 'Grammatical functions, encoded in edge labels, e.g.', '59': 'SB (subject), MO (modifier), HD (head).', '60': 'Syntactic categories, expressed by category labels assigned to non-terminal nodes and by part-of-speech tags assigned to terminals.', '61': 'A structure for (2) is shown in fig.', '62': '2.', '63': ""(2) schade, daB kein Arzt anwesend ist, der pity that no doctor present is who sich a.uskennt is competent 'Pity that no competent doctor is here' Note that the root node does not have a head descendant (HD) as the sentence is a predicative construction consisting of a subject (SB) and a predicate (PD) without a copula."", '64': ""The subject is itself a sentence in which the copula (zA) does occur and is assigned the tag HD'."", '65': 'The tree resembles traditional constituent structures.', '66': 'The difference is its word order independence: structural units (&quot;phrases&quot;) need not be contiguous substrings.', '67': 'For instance, the extraposed relative clause (RC) is still treated as part of the subject NP.', '68': 'As the annotation scheme does not distinguish different bar levels or any similar intermediate categories, only a small set of node labels is needed (currently 16 tags, S, NP, AP ...).', '69': 'Due to the rudimentary character of the argument structure representations, a great deal of information has to be expressed by grammatical functions.', '70': ""Their further classification must reflect different kinds of linguistic information: morphology (e.g., case, inflection), category, dependency type (complementation vs. modification), thematic role, etc.'"", '71': 'However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.', '72': 'In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.', '73': 'While in the first phase each annotator has to annotate structures as well as categories and functions, the refinement call be done separately for each representation level.', '74': 'During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.', '75': 'Modifiers are assigned the label MO (further classification with respect to thematic roles is planned).', '76': 'Separate labels are defined for dependencies that do not fit the complement/modifier dichotomy, e.g., pre- (GL) and postnominal genitives (GR).', '77': 'Headed and non-headed structures are distinguished by the presence or absence of a branch labeled HD.', '78': 'Morphological information: Another set of labels represents morphological information.', '79': 'PM stands for morphological particle, a label for German infinitival Z7t and superlative am.', '80': 'Separable verb prefixes are labeled SVP.', '81': 'During the second annotation stage, the annotation is enriched with information about thematic roles, quantifier scope and anaphoric reference.', '82': 'As already mentioned, this is done separately for each of the three information areas.', '83': 'A phrase or a lexical item can perform multiple functions in a sentence.', '84': 'Consider (qui verbs where the subject of the infinitival VP is not realised syntactically, but co-referent with the subject or object. of the matrix equi verb: (3) er bat mich zu kommen he asked me to come (mich is the understood subject. of kommt,n).', '85': 'In such cases, an additional edge is drawn from the embedded VP node to the controller, thus changing the syntactic tree into a graph.', '86': 'We call such additional edges secondary links and represent them as dotted lines, see fig.', '87': '4, showing the structure of (3).', '88': 'As theory-independence is one of our objectives, the annotation scheme incorporates a number of widely accepted linguistic analyses, especially in the area of verbal, adverbial and adjectival syntax.', '89': ""However, some other standard analysts turn out to be problematic, mainly due to the partial, idealised character of competence grammars, which often marginalise or ignore such important. phenomena. as 'deficient' (e.g. headless) constructions, appositions, temporal expressions, etc."", '90': 'In the following paragraphs, we give annotations for a number of such phenomena..', '91': 'Most linguistic theories treat NPs as structures headed by a unique lexical item (noun).', '92': 'However, this idealised model needs several additional assumptions in order to account for such important phenomena as complex nominal NP components (cf.', '93': '(4)) or nominalised adjectives (cf.', '94': '(5)).', '95': 'In (4), different theories make different headedness predictions.', '96': 'In (5), either a lexical nominalisation rule for the adjective Gliickliche is stipulated, or the existence of an empty nominal head.', '97': 'Moreover, the so-called DP analysis views the article der as the head of the phrase.', '98': 'Further differences concern the attachment of the degree modifier sehr.', '99': 'Because of the intended theory-independence of the scheme, we annotate only the common minimum.', '100': 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', '101': 'All components of this kernel are assigned the label NK and treated as sibling nodes.', '102': ""The difference between the particular NK's lies in the positional and part-of-speech information, which is also sufficient to recover theory-specific structures from our `underspecified' representations."", '103': ""For instance, the first, determiner among the NK's can be treated as the specifier of the phrase."", '104': 'The head of the phrase can be determined in a similar way according to theory-specific assumptions.', '105': 'In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).', '106': ""They are all treated as siblings of NK's regardless of their position (in situ or extraposed)."", '107': 'Adjunct attachment often gives rise to structural ambiguities or structural uncertainty.', '108': 'However, full or partial disambiguation takes place in context, and the annotators do not consider unrealistic readings.', '109': 'In addition, we have adopted a simple convention for those cases in which context information is insufficient, for total disambiguation: the highest possible attachment, site is chosen.', '110': 'A similar convention ha.s been adopted for constructions in which scope ambiguities have syntactic effects but a one-to-one correspondence between scope and attachment. does not seem reasonable, cf. focus particles such as only or also.', '111': 'If the scope of such a word does not directly correspond to a tree node, the word is attached to the lowest node dominating all subconstituents appearing in its scope.', '112': 'A problem for the rudimentary argument. structure representations is the use of incomplete structures in natural language, i.e. phenomena such as coordination and ellipsis.', '113': 'Since a precise structural description of non-constituent coordination would require a. rich inventory of incomplete phrase types, we have agreed on a sort of unde.rspe.cified representations: the coordinated units are assigned structures in which missing lexical material is not represented at the level of primary links.', '114': 'Fig.', '115': ""3 shows the representation of the sentence: (6) sic wurde von preuBischen Truppen besetzt she was by Prussian troops occupied und 1887 dem preuliischen Staat angegliedert and 1887 to-the Prussian state incorporated 'it was occupied by Prussian troops and incorporated into Prussia in 1887' The category of the coordination is labeled CVP here, where C stands for coordination, and VP for the actual category."", '116': ""This extra marking makes it easy to distinguish between 'normal' and coordinated categories."", '117': 'Multiple coordination as well as enumerations are annotated in the same way.', '118': 'An explicit coordinating conjunction need not be present.', '119': 'Structure-sharing is expressed using secondary links.', '120': 'The development of linguistically interpreted corpora presents a laborious and time-consuming task.', '121': 'In order to make the annotation process more efficient, extra effort has been put. into the development of an annotation tool.', '122': 'The tool supports immediate graphical feedback and automatic error checking.', '123': 'Since our scheme permits crossing edges, visualisation as bracketing and indentation would be insufficient..', '124': 'Instead, the complete structure should be represented.', '125': 'The tool should also permit a convenient handling of node and edge labels.', '126': 'In particular, variable tagsets and label collections should be allowed.', '127': 'As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.', '128': 'In the first phase, the main functionality for building and displaying unordered trees is supplied.', '129': 'In the second phase, secondary links and additional structural functions are supported.', '130': 'The implementation of the first phase as described in the following paragraphs is completed.', '131': 'As keyboard input, is more efficient than mouse input (cf.', '132': '(Lehmann et al., 1996)) most effort, has been put in developing an efficient keyboard interface.', '133': 'Menus are supported as a useful way of getting help on commands and labels.', '134': 'In addition to pure annotation, we can attach comments to structures.', '135': 'Figure 1 shows a screen dump of the tool.', '136': 'The largest part of the window contains the graphical representation of the structure being annotated.', '137': 'The following commands are available: The three tagsets used by the annotation tool (for words, phrases, and edges) are variable and are stored together with the corpus.', '138': 'This allows easy modification if needed.', '139': 'The tool checks the appropriateness of the input.', '140': 'For the implementation, we used Tcl/Tk Version 4.1.', '141': 'The corpus is stored in a SQL database.', '142': 'The degree of automation increases with the amount of data available.', '143': 'Sentences annotated in previous steps are used as training material for further processing.', '144': 'We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.', '145': 'This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).', '146': 'Grammatical functions are assigned using standard statistical part-of-speech tagging methods (cf. e.g.', '147': '(Cutting et al., 1992) and (Feldweg, 1995)).', '148': 'For a phrase Q with children of type T„..., Ta and grammatical functions G„...,GA., we use the lexical probabilities and the contextual (trigram) probabilities The lexical and contextual probabilities are determined separately for each type of phrase.', '149': 'During annotation, the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure, i.e., we calculate argma.x11 PQ (Ti 1Z-1, Ti.-2) PQ (Gi ITi).', '150': 'To keep the human annotator from missing errors made by the tagger, we additionally calculate the strongest competitor for each label G. If its probability is close to the winner (closeness is defined by a threshold on the quotient), the assignment is regarded as unreliable, and the annotator is asked to confirm the assignment.', '151': 'For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).', '152': 'The procedure was repeated 10 times with different. partitionings.', '153': 'The tagger rates 90% of all assignments as reliable and carries them out fully automatically.', '154': 'Accuracy for these cases is 97%.', '155': 'Most errors are due to wrong identification of the subject and different kinds of objects in sentences and VPs.', '156': 'Accuracy of the unreliable 10% of assignments is 75%, i.e., the annotator has to alter the choice in 1 of 4 cases when asked for confirmation.', '157': 'Overall accuracy of the tagger is 95%.', '158': 'Owing to the partial automation, the average annotation efficiency improves by 25% (from around 4 minutes to 3 minutes per sentence).', '159': 'As the annotation scheme described in this paper focusses on annotating argument structure rather than constituent trees, it differs from existing treebanks in several aspects.', '160': 'These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.', '161': 'The following features of our formalism are then of particular importance: The current tagset comprises only 16 node labels and 34 function tags, yet a. finely grinned classification will take place in the near future.', '162': 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', '163': 'In general, the resulting interpreted data also are closer to semantic annotation and more neutral with respect to particular syntactic theories.', '164': 'As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data., interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.', '165': 'In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.', '166': 'Syntactically annotated corpora of German have been missing until now.', '167': 'In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.', '168': 'We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.', '169': 'Since the combinatorics of syntactic constructions creates a. demand for very large corpora., efficiency of annotation is an important. criterion for the success of the developed methodology and tools.', '170': 'Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.', '171': 'Partial automation included in the current version significantly reduces the manna.1 effort.', '172': 'Its extension is subject to further investigations.', '173': 'This work is part of the DFG Sonderforschungsbereich 378 Rcsource-Adaptim Cogniiivc Proccsses, We wish to thank Tania Avgustinova, Berthold Crysmann, Lars Konieczny, Stephan Oepen, Karel Oliva., Christian Weil3 and two anonymous reviewers for their helpful comments on the content of this paper.', '174': 'We also wish to thank Robert MacIntyre and Ann Taylor for valuable discussions on the Penn Treebank annotation.', '175': 'Special thanks go t,o Oliver Plaehn, who implemented the annotation tool, and to our fearless annotators Roland Hendriks, Kerstin Klockner, Thomas Schulz, and Bernd-Paul Simon.'}","['A97-1014_vardha', 'A97-1014_sweta', 'A97-1014_swastika']","['../data/summaries/A97-1014_vardha.txt', '../data/summaries/A97-1014_sweta.txt', '../data/summaries/A97-1014_swastika.txt']","['../data/tba/A97-1014_vardha.json', '../data/tba/A97-1014_sweta.json', '../data/tba/A97-1014_swastika.json']"
C00-2123,Word Re-ordering and DP-based Search in Statistical Machine Translation,../data/papers/C00-2123.xml,"{'0': 'Word Re-ordering and DP-based Search in Statistical Machine Translation', '1': 'In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).', '2': 'Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÃ\x86cient search algorithm.', '3': 'A search restriction especially useful for the translation direction from German to English is presented.', '4': 'The experimental tests are carried out on the Verbmobil task (GermanEnglish, 8000-word vocabulary), which is a limited-domain spoken-language task.', '5': 'The goal of machine translation is the translation of a text given in some source language into a target language.', '6': 'We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.', '7': 'Pr(eI 1) is the language model of the target language, whereas Pr(fJ 1 jeI1) is the transla tion model.', '8': 'Our approach uses word-to-word dependencies between source and target words.', '9': 'The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).', '10': 'These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition.', '11': 'The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.', '12': 'A simple extension will be used to handle this problem.', '13': 'In Section 2, we brie y review our approach to statistical machine translation.', '14': 'In Section 3, we introduce our novel concept to word reordering and a DP-based search, which is especially suitable for the translation direction from German to English.', '15': 'This approach is compared to another reordering scheme presented in (Berger et al., 1996).', '16': 'In Section 4, we present the performance measures used and give translation results on the Verbmobil task.', '17': 'In this section, we brie y review our translation approach.', '18': 'In Eq.', '19': '(1), Pr(eI 1) is the language model, which is a trigram language model in this case.', '20': 'For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.', '21': 'The alignment model uses two kinds of parameters: alignment probabilities p(aj jajô\x80\x80\x801; I; J), where the probability of alignment aj for position j depends on the previous alignment position ajô\x80\x80\x801 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).', '22': 'When aligning the words in parallel texts (for language pairs like SpanishEnglish, French-English, ItalianGerman,...), we typically observe a strong localization effect.', '23': 'In many cases, there is an even stronger restriction: over large portions of the source string, the alignment is monotone.', '24': '2.1 Inverted Alignments.', '25': 'To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).', '26': 'An inverted alignment is defined as follows: inverted alignment: i ! j = bi: Target positions i are mapped to source positions bi.', '27': ""What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be 'hit' exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijeiô\x80\x80\x801 iô\x80\x80\x802) max bI 1 I Yi=1 [p(bijbiô\x80\x80\x801; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijeiô\x80\x80\x801 iô\x80\x80\x802) p(bijbiô\x80\x80\x801; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijeiô\x80\x80\x801 iô\x80\x80\x802) is the trigram language model probability."", '28': 'The inverted alignment probability p(bijbiô\x80\x80\x801; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration.', '29': 'The details are given in (Och and Ney, 2000).', '30': 'The sentence length probability p(JjI) is omitted without any loss in performance.', '31': 'For the inverted alignment probability p(bijbiô\x80\x80\x801; I; J), we drop the dependence on the target sentence length I. 2.2 Word Joining.', '32': ""The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment."", '33': 'We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.', '34': 'The word joining is done on the basis of a likelihood criterion.', '35': 'An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.', '36': ""E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment."", '37': 'In the following, we assume that this word joining has been carried out.', '38': 'Machine Translation In this case my colleague can not visit on I n d i e s e m F a l l ka nn m e i n K o l l e g e a m the v i e r t e n M a i n i c h t b e s u c h e n S i e you fourth of May Figure 1: Reordering for the German verbgroup.', '39': 'In order to handle the necessary word reordering as an optimization problem within our dynamic programming approach, we describe a solution to the traveling salesman problem (TSP) which is based on dynamic programming (Held, Karp, 1962).', '40': 'The traveling salesman problem is an optimization problem which is defined as follows: given are a set of cities S = s1; ; sn and for each pair of cities si; sj the cost dij > 0 for traveling from city si to city sj . We are looking for the shortest tour visiting all cities exactly once while starting and ending in city s1.', '41': 'A straightforward way to find the shortest tour is by trying all possible permutations of the n cities.', '42': 'The resulting algorithm has a complexity of O(n!).', '43': 'However, dynamic programming can be used to find the shortest tour in exponential time, namely in O(n22n), using the algorithm by Held and Karp.', '44': 'The approach recursively evaluates a quantity Q(C; j), where C is the set of already visited cities and sj is the last visited city.', '45': 'Subsets C of increasing cardinality c are processed.', '46': 'The algorithm works due to the fact that not all permutations of cities have to be considered explicitly.', '47': 'For a given partial hypothesis (C; j), the order in which the cities in C have been visited can be ignored (except j), only the score for the best path reaching j has to be stored.', '48': 'This algorithm can be applied to statistical machine translation.', '49': 'Using the concept of inverted alignments, we explicitly take care of the coverage constraint by introducing a coverage set C of source sentence positions that have been already processed.', '50': 'The advantage is that we can recombine search hypotheses by dynamic programming.', '51': 'The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.', '52': 'input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max Ã\x86;e00 j02Cnfjg fp(jjj0; J) p(Ã\x86) pÃ\x86(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once.', '53': 'Subsets of partial hypotheses with coverage sets C of increasing cardinality c are processed.', '54': 'For a trigram language model, the partial hypotheses are of the form (e0; e; C; j).', '55': 'e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.', '56': 'Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities.', '57': 'The following auxiliary quantity is defined: Qe0 (e; C; j) := probability of the best partial hypothesis (ei 1; bi 1), where C = fbkjk = 1; ; ig, bi = j, ei = e and eiô\x80\x80\x801 = e0.', '58': 'The type of alignment we have considered so far requires the same length for source and target sentence, i.e. I = J. Evidently, this is an unrealistic assumption, therefore we extend the concept of inverted alignments as follows: When adding a new position to the coverage set C, we might generate either Ã\x86 = 0 or Ã\x86 = 1 new target words.', '59': 'For Ã\x86 = 1, a new target language word is generated using the trigram language model p(eje0; e00).', '60': 'For Ã\x86 = 0, no new target word is generated, while an additional source sentence position is covered.', '61': 'A modified language model probability pÃ\x86(eje0; e00) is defined as follows: pÃ\x86(eje0; e00) =  1:0 if Ã\x86 = 0 p(eje0; e00) if Ã\x86 = 1 : We associate a distribution p(Ã\x86) with the two cases Ã\x86 = 0 and Ã\x86 = 1 and set p(Ã\x86 = 1) = 0:7.', '62': 'The above auxiliary quantity satisfies the following recursive DP equation: Qe0 (e; C; j) = Initial Skip Verb Final 1.', '63': 'In.', '64': '2.', '65': 'diesem 3.', '66': 'Fall.', '67': '4.', '68': 'mein 5.', '69': 'Kollege.', '70': '6.', '71': 'kann 7.nicht 8.', '72': 'besuchen 9.', '73': 'Sie.', '74': '10.', '75': 'am 11.', '76': 'vierten 12.', '77': 'Mai.', '78': '13.', '79': 'Figure 2: Order in which source positions are visited for the example given in Fig.1.', '80': '= p(fj je) max Ã\x86;e00 j02Cnfjg np(jjj0; J) p(Ã\x86) pÃ\x86(eje0; e00) Qe00 (e0;C n fjg; j 0 )o: The DP equation is evaluated recursively for each hypothesis (e0; e; C; j).', '81': 'The resulting algorithm is depicted in Table 1.', '82': 'The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.', '83': '3.1 Word ReOrdering with Verbgroup.', '84': 'Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.', '85': 'On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.', '86': 'No: Predecessor coverage set Successor coverage set 1 (f1; ;mg n flg ; l0) !', '87': '(f1; ;mg ; l) 2 (f1; ;mg n fl; l1g ; l0) !', '88': '(f1; ;mg n fl1g ; l) 3 (f1; ;mg n fl; l1; l2g ; l0) !', '89': '(f1; ;mg n fl1; l2g ; l) 4 (f1; ;m ô\x80\x80\x80 1g n fl1; l2; l3g ; l0) !', '90': '(f1; ;mg n fl1; l2; l3g ;m) German to English the monotonicity constraint is violated mainly with respect to the German verbgroup.', '91': 'In German, the verbgroup usually consists of a left and a right verbal brace, whereas in English the words of the verbgroup usually form a sequence of consecutive words.', '92': 'Our new approach, which is called quasi-monotone search, processes the source sentence monotonically, while explicitly taking into account the positions of the German verbgroup.', '93': 'A typical situation is shown in Figure 1.', '94': ""When translating the sentence monotonically from left to right, the translation of the German finite verb 'kann', which is the left verbal brace in this case, is postponed until the German noun phrase 'mein Kollege' is translated, which is the subject of the sentence."", '95': ""Then, the German infinitive 'besuchen' and the negation particle 'nicht' are translated."", '96': 'The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions.', '97': 'To formalize the approach, we introduce four verbgroup states S: Initial (I): A contiguous, initial block of source positions is covered.', '98': 'Skipped (K): The translation of up to one word may be postponed . Verb (V): The translation of up to two words may be anticipated.', '99': 'Final (F): The rest of the sentence is processed monotonically taking account of the already covered positions.', '100': 'While processing the source sentence monotonically, the initial state I is entered whenever there are no uncovered positions to the left of the rightmost covered position.', '101': 'The sequence of states needed to carry out the word reordering example in Fig.', '102': '1 is given in Fig.', '103': '2.', '104': 'The 13 positions of the source sentence are processed in the order shown.', '105': 'A position is presented by the word at that position.', '106': 'Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) !', '107': '(S; C; j); Not only the coverage set C and the positions j; j0, but also the verbgroup states S; S0 are taken into account.', '108': 'To be short, we omit the target words e; e0 in the formulation of the search hypotheses.', '109': 'There are 13 types of extensions needed to describe the verbgroup reordering.', '110': 'The details are given in (Tillmann, 2000).', '111': 'For each extension a new position is added to the coverage set.', '112': 'Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).', '113': 'Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence.', '114': 'The search starts in the hypothesis (I; f;g; 0).', '115': 'f;g denotes the empty set, where no source sentence position is covered.', '116': 'The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Ã\x86;e00 np(jjj0; J) p(Ã\x86) pÃ\x86(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).', '117': 'f1; ; Jg denotes a coverage set including all positions from the starting position 1 to position J and j 2 fJ ô\x80\x80\x80L; ; Jg.', '118': 'The final score is obtained from: max e;e0 j2fJô\x80\x80\x80L;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.', '119': 'The complexity of the quasimonotone search is O(E3 J (R2+LR)).', '120': 'The proof is given in (Tillmann, 2000).', '121': '3.2 Reordering with IBM Style.', '122': 'Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).', '123': 'A detailed description of the search procedure used is given in this patent.', '124': 'Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1.', '125': 'A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.', '126': 'Here, we process only full-form words within the translation procedure.', '127': 'the number of permutations carried out for the word reordering is given.', '128': 'During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet.', '129': 'Only one of the first n positions which are not already aligned in a partial hypothesis may be chosen, where n is set to 4.', '130': 'The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.', '131': 'This number must be less than or equal to n ô\x80\x80\x80 1.', '132': 'Otherwise for the predecessor search hypothesis, we would have chosen a position that would not have been among the first n uncovered positions.', '133': 'Ignoring the identity of the target language words e and e0, the possible partial hypothesis extensions due to the IBM restrictions are shown in Table 2.', '134': 'In general, m; l; l0 6= fl1; l2; l3g and in line umber 3 and 4, l0 must be chosen not to violate the above reordering restriction.', '135': 'Note that in line 4 the last visited position for the successor hypothesis must be m. Otherwise , there will be four uncovered positions for the predecessor hypothesis violating the restriction.', '136': 'A dynamic programming recursion similar to the one in Eq. 2 is evaluated.', '137': 'In this case, we have no finite-state restrictions for the search space.', '138': 'The search starts in hypothesis (f;g; 0) and ends in the hypotheses (f1; ; Jg; j), with j 2 f1; ; Jg.', '139': 'This approach leads to a search procedure with complexity O(E3 J4).', '140': 'The proof is given in (Tillmann, 2000).', '141': '4.1 The Task and the Corpus.', '142': 'We have tested the translation system on the Verbmobil task (Wahlster 1993).', '143': 'The Verbmobil task is an appointment scheduling task.', '144': 'Two subjects are each given a calendar and they are asked to schedule a meeting.', '145': 'The translation direction is from German to English.', '146': 'A summary of the corpus used in the experiments is given in Table 3.', '147': 'The perplexity for the trigram language model used is 26:5.', '148': 'Although the ultimate goal of the Verbmobil project is the translation of spoken language, the input used for the translation experiments reported on in this paper is the (more or less) correct orthographic transcription of the spoken sentences.', '149': 'Thus, the effects of spontaneous speech are present in the corpus, e.g. the syntactic structure of the sentence is rather less restricted, however the effect of speech recognition errors is not covered.', '150': 'For the experiments, we use a simple preprocessing step.', '151': 'German city names are replaced by category markers.', '152': 'The translation search is carried out with the category markers and the city names are resubstituted into the target sentence as a postprocessing step.', '153': 'Table 3: Training and test conditions for the Verbmobil task (*number of words without punctuation marks).', '154': 'German English Training: Sentences 58 073 Words 519 523 549 921 Words* 418 979 453 632 Vocabulary Size 7939 4648 Singletons 3454 1699 Test-147: Sentences 147 Words 1 968 2 173 Perplexity { 26:5 Table 4: Multi-reference word error rate (mWER) and subjective sentence error rate (SSER) for three different search procedures.', '155': 'Search CPU time mWER SSER Method [sec] [%] [%] MonS 0:9 42:0 30:5 QmS 10:6 34:4 23:8 IbmS 28:6 38:2 26:2 4.2 Performance Measures.', '156': 'The following two error criteria are used in our experiments: mWER: multi-reference WER: We use the Levenshtein distance between the automatic translation and several reference translations as a measure of the translation errors.', '157': 'On average, 6 reference translations per automatic translation are available.', '158': 'The Levenshtein distance between the automatic translation and each of the reference translations is computed, and the minimum Levenshtein distance is taken.', '159': 'This measure has the advantage of being completely automatic.', '160': 'SSER: subjective sentence error rate: For a more detailed analysis, the translations are judged by a human test person.', '161': 'For the error counts, a range from 0:0 to 1:0 is used.', '162': 'An error count of 0:0 is assigned to a perfect translation, and an error count of 1:0 is assigned to a semantically and syntactically wrong translation.', '163': '4.3 Translation Experiments.', '164': 'For the translation experiments, Eq. 2 is recursively evaluated.', '165': 'We apply a beam search concept as in speech recognition.', '166': 'However there is no global pruning.', '167': 'Search hypotheses are processed separately according to their coverage set C. The best scored hypothesis for each coverage set is computed: QBeam(C) = max e;e0 ;S;j Qe0 (e; S; C; j) The hypothesis (e0; e; S; C; j) is pruned if: Qe0 (e; S; C; j) < t0 QBeam(C); where t0 is a threshold to control the number of surviving hypotheses.', '168': 'Additionally, for a given coverage set, at most 250 different hypotheses are kept during the search process, and the number of different words to be hypothesized by a source word is limited.', '169': 'For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is suÃ\x86cient to consider only the best 50 words.', '170': 'We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.', '171': 'Table 4 shows translation results for the three approaches.', '172': 'The computing time is given in terms of CPU time per sentence (on a 450MHz PentiumIIIPC).', '173': 'Here, the pruning threshold t0 = 10:0 is used.', '174': 'Translation errors are reported in terms of multireference word error rate (mWER) and subjective sentence error rate (SSER).', '175': 'The monotone search performs worst in terms of both error rates mWER and SSER.', '176': 'The computing time is low, since no reordering is carried out.', '177': 'The quasi-monotone search performs best in terms of both error rates mWER and SSER.', '178': 'Additionally, it works about 3 times as fast as the IBM style search.', '179': 'For our demonstration system, we typically use the pruning threshold t0 = 5:0 to speed up the search by a factor 5 while allowing for a small degradation in translation accuracy.', '180': 'The effect of the pruning threshold t0 is shown in Table 5.', '181': 'The computing time, the number of search errors, and the multi-reference WER (mWER) are shown as a function of t0.', '182': 'The negative logarithm of t0 is reported.', '183': 'The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series, we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0, and this number is reported as the number of search errors.', '184': 'Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors.', '185': 'Decreasing the threshold results in higher mWER due to additional search errors.', '186': 'Table 5: Effect of the beam threshold on the number of search errors (147 sentences).', '187': 'Search t0 CPU time #search mWER Method [sec] error [%] QmS 0.0 0.07 108 42:6 1.0 0.13 85 37:8 2.5 0.35 44 36:6 5.0 1.92 4 34:6 10.0 10.6 0 34:5 IbmS 0.0 0.14 108 43:4 1.0 0.3 84 39:5 2.5 0.8 45 39:1 5.0 4.99 7 38:3 10.0 28.52 0 38:2 Table 6 shows example translations obtained by the three different approaches.', '188': 'Again, the monotone search performs worst.', '189': 'In the second and third translation examples, the IbmS word reordering performs worse than the QmS word reordering, since it can not take properly into account the word reordering due to the German verbgroup.', '190': ""The German finite verbs 'bin' (second example) and 'k\x7fonnten' (third example) are too far away from the personal pronouns 'ich' and 'Sie' (6 respectively 5 source sentence positions)."", '191': 'In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable.', '192': 'In this paper, we have presented a new, eÃ\x86cient DP-based search procedure for statistical machine translation.', '193': 'The approach assumes that the word reordering is restricted to a few positions in the source sentence.', '194': 'The approach has been successfully tested on the 8 000-word Verbmobil task.', '195': 'Future extensions of the system might include: 1) An extended translation model, where we use more context to predict a source word.', '196': '2) An improved language model, which takes into account syntactic structure, e.g. to ensure that a proper English verbgroup is generated.', '197': '3) A tight coupling with the speech recognizer output.', '198': 'This work has been supported as part of the Verbmobil project (contract number 01 IV 601 A) by the German Federal Ministry of Education, Science, Research and Technology and as part of the Eutrans project (ESPRIT project number 30268) by the European Community.', '199': 'Table 6: Example Translations for the Verbmobil task.', '200': 'Input: Ja , wunderbar . K\x7fonnen wir machen . MonS: Yes, wonderful.', '201': 'Can we do . QmS: Yes, wonderful.', '202': 'We can do that . IbmS: Yes, wonderful.', '203': 'We can do that . Input: Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin . Genaugenommen nur am dritten . Wie w\x7fare es denn am \x7fahm Samstag , dem zehnten Februar ? MonS: That is too tight , because I from the third in Kaiserslautern . In fact only on the third . How about \x7fahm Saturday , the tenth of February ? QmS: That is too tight , because I am from the third in Kaiserslautern . In fact only on the third . \x7fAhm how about Saturday , February the tenth ? IbmS: That is too tight , from the third because I will be in Kaiserslautern . In fact only on the third . \x7fAhm how about Saturday , February the tenth ? Input: Wenn Sie dann noch den siebzehnten k\x7fonnten , w\x7fare das toll , ja . MonS: If you then also the seventeenth could , would be the great , yes . QmS: If you could then also the seventeenth , that would be great , yes . IbmS: Then if you could even take seventeenth , that would be great , yes . Input: Ja , das kommt mir sehr gelegen . Machen wir es dann am besten so . MonS: Yes , that suits me perfectly . Do we should best like that . QmS: Yes , that suits me fine . We do it like that then best . IbmS: Yes , that suits me fine . We should best do it like that .'}",['C00-2123'],['../data/summaries/C00-2123.txt'],['../data/tba/C00-2123.json']
C02-1025,Named Entity Recognition: A Maximum Entropy Approach Using Global Information,../data/papers/C02-1025.xml,"{'0': 'Named Entity Recognition: A Maximum Entropy Approach Using Global Information', '1': 'This paper presents a maximum entropy-based named entity recognizer (NER).', '2': 'It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier.', '3': 'Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence- based classifier.', '4': 'In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC6 and MUC7 test data.', '5': 'Considerable amount of work has been done in recent years on the named entity recognition task, partly due to the Message Understanding Conferences (MUC).', '6': 'A named entity recognizer (NER) is useful in many NLP applications such as information extraction, question answering, etc. On its own, a NER can also provide users who are looking for person or organization names with quick information.', '7': 'In MUC6 and MUC7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (Chinchor, 1998; Sundheim, 1995) Machine learning systems in MUC6 and MUC 7 achieved accuracy comparable to rule-based systems on the named entity task.', '8': 'Statistical NERs usually find the sequence of tags that maximizes the probability , where is the sequence of words in a sentence, and is the sequence of named-entity tags assigned to the words in . Attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al., 1998; Borthwick, 1999).', '9': 'We propose maximizing , where is the sequence of named- entity tags assigned to the words in the sentence , and is the information that can be extracted from the whole document containing . Our system is built on a maximum entropy classifier.', '10': 'By making use of global context, it has achieved excellent results on both MUC6 and MUC7 official test data.', '11': 'We will refer to our system as MENERGI (Maximum Entropy Named Entity Recognizer using Global Information).', '12': 'As far as we know, no other NERs have used information from the whole document (global) as well as information within the same sentence (local) in one framework.', '13': 'The use of global features has improved the performance on MUC6 test data from 90.75% to 93.27% (27% reduction in errors), and the performance on MUC7 test data from 85.22% to 87.24% (14% reduction in errors).', '14': 'These results are achieved by training on the official MUC6 and MUC7 training data, which is much less training data than is used by other machine learning systems that worked on the MUC6 or MUC7 named entity task (Bikel et al., 1997; Bikel et al., 1999; Borth- wick, 1999).', '15': 'We believe it is natural for authors to use abbreviations in subsequent mentions of a named entity (i.e., first â\x80\x9cPresident George Bushâ\x80\x9d then â\x80\x9cBushâ\x80\x9d).', '16': 'As such, global information from the whole context of a document is important to more accurately recognize named entities.', '17': 'Although we have not done any experiments on other languages, this way of using global features from a whole document should be applicable to other languages.', '18': 'Recently, statistical NERs have achieved results that are comparable to hand-coded systems.', '19': ""Since MUC6, BBN' s Hidden Markov Model (HMM) based IdentiFinder (Bikel et al., 1997) has achieved remarkably good performance."", '20': ""MUC7 has also seen hybrids of statistical NERs and hand-coded systems (Mikheev et al., 1998; Borthwick, 1999), notably Mikheev' s system, which achieved the best performance of 93.39% on the official NE test data."", '21': 'MENE (Maximum Entropy Named Entity) (Borth- wick, 1999) was combined with Proteus (a hand- coded system), and came in fourth among all MUC 7 participants.', '22': 'MENE without Proteus, however, did not do very well and only achieved an F measure of 84.22% (Borthwick, 1999).', '23': 'Among machine learning-based NERs, Identi- Finder has proven to be the best on the official MUC6 and MUC7 test data.', '24': 'MENE (without the help of hand-coded systems) has been shown to be somewhat inferior in performance.', '25': 'By using the output of a hand-coded system such as Proteus, MENE can improve its performance, and can even outperform IdentiFinder (Borthwick, 1999).', '26': 'Mikheev et al.', '27': '(1998) did make use of information from the whole document.', '28': 'However, their system is a hybrid of hand-coded rules and machine learning methods.', '29': 'Another attempt at using global information can be found in (Borthwick, 1999).', '30': 'He used an additional maximum entropy classifier that tries to correct mistakes by using reference resolution.', '31': 'Reference resolution involves finding words that co-refer to the same entity.', '32': 'In order to train this error-correction model, he divided his training corpus into 5 portions of 20% each.', '33': 'MENE is then trained on 80% of the training corpus, and tested on the remaining 20%.', '34': 'This process is repeated 5 times by rotating the data appropriately.', '35': 'Finally, the concatenated 5 * 20% output is used to train the reference resolution component.', '36': ""We will show that by giving the first model some global features, MENERGI outperforms Borthwick' s reference resolution classifier."", '37': 'On MUC6 data, MENERGI also achieves performance comparable to IdentiFinder when trained on similar amount of training data.', '38': 'both MENE and IdentiFinder used more training data than we did (we used only the official MUC 6 and MUC7 training data).', '39': 'On the MUC6 data, Bikel et al.', '40': '(1997; 1999) do have some statistics that show how IdentiFinder performs when the training data is reduced.', '41': 'Our results show that MENERGI performs as well as IdentiFinder when trained on comparable amount of training data.', '42': 'The system described in this paper is similar to the MENE system of (Borthwick, 1999).', '43': 'It uses a maximum entropy framework and classifies each word given its features.', '44': 'Each name class is subdivided into 4 sub-classes, i.e., N begin, N continue, N end, and N unique.', '45': 'Hence, there is a total of 29 classes (7 name classes 4 sub-classes 1 not-a-name class).', '46': '3.1 Maximum Entropy.', '47': 'The maximum entropy framework estimates probabilities based on the principle of making as few assumptions as possible, other than the constraints imposed.', '48': 'Such constraints are derived from training data, expressing some relationship between features and outcome.', '49': 'The probability distribution that satisfies the above property is the one with the highest entropy.', '50': 'It is unique, agrees with the maximum-likelihood distribution, and has the exponential form (Della Pietra et al., 1997): where refers to the outcome, the history (or context), and is a normalization function.', '51': 'In addition, each feature function is a binary function.', '52': 'For example, in predicting if a word belongs to a word class, is either true or false, and refers to the surrounding context: if = true, previous word = the otherwise The parameters are estimated by a procedure called Generalized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972).', '53': 'This is an iterative method that improves the estimation of the parameters at each iteration.', '54': 'We have used the Java-based opennlp maximum entropy package1.', '55': 'In Section 5, we try to compare results of MENE, IdentiFinder, and MENERGI.', '56': 'However, 1 http://maxent.sourceforge.net 3.2 Testing.', '57': 'During testing, it is possible that the classifier produces a sequence of inadmissible classes (e.g., person begin followed by location unique).', '58': 'To eliminate such sequences, we define a transition probability between word classes to be equal to 1 if the sequence is admissible, and 0 otherwise.', '59': 'The probability of the classes assigned to the words in a sentence in a document is defined as follows: where is determined by the maximum entropy classifier.', '60': 'A dynamic programming algorithm is then used to select the sequence of word classes with the highest probability.', '61': 'The features we used can be divided into 2 classes: local and global.', '62': 'Local features are features that are based on neighboring tokens, as well as the token itself.', '63': 'Global features are extracted from other occurrences of the same token in the whole document.', '64': ""The local features used are similar to those used in BBN' s IdentiFinder (Bikel et al., 1999) or MENE (Borthwick, 1999)."", '65': 'However, to classify a token , while Borthwick uses tokens from to (from two tokens before to two tokens after ), we used only the tokens , , and . Even with local features alone, MENERGI outperforms MENE (Borthwick, 1999).', '66': 'This might be because our features are more comprehensive than those used by Borthwick.', '67': 'In IdentiFinder, there is a priority in the feature assignment, such that if one feature is used for a token, another feature lower in priority will not be used.', '68': 'In the maximum entropy framework, there is no such constraint.', '69': 'Multiple features can be used for the same token.', '70': 'Feature selection is implemented using a feature cutoff: features seen less than a small count during training will not be used.', '71': 'We group the features used into feature groups.', '72': 'Each feature group can be made up of many binary features.', '73': 'For each token , zero, one, or more of the features in each feature group are set to 1.', '74': '4.1 Local Features.', '75': 'The local feature groups are: Non-Contextual Feature: This feature is set to 1 for all tokens.', '76': 'This feature imposes constraints Table 1: Features based on the token string that are based on the probability of each name class during training.', '77': 'Zone: MUC data contains SGML tags, and a document is divided into zones (e.g., headlines and text zones).', '78': 'The zone to which a token belongs is used as a feature.', '79': 'For example, in MUC6, there are four zones (TXT, HL, DATELINE, DD).', '80': 'Hence, for each token, one of the four features zone-TXT, zone- HL, zone-DATELINE, or zone-DD is set to 1, and the other 3 are set to 0.', '81': 'Case and Zone: If the token starts with a capital letter (initCaps), then an additional feature (init- Caps, zone) is set to 1.', '82': 'If it is made up of all capital letters, then (allCaps, zone) is set to 1.', '83': 'If it starts with a lower case letter, and contains both upper and lower case letters, then (mixedCaps, zone) is set to 1.', '84': 'A token that is allCaps will also be initCaps.', '85': 'This group consists of (3 total number of possible zones) features.', '86': 'Case and Zone of and : Similarly, if (or ) is initCaps, a feature (initCaps, zone) (or (initCaps, zone) ) is set to 1, etc. Token Information: This group consists of 10 features based on the string , as listed in Table 1.', '87': 'For example, if a token starts with a capital letter and ends with a period (such as Mr.), then the feature InitCapPeriod is set to 1, etc. First Word: This feature group contains only one feature firstword.', '88': 'If the token is the first word of a sentence, then this feature is set to 1.', '89': 'Otherwise, it is set to 0.', '90': 'Lexicon Feature: The string of the token is used as a feature.', '91': 'This group contains a large number of features (one for each token string present in the training data).', '92': 'At most one feature in this group will be set to 1.', '93': 'If is seen infrequently during training (less than a small count), then will not be selected as a feature and all features in this group are set to 0.', '94': 'Lexicon Feature of Previous and Next Token: The string of the previous token and the next token is used with the initCaps information of . If has initCaps, then a feature (initCaps, ) is set to 1.', '95': 'If is not initCaps, then (not-initCaps, ) is set to 1.', '96': 'Same for . In the case where the next token is a hyphen, then is also used as a feature: (init- Caps, ) is set to 1.', '97': 'This is because in many cases, the use of hyphens can be considered to be optional (e.g., third-quarter or third quarter).', '98': 'Out-of-Vocabulary: We derived a lexicon list from WordNet 1.6, and words that are not found in this list have a feature out-of-vocabulary set to 1.', '99': 'Dictionaries: Due to the limited amount of training material, name dictionaries have been found to be useful in the named entity task.', '100': 'The importance of dictionaries in NERs has been investigated in the literature (Mikheev et al., 1999).', '101': 'The sources of our dictionaries are listed in Table 2.', '102': 'For all lists except locations, the lists are processed into a list of tokens (unigrams).', '103': 'Location list is processed into a list of unigrams and bigrams (e.g., New York).', '104': 'For locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.', '105': 'A list of words occurring more than 10 times in the training data is also collected (commonWords).', '106': 'Only tokens with initCaps not found in commonWords are tested against each list in Table 2.', '107': 'If they are found in a list, then a feature for that list will be set to 1.', '108': 'For example, if Barry is not in commonWords and is found in the list of person first names, then the feature PersonFirstName will be set to 1.', '109': 'Similarly, the tokens and are tested against each list, and if found, a corresponding feature will be set to 1.', '110': 'For example, if is found in the list of person first names, the feature PersonFirstName is set to 1.', '111': 'Month Names, Days of the Week, and Numbers: If is initCaps and is one of January, February, . . .', '112': ', December, then the feature MonthName is set to 1.', '113': 'If is one of Monday, Tuesday, . . .', '114': ', Sun day, then the feature DayOfTheWeek is set to 1.', '115': 'If is a number string (such as one, two, etc), then the feature NumberString is set to 1.', '116': 'Suffixes and Prefixes: This group contains only two features: Corporate-Suffix and Person-Prefix.', '117': 'Two lists, Corporate-Suffix-List (for corporate suffixes) and Person-Prefix-List (for person prefixes), are collected from the training data.', '118': 'For corporate suffixes, a list of tokens cslist that occur frequently as the last token of an organization name is collected from the training data.', '119': 'Frequency is calculated by counting the number of distinct previous tokens that each token has (e.g., if Electric Corp. is seen 3 times, and Manufacturing Corp. is seen 5 times during training, and Corp. is not seen with any other preceding tokens, then the â\x80\x9cfrequencyâ\x80\x9d of Corp. is 2).', '120': 'The most frequently occurring last words of organization names in cslist are compiled into a list of corporate suffixes, Corporate-Suffix- List.', '121': 'A Person-Prefix-List is compiled in an analogous way.', '122': 'For MUC6, for example, Corporate- Suffix-List is made up of ltd., associates, inc., co, corp, ltd, inc, committee, institute, commission, university, plc, airlines, co., corp. and Person-Prefix- List is made up of succeeding, mr., rep., mrs., secretary, sen., says, minister, dr., chairman, ms. . For a token that is in a consecutive sequence of init then a feature Corporate-Suffix is set to 1.', '123': 'If any of the tokens from to is in Person-Prefix- List, then another feature Person-Prefix is set to 1.', '124': 'Note that we check for , the word preceding the consecutive sequence of initCaps tokens, since person prefixes like Mr., Dr., etc are not part of person names, whereas corporate suffixes like Corp., Inc., etc are part of corporate names.', '125': '4.2 Global Features.', '126': 'Context from the whole document can be important in classifying a named entity.', '127': 'A name already mentioned previously in a document may appear in abbreviated form when it is mentioned again later.', '128': 'Previous work deals with this problem by correcting inconsistencies between the named entity classes assigned to different occurrences of the same entity (Borthwick, 1999; Mikheev et al., 1998).', '129': 'We often encounter sentences that are highly ambiguous in themselves, without some prior knowledge of the entities concerned.', '130': 'For example: McCann initiated a new global system.', '131': '(1) CEO of McCann . . .', '132': '(2) Description Source Location Names http://www.timeanddate.com http://www.cityguide.travel-guides.com http://www.worldtravelguide.net Corporate Names http://www.fmlx.com Person First Names http://www.census.gov/genealogy/names Person Last Names Table 2: Sources of Dictionaries The McCann family . . .', '133': '(3)In sentence (1), McCann can be a person or an orga nization.', '134': 'Sentence (2) and (3) help to disambiguate one way or the other.', '135': 'If all three sentences are in the same document, then even a human will find it difficult to classify McCann in (1) into either person or organization, unless there is some other information provided.', '136': 'The global feature groups are: InitCaps of Other Occurrences (ICOC): There are 2 features in this group, checking for whether the first occurrence of the same word in an unambiguous position (non first-words in the TXT or TEXT zones) in the same document is initCaps or not-initCaps.', '137': 'For a word whose initCaps might be due to its position rather than its meaning (in headlines, first word of a sentence, etc), the case information of other occurrences might be more accurate than its own.', '138': 'For example, in the sentence that starts with â\x80\x9cBush put a freeze on . . .', '139': 'â\x80\x9d, because Bush is the first word, the initial caps might be due to its position (as in â\x80\x9cThey put a freeze on . . .', '140': 'â\x80\x9d).', '141': 'If somewhere else in the document we see â\x80\x9crestrictions put in place by President Bushâ\x80\x9d, then we can be surer that Bush is a name.', '142': 'Corporate Suffixes and Person Prefixes of Other Occurrences (CSPP): If McCann has been seen as Mr. McCann somewhere else in the document, then one would like to give person a higher probability than organization.', '143': 'On the other hand, if it is seen as McCann Pte.', '144': 'Ltd., then organization will be more probable.', '145': 'With the same Corporate- Suffix-List and Person-Prefix-List used in local features, for a token seen elsewhere in the same document with one of these suffixes (or prefixes), another feature Other-CS (or Other-PP) is set to 1.', '146': 'Acronyms (ACRO): Words made up of all capitalized letters in the text zone will be stored as acronyms (e.g., IBM).', '147': 'The system will then look for sequences of initial capitalized words that match the acronyms found in the whole document.', '148': 'Such sequences are given additional features of A begin, A continue, or A end, and the acronym is given a feature A unique.', '149': 'For example, if FCC and Federal Communications Commission are both found in a document, then Federal has A begin set to 1, Communications has A continue set to 1, Commission has A end set to 1, and FCC has A unique set to 1.', '150': 'Sequence of Initial Caps (SOIC): In the sentence Even News Broadcasting Corp., noted for its accurate reporting, made the erroneous announcement., a NER may mistake Even News Broadcasting Corp. as an organization name.', '151': 'However, it is unlikely that other occurrences of News Broadcasting Corp. in the same document also co-occur with Even.', '152': 'This group of features attempts to capture such information.', '153': 'For every sequence of initial capitalized words, its longest substring that occurs in the same document as a sequence of initCaps is identified.', '154': 'For this example, since the sequence Even News Broadcasting Corp. only appears once in the document, its longest substring that occurs in the same document is News Broadcasting Corp. In this case, News has an additional feature of I begin set to 1, Broadcasting has an additional feature of I continue set to 1, and Corp. has an additional feature of I end set to 1.', '155': 'Unique Occurrences and Zone (UNIQ): This group of features indicates whether the word is unique in the whole document.', '156': 'needs to be in initCaps to be considered for this feature.', '157': 'If is unique, then a feature (Unique, Zone) is set to 1, where Zone is the document zone where appears.', '158': 'As we will see from Table 3, not much improvement is derived from this feature.', '159': 'The baseline system in Table 3 refers to the maximum entropy system that uses only local features.', '160': 'As each global feature group is added to the list of features, we see improvements to both MUC6 and MUC6 MUC7 Baseline 90.75% 85.22% + ICOC 91.50% 86.24% + CSPP 92.89% 86.96% + ACRO 93.04% 86.99% + SOIC 93.25% 87.22% + UNIQ 93.27% 87.24% Table 3: F-measure after successive addition of each global feature group Table 5: Comparison of results for MUC6 Systems MUC6 MUC7 No.', '161': 'of Articles No.', '162': 'of Tokens No.', '163': 'of Articles No.', '164': 'of Tokens MENERGI 318 160,000 200 180,000 IdentiFinder â\x80\x93 650,000 â\x80\x93 790,000 MENE â\x80\x93 â\x80\x93 350 321,000 Table 4: Training Data MUC7 test accuracy.2 For MUC6, the reduction in error due to global features is 27%, and for MUC7,14%.', '165': 'ICOC and CSPP contributed the greatest im provements.', '166': 'The effect of UNIQ is very small on both data sets.', '167': 'All our results are obtained by using only the official training data provided by the MUC conferences.', '168': 'The reason why we did not train with both MUC6 and MUC7 training data at the same time is because the task specifications for the two tasks are not identical.', '169': 'As can be seen in Table 4, our training data is a lot less than those used by MENE and IdentiFinder3.', '170': ""In this section, we try to compare our results with those obtained by IdentiFinder ' 97 (Bikel et al., 1997), IdentiFinder ' 99 (Bikel et al., 1999), and MENE (Borthwick, 1999)."", '171': ""IdentiFinder ' 99' s results are considerably better than IdentiFinder ' 97' s. IdentiFinder' s performance in MUC7 is published in (Miller et al., 1998)."", '172': 'MENE has only been tested on MUC7.', '173': 'For fair comparison, we have tabulated all results with the size of training data used (Table 5 and Table 6).', '174': 'Besides size of training data, the use of dictionaries is another factor that might affect performance.', '175': 'Bikel et al.', '176': '(1999) did not report using any dictionaries, but mentioned in a footnote that they have added list membership features, which have helped marginally in certain domains.', '177': 'Borth 2MUC data can be obtained from the Linguistic Data Consortium: http://www.ldc.upenn.edu 3Training data for IdentiFinder is actually given in words (i.e., 650K & 790K words), rather than tokens Table 6: Comparison of results for MUC7 wick (1999) reported using dictionaries of person first names, corporate names and suffixes, colleges and universities, dates and times, state abbreviations, and world regions.', '178': 'In MUC6, the best result is achieved by SRA (Krupka, 1995).', '179': 'In (Bikel et al., 1997) and (Bikel et al., 1999), performance was plotted against training data size to show how performance improves with training data size.', '180': ""We have estimated the performance of IdentiFinder ' 99 at 200K words of training data from the graphs."", '181': 'For MUC7, there are also no published results on systems trained on only the official training data of 200 aviation disaster articles.', '182': 'In fact, training on the official training data is not suitable as the articles in this data set are entirely about aviation disasters, and the test data is about air vehicle launching.', '183': 'Both BBN and NYU have tagged their own data to supplement the official training data.', '184': ""Even with less training data, MENERGI outperforms Borthwick' s MENE + reference resolution (Borthwick, 1999)."", '185': 'Except our own and MENE + reference resolution, the results in Table 6 are all official MUC7 results.', '186': 'The effect of a second reference resolution classifier is not entirely the same as that of global features.', '187': 'A secondary reference resolution classifier has information on the class assigned by the primary classifier.', '188': 'Such a classification can be seen as a not-always-correct summary of global features.', '189': 'The secondary classifier in (Borthwick, 1999) uses information not just from the current article, but also from the whole test corpus, with an additional feature that indicates if the information comes from the same document or from another document.', '190': 'We feel that information from a whole corpus might turn out to be noisy if the documents in the corpus are not of the same genre.', '191': 'Moreover, if we want to test on a huge test corpus, indexing the whole corpus might prove computationally expensive.', '192': 'Hence we decided to restrict ourselves to only information from the same document.', '193': 'Mikheev et al.', '194': '(1998) have also used a maximum entropy classifier that uses already tagged entities to help tag other entities.', '195': 'The overall performance of the LTG system was outstanding, but the system consists of a sequence of many hand-coded rules and machine-learning modules.', '196': 'We have shown that the maximum entropy framework is able to use global information directly.', '197': 'This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al., 1997).', '198': 'Using less training data than other systems, our NER is able to perform as well as other state-of-the-art NERs.', '199': 'Information from a sentence is sometimes insufficient to classify a name correctly.', '200': 'Global context from the whole document is available and can be exploited in a natural manner with a maximum entropy classifier.', '201': 'We believe that the underlying principles of the maximum entropy framework are suitable for exploiting information from diverse sources.', '202': 'Borth- wick (1999) successfully made use of other hand- coded systems as input for his MENE system, and achieved excellent results.', '203': 'However, such an approach requires a number of hand-coded systems, which may not be available in languages other than English.', '204': 'We believe that global context is useful in most languages, as it is a natural tendency for authors to use abbreviations on entities already mentioned previously.'}",['C02-1025'],['../data/summaries/C02-1025.txt'],['../data/tba/C02-1025.json']
C08-1098,Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging,../data/papers/C08-1098.xml,"{'0': 'Estimation of Conditional ProbabilitiesWith Decision Trees and an Application to Fine-Grained POS Tagging', '1': 'We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags.', '2': 'It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs.', '3': 'In experiments on German and Czech data, our tagger outperformed state- of-the-art POS taggers.', '4': 'A Hidden-Markov-Model part-of-speech tagger (Brants, 2000, e.g.) computes the most probable POS tag sequence tË\x86N = tË\x861, ..., tË\x86N for a given word sequence wN . POS taggers are usually trained on corpora with between 50 and 150 different POS tags.', '5': 'Tagsets of this size contain little or no information about number, gender, case and similar morphosyntac- tic features.', '6': 'For languages with a rich morphology such as German or Czech, more fine-grained tagsets are often considered more appropriate.', '7': 'The additional information may also help to disambiguate the (base) part of speech.', '8': 'Without gender information, for instance, it is difficult for a tagger to correctly disambiguate the German sentence Ist das RealitaÂ¨ t?', '9': '(Is that reality?).', '10': 'The word das is ambiguous between an article and a demonstrative.', '11': 'Because of the lack of gender agreement between das (neuter) and the noun RealitaÂ¨ t (feminine), the article reading must be wrong.', '12': 'The German Tiger treebank (Brants et al., 2002) is an example of a corpus with a more fine-grained tagset (over 700 tags overall).', '13': 'Large tagsets aggravate sparse data problems.', '14': 'As an example, take the German sentence Das zu versteuernde Einkommen sinkt (â\x80\x9cThe to be taxed income decreasesâ\x80\x9d; The tË\x86N N N 1 = arg max p(t1 , w1 ) 1 The joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all POS tags: N taxable income decreases).', '15': 'This sentence should be tagged as shown in table 1.', '16': 'Das ART.Def.Nom.Sg.Neut zu PART.Zu versteuernde ADJA.Pos.Nom.Sg.Neut Einkommen N.Reg.Nom.Sg.Neut p(tN , wN ) = n 1 1 i=1 p(ti|tiâ\x88\x921 ) iâ\x88\x92k p(wi|ti) le .', '17': '(1) context prob.', '18': 'xical prob HMM taggers are fast and were successfully applied to a wide range of languages and training corpora.', '19': 'Qc 2008.', '20': 'Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).', '21': 'Some rights reserved.', '22': 'Table 1: Correct POS tags for the German sentence Das zu versteuernde Einkommen sinkt.', '23': 'Unfortunately, the POS trigram consisting of the tags of the first three words does not occur in the Tiger corpus.', '24': '(Neither does the pair consisting of the first two tags.)', '25': 'The unsmoothed 777 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 777â\x80\x93784 Manchester, August 2008 context probability of the third POS tag is therefore 0.', '26': 'If the probability is smoothed with the backoff distribution p(â\x80¢|P ART .Z u), the most probable tag is ADJA.Pos.Acc.Sg.Fem rather than ADJA.Pos.Nom.Sg.Neut.', '27': 'Thus, the agreement between the article and the adjective is not checked anymore.', '28': 'A closer inspection of the Tiger corpus reveals that it actually contains all the information needed to completely disambiguate each component of the POS tag ADJA.Pos.Nom.Sg.Neut: â\x80¢ All words appearing after an article (ART) and the infinitive particle zu (PART.zu) are attributive adjectives (ADJA) (10 of 10 cases).', '29': 'â\x80¢ All adjectives appearing after an article and a particle (PART) have the degree positive (Pos) (39 of 39 cases).', '30': 'â\x80¢ All adjectives appearing after a nominative article and a particle have nominative case (11 of 11 cases).', '31': 'â\x80¢ All adjectives appearing after a singular article and a particle are singular (32 of 32 cases).', '32': 'â\x80¢ All adjectives appearing after a neuter article and a particle are neuter (4 of 4 cases).', '33': 'By (1) decomposing the context probability of ADJA.Pos.Nom.Sg.Neut into a product of attribute probabilities p(ADJA | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu) â\x88\x97 p(Pos| 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA) â\x88\x97 p(Nom | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos) â\x88\x97 p(Sg | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom) â\x88\x97 p(Neut | 2:ART, 2:ART.Def, 2:ART.Nom, 2:ART.Sg, 2:ART.Neut, 1:PART, 1:PART.Zu, 0:ADJA, 0:ADJA.Pos, 0:ADJA.Nom, 0:ADJA.Sg) and (2) selecting the relevant context attributes for the prediction of each attribute, we obtain the â\x88\x97 p(Sg | 2:ART.Sg, 1:PART.Zu, 0:ADJA) â\x88\x97 p(Neut | 2:ART.Neut, 1:PART.Zu, 0:ADJA) The conditional probability of each attribute is 1.', '34': 'Hence the context probability of the whole tag is. also 1.', '35': 'Without having observed the given context, it is possible to deduce that the observed POS tag is the only possible tag in this context.', '36': 'These considerations motivate an HMM tagging approach which decomposes the POS tags into a set of simple attributes, and uses decision trees to estimate the probability of each attribute.', '37': 'Decision trees are ideal for this task because the identification of relevant attribute combinations is at the heart of this method.', '38': 'The backoff smoothing methods of traditional n-gram POS taggers require an ordering of the reduced contexts which is not available, here.', '39': 'Discriminatively trained taggers, on the other hand, have difficulties to handle the huge number of features which are active at the same time if any possible combination of context attributes defines a separate feature.', '40': 'Decision trees (Breiman et al., 1984; Quinlan, 1993) are normally used as classifiers, i.e. they assign classes to objects which are represented as attribute vectors.', '41': 'The non-terminal nodes are labeled with attribute tests, the edges with the possible outcomes of a test, and the terminal nodes are labeled with classes.', '42': 'An object is classified by evaluating the test of the top node on the object, following the respective edge to a daughter node, evaluating the test of the daughter node, and so on until a terminal node is reached whose class is assigned to the object.', '43': 'Decision Trees are turned into probability estimation trees by storing a probability for each possible class at the terminal nodes instead of a single result class.', '44': 'Figure 1 shows a probability estimation tree for the prediction of the probability of the nominative attribute of nouns.', '45': '2.1 Induction of Decision Trees.', '46': 'Decision trees are incrementally built by first selecting the test which splits the manually annotated training sample into the most homogeneous subsets with respect to the class.', '47': 'This test, which maximizes the information gain1 wrt.', '48': 'the class, is following expression for the context probability: 1 The information gain measures how much the test de-.', '49': 'p(ADJA | ART, PART.Zu) â\x88\x97 p(Pos | 2:ART, 1:PART, 0:ADJA) â\x88\x97 p(Nom | 2:ART.Nom, 1:PART.Zu, 0:ADJA) creases the uncertainty about the class.', '50': 'It is the difference between the entropy of the empirical distribution of the class variable in the training set and the weighted average entropy yes 0:N.Name yes no 1:ART.Nom no 1:ADJA.Nom yes no which returns a probability of 0.3.', '51': 'The third tree for neuter has one non terminal and two terminal nodes returning a probability of 0.3 and 0.5, respectively.', '52': 'The sum of probabilities is therefore either 0.9 or 1.1, but never exactly 1.', '53': 'This problem 2:N.Reg p=0.999 0:N.Name 0:N.Name yes no p=0.571 p=0.938 yes no p=0.948 p=0.998 .... is solved by renormalizing the probabilities.', '54': 'The probability of an attribute (such as â\x80\x9cNomâ\x80\x9d) is always conditioned on the respective base POS (such as â\x80\x9cNâ\x80\x9d) (unless the predicted attribute is theFigure 1: Probability estimation tree for the nomi native case of nouns.', '55': 'The test 1:ART.Nom checks if the preceding word is a nominative article.', '56': 'assigned to the top node.', '57': 'The tree is recursively expanded by selecting the best test for each subset and so on, until all objects of the current subset belong to the same class.', '58': 'In a second step, the decision tree may be pruned in order to avoid overfit- ting to the training data.', '59': 'Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tree for each value.', '60': 'The motivation was that a tree which predicts a single value (say verb) does not fragment the data with tests which are only relevant for the distinction of two other values (e.g. article and possessive pronoun).2 Furthermore, we observed that such two-class decision trees require no optimization of the pruning threshold (see also section 2.2.) The tree induction algorithm only considers binary tests, which check whether some particular attribute is present or not.', '61': 'The best test for each node is selected with the standard information gain criterion.', '62': 'The recursive tree building process terminates if the information gain is 0.', '63': 'The decision tree is pruned with the pruning criterion described below.', '64': 'Since the tagger creates a separate tree for each attribute, the probabilities of a set of competing attributes such as masculine, feminine, and neuter will not exactly sum up to 1.', '65': 'To understand why, assume that there are three trees for the gender attributes.', '66': 'Two of them (say the trees for masculine and feminine) consist of a single terminal node base POS) in order to make sure that the probability of an attribute is 0 if it never appeared with the respective base POS.', '67': 'All context attributes other than the base POS are always used in combination with the base POS.', '68': 'A typical context attribute is â\x80\x9c1:ART.Nomâ\x80\x9d which states that the preceding tag is an article with the attribute â\x80\x9cNomâ\x80\x9d.', '69': 'â\x80\x9c1:ARTâ\x80\x9d is also a valid attribute specification, but â\x80\x9c1:Nomâ\x80\x9d is not.', '70': 'The tagger further restricts the set of possible test attributes by requiring that some attribute of the POS tag at position i-k (i=position of the predicted POS tag, k â\x89¥ 1) must have been used be fore an attribute of the POS tag at position i-(k+1) may be examined.', '71': 'This restriction improved the tagging accuracy for large contexts.', '72': '2.2 Pruning Criterion.', '73': 'The tagger applies3 the critical-value pruning strategy proposed by (Mingers, 1989).', '74': 'A node is pruned if the information gain of the best test multiplied by the size of the data subsample is below a given threshold.', '75': 'To illustrate the pruning, assume that D is the data of the current node with 50 positive and 25 negative elements, and that D1 (with 20 positive and 20 negative elements) and D2 (with 30 positive and 5 negative elements) are the two subsets induced by the best test.', '76': 'The entropy of D is â\x88\x922/3 log22/3 â\x88\x92 1/3 log21/3 = 0.92, the entropy of D1 is â\x88\x921/2 log21/2â\x88\x921/2 log21/2 = 1, and the entropy of D2 is â\x88\x926/7 log26/7 â\x88\x92 1/7 log21/7 = 0.59.', '77': 'The information gain is therefore 0.92 â\x88\x92 (8/15 â\x88\x97 1 â\x88\x92 7/15 â\x88\x97 0.59) = 0.11.', '78': 'The resulting score is 75 â\x88\x97 0.11 = 8.25.', '79': 'Given a threshold of 6, the node is therefore not pruned.', '80': 'We experimented with pre-pruning (where a node is always pruned if the gain is below the in the two subsets.', '81': 'The weight of each subset is proportional to its size.', '82': '2 We did not directly compare the two alternatives (two- valued vs. multi-valued tests), because the implementational effort required would have been too large.', '83': '3 We also experimented with a pruning criterion based on binomial tests, which returned smaller trees with a slightly lower accuracy, although the difference in accuracy was never larger than 0.1% for any context size.', '84': 'Thus, the simpler pruning strategy presented here was chosen.', '85': 'threshold) as well as post-pruning (where a node is only pruned if its sub-nodes are terminal nodes or pruned nodes).', '86': 'The performance of pre-pruning was slightly better and it was less dependent on the choice of the pruning threshold.', '87': 'A threshold of 6 consistently produced optimal or near optimal results for pre-pruning.', '88': 'Thus, pre-pruning with a threshold of 6 was used in the experiments.', '89': 'The tagger treats dots in POS tag labels as attribute separators.', '90': 'The first attribute of a POS tag is the main category.', '91': 'The number of additional attributes is fixed for each main category.', '92': 'The additional attributes are category-specific.', '93': 'The singular attribute of a noun and an adjective POS tag are therefore two different attributes.4 Each position in the POS tags of a given category corresponds to a feature.', '94': 'The attributes occurring at a certain position constitute the value set of the feature.', '95': 'Our tagger is a HMM tagger which decomposes the context probabilities into a product of attribute probabilities.', '96': 'The probability of an attribute given the attributes of the preceding POS tags as well asand that the context probability p(ti|tiâ\x88\x921 ) is internally computed as a product of attribute probabili ties.', '97': 'In order to increase the speed, the tagger also applies a beam-search strategy which prunes all search paths whose probability is below the probability of the best path times a threshold.', '98': 'With athreshold of 10â\x88\x923 or lower, the influence of prun ing on the tagging accuracy was negligible.', '99': '4.1 Supplementary Lexicon.', '100': 'The tagger may use an external lexicon which supplies entries for additional words which are not found in the training corpus, and additional tags for words which did occur in the training data.', '101': 'If an external lexicon is provided, the lexical probabilities are smoothed as follows: The tagger computes the average tag probabilities of all words with the same set of possible POS tags.', '102': 'The Witten-Bell method is then applied to smooth the lexical probabilities with the average probabilities.', '103': 'If the word w was observed with N different tags, and f (w, t) is the joint frequency of w and POS tag t, and p(t|[w]) is the average probability of t among words with the same set of possible tags as w, then the smoothed probability of t given w is defined as follows: f (w, t) + N p(t|[w]) the preceding attributes of the predicted POS tag is estimated with a decision tree as described be p(t|w) = f (w) + N fore.', '104': 'The probabilities at the terminal nodes of the decision trees are smoothed with the parent node probabilities (which themselves were smoothed in the same way).', '105': 'The smoothing is implemented by adding the weighted class probabilities pp(c) of the parent node to the frequencies f (c) before normalizing them to probabilities: p(c) = f (c) + Î±pp(c) Î± + ï¿½c f (c) The weight Î± was fixed to 1 after a few experiments on development data.', '106': 'This smoothing strategy is closely related to Witten-Bell smoothing.', '107': 'The probabilities are normalized by dividing them by the total probability of all attribute values of the respective feature (see section 2.1).', '108': 'The best tag sequence is computed with the Viterbi algorithm.', '109': 'The main differences of our tag- ger to a standard trigram tagger are that the order of the Markov model (the k in equation 1) is not fixed 4 This is the reason why the attribute tests in figure 1 used complex attributes such as ART.Nom rather than Nom.The smoothed estimates of p(tag|word) are di vided by the prior probability p(tag) of the tag and used instead of p(word|tag).5 4.2 Unknown Words.', '110': 'The lexical probabilities of unknown words are obtained as follows: The unknown words are divided into four disjoint classes6 with numeric expressions, words starting with an uppercase letter, words starting with a lowercase letter, and a fourth class for the other words.', '111': 'The tagger builds a suffix trie for each class of unknown words using the known word types from that class.', '112': 'The maximal length of the suffixes is 7.', '113': 'The suffix tries are pruned until (i) all suffixes have a frequency of at least 5 and (ii) the information gain multiplied by the suffix frequency and di 5 p(word|tag) is equal to p(tag|word)p(word)/p(tag) and p(word) is a constant if the tokenization is unambiguous.', '114': 'Therefore dropping the factor p(word) has no influence on the ranking of the different tag sequences.', '115': '6 In earlier experiments, we had used a much larger number of word classes.', '116': 'Decreasing their number to 4 turned out to be better.', '117': 'a threshold of 1.', '118': 'More precisely, if TÎ± is the set of POS tags that occurred with suffix Î±, |T | is the size of the set T , fÎ± is the frequency of suffix Î±, and pÎ±(t) is the probability of POS tag t among the words with suffix Î±, then the following condition must hold: tion between definite and indefinite articles, and the distinction between hyphens, slashes, left and right parentheses, quotation marks, and other symbols which the Tiger treebank annotates with â\x80\x9c$(â\x80\x9d.', '119': 'A supplementary lexicon was created by analyzing a word list which included all words from the faÎ± paÎ± (t) log paÎ±(t) < 1 training, development, and test data with a German computationa l morphology.', '120': 'The analyses gener |TaÎ±| tâ\x88\x88TaÎ± pÎ±(t) ated by the morphology were mapped to the Tiger The POS probabilities are recursively smoothed with the POS probabilities of shorter suffixes using Witten-Bell smoothing.', '121': 'Our tagger was first evaluated on data from the German Tiger treebank.', '122': 'The results were compared to those obtained with the TnT tagger (Brants, 2000) and the SVMTool (GimeÂ´nez and Ma`rquez, 2004), which is based on support vector machines.7 The training of the SVMTool took more than a day.', '123': 'Therefore it was not possible to optimize the parameters systematically.', '124': 'We took standard features from a 5 word window and M4LRL training without optimization of the regular- ization parameter C. In a second experiment, our tagger was also evaluated on the Czech Academic corpus 1.0 (HladkaÂ´ et al., 2007) and compared to the TnT tag- ger.', '125': '5.1 Tiger Corpus.', '126': 'The German Tiger treebank (Brants et al., 2002) contains over 888,000 tokens.', '127': 'It is annotated with POS tags from the coarse-grained STTS tagset and with additional features encoding information about number, gender, case, person, degree, tense, and mood.', '128': 'After deleting problematic sentences (e.g. with an incomplete annotation) and automatically correcting some easily detectable errors, 885,707 tokens were left.', '129': 'The first 80% were used as training data, the first half of the rest as development data, and the last 10% as test data.', '130': 'Some of the 54 STTS labels were mapped to new labels with dots, which reduced the number of main categories to 23.', '131': 'Examples are the nominal POS tags NN and NE which were mapped to N.Reg and N.Name.', '132': 'Some lexically decidable distinctions missing in the Tiger corpus have been tagset.', '133': 'Note that only the words, but not the POS tags from the test and development data were used, here.', '134': 'Therefore, it is always possible to create a supplementary lexicon for the corpus to be processed.', '135': 'In case of the TnT tagger, the entries of the supplementary lexicon were added to the regular lexicon with a default frequency of 1 if the word/tag- pair was unknown, and with a frequency proportional to the prior probability of the tag if the word was unknown.', '136': 'This strategy returned the best results on the development data.', '137': 'In case of the SVM- Tool, we were not able to successfully integrate the supplementary lexicon.', '138': '5.1.1 Refined Tagset Prepositions are not annotated with case in the Tiger treebank, although this information is important for the disambiguation of the case of the next noun phrase.', '139': 'In order to provide the tagger with some information about the case of prepositions, a second training corpus was created in which prepositions which always select the same case, such as durch (through), were annotated with this case (APPR.Acc).', '140': 'Prepositions which select genitive case, but also occur with dative case8, were tagged with APPR.Gen. The more frequent ones of the remaining prepositions, such as in (in), were lexicalized (APPR.in).', '141': 'The refined tagset also distinguished between the auxiliaries sein, haben, and werden, and used lexicalized tags for the coordinating conjunctions aber, doch, denn, wie, bis, noch, and als whose distribution differs from the distribution of prototypical coordinating conjunctions such as und (and) or oder (or).', '142': 'For evaluation purposes, the refined tags are mapped back to the original tags.', '143': 'This mapping is unambiguous.', '144': '7 It was planned to include also the Stanford tagger.', '145': '(Toutanova et al., 2003) in this comparison, but it was not possible to train it on the Tiger data.', '146': '8 In German, the genitive case of arguments is more and.', '147': 'more replaced by the dative.', '148': 'Table 2: Tagging accuracies on development data in percent.', '149': 'Results for 2 and for 10 preceding POS tags as context are reported for our tagger.', '150': 'much smaller.', '151': 'Table 3 shows the results of an evaluation based on the plain STTS tagset.', '152': 'The first result was obtained with TnT trained on Tiger data which was mapped to STTS before.', '153': 'The second row contains the results for the TnT tagger when it is trained on the Tiger data and the output is mapped to STTS.', '154': 'The third row gives the corresponding figures for our tagger.', '155': '5.1.2 Results Table 2 summarizes the results obtained with different taggers and tagsets on the development data.', '156': 'The accuracy of a baseline tagger which chooses the most probable tag9 ignoring the context is 67.3% without and 69.4% with the supple 92.3 92.2 92.1 92 91.9 91.8 91.7 91.6 91.5 91.4 2 3 4 5 6 7 8 9 10 mentary lexicon.', '157': 'The TnT tagger achieves 86.3% accuracy on the default tagset.', '158': 'A tag is considered correct if all attributes are correct.', '159': 'The tagset refinement increases the accuracy by about 0.6%, and the external lexicon by another 3.5%.', '160': 'The SVMTool is slightly better than the TnT tagger on the default tagset, but shows little improvement from the tagset refinement.', '161': 'Apparently, the lexical features used by the SVMTool encode most of the information of the tagset refinement.', '162': 'With a context of two preceding POS tags (similar to the trigram tagger TnT), our tagger outperforms TnT by 0.7% on the default tagset, by 1% on the refined tagset, and by 1.1% on the refined tagset plus the additional lexicon.', '163': 'A larger context of up to 10 preceding POS tags further increased the accuracy by 0.6, 0.6, and 0.7%, respectively.', '164': 'de fa ult refined ref.+lexicon T n T S T T S T n T Ti g e r 1 0 t a g s 9 7.', '165': '2 8 9 7.', '166': '1 7 97.26 97.51 9 7.', '167': '3 9 97.57 97.97 Table 3: STTS accuracies of the TnT tagger trained on the STTS tagset, the TnT tagger trained on the Tiger tagset, and our tagger trained on the Tiger tagset.', '168': 'These figures are considerably lower than e.g. the 96.7% accuracy reported in Brants (2000) for the Negra treebank which is annotated with STTS tags without agreement features.', '169': 'This is to 9 Unknown words are tagged by choosing the most frequent tag of words with the same capitalization.', '170': 'Figure 2: Tagging accuracy on development data depending on context size Figure 2 shows that the tagging accuracy tends to increase with the context size.', '171': 'The best results are obtained with a context size of 10.', '172': 'What type of information is relevant across a distance of ten words?', '173': 'A good example is the decision tree for the attribute first person of finite verbs, which looks for a first person pronoun at positions -1 through -10 (relative to the position of the current word) in this order.', '174': 'Since German is a verb-final language, these tests clearly make sense.', '175': 'Table 4 shows the performance on the test data.', '176': 'Our tagger was used with a context size of 10.', '177': 'The suffix length parameter of the TnT tagger was set to 6 without lexicon and to 3 with lexicon.', '178': 'These values were optimal on the development data.', '179': 'The accuracy of our tagger is lower than on the development data.', '180': 'This could be due to the higher rate of unknown words (10.0% vs. 7.7%).', '181': 'Relative to the TnT tagger, however, the accuracy is quite similar for test and development data.', '182': 'The differences between the two taggers are significant.10 ta gg er de fa ult refined ref.+lexicon Tn T ou r ta gg er 8 3.', '183': '4 5 84.11 89.14 8 5.', '184': '0 0 85.92 91.07 Table 4: Tagging accuracies on test data.', '185': 'By far the most frequent tagging error was the confusion of nominative and accusative case.', '186': 'If 10 726 sentences were better tagged by TnT (i.e. with few errors), 1450 sentences were better tagged by our tagger.', '187': 'The resulting score of a binomial test is below 0.001.', '188': 'this error is not counted, the tagging accuracy on the development data rises from 92.17% to 94.27%.', '189': 'Our tagger is quite fast, although not as fast as the TnT tagger.', '190': 'With a context size of 3 (10), it annotates 7000 (2000) tokens per second on a computer with an Athlon X2 4600 CPU.', '191': 'The training with a context size of 10 took about 4 minutes.', '192': '5.2 Czech Academic Corpus.', '193': 'We also evaluated our tagger on the Czech Academic corpus (HladkaÂ´ et al., 2007) which contains 652.131 tokens and about 1200 different POS tags.', '194': 'The data was divided into 80% training data, 10% development data and 10% test data.', '195': '89 88.9 88.8 Provost & Domingos (2003) noted that well- known decision tree induction algorithms such as C4.5 (Quinlan, 1993) or CART (Breiman et al., 1984) fail to produce accurate probability estimates.', '196': 'They proposed to grow the decision trees to their maximal size without pruning, and to smooth the probability estimates with add-1 smoothing (also known as the Laplace correction).', '197': 'Ferri et al.', '198': '(2003) describe a more complex backoff smoothing method.', '199': 'Contrary to them, we applied pruning and found that some pruning (threshold=6) gives better results than no pruning (threshold=0).', '200': 'Another difference is that we used N two- class trees with normalization to predict the probabilities of N classes.', '201': 'These two-class trees can be pruned with a fixed pruning threshold.', '202': 'Hence there is no need to put aside training data for parameter tuning.', '203': '88.7 88.6 88.5 â\x80\x99 c o n t e x t d a t a 2 â\x80\x99 2 3 4 5 6 7 8 9 10 A n ope n que stio n is wh eth er the SV MT ool (or oth er dis cri min ativ ely trai ned tag ger s) cou ld out - perf orm the pre sen ted tag ger if the sa me dec om positi on of PO S tag s and the sa me con text size wasFigure 3: Accuracy on development data depend ing on context size The best accuracy of our tagger on the development set was 88.9% obtained with a context of 4 preceding POS tags.', '204': 'The best accuracy of the TnT tagger was 88.2% with a maximal suffix length of 5.', '205': 'The corresponding figures for the test data are.', '206': '89.53% for our tagger and 88.88% for the TnT tag- ger.', '207': 'The difference is significant.', '208': 'Our tagger combines two ideas, the decomposition of the probability of complex POS tags into a product of feature probabilities, and the estimation of the conditional probabilities with decision trees.', '209': 'A similar idea was previously presented in Kempe (1994), but apparently never applied again.', '210': 'The tagging accuracy reported by Kempe was below that of a traditional trigram tagger.', '211': 'Unlike him, we found that our tagging method outperformed state-of-the-art POS taggers on fine-grained POS tagging even if only a trigram context was used.', '212': 'Schmid (1994) and Ma`rquez (1999) used decision trees for the estimation of contextual tag probabilities, but without a decomposition of the tag probability.', '213': 'Magerman (1994) applied probabilistic decision trees to parsing, but not with a generative model.', '214': 'used.', '215': 'We think that this might be the case if the SVM features are restricted to the set of relevant attribute combinations discovered by the decision tree, but we doubt that it is possible to train the SVMTool (or other discriminatively trained tag- gers) without such a restriction given the difficulties to train it with the standard context size.', '216': 'Czech POS tagging has been extensively studied in the past (HajicË\x87 and VidovaÂ´-HladkaÂ´, 1998; HajicË\x87 et al., 2001; Votrubec, 2006).', '217': 'Spoustov et al.', '218': '(2007) compared several POS taggers including an n-gram tagger and a discriminatively trained tagger (MorcË\x87e), and evaluated them on the Prague Dependency Treebank (PDT 2.0).', '219': 'MorcË\x87eâ\x80\x99s tagging accuracy was 95.12%, 0.3% better than the n-gram tagger.', '220': 'A hybrid system based on four different tagging methods reached an accuracy of 95.68%.', '221': 'Because of the different corpora used and the different amounts of lexical information available, a direct comparison to our results is difficult.', '222': 'Furthermore, our tagger uses no corpus-specific heuristics, whereas MorcË\x87e e.g. is optimized for Czech POS tagging.', '223': 'The German tagging results are, to the best of our knowledge, the first published results for fine- grained POS tagging with the Tiger tagset.', '224': 'We presented a HMM POS tagger for fine-grained tagsets which splits the POS tags into attribute vectors and estimates the conditional probabilities of the attributes with decision trees.', '225': 'In experiments with German and Czech corpora, this method achieved a higher tagging accuracy than two state-of-the-art general-purpose POS taggers (TnT and SVMTool).'}",['C08-1098'],['../data/summaries/C08-1098.txt'],['../data/tba/C08-1098.json']
C10-1045,"Better Arabic Parsing: Baselines, Evaluations, and Analysis",../data/papers/C10-1045.xml,"{'0': 'Better Arabic Parsing: Baselines, Evaluations, and Analysis', '1': 'In this paper, we offer broad insight into the underperformance of Arabic constituency parsing by analyzing the interplay of linguistic phenomena, annotation choices, and model design.', '2': 'First, we identify sources of syntactic ambiguity understudied in the existing parsing literature.', '3': 'Second, we show that although the Penn Arabic Treebank is similar to other tree- banks in gross statistical terms, annotation consistency remains problematic.', '4': 'Third, we develop a human interpretable grammar that is competitive with a latent variable PCFG.', '5': 'Fourth, we show how to build better models for three different parsers.', '6': 'Finally, we show that in application settings, the absence of gold segmentation lowers parsing performance by 2â\x80\x935% F1.', '7': 'It is well-known that constituency parsing models designed for English often do not generalize easily to other languages and treebanks.1 Explanations for this phenomenon have included the relative informativeness of lexicalization (Dubey and Keller, 2003; Arun and Keller, 2005), insensitivity to morphology (Cowan and Collins, 2005; Tsarfaty and Simaâ\x80\x99an, 2008), and the effect of variable word order (Collins et al., 1999).', '8': 'Certainly these linguistic factors increase the difficulty of syntactic disambiguation.', '9': 'Less frequently studied is the interplay among language, annotation choices, and parsing model design (Levy and Manning, 2003; KuÂ¨ bler, 2005).', '10': '1 The apparent difficulty of adapting constituency models to non-configurational languages has been one motivation for dependency representations (HajicË\x87 and ZemaÂ´nek, 2004; Habash and Roth, 2009).', '11': 'To investigate the influence of these factors, we analyze Modern Standard Arabic (henceforth MSA, or simply â\x80\x9cArabicâ\x80\x9d) because of the unusual opportunity it presents for comparison to English parsing results.', '12': 'The Penn Arabic Treebank (ATB) syntactic guidelines (Maamouri et al., 2004) were purposefully borrowed without major modification from English (Marcus et al., 1993).', '13': 'Further, Maamouri and Bies (2004) argued that the English guidelines generalize well to other languages.', '14': 'But Arabic contains a variety of linguistic phenomena unseen in English.', '15': 'Crucially, the conventional orthographic form of MSA text is unvocalized, a property that results in a deficient graphical representation.', '16': 'For humans, this characteristic can impede the acquisition of literacy.', '17': 'How do additional ambiguities caused by devocalization affect statistical learning?', '18': 'How should the absence of vowels and syntactic markers influence annotation choices and grammar development?', '19': 'Motivated by these questions, we significantly raise baselines for three existing parsing models through better grammar engineering.', '20': 'Our analysis begins with a description of syntactic ambiguity in unvocalized MSA text (Â§2).', '21': 'Next we show that the ATB is similar to other tree- banks in gross statistical terms, but that annotation consistency remains low relative to English (Â§3).', '22': 'We then use linguistic and annotation insights to develop a manually annotated grammar for Arabic (Â§4).', '23': 'To facilitate comparison with previous work, we exhaustively evaluate this grammar and two other parsing models when gold segmentation is assumed (Â§5).', '24': 'Finally, we provide a realistic eval uation in which segmentation is performed both in a pipeline and jointly with parsing (Â§6).', '25': 'We quantify error categories in both evaluation settings.', '26': 'To our knowledge, ours is the first analysis of this kind for Arabic parsing.', '27': 'Arabic is a morphologically rich language with a root-and-pattern system similar to other Semitic languages.', '28': 'The basic word order is VSO, but SVO, VOS, and VO configurations are also possible.2 Nouns and verbs are created by selecting a consonantal root (usually triliteral or quadriliteral), which bears the semantic core, and adding affixes and diacritics.', '29': 'Particles are uninflected.', '30': ""Word Head Of Complement POS 1 '01 inna â\x80\x9cIndeed, trulyâ\x80\x9d VP Noun VBP 2 '01 anna â\x80\x9cThatâ\x80\x9d SBAR Noun IN 3 01 in â\x80\x9cIfâ\x80\x9d SBAR Verb IN 4 01 an â\x80\x9ctoâ\x80\x9d SBAR Verb IN Table 1: Diacritized particles and pseudo-verbs that, after orthographic normalization, have the equivalent surface form 0 an."", '31': 'The distinctions in the ATB are linguistically justified, but complicate parsing.', '32': 'Table 8a shows that the best model recovers SBAR at only 71.0% F1.', '33': 'Diacritics can also be used to specify grammatical relations such as case and gender.', '34': 'But diacritics are not present in unvocalized text, which is the standard form of, e.g., news media documents.3 VBD she added VP PUNC S VP VBP NP ...', '35': 'VBD she added VP PUNC â\x80\x9c SBAR IN NP 0 NN.', '36': 'Let us consider an example of ambiguity caused by devocalization.', '37': 'Table 1 shows four words â\x80\x9c 0 Indeed NN Indeed Saddamwhose unvocalized surface forms 0 an are indistinguishable.', '38': 'Whereas Arabic linguistic theory as Saddam (a) Reference (b) Stanford signs (1) and (2) to the class of pseudo verbs 01 +i J>1ï¿½ inna and her sisters since they can beinflected, the ATB conventions treat (2) as a com plementizer, which means that it must be the head of SBAR.', '39': 'Because these two words have identical complements, syntax rules are typically unhelpful for distinguishing between them.', '40': 'This is especially true in the case of quotationsâ\x80\x94which are common in the ATBâ\x80\x94where (1) will follow a verb like (2) (Figure 1).', '41': 'Even with vocalization, there are linguistic categories that are difficult to identify without semantic clues.', '42': 'Two common cases are the attribu tive adjective and the process nominal _; maSdar, which can have a verbal reading.4 At tributive adjectives are hard because they are or- thographically identical to nominals; they are inflected for gender, number, case, and definiteness.', '43': 'Moreover, they are used as substantives much 2 Unlike machine translation, constituency parsing is not significantly affected by variable word order.', '44': 'However, when grammatical relations like subject and object are evaluated, parsing performance drops considerably (Green et al., 2009).', '45': 'In particular, the decision to represent arguments in verb- initial clauses as VP internal makes VSO and VOS configurations difficult to distinguish.', '46': 'Topicalization of NP subjects in SVO configurations causes confusion with VO (pro-drop).', '47': '3 Techniques for automatic vocalization have been studied (Zitouni et al., 2006; Habash and Rambow, 2007).', '48': 'However, the data sparsity induced by vocalization makes it difficult to train statistical models on corpora of the size of the ATB, so vocalizing and then parsing may well not help performance.', '49': ""4 Traditional Arabic linguistic theory treats both of these types as subcategories of noun ï¿½ '.i . Figure 1: The Stanford parser (Klein and Manning, 2002) is unable to recover the verbal reading of the unvocalized surface form 0 an (Table 1)."", '50': 'more frequently than is done in English.', '51': 'Process nominals name the action of the transitive or ditransitive verb from which they derive.', '52': 'The verbal reading arises when the maSdar has an NP argument which, in vocalized text, is marked in the accusative case.', '53': 'When the maSdar lacks a determiner, the constituent as a whole resem bles the ubiquitous annexation construct ï¿½ ?f iDafa.', '54': 'Gabbard and Kulick (2008) show that there is significant attachment ambiguity associated with iDafa, which occurs in 84.3% of the trees in our development set.', '55': 'Figure 4 shows a constituent headed by a process nominal with an embedded adjective phrase.', '56': 'All three models evaluated in this paper incorrectly analyze the constituent as iDafa; none of the models attach the attributive adjectives properly.', '57': 'For parsing, the most challenging form of ambiguity occurs at the discourse level.', '58': 'A defining characteristic of MSA is the prevalence of discourse markers to connect and subordinate words and phrases (Ryding, 2005).', '59': 'Instead of offsetting new topics with punctuation, writers of MSA in sert connectives such as ï¿½ wa and ï¿½ fa to link new elements to both preceding clauses and the text as a whole.', '60': 'As a result, Arabic sentences are usually long relative to English, especially after Length English (WSJ) Arabic (ATB) â\x89¤ 20 41.9% 33.7% â\x89¤ 40 92.4% 73.2% â\x89¤ 63 99.7% 92.6% â\x89¤ 70 99.9% 94.9% Table 2: Frequency distribution for sentence lengths in the WSJ (sections 2â\x80\x9323) and the ATB (p1â\x80\x933).', '61': 'English parsing evaluations usually report results on sentences up to length 40.', '62': 'Arabic sentences of up to length 63 would need to be.', '63': 'evaluated to account for the same fraction of the data.', '64': 'We propose a limit of 70 words for Arabic parsing evaluations.', '65': 'ATB CTB6 Negra WSJ Trees 23449 28278 20602 43948 Word Typess 40972 45245 51272 46348 Tokens 738654 782541 355096 1046829 Tags 32 34 499 45 Phrasal Cats 22 26 325 27 Test OOV 16.8% 22.2% 30.5% 13.2% Per Sentence Table 4: Gross statistics for several different treebanks.', '66': 'Test set OOV rate is computed using the following splits: ATB (Chiang et al., 2006); CTB6 (Huang and Harper, 2009); Negra (Dubey and Keller, 2003); English, sections 221 (train) and section 23 (test).', '67': 'Table 3: Dev set frequencies for the two most significant discourse markers in Arabic are skewed toward analysis as a conjunction.', '68': 'segmentation (Table 2).', '69': 'The ATB gives several different analyses to these words to indicate different types of coordination.', '70': 'But it conflates the coordinating and discourse separator functions of wa (<..4.b ï¿½ ï¿½) into one analysis: conjunction(Table 3).', '71': 'A better approach would be to distin guish between these cases, possibly by drawing on the vast linguistic work on Arabic connectives (AlBatal, 1990).', '72': 'We show that noun-noun vs. discourse-level coordination ambiguity in Arabic is a significant source of parsing errors (Table 8c).', '73': '3.1 Gross Statistics.', '74': 'Linguistic intuitions like those in the previous section inform language-specific annotation choices.', '75': 'The resulting structural differences between tree- banks can account for relative differences in parsing performance.', '76': 'We compared the ATB5 to tree- banks for Chinese (CTB6), German (Negra), and English (WSJ) (Table 4).', '77': 'The ATB is disadvantaged by having fewer trees with longer average 5 LDC A-E catalog numbers: LDC2008E61 (ATBp1v4), LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).', '78': 'We map the ATB morphological analyses to the shortened â\x80\x9cBiesâ\x80\x9d tags for all experiments.', '79': 'yields.6 But to its great advantage, it has a high ratio of non-terminals/terminals (Î¼ Constituents / Î¼ Length).', '80': 'Evalb, the standard parsing metric, is biased toward such corpora (Sampson and Babarczy, 2003).', '81': 'Also surprising is the low test set OOV rate given the possibility of morphological variation in Arabic.', '82': 'In general, several gross corpus statistics favor the ATB, so other factors must contribute to parsing underperformance.', '83': '3.2 Inter-annotator Agreement.', '84': 'Annotation consistency is important in any supervised learning task.', '85': 'In the initial release of the ATB, inter-annotator agreement was inferior to other LDC treebanks (Maamouri et al., 2008).', '86': 'To improve agreement during the revision process, a dual-blind evaluation was performed in which 10% of the data was annotated by independent teams.', '87': 'Maamouri et al.', '88': '(2008) reported agreement between the teams (measured with Evalb) at 93.8% F1, the level of the CTB.', '89': 'But Rehbein and van Genabith (2007) showed that Evalb should not be used as an indication of real differenceâ\x80\x94 or similarityâ\x80\x94between treebanks.', '90': 'Instead, we extend the variation n-gram method of Dickinson (2005) to compare annotation error rates in the WSJ and ATB.', '91': 'For a corpus C, let M be the set of tuples â\x88\x97n, l), where n is an n-gram with bracketing label l. If any n appears 6 Generative parsing performance is known to deteriorate with sentence length.', '92': 'As a result, Habash et al.', '93': '(2006) developed a technique for splitting and chunking long sentences.', '94': 'In application settings, this may be a profitable strategy.', '95': 'NN ï¿½ .e NP NNP NP DTNNP NN ï¿½ .e NP NP NNP NP Table 5: Evaluation of 100 randomly sampled variation nuclei types.', '96': 'The samples from each corpus were independently evaluated.', '97': 'The ATB has a much higher fraction of nuclei per tree, and a higher type-level error rate.', '98': 'summit Sharm (a) Al-Sheikh summit Sharm (b) DTNNP Al-Sheikh in a corpus position without a bracketing label, then we also add â\x88\x97n, NIL) to M. We call the set of unique n-grams with multiple labels in M the variation nuclei of C. Bracketing variation can result from either annotation errors or linguistic ambiguity.', '99': 'Human evaluation is one way to distinguish between the two cases.', '100': 'Following Dickinson (2005), we randomly sampled 100 variation nuclei from each corpus and evaluated each sample for the presence of an annotation error.', '101': 'The human evaluators were a non-native, fluent Arabic speaker (the first author) for the ATB and a native English speaker for the WSJ.7 Table 5 shows type- and token-level error rates for each corpus.', '102': 'The 95% confidence intervals for type-level errors are (5580, 9440) for the ATB and (1400, 4610) for the WSJ.', '103': 'The results clearly indicate increased variation in the ATB relative to the WSJ, but care should be taken in assessing the magnitude of the difference.', '104': 'On the one hand, the type-level error rate is not calibrated for the number of n-grams in the sample.', '105': 'At the same time, the n-gram error rate is sensitive to samples with extreme n-gram counts.', '106': 'For example, one of the ATB samples was the determiner -"""" ; dhalikâ\x80\x9cthat.â\x80\x9d The sample occurred in 1507 corpus po sitions, and we found that the annotations were consistent.', '107': 'If we remove this sample from the evaluation, then the ATB type-level error rises to only 37.4% while the n-gram error rate increases to 6.24%.', '108': 'The number of ATB n-grams also falls below the WSJ sample size as the largest WSJ sample appeared in only 162 corpus positions.', '109': '7 Unlike Dickinson (2005), we strip traces and only con-.', '110': 'Figure 2: An ATB sample from the human evaluation.', '111': 'The ATB annotation guidelines specify that proper nouns should be specified with a flat NP (a).', '112': 'But the city name Sharm Al- Sheikh is also iDafa, hence the possibility for the incorrect annotation in (b).', '113': 'We can use the preceding linguistic and annotation insights to build a manually annotated Arabic grammar in the manner of Klein and Manning (2003).', '114': 'Manual annotation results in human in- terpretable grammars that can inform future tree- bank annotation decisions.', '115': 'A simple lexicalized PCFG with second order Markovization gives relatively poor performance: 75.95% F1 on the test set.8 But this figure is surprisingly competitive with a recent state-of-the-art baseline (Table 7).', '116': 'In our grammar, features are realized as annotations to basic category labels.', '117': 'We start with noun features since written Arabic contains a very high proportion of NPs.', '118': 'genitiveMark indicates recursive NPs with a indefinite nominal left daughter and an NP right daughter.', '119': 'This is the form of recursive levels in iDafa constructs.', '120': 'We also add an annotation for one-level iDafa (oneLevelIdafa) constructs since they make up more than 75% of the iDafa NPs in the ATB (Gabbard and Kulick, 2008).', '121': 'For all other recursive NPs, we add a common annotation to the POS tag of the head (recursiveNPHead).', '122': 'Base NPs are the other significant category of nominal phrases.', '123': 'markBaseNP indicates these non-recursive nominal phrases.', '124': 'This feature includes named entities, which the ATB marks with a flat NP node dominating an arbitrary number of NNP pre-terminal daughters (Figure 2).', '125': 'For verbs we add two features.', '126': 'First we mark any node that dominates (at any level) a verb sider POS tags when pre-terminals are the only intervening nodes between the nucleus and its bracketing (e.g., unaries, base NPs).', '127': 'Since our objective is to compare distributions of bracketing discrepancies, we do not use heuristics to prune the set of nuclei.', '128': '8 We use head-finding rules specified by a native speaker.', '129': 'of Arabic.', '130': 'This PCFG is incorporated into the Stanford Parser, a factored model that chooses a 1-best parse from the product of constituency and dependency parses.', '131': 'termined by the category of the word that follows it.', '132': 'Because conjunctions are elevated in the parse trees when they separate recursive constituents, we choose the right sister instead of the category of the next word.', '133': 'We create equivalence classes for verb, noun, and adjective POS categories.', '134': 'Table 6: Incremental dev set results for the manually annotated grammar (sentences of length â\x89¤ 70).', '135': 'phrase (markContainsVerb).', '136': 'This feature has a linguistic justification.', '137': ""Historically, Arabic grammar has identified two sentences types: those that begin with a nominal (ï¿½ '.i ï¿½u _.."", '138': '), and thosethat begin with a verb (ï¿½ ub..i ï¿½u _..', '139': 'But for eign learners are often surprised by the verbless predications that are frequently used in Arabic.', '140': 'Although these are technically nominal, they have become known as â\x80\x9cequationalâ\x80\x9d sentences.', '141': 'mark- ContainsVerb is especially effective for distinguishing root S nodes of equational sentences.', '142': 'We also mark all nodes that dominate an SVO configuration (containsSVO).', '143': 'In MSA, SVO usually appears in non-matrix clauses.', '144': 'Lexicalizing several POS tags improves performance.', '145': 'splitIN captures the verb/preposition idioms that are widespread in Arabic.', '146': 'Although this feature helps, we encounter one consequence of variable word order.', '147': 'Unlike the WSJ corpus which has a high frequency of rules like VP â\x86\x92VB PP, Arabic verb phrases usually have lexi calized intervening nodes (e.g., NP subjects and direct objects).', '148': 'For example, we might have VP â\x86\x92 VB NP PP, where the NP is the subject.', '149': 'This annotation choice weakens splitIN.', '150': 'We compare the manually annotated grammar, which we incorporate into the Stanford parser, to both the Berkeley (Petrov et al., 2006) and Bikel (Bikel, 2004) parsers.', '151': 'All experiments use ATB parts 1â\x80\x933 divided according to the canonical split suggested by Chiang et al.', '152': '(2006).', '153': 'Preprocessing the raw trees improves parsing performance considerably.9 We first discard all trees dominated by X, which indicates errors and non-linguistic text.', '154': 'At the phrasal level, we remove all function tags and traces.', '155': 'We also collapse unary chains withidentical basic categories like NP â\x86\x92 NP.', '156': 'The pre terminal morphological analyses are mapped to the shortened â\x80\x9cBiesâ\x80\x9d tags provided with the tree- bank.', '157': 'Finally, we add â\x80\x9cDTâ\x80\x9d to the tags for definite nouns and adjectives (Kulick et al., 2006).', '158': 'The orthographic normalization strategy we use is simple.10 In addition to removing all diacritics, we strip instances of taTweel J=J4.i, collapse variants of alif to bare alif,11 and map Ara bic punctuation characters to their Latin equivalents.', '159': 'We retain segmentation markersâ\x80\x94which are consistent only in the vocalized section of the treebankâ\x80\x94to differentiate between e.g. ï¿½ â\x80\x9ctheyâ\x80\x9d and ï¿½ + â\x80\x9ctheir.â\x80\x9d Because we use the vocalized section, we must remove null pronoun markers.', '160': 'In Table 7 we give results for several evaluation metrics.', '161': 'Evalb is a Java re-implementation of the standard labeled precision/recall metric.12 The ATB gives all punctuation a single tag.', '162': 'For parsing, this is a mistake, especially in the case of interrogatives.', '163': 'splitPUNC restores the convention of the WSJ.', '164': 'We also mark all tags that dominate a word with the feminine ending :: taa mar buuTa (markFeminine).', '165': 'To differentiate between the coordinating and discourse separator functions of conjunctions (Table 3), we mark each CC with the label of its right sister (splitCC).', '166': 'The intuition here is that the role of a discourse marker can usually be de 9 Both the corpus split and pre-processing code are avail-.', '167': 'able at http://nlp.stanford.edu/projects/arabic.shtml.', '168': '10 Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation.', '169': '11 taTweel (-) is an elongation character used in Arabic script to justify text.', '170': 'It has no syntactic function.', '171': 'Variants of alif are inconsistently used in Arabic texts.', '172': 'For alif with hamza, normalization can be seen as another level of devocalization.', '173': '12 For English, our Evalb implementation is identical to the most recent reference (EVALB20080701).', '174': 'For Arabic we M o d e l S y s t e m L e n g t h L e a f A n c e s t o r Co rpu s Sent Exact E v a l b L P LR F1 T a g % B a s e l i n e 7 0 St an for d (v 1.', '175': '6. 3) all G o l d P O S 7 0 0.7 91 0.825 358 0.7 73 0.818 358 0.8 02 0.836 452 80.', '176': '37 79.', '177': '36 79.', '178': '86 78.', '179': '92 77.', '180': '72 78.', '181': '32 81.', '182': '07 80.', '183': '27 80.', '184': '67 95.', '185': '58 95.', '186': '49 99.', '187': '95 B a s e li n e ( S e lf t a g ) 70 a l l B i k e l ( v 1 . 2 ) B a s e l i n e ( P r e t a g ) 7 0 a l l G o l d P O S 70 0.7 70 0.801 278 0.7 52 0.794 278 0.7 71 0.804 295 0.7 52 0.796 295 0.7 75 0.808 309 77.', '188': '92 76.', '189': '00 76.', '190': '95 76.', '191': '96 75.', '192': '01 75.', '193': '97 78.', '194': '35 76.', '195': '72 77.', '196': '52 77.', '197': '31 75.', '198': '64 76.', '199': '47 78.', '200': '83 77.', '201': '18 77.', '202': '99 94.', '203': '64 94.', '204': '63 95.', '205': '68 95.', '206': '68 96.', '207': '60 ( P e tr o v, 2 0 0 9 ) all B e r k e l e y ( S e p . 0 9 ) B a s e l i n e 7 0 a l l G o l d P O S 70 â\x80\x94 â\x80\x94 â\x80\x94 0 . 8 0 9 0.839 335 0 . 7 9', '208': '0 . 8 3 1 0.859 496 76.', '209': '40 75.', '210': '30 75.', '211': '85 82.', '212': '32 81.', '213': '63 81.', '214': '97 81.', '215': '43 80.', '216': '73 81.', '217': '08 84.', '218': '37 84.', '219': '21 84.', '220': '29 â\x80\x94 95.', '221': '07 95.', '222': '02 99.', '223': '87 Table 7: Test set results.', '224': 'Maamouri et al.', '225': '(2009b) evaluated the Bikel parser using the same ATB split, but only reported dev set results with gold POS tags for sentences of length â\x89¤ 40.', '226': 'The Bikel GoldPOS configuration only supplies the gold POS tags; it does not force the parser to use them.', '227': 'We are unaware of prior results for the Stanford parser.', '228': 'F1 85 Berkeley 80 Stanford.', '229': 'Bikel 75 training trees 5000 10000 15000 Figure 3: Dev set learning curves for sentence lengths â\x89¤ 70.', '230': 'All three curves remain steep at the maximum training set size of 18818 trees.', '231': 'The Leaf Ancestor metric measures the cost of transforming guess trees to the reference (Sampson and Babarczy, 2003).', '232': 'It was developed in response to the non-terminal/terminal bias of Evalb, but Clegg and Shepherd (2005) showed that it is also a valuable diagnostic tool for trees with complex deep structures such as those found in the ATB.', '233': 'For each terminal, the Leaf Ancestor metric extracts the shortest path to the root.', '234': 'It then computes a normalized Levenshtein edit distance between the extracted chain and the reference.', '235': 'The range of the score is between 0 and 1 (higher is better).', '236': 'We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores along add a constraint on the removal of punctuation, which has a single tag (PUNC) in the ATB.', '237': 'Tokens tagged as PUNC are not discarded unless they consist entirely of punctuation.', '238': 'with the number of exactly matching guess trees.', '239': '5.1 Parsing Models.', '240': 'The Stanford parser includes both the manually annotated grammar (Â§4) and an Arabic unknown word model with the following lexical features: 1.', '241': 'Presence of the determiner J Al. 2.', '242': 'Contains digits.', '243': '3.', '244': 'Ends with the feminine affix :: p. 4.', '245': 'Various verbal (e.g., ï¿½, .::) and adjectival.', '246': 'suffixes (e.g., ï¿½=) Other notable parameters are second order vertical Markovization and marking of unary rules.', '247': 'Modifying the Berkeley parser for Arabic is straightforward.', '248': 'After adding a ROOT node to all trees, we train a grammar using six split-and- merge cycles and no Markovization.', '249': 'We use the default inference parameters.', '250': 'Because the Bikel parser has been parameter- ized for Arabic by the LDC, we do not change the default model settings.', '251': 'However, when we pre- tag the inputâ\x80\x94as is recommended for Englishâ\x80\x94 we notice a 0.57% F1 improvement.', '252': 'We use the log-linear tagger of Toutanova et al.', '253': '(2003), which gives 96.8% accuracy on the test set.', '254': '5.2 Discussion.', '255': 'The Berkeley parser gives state-of-the-art performance for all metrics.', '256': 'Our baseline for all sentence lengths is 5.23% F1 higher than the best previous result.', '257': 'The difference is due to more careful S-NOM NP NP NP VP VBG :: b NP restoring NP ADJP NN :: b NP NN NP NP ADJP DTJJ ADJP DTJJ NN :: b NP NP NP ADJP ADJP DTJJ J ..i NN :: b NP NP NP ADJP ADJP DTJJ NN _;ï¿½ NP PRP DTJJ DTJJ J ..i _;ï¿½ PRP J ..i NN _;ï¿½ NP PRP DTJJ NN _;ï¿½ NP PRP DTJJ J ..i role its constructive effective (b) Stanford (c) Berkeley (d) Bik el (a) Reference Figure 4: The constituent Restoring of its constructive and effective role parsed by the three different models (gold segmentation).', '258': 'The ATB annotation distinguishes between verbal and nominal readings of maSdar process nominals.', '259': 'Like verbs, maSdar takes arguments and assigns case to its objects, whereas it also demonstrates nominal characteristics by, e.g., taking determiners and heading iDafa (Fassi Fehri, 1993).', '260': 'In the ATB, :: b astaâ\x80\x99adah is tagged 48 times as a noun and 9 times as verbal noun.', '261': 'Consequently, all three parsers prefer the nominal reading.', '262': 'Table 8b shows that verbal nouns are the hardest pre-terminal categories to identify.', '263': 'None of the models attach the attributive adjectives correctly.', '264': 'pre-processing.', '265': 'However, the learning curves in Figure 3 show that the Berkeley parser does not exceed our manual grammar by as wide a margin as has been shown for other languages (Petrov, 2009).', '266': 'Moreover, the Stanford parser achieves the most exact Leaf Ancestor matches and tagging accuracy that is only 0.1% below the Bikel model, which uses pre-tagged input.', '267': 'In Figure 4 we show an example of variation between the parsing models.', '268': 'We include a list of per-category results for selected phrasal labels, POS tags, and dependencies in Table 8.', '269': 'The errors shown are from the Berkeley parser output, but they are representative of the other two parsing models.', '270': '6 Joint Segmentation and Parsing.', '271': 'Although the segmentation requirements for Arabic are not as extreme as those for Chinese, Arabic is written with certain cliticized prepositions, pronouns, and connectives connected to adjacent words.', '272': 'Since these are distinct syntactic units, they are typically segmented.', '273': 'The ATB segmentation scheme is one of many alternatives.', '274': 'Until now, all evaluations of Arabic parsingâ\x80\x94including the experiments in the previous sectionâ\x80\x94have assumed gold segmentation.', '275': 'But gold segmentation is not available in application settings, so a segmenter and parser are arranged in a pipeline.', '276': 'Segmentation errors cascade into the parsing phase, placing an artificial limit on parsing performance.', '277': 'Lattice parsing (Chappelier et al., 1999) is an alternative to a pipeline that prevents cascading errors by placing all segmentation options into the parse chart.', '278': 'Recently, lattices have been used successfully in the parsing of Hebrew (Tsarfaty, 2006; Cohen and Smith, 2007), a Semitic language with similar properties to Arabic.', '279': 'We extend the Stanford parser to accept pre-generated lattices, where each word is represented as a finite state automaton.', '280': 'To combat the proliferation of parsing edges, we prune the lattices according to a hand-constructed lexicon of 31 clitics listed in the ATB annotation guidelines (Maamouri et al., 2009a).', '281': 'Formally, for a lexicon L and segments I â\x88\x88 L, O â\x88\x88/ L, each word automaton accepts the language Iâ\x88\x97(O + I)Iâ\x88\x97.', '282': 'Aside from adding a simple rule to correct alif deletion caused by the preposition J, no other language-specific processing is performed.', '283': 'Our evaluation includes both weighted and un- weighted lattices.', '284': 'We weight edges using a unigram language model estimated with Good- Turing smoothing.', '285': 'Despite their simplicity, uni- gram weights have been shown as an effective feature in segmentation models (Dyer, 2009).13 The joint parser/segmenter is compared to a pipeline that uses MADA (v3.0), a state-of-the-art Arabic segmenter, configured to replicate ATB segmentation (Habash and Rambow, 2005).', '286': 'MADA uses an ensemble of SVMs to first re-rank the output of a deterministic morphological analyzer.', '287': 'For each 13 Of course, this weighting makes the PCFG an improper distribution.', '288': 'However, in practice, unknown word models also make the distribution improper.', '289': 'Parent Head Modif er Dir # gold F1 Label # gold F1 NP NP TAG R 946 0.54 ADJP 1216 59.45 S S S R 708 0.57 SBAR 2918 69.81 NP NP ADJ P R 803 0.64 FRAG 254 72.87 NP NP N P R 2907 0.66 VP 5507 78.83 NP NP SBA R R 1035 0.67 S 6579 78.91 NP NP P P R 2713 0.67 PP 7516 80.93 VP TAG P P R 3230 0.80 NP 34025 84.95 NP NP TAG L 805 0.85 ADVP 1093 90.64 VP TAG SBA R R 772 0.86 WHN P 787 96.00 S VP N P L 961 0.87 (a) Major phrasal categories (b) Major POS categories (c) Ten lowest scoring (Collins, 2003)-style dependencies occurring more than 700 times Table 8: Per category performance of the Berkeley parser on sentence lengths â\x89¤ 70 (dev set, gold segmentation).', '290': '(a) Of the high frequency phrasal categories, ADJP and SBAR are the hardest to parse.', '291': 'We showed in Â§2 that lexical ambiguity explains the underperformance of these categories.', '292': '(b) POS tagging accuracy is lowest for maSdar verbal nouns (VBG,VN) and adjectives (e.g., JJ).', '293': 'Richer tag sets have been suggested for modeling morphologically complex distinctions (Diab, 2007), but we find that linguistically rich tag sets do not help parsing.', '294': '(c) Coordination ambiguity is shown in dependency scores by e.g., â\x88\x97SSS R) and â\x88\x97NP NP NP R).', '295': 'â\x88\x97NP NP PP R) and â\x88\x97NP NP ADJP R) are both iDafa attachment.', '296': 'input token, the segmentation is then performed deterministically given the 1-best analysis.', '297': 'Since guess and gold trees may now have different yields, the question of evaluation is complex.', '298': 'Cohen and Smith (2007) chose a metric like SParseval (Roark et al., 2006) that first aligns the trees and then penalizes segmentation errors with an edit-distance metric.', '299': 'But we follow the more direct adaptation of Evalb suggested by Tsarfaty (2006), who viewed exact segmentation as the ultimate goal.', '300': 'Therefore, we only score guess/gold pairs with identical character yields, a condition that allows us to measure parsing, tagging, and segmentation accuracy by ignoring whitespace.', '301': 'Table 9 shows that MADA produces a high quality segmentation, and that the effect of cascading segmentation errors on parsing is only 1.92% F1.', '302': 'However, MADA is language-specific and relies on manually constructed dictionaries.', '303': 'Conversely, the lattice parser requires no linguistic resources and produces segmentations of comparable quality.', '304': 'Nonetheless, parse quality is much lower in the joint model because a lattice is effectively a long sentence.', '305': 'A cell in the bottom row of the parse chart is required for each potential whitespace boundary.', '306': 'As we have said, parse quality decreases with sentence length.', '307': 'Finally, we note that simple weighting gives nearly a 2% F1 improvement, whereas Goldberg and Tsarfaty (2008) found that unweighted lattices were more effective for Hebrew.', '308': 'Table 9: Dev set results for sentences of length â\x89¤ 70.', '309': 'Coverage indicates the fraction of hypotheses in which the character yield exactly matched the reference.', '310': 'Each model was able to produce hypotheses for all input sentences.', '311': 'In these experiments, the input lacks segmentation markers, hence the slightly different dev set baseline than in Table 6.', '312': 'By establishing significantly higher parsing baselines, we have shown that Arabic parsing performance is not as poor as previously thought, but remains much lower than English.', '313': 'We have described grammar state splits that significantly improve parsing performance, catalogued parsing errors, and quantified the effect of segmentation errors.', '314': 'With a human evaluation we also showed that ATB inter-annotator agreement remains low relative to the WSJ corpus.', '315': 'Our results suggest that current parsing models would benefit from better annotation consistency and enriched annotation in certain syntactic configurations.', '316': 'Acknowledgments We thank Steven Bethard, Evan Rosen, and Karen Shiells for material contributions to this work.', '317': 'We are also grateful to Markus Dickinson, Ali Farghaly, Nizar Habash, Seth Kulick, David McCloskey, Claude Reichard, Ryan Roth, and Reut Tsarfaty for constructive discussions.', '318': 'The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship.', '319': 'This paper is based on work supported in part by DARPA through IBM.', '320': 'The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.'}",['C10-1045'],['../data/summaries/C10-1045.txt'],['../data/tba/C10-1045.json']
C90-2039,Strategic Lazy Incremental Copy Graph Unification,../data/papers/C90-2039.xml,"{'0': 'Strategic Lazy Incremental Copy Graph Unification', '1': 'The strategic lazy incremental copy graph unification method is a combination of two methods for unifying hmture structures.', '2': 'One, called the lazy incremental copy graph unification method, achieves structure sharing with constant order data access time which reduces the cequired memory.', '3': ""The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify :;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation."", '4': 'The combined method .makes each feature structure unification efficient and also reduces garbage collection and page swapping occurrences, thus increasing the total efficiency of natural language processing systems mainly based on I.yped feature structure unification such as natural language analysis and generation sysl~ems.', '5': ""Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871."", '6': ""These furnmlisms were developed relatively independentIy but actually had common properties; th'~t is, they used data structures called ftmctional structures or feature structures and they were based on unilieathm operation on these data structures."", '7': 'These formalisms were applied in the field of natural language processing and, based on these formalisms, ~:~ystems such as machine translation systems were developed [l<ol;u, e et a l 8gJ.', '8': 'In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation.', '9': 'The efficiency of systems based on ..~uch formalisms, such as natural language analysis and generation systems very much depends on their FS ~lnifieatlon efficiencies.', '10': 'Tiffs dependency is especially crucial for lexicon-driven approaches such as tlPSO[Pollard and Sag 861 and JPSG[Gunji 871 because rich lexieal information and phrase structure information is described in terms of FSs.', '11': 'For example, a spoken Present.', '12': 'affiliation: Infi)rmation Science Research 1,aboratory, NTT Basic Research i.aboratories.', '13': ""lh'esenl, address: 9 11, Midori cho 3-theme, Musashinoshi, Tokyo 180, Japan."", '14': 'Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.', '15': ""Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871."", '16': 'These methods uses rooted directed graphs (DGs) to represent FSs.', '17': 'These methods take two DGs as their inputs and give a unification result DG.', '18': 'Previous research identified DG copying as a significant overhead.', '19': 'Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).', '20': 'Ile proposed an incremental copy graph unification method to avoid over copying and early copying.', '21': 'itowever, the problem with his method is that a unitication result graph consists only of newly created structures.', '22': 'This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.', '23': 'Copying sharable parts is called redundant copying.', '24': 'A better method would nfinimize the copying of sharable varts.', '25': 'The redundantly copied parts are relatively large when input graphs have few common feature paths.', '26': 'In natural language processing, such cases are ubiquitous.', '27': 'I""or example, in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure, such eases occur very h\'equent, ly.', '28': ""In Kasper's disjunctive feature description unification [Kasper 861, such cases occur very h'equently in unifying definite and disjunct's definite parts."", '29': 'Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.', '30': 'I)eveloping a method which avoids memory wastage is very important.', '31': ""Pereira's structure sharing FS unification method can avoid this problem."", '32': 'The method achieves structure sharing by importing the Bayer and Moore approach for term structurestl~oyer and Moore 721.', '33': 'The method uses a data structure consisting of a skeleton part to represent original information and an environment part to represent updated information.', '34': ""3'he skeleton part is shared by one of the input FSs and the result FS."", '35': ""Therefore, Pereira's method needs relatively few new structures when two input FSs are difference in size and which input is larger are known before unification."", '36': ""However, Pereira's method can create skeleton-enviromnent structures that are deeply embedded, for example, in reeursively constructing large phrase structure fl'om their parts."", '37': 'This causes O(log d) graph node access time overhead in assembling the whole DG from the skeleton and environments where d is the number of nodes in the DG.', '38': 'Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing.', '39': ""This paper proposes an FS unification method that allows structure sharing with constant m'der node access time."", '40': ""This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method."", '41': 'The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short).', '42': 'In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms.', '43': 'The advantages of such a system include: (1)rule writers are not required to describe control infimnation such as eonstraiut application order in a rule, and (12)rule descriptions can be used iu different processing directions, i.e., analysis and general,ion.', '44': 'However, these advantages in describing rules are disadvantages in applying them because of tt~e lack of control information.', '45': ""For example, when constructing a phrase structure from its parts (e.g., a sentence fi'om a subject NP and VP), unueeessary computation can be reduced if the semantic representation is assembled after checking constraints such as grammatical agreements, which can fail."", '46': 'This is impossible in straightforward unification-based formalisms.', '47': ""In contrast, in a procedure-based system which uses IF-TItEN style rules (i.e., consisting of explicit test and structure-building operations), it is possible to construct the semantic representation (TIIEN par'g) after checking the agreement (IF part)."", '48': 'Such a system has the advantage of processing efficiency but the disadvantage of lacking multidirectionality.', '49': 'In this paper, some of the efficiency of the procedure- based system is introduced into an FS unification-based system.', '50': 'That is, an FS unification method is proposed that introduces a strategy called the e_arly failure Â£inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures.', '51': 'This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).', '52': 'These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).', '53': 'Section 2 explains typed feature structures (TFSs) and unification on them.', '54': ""Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method."", '55': 'The section also introduces the key idea of the EFF strategy wlfich comes from observations of his method.', '56': 'Section 3 and 4 introduce the LING method and the SING method, respectively.', '57': 'Ordinary FSs used in unification-based grammar formalisms such as PAT].{[Shieher 851 arc classified into two classes, namely, atomic leSs and complex FSs.', '58': 'An atomic FS is represented by an atomic symbol and a complex FS is represented by a set of feature-value pairs.', '59': 'Complex FSs are used to partially describe objects by specifying values for certain features or attributes of described objects.', '60': 'Complex FSs can have complex FSs as their feature values and can share certain values among features.', '61': 'For ordinary FSs, unification is defined by using partial ordering based on subsumption relationships.', '62': 'These properties enable flexible descriptions.', '63': 'An extension allows complex FSs to have type symbols which define a lattice structure on them, for example, as in [Pollard and Sag 8""11.', '64': 'The type symbol lattice contains the greatest type symbol Top, which subsumes every type symbol, and the least type symbol Bottom, which is subsumed by every I.ype symbol.', '65': 'An example of a type symbol lattice is shown in Fig.', '66': '1.', '67': 'An extended complex FS is represented by a type symbol and a set of feature-value pairs.', '68': 'Once complex IeSs are extended as above, an atomic FS can be seen as an extended complex FS whose type symbol has only Top as its greater type symbol and only Bottom as its lesser type symbol and which has an empty set of feature value pairs.', '69': 'Extended complex FSs are called typed feature structures (TFSs).', '70': 'TFSs are denoted by feature-value pair matrices or rooted directed graphs as shown in Fig.', '71': '2.', '72': ""Among such structures, unification c'm be defined IAP,- Kaci 861 by using the following order; ATFS tl is less than or equal to a TFS t2 if and only if: Â\x95 the type symbol of tl is less than or equal to the type syn'bol of/2; and Â\x95 each of the features of t2 exists in t1 and."", '73': 'has as its value a TFS which is not less than its counterpart in tl ; and each of the coreference relationships in t2 is also held in tl.', '74': 'Top Sign Syn Head List POS /77 Lexical Phrase Sign NonEmpty Empty V N P ADV Slgn Li.', '75': 'Lis~ ust I I I I NonEmpty Emply I I i I Sign Sign I I/ / List List 5/ /5 ....', '76': 'U_ Bottom Figure 1: Exainple of a type symbol lattice --2-- peSymbÂ°10 eaturel TypeSymboll ] ]] I feature2 TypeSymbol2 I feature3 ?Tag T ypeSymbol3 ] ]feature4 TypeSymbol4 L [.feature5 TypeSymbol5 TIeature3 7Tag (a) feature-value matrix notation ""?"" i~ the prefix for a tag and TFSs with the same tag are token-identical.', '77': 'TypeSym bol/~ feo~.,o/ I TypeSymboll ~ [.', '78': 'TypeSymbol2 4Â¢"" \'~Â°~\'~/.~ypeSymbol3 featury ""X~ature5 TypeSymbol4 4r ""~TypeSymbol5 (b) directed graph notation Figure 2: TFS notations Phrase [sub(at ?X2 SignList ] dtrs CHconst Sign U Syn i\'oo I syn I head ?Xl . ] ubcat NonEmptySignLIst | [\'first ]1 ?Ã\x973 Lrest ?X2 J j Phrase -dtrs CHconst hdtr LexicalSignsyn Syn -head Head pos P orm Ga subcat NonEmptySignList Sign ,11 yn Synead Head L~,os N] Irest EmptySignkist Phrase ""syn Syn head ?X1 Head Fpos P Lform Ga ] Lsubcat ?X2 Empl.ySignList dtrs CHconst ccltr ?X3 Sign syn iyn head Head _ [pos N hdtr LexicalSign l-syn Syn l I F head :x~ 7/ Lsubcat [ NonEinptySignList l l P"""" ~Ã\x97~ llll Lrest ?X2 JJjJ Figure 3: Example of TFS unification Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet.', '79': 'A unification example is shown in Fig.', '80': '3.', '81': 'In tile directed graph notation, TFS unification corresponds to graph mergi ng.', '82': 'TFSs are very convenient for describing linguistic information in unlfication-based formalisms.', '83': ""In TFS unification based on Wrobtewski's method, a DG is represented by tile NODE and ARC structures corresponding to a TFS and a feature-value pair respectively, as shown in Fig."", '84': '4.', '85': 'The NODE structure has the slots TYPESYMBOL to represent a type symbol, ARCS to represent a set of feature-value pairs, GENERATION to specify the unification process in which the structure has been created, FORWARD, and COPY.', '86': ""When a NODE's GENERATION value is equal to the global value specifying the current unit]cation process, the structure has been created in the current process or that the structure is currel~l. The characteristics which allow nondestructive incremental copy are the NODE's two different slots, FORWARD and COPY, for representing forwarding relationships."", '87': 'A FORWARD slot value represents an eternal relationship while a COPY slot value represents a temporary relationship.', '88': 'When a NODE node1 has a NODE node2 as its FORWARD value, the other contents of tile node1 are ignored and tim contents of node2 are used.', '89': 't{owever, when a NODE has another NODE as its COPY value, the contents of the COPY value are used only when the COPY value is cub:rent.', '90': 'After the process finishes, all COPY slot values are ignored and thus original structures are not destroyed.', '91': 'The unification procedure based on this method takes as its input two nodes which are roots of the DGs to be unified.', '92': 'The procedure incrementally copies nodes and ares on the subgraphs of each input 1)G until a node with an empty ARCS value is found.', '93': 'The procedure first dereferences both root nodes of the input DGs (i.e., it follows up FORWARD and COPY slot values).', '94': 'If the dereferenee result nodes arc identical, the procedure finishes and returns one of the dereference result nodes.', '95': 'Next, the procedure calculates the meet of their type symbol.', '96': 'If the meet is Bottom, which means inconsistency, the procedure finishes and returns Bottom.', '97': 'Otherwise, the procedure obtains the output node with the meet as its TYPESYMBOL.', '98': 'The output node has been created only when neither input node is current; or otherwise the output node is an existing current node.', '99': 'Next, the procedure treats arcs.', '100': 'The procedure assumes the existence of two procedures, namely, SharedArcs and ComplementArcs.', '101': 'The SharedArcs procedure takes two lists of arcs as its arguments and gives two lists of arcs each of which contains arcs whose labels exists in both lists with the same arc label order.', '102': 'The ComplementArcs procedure takes two lists of arcs as NODE TYPESYMBOL: <symbol> [ ARCS: <a list of ARC structures > FORWARD: ""<aNODEstructure orNIL> / COPY: < a NODEstructure or Nil, > GENERATION: <an integer> ARC LABEL: <symbol> VALUE: <:a NODEstructure> Figure 4: Data Structures for Wroblewski\'s method Input graph GI Input graph 62 Â¢ .......\'77 ........ i : Sobg,\'aphs not required to be copied L ...........................................', '103': 'Output graph G3 Figure 5: Incremental copy graph unification In this figure, type symbols are omitted.', '104': 'its arguments and gives one list of arcs whose labels are unique to one input list.', '105': 'The unification procedure first treats arc pairs obtained by SharedArcs.', '106': ""The procedure applies itself ,'ecursively to each such arc pair values and adds to the output node every arc with the same label as its label and the unification result of their values unless the tmification result is Bottom."", '107': 'Next, the procedure treats arcs obtained by ComplementArcs.', '108': 'Each arc value is copied and an arc with the same label and the copied value is added to the output node.', '109': 'For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig.', '110': '5.', '111': 'The unification procedure is applied recursively to feature a values of the input nodes.', '112': ""The node specified by the feature path <a> fi'om input graph G1 (Gl/<a>) has an arc with the label c and the corresponding node of input graph G2 does not."", '113': 'The whole subgraph rooted by 6 l/<a c> is then copied.', '114': 'This is because such subgraphs can be modified later.', '115': 'For example, the node Y(G3/<o c g>) will be modified to be the unification result of G 1/<a c g> (or G1/<b d>) and G2/<b d> when the feature path <b d> will be treated.', '116': 'Incremental Copy Graph Unification PROCEDURE Unify(node1, node2) node1 = Dereference(nodel).', '117': 'node2 = Dereferencelnode2).', '118': 'IF Eq?(nodel, node2) THEN Return(node1).', '119': 'ELSE meet = Meet(nodel.typesymbol, node2.typesymbol) IF Equal?(meet, Bottom) THEN Return(Bottom).', '120': 'ELSE outnode = GetOutNode(nodel, node2, meet).', '121': '(sharedst, shareds2) = SharedArcs(nodel.arcs, node2.arcs).', '122': 'complements1 = ComplementArcs(node|.arcs, node2.arcs).', '123': 'complements2 = ComplementArcs(node2.arcs, nodel.arcs).', '124': 'FOR ALL (sharedt, shared2) IN (sharedsl, shareds2) DO arcnode = Unify(sharedl.value, shared2.value).', '125': 'IF Equal?(arcnode, Bottom) ]HEN Return(Bottom).', '126': 'ELSE AddArc(outnode, sharedl.label, arcnode).', '127': ""ENDIF IF Eq?(outnode, node1) THEN coi'nplements = complement2."", '128': 'ELSE IF Eq?(outnode, node2) THEN complements = complementL ELSE complements = Append(complements1, complements2].', '129': 'ENDIF FORALL complement IN complements DO newnode = CopyNode(complement.value).', '130': 'AddArc(outnode, complement.label, newnode).', '131': 'Return(outnode).', '132': ""ENDIF ENDIE ENDPROCEDURE Figure 6: Incremental copy graph unification procedure The problem with Wroblewski's method is that tile whole result DG is created by using only newly created structures."", '133': 'In the example in Fig.', '134': ""5, the subgraphs of the result DG surrounded by the dashed rectangle can be shared with subgraphs of input structures G1 and G2, Section 4 proposes a method t.hat avoids this problem, Wroblewski's method first treats arcs with labels that exist in both input nodes and then treats arcs with unique labels."", '135': 'This order is related to the unification failure tendency.', '136': 'Unification fails in treating arcs with common labels more often than in treating arcs with unique labels.', '137': 'Finding a failure can stop further computation as previously described, and thus finding failures first reduces unnecessary computation.', '138': 'This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels.', '139': 'In Section 5, a method which uses this generalized strategy is proposed.', '140': ""In Wroblewski's method, copying unique label arc values whole in order to treat cases like ]Pig."", '141': '5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.', '142': 'With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.', '143': '5 due to a change of node Y G3/<a c g>).', '144': 'To achieve this, I, he LING unification method, which uses copy dependency information, was developed.', '145': 'The LING unification procedure uses a revised CopyNode procedure which does not copy structures immediately.', '146': 'The revised procedure uses a newly introduced slot COPY-DEPENDENCY.', '147': 'The slot has pairs consisting of nodes and arcs as its value.', '148': ""The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig."", '149': '7): (1) if nodel \', the dereference result of node/, is current, then CopyNode returns node l"" to indicate that the ancestor node node2 must be coiffed immediately; (2)otherwise, CopyArcs is applied to node1"" and if it returns ,~;everal arc copies, CopyNode creates a new copy node.', '150': 'It then adds the arc copies and arcs of node/\' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1"" and returns Nil_.', '151': "",',:opyArcs applies CopyNode to each arc value with node l' as the new ancestor node and returns the set of new arcs for non-Nil_ CopyNode results."", '152': ""When a new copy of a node is needed later, the LING unification procedure will actually copy structures using the COPY-DEPENDENCY slot value of the node (in GetOutNode procedure in lJ'ig."", '153': '6).', '154': 'It substitutes arcs with newly copied nodes for existing arcs.', '155': 'That is, antecedent nodes in the COPY-DEPENDENCY values are also copied.', '156': 'In the above explanation, both COPY-DEPENDENCY and COPY slots are used for the sake of simplicity.', '157': ']lowever, this method can be achieved with only the COPY slot because a node does not have non-NIL COPY-I)EPENDENCY and COPY values simultaneously.', '158': 'The data in the COPY-DEPENDENCY slot are I;emporary and they are discarded during an extensive process such as analyzing a sentence, ttowever, this does not result in any incompleteness or in any partial analysis structure being test.', '159': ""Moreover, data can be accessed in a constant order time relative to the number of DG nodes and need not be reconstructed because this method does not use a data structure consisl, ing of ,';keleton and environments as does Pereira's method."", '160': 'The efficiency of the LING unification method depends on the proportion of newly created structures in the unification result structures.', '161': ""Two worst eases can be considered: (t) If there are no arcs whose labels are unique to an input node witlh respect to each other, the procedure in LING unification method behaves in the same way as the procedure in the Wroblewski's method."", '162': '(2) In the worst eases, in which there are unique label arcs but all result structures are newly created, the method CopyNode PROCEDURE CopyNode(node, arc, ancestor) node = Dereference(node).', '163': 'IF Current?(node) THEN Return(node).', '164': 'ELSE IF NotEmpty?(newarcs = CopyArcs(node)) THEN newnode = Create(node.typesymbol).', '165': 'node.copy = newnode.', '166': 'FOR ALL arc IN node.arcs DO IF NotNIL?(newarc = FindArc(arc.label, newarcs)) THEN AddArc(newnode, newarc.label, newarc.value}.', '167': 'ELSE AddArc(newnode, arc.label, arc.value).', '168': 'ENDIF Returo(newnode).', '169': 'ELSE node.copy-dependency = node.copy-dependency U {Cons(ancestor, arc)}.', '170': 'Return(Nil_).', '171': 'ENDIF ENDPROCEDURE CopyArcs PROCEDURE AlcsCopied(node) newarcs = O- FOR ALL arc IN node.arcs DO newnode = CopyNode(arc.value, arc, node).', '172': 'IF NotNIL?(newnode) THEN newarc = CreateArc(arc.label, newnode).', '173': 'newarcs = {newarc} U newarcs.', '174': 'ENDIF Return(newarcs).', '175': 'ENDPROCEDURE Figure 7: The revised CopyNode procedure has the disadvantage of treating copy dependency information.', '176': 'However, these two cases are very rare.', '177': 'Usually, the number of features in two input structures is relatively small and the sizes of the two input structures are often very different.', '178': 'For example, in Kasper\'s disjunctive feature description unification, a definite part [""S is larger than a disjunet definite part t""S.', '179': 'Method In a system where FS unification is applied, there are features whose values fail relatively often in unification with other values and there are features whose values do not fail so often.', '180': 'For example, in Japanese sentence analysis, unification of features for conjugation forms, case markers, and semantic selectional restrictions tends to fail but unification of features for semantic representations does not fail.', '181': 'In such cases, application of the EFF strategy, that is, treating features tending to fall in unification first, reduces unnecessary computation when the unification finally fails.', '182': 'For example, when unification of features for case markers does fail, treating these features first avoids treating features for senmntic representations.', '183': 'The SING unification method uses this failure tendency infornmtion.', '184': 'These unification failure tendencies depend on systems such as analysis systems or generation systems.', '185': 'Unlike the analysis case, unification of features for semantic representations tends to fail.', '186': 'in this method, theretbre, the failure tendency information is acquired by a learning process.', '187': 'That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.', '188': 'in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.', '189': ""As in TFS unification, failure tendency information is recorded in terms of a triplet consisting of the greatest lower bound type symbol of the input TFSs' type symbols, a feature and success/failure flag."", '190': ""This is because the type symbol of a 'rFS represents salient information on the whole TFS."", '191': 'By using learned failure tendency information, feature value unification is applied in an order that first treats features with the greatest tendency to fail.', '192': 'This is achieved by the sorting procedure of common label arc pairs attached to the meet type symbol.', '193': 'The arc pairs obtained by the SharedArcs procedure are sorted before treating arcs.', '194': 'The efficiency of the SING unification method depends on the following factors: (1) The overall FS unification failure rate of the process: in extreme cases, if Go unification failure occurs, the method has no advantages except the overhead of feature unification order sorting.', '195': 'However, such cases do not occur in practice.', '196': '(2) Number of features FSs have: if each FS has only a small number of features, the efficiency gain from the SING unification method is small.', '197': '(3) Unevenness of FS unification failure tendency: in extreme cases, if every feature has the same unification failure tendency, this method has no advantage.', '198': 'However, such cases do not occur or are very rare, and for example, in many cases of natural language analysis, FS unification failures occur in treating only limited kinds of features related to grammatical agreement such as number and/or person agreement and semantic selectional constraints.', '199': 'In such cases, the SING unification method obtains efl]ciency gains.', '200': 'The above factors can be examined by inspecting failure tendency information, from which the efficiency gain from the SING method can be predicted.', '201': 'Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not.', '202': 'The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.', '203': ""The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method."", '204': ""Structure sharing avoids memory wastage'."", '205': 'Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.', '206': 'This reduces repeated calculation of substructures.', '207': 'The SING unification method introduces the concept of feature unification strategy.', '208': ""'the method treats features tending to fail in unification first."", '209': ""Thus, the efficiency gain fi'om this method is high when the overall FS unification failure rate of the application process is high."", '210': ""The combined method Inakes each FS unification efficient and also reduces garbage collection and page swapping occurrences by avoiding memory wastage, thus increasing the total efficiency of li'S unification-based natural language processing systems such aa analysis and generation systems based on IlI'SG.""}",['C90-2039'],['../data/summaries/C90-2039.txt'],['../data/tba/C90-2039.json']
C94-2154,THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES,../data/papers/C94-2154.xml,"{'0': 'THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICATIONS FOR TYPED FEATURE STRUCTURES', '1': ""in this pa,per, we argue tha, t type inferencing incorrectly implements a.pl)rolwiateness specifica.tions for typed [ea.ture structures, promote a combina.tion of l;ype resolution and unfilling a,s a. correct a.nd ef'~ ticient Mternative, and consider the expressive limits of this a.lterna.tive approa.ch."", '2': ""!['hroughout, we use feature cooccurence restrictions as illustration and linguistic motivation."", '3': 'Unification lbrmMisms ma.y be either un-typed (DCC~s, PATRII, 1,F(;) or typed (npsG).', '4': ""A m~L,ior reason for adding types to ~ forma,lism is to express restrictions on fea.ture cooccurences a.s in (;l's(:: [5] in order to rule out nonexista.nt tyl)es of objects."", '5': 'For example, there a.re no verbs which have the [km.ture +R. The simplest way to express such restrictions is by mea.ns of a.n a.ppropria.teness pa.r-tim flmction Approp: Type × Feat ~ Type.', '6': 'With such a.n a.pl)rol)riatleness specifica.- tion lrla.tly Sllch restrictioi,s may be expressed, though no restrictions involving reentrancies ma.y be expressed.', '7': 'In this pal)er, we will first in §2 survey the range of type eonstra.ints tha.t ma.y be expressed with just a. type hiera.rchy and *\']\'he resea.rch pl\'eS(!lllL(\'d ill |,his; paper was pay tia.lly sponsored hy \'[kfilprojekt B4 ""(;onsl.rahH.s on Grammar fl~r Efficient Ck:neration"" of the Soi,der forschungsbereich 340 of the Deutsche [""orschungsgemeinscha, ft. ""VVe would also like to thank \'l\'hilo GStz for helph,l comments ou thc ideas present.ed here.', '8': 'All mistakes a.rc of collrsc our OWll.', '9': 'IKI.', '10': 'Wilhehnstr.', '11': ""113, |)-721174Tfilfi,,ge,, (ler- ma.ny, {rig,King} g'~sfs.n phil.uni-I uebingen.de."", '12': 'a.n N)propria.teness specification.', '13': ""Then in ~3, we discuss how such type cons|fronts linty be mainta.ined under unification as exemplilied in the na.tura.1 language D~rs- ing/generation system '.l'ro]l [7]."", '14': '1 Unlike previous systems such as ALl,:, Troll does not employ a.ny type infereneing, inste~M, a, limited amount of named disjunction ([1 1], [12], [6])is introduced to record type resol u tion possibilities.', '15': 'The a.lnount of dis- junction is a.lso kept small by the technique of unlilli,g described in [9].', '16': ""This strategy a.ctua.lly ma.inta.ins apl)ropri~tteness conditions in some ca.ses in which a. type in-ferencing stra.tegy would fa.il, l)'inMly, in §4, we discuss the possibilities for genera lizillg this a.pl)roa.ch to ha.ndle a bro~Mer r~tnge of constra.ints, including constraints inw)lving reentran cies."", '17': 'As discussed iu Gerdemann ,~ King [8], one ca.n view a.pl}rol)ria.teness CO[lditions as (lelining GPSG style fea,1;tl re cooccurence restrict:ions (FCRs).', '18': 'In [8], we divided FCRs into co,j,,ctive and di.q,,~ctive ct~sses.', '19': 'A conjunctive FCI/.', '20': ""is a constra.int of the following fornl : i[' a.n object is of ;~ cert;fin kind then ill deserves certa.in fea.tures with wdues of cert~till kinds An FCI~ stat:ing tha,2: a. verb must h~we v and N t'eatures with values A- and -respectively is a.ll example of a. conjunctive FCI{."", '21': 'A disjunctive I""CI{.', '22': 'is of the form: l rl\'he ""]\'roll ,qysl.em was implemented in Quintus Prolog by Dale (lerdemann and \'[\'hilo (]Stz.', '23': ""if an object is of a. cel'taiu kiud then it deserves cerl;a.in [ca,1;tll'C~s with vMues of certa.hi kinds, or it deserves cerl.ahi (pei'ha.liS other) fea.1;u res \\vil, h viiiues of terra.in (perlla.ps other) kinds, or ..."", '24': ""(31:it i:leserw.;s i:erl;a.in (lmrhal)S other) fea,1;llres wil.h Vi, l.[ll(~S o[ certain (perha.ps other) khi,<ls lo::I exa~]nple, the following F(',|/."", '25': ""sl.a.t.iug tha,t inverCed verbs lilt|S1, lie a.uxili;tries is disjunctive: a verb Ilitisl; ha.re the ['(~il.l.tll'(~s INV and AUX with va.l/ies d a.Iid I, -a.iitl i L-, or -;Mid -respectivel.y. Both o| these |el'illS or l,'(',lls iiHly I)(!"", '26': ""expressed in a. foi'llla.iiSlli euiployhi<~ fiiiil.e lia,rtia.[ order (Type, E) o| types tllldel' sub- 8illnptioli> a, finite sel."", '27': 'Feat of ro;./.t;tll.(~s, and an a.pprol)ria.teness parl, ial rliilcl.ion Approp:Type X Feat -~ Type.', '28': '[uluitively; the l, ypes fornla.lize I;lie notion ol"" kinds +,j"" objecl, t g: t,\' ill\' ca.oh oil|eel, of tyl>e t\' i~<i Mso of l;Ylle L, il, ll(] Approp(l, f) = lI ill\' (!;i(\'[I object oF type t deserves [eaA.urt~ f wil.]i :i. Vi./.]lle or type ft. ~@\'e call S/IC]I it.', '29': '[Ol\'tll;liiSlll i-i, ii ;I,])l)l""Opl\']al, olio,~/ fOl\'lllil]i~;lll.', '30': '(\',iLl\'- peliLel"",s AI,F, and (,erdeliia.', '31': ""i ;ill(| (i(~t,z's Troll are ex:-t.niples o| illilllenienl.a.Lions o| a,pF, ro]) ria, Loliess |or illa.[iSlil,s. l low an a.i)ln'oprhi.teness [orniaJisnl enco<les a conjunctive I:(',R is ob\\.'i<>us~ bll(."", '32': 'llOW it encodes a disjuiictive I""(\',1{ is less so.', '33': 'Ali exa.niple i|]usl;ral;es best how it.', '34': 'is done.', '35': ""~Ul)pOS0 that F( ',1{ [i sl.al.es l.hal, ob- .iecls (if type t deserw!"", '36': '[(!a.[./ll\'(!S f \'and .q, I)oth with boolea.I/ wdues a.ll(I [\'lll\'l,[lel\'lllOF(~ that the va.hies of f aild g iil/lSl al~r(!e, [> is the disjunct]w! I""(111.', '37': 'if a,u object is o[ type l then it deserw:s f with va.lue -I- and q with wdue +, or it deserw.~s f with va.lue a.nd 9 with value - To 0ncode [3> first iul,rodLiCe sul/l.yltes , t ~ ;+l.ll([ l"" of I (1 E I/, 1.##), O11(!', '38': ""SUl)tyl)e ['()l' ea,ch disjuuct iu the cousequenl, of'p."", '39': ""Then encode the ]'ea.tli['e/wthl~.~ <'on(!il.illliS in l, he [irst disjunct ILy putthlg Approp(t', ./) :: ~-a,nd Approp(//~ q) -+, and encode the I'eature/value conditions in the second dis-juu(:t by putting Approp(t',f) = -."", '40': 'and Approp(t\',g) = . .\'2 This a pproa,ch Ina, kes two inll)ort;a, lll, closed-world type assumptious a, bouL (.he types tli~d; Slll)SlllIle 11o ogher types (hellCe- forth species), l:irst, the p;i.rtition conditiOII states tha.t for each type t, if a.n object is (31\' type t theu the object is of ex-ax-I.ly o11(2 species subsulned by t. Second, the all-or-nothing cclndition sta, tes that 1\'(31\' each species ,q a.itd fea.ture f, either every el"" IIO ol>,iecl, or species s deserves feature .#c.3 All a.l)ltroltriM,eliess [orli+ia.lisill sllc]l a.s ALl:, ([2], [3])ti,;t.l. does not uieet both c.ouditions llla.y llOt; ]lroper[y el|cOde a, disjull(\'- five l""(:l/.', '41': 'For exalnple, consider disjunctive I""CI{.', '42': 'p. An a.I)prl;)pria.l, elleSS [ornia.l--iSlli I/lily l/O( properly encode 1,hi~t t / a.lld t"" i\'el)rt,selil, MI a.lid oilly the disjuncl, s ill the COll.qeqll(Hlt or [i wiLhout the i)a.rl,ition COll-d]tion.', '43': ""<till a.llln'ol)riill.eness [orlila.liSlll llia,y IIOl."", '44': ""llrOl)erly encode the [t~ii.l.llle/vii.hle (:(lll-<liiriOii: deinanded liy em'h disjuncl, hi the COli.~t!qllelil."", '45': ""o| p wilhoul, the a.i[-Ol'-liot;hilig c(m(til.ion."", '46': 'As indicat.ed a.bove, AI, I.; is iLIi exa.tlli)le o| it.', '47': ""f(n'liialiSlU I.ha.l, does it(it ineel; llol;h o| 1.hese closed world aS,glllnlil,iOli.g. In AI+E :-/."", '48': ""['eli.l.tlr(~ st.i'llCtlile i.<4 won typed ifl' for ea.ch arc iit the te:+d.ure sI.l'tlCl;tlr0, if' 1,he SOtll'('(~ node is labelled wil.h type /., the targel; node is lallelled with 1;ype l / a.lld the il.i'c is IMlelled with [ea.tlll'(~ f 1,lien Approp(/.> .f) [ l/."", '49': ""Furl.her|note> a ['eal, urt~ strut(tire is >l'lds exanll)h: I:(JR is, for eXlmsil.ory l)nrl)oses, quilt simph'."", '50': '""l\'hc prolileni o[ c.xpr(\'.sshig F(Jl/\'s, however, is a l\'Cal Iiuguisl.ic i)rol)lcin.', '51': 'As noted I)y Copcstakc.', '52': 'ct al. [4], it.', '53': ""was inipossihlc I.o c.xpress CV('II Ihc .~ilii[)]oM."", '54': 'forilis o[ l""(JRs in l.hc.ii7 c×tciidcd VCISiOII (it\' AI.E. \'[\'hc basic principle of expressing l""Clls also ex lends Io I""(\',[(s iuvolviug longer palhs.', '55': ""For example, to (:llSllt't: thai."", '56': ""for the type l, I.he path (fg) lakes a vahie subsuuied I)y .% one nlust tirst hll, ro ducc the chaiu Approp(/, f) = .,, Approp('a, g) = .~."", '57': ""Silch ilil.crlllCdialc I.'~'l)lts COllid ll(!"", '58': 'hll.rodllced a.<-; part o[ a (onilli[al.iou sl.age..', '59': '4 Nob: I.hal.', '60': ""Ihesc cl,>s<,d world assulnplions art' explicitly made in Pollard ,t,."", '61': 'Sag (rorthcoming) [14]..', '62': 'well-typable iff the feature structure subsumes a well-typed feature structure, in ALl.:, type infereneing is employed to ensure that all feature structures are well-typable--in fact, all feature structures are well typed.', '63': 'Unfortunately, well-typability is not sufficient to ensure that disjunctive FCRs are satisfied.', '64': 'Consider, For exam- pie, our encoding of the disjunctive FCR p and suppose that 99 is the fe, ature structure t[f : +,9 : -].', '65': '90 is well-typed, and hence trivially well-typable.', '66': 'Unfortunately, 99 vb elates the encoded disjunctive FCR p. The only way one could interpret ~ as well-formed.', '67': 'By contrast, the Troll system described in this paper has an etfeetive algorithm f<>r deciding well-formedness, which is based on the idea of efficiently representing disjunctive possibilities within the feature structure, Call a well-typed feature structure in which all nodes are labelled with species a resolved feature structure and call a set of resolved feature structures that have the same underlying graph (that is, they differ only in their node labellings) a disjunctive resolved feature structure.', '68': ""We write fS, ~vf8 and 'D~.)c$ for the collections of feature structures, resolved feature structures and disjunctive resolved feature structures respectively."", '69': ""Say that F'E RFS is a resolvant of F E FS iff F and F' have the same underlying graph and F subsumes F'."", '70': 'Let type resolution be the total function R:->DRFS such that R(F) is the set of all resolvants of F. Guided by the partition and all-or-nothing conditions, King [13] has formulated a semantics of feature structures and developed a notion of a satisfiable feature structure such that F E FS is satisfiable iff R(F) 0.', '71': ""T$ is satisfial~le if[' 7~(F) 7 ~ (7)."", '72': 'C, erdemann ,% King [8] have also shown that a feature strtlcture l]leets all encoded FCRs ifl"" the feature structure is satisfiable.', '73': 'The Troll system, which is based on this idea, effectively inqflements type resolution.', '74': 'Why does type resohttion succeed where.', '75': 'type inferencing fails?', '76': 'Consider again the encoding of p and the feature structure 9~.', '77': 'Loosely speaking, the appropriateness sl)eeifieations for type t encode the part of p that sta, tes that an object of tyl)e t deserves features f and g, both with boolean vahles.', '78': 'However, the appropriateness specifications for the speci- ate sul)types t\' and t"" of type t encode the part of p that states that these val-lies lnust agree.', '79': 'Well-typability only considers species if forced to.', '80': 'In the case of ~, well-typability can be estahlished by consklering type t alone, without the l)artition condition forcing one to find a well-typed species subsumed hy t. Consequently, well-tyl)ahility overlooks the part offl exehisively encoded by the ai)propriateness specifications for t\' and t"".', '81': 'Type resolution, on the other hand, always considers species.', '82': ""Thus, type resolving 9o cannot overlook the part of p exclusively encoded by tile appropriateness specifications for t' and t'."", '83': ""APPROPRIATENES S CONDITIONS A very important property of the class of DRFS is that they are closed under unification, i.e., if F and F'E DRFS then F U F' E DRFS.4 Given this property, it would in principle he possible to use the disjunctive resolved feature structures in an implementation without any additional type inferencing procedure to maintain satisfiability."", '84': 'It would, of course, not be very efficient to work with such large disjunctions of feature structures.', '85': '4In fa.ct., it ~:~rl~ I)~ SI~OW ~ that if t"" a.nd 1\'"" 6 fS then ""R ( F) tJ 1""(1""\') = ""R ( F tO F\').', '86': 'Uni/ication of sets of fca.ture structures is defined here ill the standard way: S t2 ,S"" = {1""[ I""\' 6 S and l"""" G S"" and 1"" = 1""\' H 1""""}.', '87': ""(!rty a.llows a. disjultctivo fesolv(,d featur(, structti re to I)e r(;l)rosetd,(~d more et[icieutly a,s ~t sitlgle untyl)(~d l'eatur(' st.l'll(:l.llfe plus a, sel; of d(;pondlmt node la.h(~liugs, which ca.n be further (;oml)a,(:t(~d using mi, Nie(l dis."", '88': ""junction a.s in (',(~rdemann [(i], I)i'~['re t(: Fo]' exanH)le , SUl)l)OS(~ \\v(~ I,,yl)(~ r(~solvc the [ea, l, urc st, ructure t[,f ; bool,fl; bool] using our encoding of p."", '89': '()he can (rosily see tha.t this fea.tur(~ strut:fur(, has only two I\'e solwl, nts, which ca, n I)e colla.ps(~d iuto one fea,1;ure strlll:ttlro with llallV2d d]sjunci.ion a,s shown below: f:k , : :> f: (I t -) II\'ll;1} [""\'""\' ] 0:t-LU: J ,u: (I t ) We now ha,vo a, [;(mSolml)ly COml)a(:l l\'q)-resentaJ;ion hi which t.ho l""(il{, ha.s lie(Hi tl\';tllsl;I,t(~([ iul,o a. Ila, ill(!(I ([iS.]llll(:l.ioli.', '90': ""Ih,w O,V(H'> (Hie should note tha, t fills dis.iun(: l;ion is only l)l'eSeUl; b(~(:aats(~ the ['oaJ, tli'O~i .f a,]l(l g ha>l)l)en 1:o I)o Fir(~s(HIt."", '91': 'Tilt!S(!', '92': 'I(,a tures would .eed l;o Im l)res(mt il w(~ wtwe enforchl<ej (Jaxpcnl,(H""s [:7] lcil, al w(ql i.yl)iug r(xluiroti]oilt ~ whhth ,qa,y\'s 1.1ial [(!al:ilr(\'s I. lial a,l:e a.llowed ilillSt 1)o pres,.ml., lllil.', '93': 'Iol.a[ well I.yping is, hi fax:t> incoinl)a.lib]e ;villi lype resolul, ioli~ since I;hore lilil$\' w(ql I)o all inli llit;(~ seL of tota, lly w(,ll iyl)od I\'esolvalil.s of ;1 l\'(;a, Lllr(J st]\'llcttir(\'~, For (~xa.llipi(~, a.ll illi(lei\'.- Sl)ocifiod list stl\'u(\'tlir(\' couhl be iT(~S()/v0(I 1.o ;~ list of length (L a. list of h:ngl.h 1, el.c, ,qhlce I, ota.I well I,yliin g is liOt i\'(!quir(!([, we lm~y i~s well a.ctiwqy un[il[ r0(lulid;lnt [\'0a, tlires, 5 ill this (!Xalli[)l(!> i[ t, li(\' f ail(l (7 fo.a, tllrOS ;~l\'e reliiovod, we a,lO lell wil, h lh(, simple, disjunction {if,/\'~}: which is (!quiv- a,lent, to l;]le or(lillaJ\'y l,Yl)(\' l.(; Thus, iu lliis ca, so> ]lO (lisjtulcl, ion a.t all ix rc!(llliro(l 10 (!11"" force the I""CIL All th\',tt is requirc(I is tim ~qntuil, ively, [eat, ui\'cs arc rodundaui it Ilwir val llCS art\'.', '94': ""eul,h'cl 5 predictaldc fl'oui ihc approluiaic .ross Sl>eCificatim,..%'c GStz [1)], (',cr,lemam, [7] k,r ;I. IIlOl;('."", '95': '[HXX:iHCforUllllalioii.', '96': '°[n this casc, il.', '97': ""would also have b(:ml l)~>,~iblc to unlill Lhc oi'i<eiuai teal, life Sll'tl<ltllc I,.I.ie I*' solviug."", '98': '/Snforl, unai,e, ly, llmvcvcr, this i~; l.>i ;ihvay~.', '99': 'the (:asc, as C;lll |)(!', '100': ""S(!t'II in the [ollowiug (!Xalll])lC: t{j: +] :> {C/: +]} ~ ~'."", '101': 'asSUml)tion tha.t t will only be ext(mded I)y unil\'ying il with a.lmther (t;Oml)a.ct(~d) m(mll)(!r o[\' ""l)\']?.Jr,_c,.', '102': 'This, h.w(wer, wa.s a. simple ca.se iu which a.I1 of the named dis.jun(:tion could ho removed.', '103': 'It would not lmve I)(\'en i)os sihle to relnov(\' tim fea.tur(\'s f ~tll(I g if thest~ 17,atu[\'es had I)oen involved iu re(m-tranci(+s of i[\' tlt(,se lim.tures ha.d ha.d t:om- i)h+x va.lu(\'s, lu gt+tlera.I, howover, our eXl)e- ri(!ll(:(~ ha,s I)(~(ql that, eV(;l! wil, li very (:()tit pl(\'x type hi(~ra, rchi(~s a.nd |\'(m, tur(; SLI\'UC-l, lll\'eS [()1"" liPS(i, very i\'ow named (lisjunc-lions a, re introdu(\'e(l. 7 q\'hus~ uuilica.1;ion is e;(merally uo more (~xp(msive tha.n unifica.- li,:)H with unlylmd l(mt.ur(~ sl.fu(:l.ur(\'s.', '104': ""\\% havc~ sh,:Y, vu in this i~al),:~r tha.t the kind of consl raints ,:~Xl)r.t~ssihlo Ity api)Vol)rh~,l;or.~ss c~mdit.ions call he imlflemc'.nted iN a i.'actical .,.D, sle]n e,ul)loyinK typ,M featu r,:'~ st.ru(:t.uf(,s and utdlica.Lion a.s I.he I:,ritna.ry Ol)(U'a,t, ic:,n on t(>;t,l, ur<+ ,'-;t, ruct, ure~."", '105': 'Ilut what.', '106': ""Of IIlOl'(' COIII[)I(~N l;yp(~ CC'IIH|,F.~LilI|,,q it~v'.)l',.' h~y; r(~enl;ram:ies': [ntro(IL~ciug reeJH.ra.ncies illl."", '107': "",::<rest ralid.s allows E.' the F,O~sihillty of d(~liNiu/,, recursivc l.yl),:~s ~ such a.s the (leli nitkm of append in [I]."", '108': ""(;lea['ly the re ~olv;-~nl.~, o[ such a. recursiv(~ l.yl)(', could Not I)(~ l,reCOmlfiled a.s r,.~quiI'oxl in Troll."", '109': ""Oue might, uew'rtholoss, considm' a l- [OWil]l[ f('(Hl(, f a, ll('y- ('OIls t f a hI| S oll llollrecursiv(qy defiltcd l.ypcs."", '110': 'A ])ro/)leul still arises; nantcly, il lhe l\'eSo[va.itts of a Frail, till\'t1 .qll\'tlCI411""(~ ill(:ludcd sonic with a pa.rticu lar r(~onll\'all(:y a.nd s()Tn(~ \\viLh(\',ul, then the (:,.)mliti()ll iliad, a.II resc)lva.uts ha.v(~ th,:~ same shal)(~ would m)lon~e[\' hold.', '111': ""()ue v.,ottkl l.her(q'or,.~ no(~(l i.o eml)loy a moue COml)l(,x vorsion .r ,a.med (lis.it, f,t:tio, (Ill], [12], tit)I)."", '112': 'It.', '113': 'ig (i,.L(~sti,.malfl(~ wh(\'thef such a.d ditional (:()mpl(~xit.y would I)e justified to \'Our CXl)ericl~(:c is derived l,\'imarily flora test-i.I"" Ihc \'l\'loll system (m a tat, her lar<e,e e, ramul;G for (\',(!l>lll;lll imfiial vcrh I>lHases, which was wiit-t(\'n I)y I\'hhard Ilillrichs a.d Tsum:ko Na, kazawa aud iinl)lclncut,cd by I)clmar McuH_:J\'s.', '114': 'handle this limited class of reentrancy- constraints.', '115': 'It seems then, that the class of constraints that can be expressed by appropriateness conditions corresponds closely to the class of constraints that can be efficiently preeompiled.', '116': 'We take this as a justification for appropriateness formalisms in general.', '117': 'It makes sense to ~d)straet out the efficiently processable constraints and then allow another mechalfiSm, such as attachments of definite clauses, to express more complex constraints.'}",['C94-2154'],['../data/summaries/C94-2154.txt'],['../data/tba/C94-2154.json']
D09-1092,Polylingual Topic Models,../data/papers/D09-1092.xml,"{'0': 'Polylingual Topic Models', '1': 'Topic models are a useful tool for analyzing large text collections, but have previously been applied in only monolingual, or at most bilingual, contexts.', '2': 'Meanwhile, massive collections of interlinked documents in dozens of languages, such as Wikipedia, are now widely available, calling for tools that can characterize content in many languages.', '3': 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', '4': 'We explore the model’s characteristics using two large corpora, each with over ten different languages, and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', '5': 'Statistical topic models have emerged as an increasingly useful analysis tool for large text collections.', '6': 'Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).', '7': 'Much of this work, however, has occurred in monolingual contexts.', '8': 'In an increasingly connected world, the ability to access documents in many languages has become both a strategic asset and a personally enriching experience.', '9': 'In this paper, we present the polylingual topic model (PLTM).', '10': 'We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).', '11': 'There are many potential applications for polylingual topic models.', '12': 'Although research literature is typically written in English, bibliographic databases often contain substantial quantities of work in other languages.', '13': 'To perform topic-based bibliometric analysis on these collections, it is necessary to have topic models that are aligned across languages.', '14': 'Such analysis could be significant in tracking international research trends, where language barriers slow the transfer of ideas.', '15': 'Previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations.', '16': 'However, the growth of the internet, and in particular Wikipedia, has made vast corpora of topically comparable texts—documents that are topically similar but are not direct translations of one another—considerably more abundant than ever before.', '17': 'We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.', '18': 'In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.', '19': 'We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', '20': 'We also explore how the characteristics of different languages affect topic model performance.', '21': 'The second corpus, Wikipedia articles in twelve languages, contains sets of documents that are not translations of one another, but are very likely to be about similar concepts.', '22': 'We use this corpus to explore the ability of the model both to infer similarities between vocabularies in different languages, and to detect differences in topic emphasis between languages.', '23': 'The internet makes it possible for people all over the world to access documents from different cultures, but readers will not be fluent in this wide variety of languages.', '24': 'By linking topics across languages, polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.', '25': 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing, 2007).', '26': 'Tam, Lane and Schultz (Tam et al., 2007) also show improvements in machine translation using bilingual topic models.', '27': 'Both of these translation-focused topic models infer word-to-word alignments as part of their inference procedures, which would become exponentially more complex if additional languages were added.', '28': 'We take a simpler approach that is more suitable for topically similar document tuples (where documents are not direct translations of one another) in more than two languages.', '29': 'A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.', '30': 'However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.', '31': 'They also provide little analysis of the differences between polylingual and single-language topic models.', '32': 'Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.', '33': 'They find, for example, that English blog posts about the Nintendo Wii often relate to a hack, which cannot be mentioned in Japanese posts due to Japanese intellectual property law.', '34': 'Similarly, posts about whaling often use (positive) nationalist language in Japanese and (negative) environmentalist language in English.', '35': 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.', '36': 'Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.', '37': 'PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics.', '38': 'This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.', '39': 'Additionally, PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1, ... , L. In other words, rather than using a single set of topics Φ = {φ1, ... , φT}, as in LDA, there are L sets of language-specific topics, Φ1, ... , ΦL, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.', '40': 'Anew document tuple w = (w1, ... , wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then, for each language l, a latent topic assignment is drawn for each token in that language: Finally, the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl,Φl) = 11n φlwl |zl .', '41': '(3) The graphical model is shown in figure 1.', '42': ""Given a corpus of training and test document tuples—W and W', respectively—two possible inference tasks of interest are: computing the probability of the test tuples given the training tuples and inferring latent topic assignments for test documents."", '43': 'These tasks can either be accomplished by averaging over samples of Φ1, .', '44': '.', '45': '.', '46': "", ΦL and αm from P(Φ1, ... , ΦL, αm  |W', β) or by evaluating a point estimate."", '47': 'We take the latter approach, and use the MAP estimate for αm and the predictive distributions over words for Φ1, .', '48': '.', '49': '.', '50': "", ΦL.The probability of held-out document tuples W' given training tuples W is then approximated by Topic assignments for a test document tuple sampling."", '51': 'Gibbs sampling involves sequentially resampling each zln from its conditional posterior: where z\\l,n is the current set of topic assignments for all other tokens in the tuple, while (Nt)\\l,n is the number of occurrences of topic t in the tuple, excluding zln, the variable being resampled.', '52': 'Our first set of experiments focuses on document tuples that are known to consist of direct translations.', '53': 'In this case, we can be confident that the topic distribution is genuinely shared across all languages.', '54': 'Although direct translations in multiple languages are relatively rare (in contrast with comparable documents), we use direct translations to explore the characteristics of the model.', '55': 'The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.', '56': 'These texts consist of roughly a decade of proceedings of the European parliament.', '57': 'For our purposes we use alignments at the speech level rather than the sentence level, as in many translation tasks using this corpus.', '58': 'We also remove the twenty-five most frequent word types for efficiency reasons.', '59': 'The remaining collection consists of over 121 million words.', '60': 'Details by language are shown in Table 1.', '61': 'ES otros otras otro otra parte dem‡s FI muiden toisaalta muita muut muihin muun FR autres autre part cTMt6 ailleurs m6me IT altri altre altro altra dall parte NL andere anderzijds anderen ander als kant PT outros outras outro lado outra noutros SV andra sidan Œ annat ena annan The concentration parameter α for the prior over document-specific topic distributions is initialized to 0.01 T, while the base measure m is initialized to the uniform distribution.', '62': 'Hyperparameters αm are re-estimated every 10 Gibbs iterations.', '63': 'Figure 2 shows the most probable words in all languages for four example topics, from PLTM with 400 topics.', '64': 'The first topic contains words relating to the European Central Bank.', '65': 'This topic provides an illustration of the variation in technical terminology captured by PLTM, including the wide array of acronyms used by different languages.', '66': 'The second topic, concerning children, demonstrates the variability of everyday terminology: although the four Romance languages are closely related, they use etymologically unrelated words for children.', '67': '(Interestingly, all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)', '68': 'The third topic demonstrates differences in inflectional variation.', '69': 'English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words, while Greek and Finnish are dominated by inflected variants of the same lexical item.', '70': 'The final topic demonstrates that PLTM effectively clusters “syntactic” words, as well as more semantically specific nouns, adjectives and verbs.', '71': 'Although the topics in figure 2 seem highly focused, it is interesting to ask whether the model is genuinely learning mixtures of topics or simply assigning entire document tuples to single topics.', '72': 'To answer this question, we compute the posterior probability of each topic in each tuple under the trained model.', '73': 'If the model assigns all tokens in a tuple to a single topic, the maximum posterior topic probability for that tuple will be near to 1.0.', '74': 'If the model assigns topics uniformly, the maximum topic probability will be near 1/T.', '75': 'We compute histograms of these maximum topic probabilities for T ∈ {50,100, 200, 400, 800}.', '76': 'For clarity, rather than overlaying five histograms, figure 3 shows the histograms converted into smooth curves using a kernel density estimator.1 Although there is a small bump around 1.0 (for extremely short documents, e.g., “Applause”), values are generally closer to, but greater than, 1/T.', '77': 'Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.', '78': 'Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple), we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.', '79': 'For each tuple we can then calculate the JensenShannon divergence (the average of the KL divergences between each distribution and a mean distribution) between these distributions.', '80': 'Figure 4 shows the density of these divergences for different numbers of topics.', '81': 'As with the previous figure, there are a small number of documents that contain only one topic in all languages, and thus have zero divergence.', '82': 'These tend to be very short, formulaic parliamentary responses, however.', '83': 'The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that, for each tuple, the model is not simply assigning all tokens in a particular language to a single topic.', '84': 'As the number of topics increases, greater variability in topic distributions causes divergence to increase.', '85': 'Smoothed histograms of inter−language JS divergence A topic model specifies a probability distribution over documents, or in the case of PLTM, document tuples.', '86': ""Given a set of training document tuples, PLTM can be used to obtain posterior estimates of Φ', ... , ΦL and αm."", '87': 'The probability of previously unseen held-out document tuples given these estimates can then be computed.', '88': 'The higher the probability of the held-out document tuples, the better the generalization ability of the model.', '89': 'Analytically calculating the probability of a set of held-out document tuples given Φ1, ... , ΦL and αm is intractable, due to the summation over an exponential number of topic assignments for these held-out documents.', '90': 'However, recently developed methods provide efficient, accurate estimates of this probability.', '91': 'We use the “left-to-right” method of (Wallach et al., 2009).', '92': 'We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.', '93': 'Table 2 shows the log probability of held-out data in nats per word for PLTM and LDA, both trained with 200 topics.', '94': 'There is substantial variation between languages.', '95': 'Additionally, the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.', '96': 'It is important to note, however, that these results do not imply that LDA should be preferred over PLTM—that choice depends upon the needs of the modeler.', '97': 'Rather, these results are intended as a quantitative analysis of the difference between the two models.', '98': 'As the number of topics is increased, the word counts per topic become very sparse in monolingual LDA models, proportional to the size of the vocabulary.', '99': 'Figure 5 shows the proportion of all tokens in English and Finnish assigned to each topic under LDA and PLTM with 800 topics.', '100': 'More than 350 topics in the Finnish LDA model have zero tokens assigned to them, and almost all tokens are assigned to the largest 200 topics.', '101': 'English has a larger tail, with non-zero counts in all but 16 topics.', '102': 'In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.', '103': 'PLTM topics therefore have a higher granularity – i.e., they are more specific.', '104': 'This result is important: informally, we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.', '105': 'An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.', '106': 'For example, a journal might publish papers in English, French, German and Italian.', '107': 'No paper is exactly comparable to any other paper, but they are all roughly topically similar.', '108': 'If we wish to perform topic-based bibliometric analysis, it is vital to be able to track the same topics across all languages.', '109': 'One simple way to achieve this topic alignment is to add a small set of comparable document tuples that provide sufficient “glue” to bind the topics together.', '110': 'Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.', '111': 'In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e., we put each of these documents in a single-document tuple.', '112': 'To do this, we divide the corpus W into two sets of document tuples: a “glue” set G and a “separate” set S such that |G |/ |W |= p. In other words, the proportion of tuples in the corpus that are treated as “glue” (i.e., placed in G) is p. For every tuple in S, we assign each document in that tuple to a new singledocument tuple.', '113': 'By doing this, every document in S has its own distribution over topics, independent of any other documents.', '114': 'Ideally, the “glue” documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.', '115': 'FR russie tch´etch´enie union avec russe r´egion IT ho presidente mi perch´e relazione votato lang Topics at P = 0.25 DE rußland russland russischen tschetschenien ukraine EN russia russian chechnya cooperation region belarus FR russie tch´etch´enie avec russe russes situation IT russia unione cooperazione cecenia regione russa We train PLTM with 100 topics on corpora with p E 10.01, 0.05, 0.1, 0.25, 0.5}.', '116': 'We use 1000 iterations of Gibbs sampling with Q = 0.01.', '117': 'Hyperparameters αm are re-estimated every 10 iterations.', '118': 'We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.', '119': 'The lower the divergence, the more similar the distributions are to each other.', '120': 'From the results in figure 4, we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.', '121': 'Table 3 shows mean JS divergences for each value of p. As expected, JS divergence is greater than that obtained when all tuples are left intact.', '122': 'Divergence drops significantly when the proportion of “glue” tuples increases from 0.01 to 0.25.', '123': 'Example topics for p = 0.01 and p = 0.25 are shown in table 4.', '124': 'At p = 0.01 (1% “glue” documents), German and French both include words relating to Russia, while the English and Italian word distributions appear locally consistent but unrelated to Russia.', '125': 'At p = 0.25, the top words for all four languages are related to Russia.', '126': 'These results demonstrate that PLTM is appropriate for aligning topics in corpora that have only a small subset of comparable documents.', '127': 'One area for future work is to explore whether initialization techniques or better representations of topic co-occurrence might result in alignment of topics with a smaller proportion of comparable texts.', '128': 'Although the PLTM is clearly not a substitute for a machine translation system—it has no way to represent syntax or even multi-word phrases—it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.', '129': 'We therefore evaluate the ability of the PLTM to generate bilingual lexica, similar to other work in unsupervised translation modeling (Haghighi et al., 2008).', '130': 'In the early statistical translation model work at IBM, these representations were called “cepts,” short for concepts (Brown et al., 1993).', '131': 'We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).', '132': 'Unlike previous work (Koehn and Knight, 2002), we evaluate all words, not just nouns.', '133': 'We collected bilingual lexica mapping English words to German, Greek, Spanish, French, Italian, Dutch and Swedish.', '134': 'Each lexicon is a set of pairs consisting of an English word and a translated word, 1we, wt}.', '135': 'We do not consider multi-word terms.', '136': 'We expect that simple analysis of topic assignments for sequential words would yield such collocations, but we leave this for future work.', '137': 'For every topic t we select a small number K of the most probable words in English (e) and in each “translation” language (E): Wte and Wtt, respectively.', '138': 'We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.', '139': 'Results for K = 1, that is, considering only the single most probable word for each language, are shown in figure 6.', '140': 'Precision at this level is relatively high, above 50% for Spanish, French and Italian with T = 400 and 800.', '141': 'Many of the candidate pairs that were not in the bilingual lexica were valid translations (e.g.', '142': 'EN “comitology” and IT lang Topics at P = 0.01 “comitalogia”) that simply were not in the lexica.', '143': 'We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften,” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3, T = 800, 1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?', '144': 'The number of such pairs that appear in bilingual lexica is shown on the y-axis.', '145': 'For T = 800, the top English and Spanish words in 448 topics were exact translations of one another.', '146': 'In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', '147': 'These aligned document pairs could then be fed into standard machine translation systems as training data.', '148': 'To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.', '149': 'It is not necessarily clear that PLTM will be effective at identifying translations.', '150': 'In finding a lowdimensional semantic representation, topic models deliberately smooth over much of the variation present in language.', '151': 'We are therefore interested in determining whether the information in the document-specific topic distributions is sufficient to identify semantically identical documents.', '152': 'We begin by dividing the data into a training set of 69,550 document tuples and a test set of 17,435 document tuples.', '153': 'In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.', '154': 'We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al., 2009).', '155': 'Finally, for each pair of languages (“query” and “target”) we calculate the difference between the topic distribution for each held-out document in the query language and the topic distribution for each held-out document in the target language.', '156': 'We use both Jensen-Shannon divergence and cosine distance.', '157': 'For each document in the query language we rank all documents in the target language and record the rank of the actual translation.', '158': 'Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.', '159': 'Cosine-based rankings are significantly worse.', '160': 'It is important to note that the length of documents matters.', '161': 'As noted before, many of the documents in the EuroParl collection consist of short, formulaic sentences.', '162': 'Restricting the query/target pairs to only those with query and target documents that are both longer than 50 words results in significant improvement and reduced variance: the average proportion of query documents for which the true translation is ranked highest goes from 53.9% to 72.7%.', '163': 'Performance continues to improve with longer documents, most likely due to better topic inference.', '164': 'Results vary by language.', '165': 'Table 5 shows results for all target languages with English as a query language.', '166': 'Again, English generally performs better with Romance languages than Germanic languages.', '167': 'Directly parallel translations are rare in many languages and can be extremely expensive to produce.', '168': 'However, the growth of the web, and in particular Wikipedia, has made comparable text corpora – documents that are topically similar but are not direct translations of one another – considerably more abundant than true parallel corpora.', '169': 'In this section, we explore two questions relating to comparable text corpora and polylingual topic modeling.', '170': 'First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.', '171': 'This property is useful for building machine translation systems as well as for human readers who are either learning new languages or analyzing texts in languages they do not know.', '172': 'Second, because comparable texts may not use exactly the same topics, it becomes crucially important to be able to characterize differences in topic prevalence at the document level (do different languages have different perspectives on the same article?) and at the language-wide level (which topics do particular languages focus on?).', '173': 'We downloaded XML copies of all Wikipedia articles in twelve different languages: Welsh, German, Greek, English, Farsi, Finnish, French, Hebrew, Italian, Polish, Russian and Turkish.', '174': 'These versions of Wikipedia were selected to provide a diverse range of language families, geographic areas, and quantities of text.', '175': 'We preprocessed the data by removing tables, references, images and info-boxes.', '176': 'We dropped all articles in non-English languages that did not link to an English article.', '177': 'In the English version of Wikipedia we dropped all articles that were not linked to by any other language in our set.', '178': 'For efficiency, we truncated each article to the nearest word after 1000 characters and dropped the 50 most common word types in each language.', '179': 'Even with these restrictions, the size of the corpus is 148.5 million words.', '180': 'We present results for a PLTM with 400 topics.', '181': '1000 Gibbs sampling iterations took roughly four days on one CPU with current hardware.', '182': 'As with EuroParl, we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.', '183': 'We can then average over all such document-document divergences for each pair of languages to get an overall “disagreement” score between languages.', '184': 'Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.', '185': 'Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.', '186': 'Although we find that if Wikipedia contains an article on a particular subject in some language, the article will tend to be topically similar to the articles about that subject in other languages, we also find that across the whole collection different languages emphasize topics to different extents.', '187': 'To demonstrate the wide variation in topics, we calculated the proportion of tokens in each language assigned to each topic.', '188': 'Figure 8 represents the estimated probabilities of topics given a specific language.', '189': 'Competitive cross-country skiing (left) accounts for a significant proportion of the text in Finnish, but barely exists in Welsh and the languages in the Southeastern region.', '190': 'Meanwhile, interest in actors and actresses (center) is consistent across all languages.', '191': 'Finally, historical topics, such as the Byzantine and Ottoman empires (right) are strong in all languages, but show geographical variation: interest centers around the empires.', '192': 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.', '193': 'We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.', '194': 'We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.', '195': 'Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.', '196': 'When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.', '197': 'The authors thank Limin Yao, who was involved in early stages of this project.', '198': 'This work was supported in part by the Center for Intelligent Information Retrieval, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant number IIS-0326249, and in part by Army prime contract number W911NF-07-1-0216 and University of Pennsylvania subaward number 103548106, and in part by National Science Foundation under NSF grant #CNS-0619337.', '199': 'Any opinions, findings and conclusions or recommendations expressed in this material are the authors’ and do not necessarily reflect those of the sponsor.'}","['D09-1092_swastika', 'D09-1092_vardha', 'D09-1092_sweta']","['../data/summaries/D09-1092_swastika.txt', '../data/summaries/D09-1092_vardha.txt', '../data/summaries/D09-1092_sweta.txt']","['../data/tba/D09-1092_swastika.json', '../data/tba/D09-1092_vardha.json', '../data/tba/D09-1092_sweta.json']"
D10-1044,Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation,../data/papers/D10-1044.xml,"{'0': 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', '1': 'We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', '2': 'This extends previous work on discriminative weighting by using a finer granularity, focusing on the properties of instances rather than corpus components, and using a simpler training procedure.', '3': 'We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.', '4': 'Domain adaptation is a common concern when optimizing empirical NLP applications.', '5': 'Even when there is training data available in the domain of interest, there is often additional data from other domains that could in principle be used to improve performance.', '6': 'Realizing gains in practice can be challenging, however, particularly when the target domain is distant from the background data.', '7': 'For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.', '8': '), which precludes a single universal approach to adaptation.', '9': 'In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material—though adequate for reasonable performance—is also available.', '10': 'This is a standard adaptation problem for SMT.', '11': 'It is difficult when IN and OUT are dissimilar, as they are in the cases we study.', '12': 'For simplicity, we assume that OUT is homogeneous.', '13': 'The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.', '14': 'There is a fairly large body of work on SMT adaptation.', '15': 'We introduce several new ideas.', '16': 'First, we aim to explicitly characterize examples from OUT as belonging to general language or not.', '17': 'Previous approaches have tried to find examples that are similar to the target domain.', '18': 'This is less effective in our setting, where IN and OUT are disparate.', '19': 'The idea of distinguishing between general and domain-specific examples is due to Daum´e and Marcu (2006), who used a maximum-entropy model with latent variables to capture the degree of specificity.', '20': 'Daum´e (2007) applies a related idea in a simpler way, by splitting features into general and domain-specific versions.', '21': 'This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiang and Zhai, 2007) to downweight domain-specific examples in OUT.', '22': 'Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.', '23': 'Our second contribution is to apply instance weighting at the level of phrase pairs.', '24': 'Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.', '25': 'For instance, the sentence Similar improvements in haemoglobin levels were reported in the scientific literature for other epoetins would likely be considered domain-specific despite the presence of general phrases like were reported in.', '26': 'Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009), who weight sentences according to sub-corpus and genre membership.', '27': 'Finally, we make some improvements to baseline approaches.', '28': 'We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.', '29': 'This is a simple and effective alternative to setting weights discriminatively to maximize a metric such as BLEU.', '30': 'A similar maximumlikelihood approach was used by Foster and Kuhn (2007), but for language models only.', '31': 'For comparison to information-retrieval inspired baselines, eg (L¨u et al., 2007), we select sentences from OUT using language model perplexities from IN.', '32': 'This is a straightforward technique that is arguably better suited to the adaptation task than the standard method of treating representative IN sentences as queries, then pooling the match results.', '33': 'The paper is structured as follows.', '34': 'Section 2 describes our baseline techniques for SMT adaptation, and section 3 describes the instance-weighting approach.', '35': 'Experiments are presented in section 4.', '36': 'Section 5 covers relevant previous work on SMT adaptation, and section 6 concludes.', '37': 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', '38': 'The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.', '39': 'Thus, provided at least this amount of IN data is available—as it is in our setting—adapting these weights is straightforward.', '40': 'We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.', '41': 'We do not adapt the alignment procedure for generating the phrase table from which the TM distributions are derived.', '42': 'The natural baseline approach is to concatenate data from IN and OUT.', '43': 'Its success depends on the two domains being relatively close, and on the OUT corpus not being so large as to overwhelm the contribution of IN.', '44': 'When OUT is large and distinct, its contribution can be controlled by training separate IN and OUT models, and weighting their combination.', '45': 'An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).', '46': 'This has the potential drawback of increasing the number of features, which can make MERT less stable (Foster and Kuhn, 2009).', '47': 'Apart from MERT difficulties, a conceptual problem with log-linear combination is that it multiplies feature probabilities, essentially forcing different features to agree on high-scoring candidates.', '48': 'This is appropriate in cases where it is sanctioned by Bayes’ law, such as multiplying LM and TM probabilities, but for adaptation a more suitable framework is often a mixture model in which each event may be generated from some domain.', '49': 'This leads to a linear combination of domain-specific probabilities, with weights in [0, 1], normalized to sum to 1.', '50': 'Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.', '51': 'For the LM, adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case), pi are the corresponding domain-specific models, and ˜p(w, h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.', '52': 'It is not immediately obvious how to formulate an equivalent to equation (1) for an adapted TM, because there is no well-defined objective for learning TMs from parallel corpora.', '53': 'This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita, 2008; Foster and Kuhn, 2007; L¨u et al., 2007).', '54': 'However, we note that the final conditional estimates p(s|t) from a given phrase table maximize the likelihood of joint empirical phrase pair counts over a word-aligned corpus.', '55': 'This suggests a direct parallel to (1): where ˜p(s, t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al., 2004).', '56': ""For the TM, this is: where cI(s, t) is the count in the IN phrase table of pair (s, t), po(s|t) is its probability under the OUT TM, and cI(t) = &quot;s, cI(s', t)."", '57': 'This is motivated by taking β po(s|t) to be the parameters of a Dirichlet prior on phrase probabilities, then maximizing posterior estimates p(s|t) given the IN corpus.', '58': 'Intuitively, it places more weight on OUT when less evidence from IN is available.', '59': 'To set β, we used the same criterion as for α, over a dev corpus: The MAP combination was used for TM probabilities only, in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney, 1995).3 Motivated by information retrieval, a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al., 2005; L¨u et al., 2007), or individual target hypotheses (Zhao et al., 2004).', '60': 'The matching sentence pairs are then added to the IN corpus, and the system is re-trained.', '61': 'Although matching is done at the sentence level, this information is subsequently discarded when all matches are pooled.', '62': 'To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.', '63': 'The number of top-ranked pairs to retain is chosen to optimize dev-set BLEU score.', '64': 'The sentence-selection approach is crude in that it imposes a binary distinction between useful and non-useful parts of OUT.', '65': 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.', '66': 'The weight on each sentence is a value in [0, 1] computed by a perceptron with Boolean features that indicate collection and genre membership.', '67': 'We extend the Matsoukas et al approach in several ways.', '68': 'First, we learn weights on individual phrase pairs rather than sentences.', '69': 'Intuitively, as suggested by the example in the introduction, this is the right granularity to capture domain effects.', '70': 'Second, rather than relying on a division of the corpus into manually-assigned portions, we use features intended to capture the usefulness of each phrase pair.', '71': 'Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.', '72': 'The original OUT counts co(s, t) are weighted by a logistic function wλ(s, t): To motivate weighting joint OUT counts as in (6), we begin with the “ideal” objective for setting multinomial phrase probabilities 0 = {p(s|t), dst}, which is the likelihood with respect to the true IN distribution pi(s, t).', '73': 'Jiang and Zhai (2007) suggest the following derivation, making use of the true OUT distribution po(s, t): where each fi(s, t) is a feature intended to charac- !0ˆ = argmax pf(s, t) log pθ(s|t) (8) terize the usefulness of (s, t), weighted by Ai. θ s,t pf(s, t)po(s, t) log pθ(s|t) The mixing parameters and feature weights (col- != argmax po (s, t) lectively 0) are optimized simultaneously using dev- θ s,t pf(s, t)co(s, t) log pθ(s|t), set maximum likelihood as before: !�argmax po (s, t) ! θ s,t �ˆ = argmax ˜p(s, t) log p(s|t; 0).', '74': '(7) φ s,t This is a somewhat less direct objective than used by Matsoukas et al, who make an iterative approximation to expected TER.', '75': 'However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.', '76': 'Dropping the conditioning on 0 for brevity, and letting ¯cλ(s, t) = cλ(s, t) + yu(s|t), and ¯cλ(t) = 4Note that the probabilities in (7) need only be evaluated over the support of ˜p(s, t), which is quite small when this distribution is derived from a dev set.', '77': 'Maximizing (7) is thus much faster than a typical MERT run. where co(s, t) are the counts from OUT, as in (6).', '78': 'This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates, and po(s|t) is an instance-weighted model derived from the OUT corpus.', '79': 'This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination, or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.', '80': 'We model po(s|t) using a MAP criterion over weighted phrase-pair counts: and from the similarity to (5), assuming y = 0, we see that wλ(s, t) can be interpreted as approximating pf(s, t)/po(s, t).', '81': 'The logistic function, whose outputs are in [0, 1], forces pp(s, t) <_ po(s, t).', '82': 'This is not unreasonable given the application to phrase pairs from OUT, but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s, t)), with outputs in [0, oo].', '83': 'We have not yet tried this.', '84': 'An alternate approximation to (8) would be to let w,\\(s, t) directly approximate pˆI(s, t).', '85': 'With the additional assumption that (s, t) can be restricted to the support of co(s, t), this is equivalent to a “flat” alternative to (6) in which each non-zero co(s, t) is set to one.', '86': 'This variant is tested in the experiments below.', '87': 'A final alternate approach would be to combine weighted joint frequencies rather than conditional estimates, ie: cI(s, t) + w,\\(s, t)co(, s, t), suitably normalized.5 Such an approach could be simulated by a MAP-style combination in which separate 0(t) values were maintained for each t. This would make the model more powerful, but at the cost of having to learn to downweight OUT separately for each t, which we suspect would require more training data for reliable performance.', '88': 'We have not explored this strategy.', '89': 'We used 22 features for the logistic weighting model, divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language, and one intended to capture similarity to the IN domain.', '90': 'The 14 general-language features embody straightforward cues: frequency, “centrality” as reflected in model scores, and lack of burstiness.', '91': 'They are: 5We are grateful to an anonymous reviewer for pointing this out.', '92': '6One of our experimental settings lacks document boundaries, and we used this approximation in both settings for consistency.', '93': 'The 8 similarity-to-IN features are based on word frequencies and scores from various models trained on the IN corpus: To avoid numerical problems, each feature was normalized by subtracting its mean and dividing by its standard deviation.', '94': 'In addition to using the simple features directly, we also trained an SVM classifier with these features to distinguish between IN and OUT phrase pairs.', '95': 'Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.', '96': 'We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.', '97': 'We carried out translation experiments in two different settings.', '98': 'The first setting uses the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus (www.statmt.org/europarl) as OUT, for English/French translation in both directions.', '99': 'The dev and test sets were randomly chosen from the EMEA corpus.', '100': 'Figure 1 shows sample sentences from these domains, which are widely divergent.', '101': 'The second setting uses the news-related subcorpora for the NIST09 MT Chinese to English evaluation8 as IN, and the remaining NIST parallel Chinese/English corpora (UN, Hong Kong Laws, and Hong Kong Hansard) as OUT.', '102': 'The dev corpus was taken from the NIST05 evaluation set, augmented with some randomly-selected material reserved from the training set.', '103': 'The NIST06 and NIST08 evaluation sets were used for testing.', '104': '(Thus the domain of the dev and test corpora matches IN.)', '105': 'Compared to the EMEA/EP setting, the two domains in the NIST setting are less homogeneous and more similar to each other; there is also considerably more IN text available.', '106': 'The corpora for both settings are summarized in table 1.', '107': 'The reference medicine for Silapo is EPREX/ERYPO, which contains epoetin alfa.', '108': 'Le m´edicament de r´ef´erence de Silapo est EPREX/ERYPO, qui contient de l’´epo´etine alfa.', '109': '— I would also like to point out to commissioner Liikanen that it is not easy to take a matter to a national court.', '110': 'Je voudrais pr´eciser, a` l’adresse du commissaire Liikanen, qu’il n’est pas ais´e de recourir aux tribunaux nationaux.', '111': 'We used a standard one-pass phrase-based system (Koehn et al., 2003), with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.', '112': 'Feature weights were set using Och’s MERT algorithm (Och, 2003).', '113': 'The corpus was wordaligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7.', '114': 'It was filtered to retain the top 30 translations for each source phrase using the TM part of the current log-linear model.', '115': 'Table 2 shows results for both settings and all methods described in sections 2 and 3.', '116': 'The 1st block contains the simple baselines from section 2.1.', '117': 'The natural baseline (baseline) outperforms the pure IN system only for EMEA/EP fren.', '118': 'Log-linear combination (loglin) improves on this in all cases, and also beats the pure IN system.', '119': 'The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.', '120': 'This significantly underperforms log-linear combination.', '121': 'The 3rd block contains the mixture baselines.', '122': 'The linear LM (lin lm), TM (lin tm) and MAP TM (map tm) used with non-adapted counterparts perform in all cases slightly worse than the log-linear combination, which adapts both LM and TM components.', '123': 'However, when the linear LM is combined with a linear TM (lm+lin tm) or MAP TM (lm+map TM), the results are much better than a log-linear combination for the EMEA setting, and on a par for NIST.', '124': 'This is consistent with the nature of these two settings: log-linear combination, which effectively takes the intersection of IN and OUT, does relatively better on NIST, where the domains are broader and closer together.', '125': 'Somewhat surprisingly, there do not appear to be large systematic differences between linear and MAP combinations.', '126': 'The 4th block contains instance-weighting models trained on all features, used within a MAP TM combination, and with a linear LM mixture.', '127': 'The iw all map variant uses a non-0 y weight on a uniform prior in p,,(s t), and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.', '128': 'Clearly, retaining the original frequencies is important for good performance, and globally smoothing the final weighted frequencies is crucial.', '129': 'This best instance-weighting model beats the equivalant model without instance weights by between 0.6 BLEU and 1.8 BLEU, and beats the log-linear baseline by a large margin.', '130': 'The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', '131': 'The general-language features have a slight advantage over the similarity features, and both are better than the SVM feature.', '132': 'We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.', '133': 'It is difficult to directly compare the Matsoukas et al results with ours, since our out-of-domain corpus is homogeneous; given heterogeneous training data, however, it would be trivial to include Matsoukas-style identity features in our instance-weighting model.', '134': 'Although these authors report better gains than ours, they are with respect to a non-adapted baseline.', '135': 'Finally, we note that Jiang’s instance-weighting framework is broader than we have presented above, encompassing among other possibilities the use of unlabelled IN data, which is applicable to SMT settings where source-only IN corpora are available.', '136': 'It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.', '137': 'At first glance, this seems only peripherally related to our work, since the specific/general distinction is made for features rather than instances.', '138': 'However, for multinomial models like our LMs and TMs, there is a one to one correspondence between instances and features, eg the correspondence between a phrase pair (s, t) and its conditional multinomial probability p(s1t).', '139': 'As mentioned above, it is not obvious how to apply Daum´e’s approach to multinomials, which do not have a mechanism for combining split features.', '140': 'Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.', '141': 'Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L¨u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L¨u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).', '142': 'There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan, 2007; Wu et al., 2005), and on dynamically choosing a dev set (Xu et al., 2007).', '143': 'Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).', '144': 'In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.', '145': 'Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.', '146': 'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', '147': 'These estimates are in turn combined linearly with relative-frequency estimates from an in-domain phrase table.', '148': 'Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.', '149': 'We obtained positive results using a very simple phrase-based system in two different adaptation settings: using English/French Europarl to improve a performance on a small, specialized medical domain; and using non-news portions of the NIST09 training material to improve performance on the news-related corpora.', '150': 'In both cases, the instanceweighting approach improved over a wide range of baselines, giving gains of over 2 BLEU points over the best non-adapted baseline, and gains of between 0.6 and 1.8 over an equivalent mixture model (with an identical training procedure but without instance weighting).', '151': 'In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.', '152': 'We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.', '153': 'Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.'}","['D10-1044_aakansha', 'D10-1044_sweta', 'D10-1044_swastika']","['../data/summaries/D10-1044_aakansha.txt', '../data/summaries/D10-1044_sweta.txt', '../data/summaries/D10-1044_swastika.txt']","['../data/tba/D10-1044_aakansha.json', '../data/tba/D10-1044_sweta.json', '../data/tba/D10-1044_swastika.json']"
D10-1083,Simple Type-Level Unsupervised POS Tagging,../data/papers/D10-1083.xml,"{'0': 'Simple Type-Level Unsupervised POS Tagging', '1': 'Part-of-speech (POS) tag distributions are known to exhibit sparsity â\x80\x94 a word is likely to take a single predominant tag in a corpus.', '2': 'Recent research has demonstrated that incorporating this sparsity constraint improves tagging accuracy.', '3': 'However, in existing systems, this expansion come with a steep increase in model complexity.', '4': 'This paper proposes a simple and effective tagging method that directly models tag sparsity and other distributional properties of valid POS tag assignments.', '5': 'In addition, this formulation results in a dramatic reduction in the number of model parameters thereby, enabling unusually rapid training.', '6': 'Our experiments consistently demonstrate that this model architecture yields substantial performance gains over more complex tagging counterparts.', '7': 'On several languages, we report performance exceeding that of more complex state-of-the art systems.1', '8': 'Since the early days of statistical NLP, researchers have observed that a part-of-speech tag distribution exhibits â\x80\x9cone tag per discourseâ\x80\x9d sparsity â\x80\x94 words are likely to select a single predominant tag in a corpus, even when several tags are possible.', '9': 'Simply assigning to each word its most frequent associated tag in a corpus achieves 94.6% accuracy on the WSJ portion of the Penn Treebank.', '10': 'This distributional sparsity of syntactic tags is not unique to English 1 The source code for the work presented in this paper is available at http://groups.csail.mit.edu/rbg/code/typetagging/.', '11': 'â\x80\x94 similar results have been observed across multiple languages.', '12': 'Clearly, explicitly modeling such a powerful constraint on tagging assignment has a potential to significantly improve the accuracy of an unsupervised part-of-speech tagger learned without a tagging dictionary.', '13': 'In practice, this sparsity constraint is difficult to incorporate in a traditional POS induction system (MeÂ´rialdo, 1994; Johnson, 2007; Gao and Johnson, 2008; GracÂ¸a et al., 2009; Berg-Kirkpatrick et al., 2010).', '14': 'These sequence models-based approaches commonly treat token-level tag assignment as the primary latent variable.', '15': 'By design, they readily capture regularities at the token-level.', '16': 'However, these approaches are ill-equipped to directly represent type-based constraints such as sparsity.', '17': 'Previous work has attempted to incorporate such constraints into token-level models via heavy-handed modifications to inference procedure and objective function (e.g., posterior regularization and ILP decoding) (GracÂ¸a et al., 2009; Ravi and Knight, 2009).', '18': 'In most cases, however, these expansions come with a steep increase in model complexity, with respect to training procedure and inference time.', '19': 'In this work, we take a more direct approach and treat a word type and its allowed POS tags as a primary element of the model.', '20': 'The model starts by generating a tag assignment for each word type in a vocabulary, assuming one tag per word.', '21': 'Then, token- level HMM emission parameters are drawn conditioned on these assignments such that each word is only allowed probability mass on a single assigned tag.', '22': 'In this way we restrict the parameterization of a Language Original case English Danish Dutch German Spanish Swedish Portuguese 94.6 96.3 96.6 95.5 95.4 93.3 95.6 Table 1: Upper bound on tagging accuracy assuming each word type is assigned to majority POS tag.', '23': 'Across all languages, high performance can be attained by selecting a single tag per word type.', '24': 'token-level HMM to reflect lexicon sparsity.', '25': 'This model admits a simple Gibbs sampling algorithm where the number of latent variables is proportional to the number of word types, rather than the size of a corpus as for a standard HMM sampler (Johnson, 2007).', '26': 'There are two key benefits of this model architecture.', '27': 'First, it directly encodes linguistic intuitions about POS tag assignments: the model structure reflects the one-tag-per-word property, and a type- level tag prior captures the skew on tag assignments (e.g., there are fewer unique determiners than unique nouns).', '28': 'Second, the reduced number of hidden variables and parameters dramatically speeds up learning and inference.', '29': 'We evaluate our model on seven languages exhibiting substantial syntactic variation.', '30': 'On several languages, we report performance exceeding that of state-of-the art systems.', '31': 'Our analysis identifies three key factors driving our performance gain: 1) selecting a model structure which directly encodes tag sparsity, 2) a type-level prior on tag assignments, and 3) a straightforward naÂ¨Ä±veBayes approach to incorporate features.', '32': 'The observed performance gains, coupled with the simplicity of model implementation, makes it a compelling alternative to existing more complex counterparts.', '33': 'Recent work has made significant progress on unsupervised POS tagging (MeÂ´rialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006; Johnson,2007; Goldwater and Griffiths, 2007; Gao and John son, 2008; Ravi and Knight, 2009).', '34': 'Our work is closely related to recent approaches that incorporate the sparsity constraint into the POS induction process.', '35': 'This line of work has been motivated by empirical findings that the standard EM-learned unsupervised HMM does not exhibit sufficient word tag sparsity.', '36': 'The extent to which this constraint is enforced varies greatly across existing methods.', '37': 'On one end of the spectrum are clustering approaches that assign a single POS tag to each word type (Schutze, 1995; Lamar et al., 2010).', '38': 'These clusters are computed using an SVD variant without relying on transitional structure.', '39': 'While our method also enforces a singe tag per word constraint, it leverages the transition distribution encoded in an HMM, thereby benefiting from a richer representation of context.', '40': 'Other approaches encode sparsity as a soft constraint.', '41': 'For instance, by altering the emission distribution parameters, Johnson (2007) encourages the model to put most of the probability mass on few tags.', '42': 'This design does not guarantee â\x80\x9cstructural zeros,â\x80\x9d but biases towards sparsity.', '43': 'A more forceful approach for encoding sparsity is posterior regularization, which constrains the posterior to have a small number of expected tag assignments (GracÂ¸a et al., 2009).', '44': 'This approach makes the training objective more complex by adding linear constraints proportional to the number of word types, which is rather prohibitive.', '45': 'A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types.', '46': 'The use of ILP in learning the desired grammar significantly increases the computational complexity of this method.', '47': 'In contrast to these approaches, our method directly incorporates these constraints into the structure of the model.', '48': 'This design leads to a significant reduction in the computational complexity of training and inference.', '49': 'Another thread of relevant research has explored the use of features in unsupervised POS induction (Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010; Hasan and Ng, 2009).', '50': 'These methods demonstrated the benefits of incorporating linguistic features using a log-linear parameterization, but requires elaborate machinery for training.', '51': 'In our work, we demonstrate that using a simple naÂ¨Ä±veBayes approach also yields substantial performance gains, without the associated training complexity.', '52': 'We consider the unsupervised POS induction problem without the use of a tagging dictionary.', '53': 'A graphical depiction of our model as well as a summary of random variables and parameters can be found in Figure 1.', '54': 'As is standard, we use a fixed constant K for the number of tagging states.', '55': 'Model Overview The model starts by generating a tag assignment T for each word type in a vocabulary, assuming one tag per word.', '56': 'Conditioned on T , features of word types W are drawn.', '57': 'We refer to (T , W ) as the lexicon of a language and Ï\x88 for the parameters for their generation; Ï\x88 depends on a single hyperparameter Î².', '58': 'Once the lexicon has been drawn, the model proceeds similarly to the standard token-level HMM: Emission parameters Î¸ are generated conditioned on tag assignments T . We also draw transition parameters Ï\x86.', '59': 'Both parameters depend on a single hyperparameter Î±.', '60': 'Once HMM parameters (Î¸, Ï\x86) are drawn, a token-level tag and word sequence, (t, w), is generated in the standard HMM fashion: a tag sequence t is generated from Ï\x86.', '61': 'The corresponding token words w are drawn conditioned on t and Î¸.2 Our full generative model is given by: K P (Ï\x86, Î¸|T , Î±, Î²) = n (P (Ï\x86t|Î±)P (Î¸t|T , Î±)) t=1 The transition distribution Ï\x86t for each tag t is drawn according to DIRICHLET(Î±, K ), where Î± is the shared transition and emission distribution hyperparameter.', '62': 'In total there are O(K 2) parameters associated with the transition parameters.', '63': 'In contrast to the Bayesian HMM, Î¸t is not drawn from a distribution which has support for each of the n word types.', '64': 'Instead, we condition on the type-level tag assignments T . Specifically, let St = {i|Ti = t} denote the indices of theword types which have been assigned tag t accord ing to the tag assignments T . Then Î¸t is drawn from DIRICHLET(Î±, St), a symmetric Dirichlet which only places mass on word types indicated by St. This ensures that each word will only be assigned a single tag at inference time (see Section 4).', '65': 'Note that while the standard HMM, has O(K n) emission parameters, our model has O(n) effective parameters.3 Token Component Once HMM parameters (Ï\x86, Î¸) have been drawn, the HMM generates a token-level corpus w in the standard way: P (w, t|Ï\x86, Î¸) = P (T , W , Î¸, Ï\x88, Ï\x86, t, w|Î±, Î²) = P (T , W , Ï\x88|Î²) [Lexicon] ï£« n n ï£\xad (w,t)â\x88\x88(w,t) j ï£¶ P (tj |Ï\x86tjâ\x88\x921 )P (wj |tj , Î¸tj )ï£¸ P (Ï\x86, Î¸|T , Î±, Î²) [Parameter] P (w, t|Ï\x86, Î¸) [Token] We refer to the components on the right hand side as the lexicon, parameter, and token component respectively.', '66': 'Since the parameter and token components will remain fixed throughout experiments, we briefly describe each.', '67': 'Parameter Component As in the standard Bayesian HMM (Goldwater and Griffiths, 2007), all distributions are independently drawn from symmetric Dirichlet distributions: 2 Note that t and w denote tag and word sequences respectively, rather than individual tokens or tags.', '68': 'Note that in our model, conditioned on T , there is precisely one t which has nonzero probability for the token component, since for each word, exactly one Î¸t has support.', '69': '3.1 Lexicon Component.', '70': 'We present several variations for the lexical component P (T , W |Ï\x88), each adding more complex pa rameterizations.', '71': 'Uniform Tag Prior (1TW) Our initial lexicon component will be uniform over possible tag assignments as well as word types.', '72': 'Its only purpose is 3 This follows since each Î¸t has St â\x88\x92 1 parameters and.', '73': 'P St = n. Î² T VARIABLES Ï\x88 Y W : Word types (W1 ,.', '74': '.., Wn ) (obs) P T : Tag assigns (T1 ,.', '75': '.., Tn ) T W Ï\x86 E w : Token word seqs (obs) t : Token tag assigns (det by T ) PARAMETERS Ï\x88 : Lexicon parameters Î¸ : Token word emission parameters Ï\x86 : Token tag transition parameters Ï\x86 Ï\x86 t1 t2 Î¸ Î¸ w1 w2 K Ï\x86 T tm O K Î¸ E wN m N N Figure 1: Graphical depiction of our model and summary of latent variables and parameters.', '76': 'The type-level tag assignments T generate features associated with word types W . The tag assignments constrain the HMM emission parameters Î¸.', '77': 'The tokens w are generated by token-level tags t from an HMM parameterized by the lexicon structure.', '78': 'The hyperparameters Î± and Î² represent the concentration parameters of the token- and type-level components of the model respectively.', '79': 'They are set to fixed constants.', '80': 'to explore how well we can induce POS tags using only the one-tag-per-word constraint.', '81': 'Specifically, the lexicon is generated as: P (T , W |Ï\x88) =P (T )P (W |T ) Word Type Features (FEATS): Past unsupervised POS work have derived benefits from features on word types, such as suffix and capitalization features (Hasan and Ng, 2009; Berg-Kirkpatrick et al.,2010).', '82': 'Past work however, has typically associ n = n P (Ti)P (Wi|Ti) = i=1 1 n K n ated these features with token occurrences, typically in an HMM.', '83': 'In our model, we associate these features at the type-level in the lexicon.', '84': 'Here, we conThis model is equivalent to the standard HMM ex cept that it enforces the one-word-per-tag constraint.', '85': 'Learned Tag Prior (PRIOR) We next assume there exists a single prior distribution Ï\x88 over tag assignments drawn from DIRICHLET(Î², K ).', '86': 'This alters generation of T as follows: n P (T |Ï\x88) = n P (Ti|Ï\x88) i=1 Note that this distribution captures the frequency of a tag across word types, as opposed to tokens.', '87': 'The P (T |Ï\x88) distribution, in English for instance, should have very low mass for the DT (determiner) tag, since determiners are a very small portion of the vocabulary.', '88': 'In contrast, NNP (proper nouns) form a large portion of vocabulary.', '89': 'Note that these observa sider suffix features, capitalization features, punctuation, and digit features.', '90': 'While possible to utilize the feature-based log-linear approach described in Berg-Kirkpatrick et al.', '91': '(2010), we adopt a simpler naÂ¨Ä±ve Bayes strategy, where all features are emitted independently.', '92': 'Specifically, we assume each word type W consists of feature-value pairs (f, v).', '93': 'For each feature type f and tag t, a multinomial Ï\x88tf is drawn from a symmetric Dirichlet distribution with concentration parameter Î².', '94': 'The P (W |T , Ï\x88) term in the lexicon component now decomposes as: n P (W |T , Ï\x88) = n P (Wi|Ti, Ï\x88) i=1 n ï£« ï£¶ tions are not modeled by the standard HMM, which = n ï£\xad n P (v|Ï\x88Ti f )ï£¸ instead can model token-level frequency.', '95': 'i=1 (f,v)â\x88\x88Wi', '96': 'For inference, we are interested in the posterior probability over the latent variables in our model.', '97': 'During training, we treat as observed the language word types W as well as the token-level corpus w. We utilize Gibbs sampling to approximate our collapsed model posterior: P (T ,t|W , w, Î±, Î²) â\x88\x9d P (T , t, W , w|Î±, Î²) 0.7 0.6 0.5 0.4 0.3 English Danish Dutch Germany Portuguese Spanish Swedish = P (T , t, W , w, Ï\x88, Î¸, Ï\x86, w|Î±, Î²)dÏ\x88dÎ¸dÏ\x86 Note that given tag assignments T , there is only one setting of token-level tags t which has mass in the above posterior.', '98': 'Specifically, for the ith word type, the set of token-level tags associated with token occurrences of this word, denoted t(i), must all take the value Ti to have nonzero mass. Thus in the context of Gibbs sampling, if we want to block sample Ti with t(i), we only need sample values for Ti and consider this setting of t(i).', '99': 'The equation for sampling a single type-level assignment Ti is given by, 0.2 0 5 10 15 20 25 30 Iteration Figure 2: Graph of the one-to-one accuracy of our full model (+FEATS) under the best hyperparameter setting by iteration (see Section 5).', '100': 'Performance typically stabilizes across languages after only a few number of iterations.', '101': 'to represent the ith word type emitted by the HMM: P (t(i)|Ti, t(â\x88\x92i), w, Î±) â\x88\x9d n P (w|Ti, t(â\x88\x92i), w(â\x88\x92i), Î±) (tb ,ta ) P (Ti, t(i)|T , W , t(â\x88\x92i), w, Î±, Î²) = P (T |tb, t(â\x88\x92i), Î±)P (ta|T , t(â\x88\x92i), Î±) â\x88\x92i (i) i i (â\x88\x92i) P (Ti|W , T â\x88\x92i, Î²)P (t |Ti, t , w, Î±) All terms are Dirichlet distributions and parameters can be analytically computed from counts in t(â\x88\x92i)where T â\x88\x92i denotes all type-level tag assignment ex cept Ti and t(â\x88\x92i) denotes all token-level tags except and w (â\x88\x92i) (Johnson, 2007).', '102': 't(i).', '103': 'The terms on the right-hand-side denote the type-level and token-level probability terms respectively.', '104': 'The type-level posterior term can be computed according to, P (Ti|W , T â\x88\x92i, Î²) â\x88\x9d Note that each round of sampling Ti variables takes time proportional to the size of the corpus, as with the standard token-level HMM.', '105': 'A crucial difference is that the number of parameters is greatly reduced as is the number of variables that are sampled during each iteration.', '106': 'In contrast to results reported in Johnson (2007), we found that the per P (Ti|T â\x88\x92i, Î²) n (f,v)â\x88\x88Wi P (v|Ti, f, W â\x88\x92i, T â\x88\x92i, Î²) formance of our Gibbs sampler on the basic 1TW model stabilized very quickly after about 10 full it All of the probabilities on the right-hand-side are Dirichlet, distributions which can be computed analytically given counts.', '107': 'The token-level term is similar to the standard HMM sampling equations found in Johnson (2007).', '108': 'The relevant variables are the set of token-level tags that appear before and after each instance of the ith word type; we denote these context pairs with the set {(tb, ta)} and they are contained in t(â\x88\x92i).', '109': 'We use w erations of sampling (see Figure 2 for a depiction).', '110': 'We evaluate our approach on seven languages: English, Danish, Dutch, German, Portuguese, Spanish, and Swedish.', '111': 'On each language we investigate the contribution of each component of our model.', '112': 'For all languages we do not make use of a tagging dictionary.', '113': 'Mo del Hy per par am . E n g li s h1 1 m-1 D a n i s h1 1 m-1 D u t c h1 1 m-1 G er m a n1 1 m-1 Por tug ues e1 1 m-1 S p a ni s h1 1 m-1 S w e di s h1 1 m-1 1T W be st me dia n 45.', '114': '2 62.6 45.', '115': '1 61.7 37.', '116': '2 56.2 32.', '117': '1 53.8 47.', '118': '4 53.7 43.', '119': '9 61.0 44.', '120': '2 62.2 39.', '121': '3 68.4 49.', '122': '0 68.4 48.', '123': '5 68.1 34.', '124': '3 54.4 33.', '125': '36.', '126': '0 55.3 34.', '127': '9 50.2 +P RI OR be st me dia n 47.', '128': '9 65.5 46.', '129': '5 64.7 42.', '130': '3 58.3 40.', '131': '0 57.3 51.', '132': '4 65.9 48.', '133': '3 60.7 50.', '134': '41.', '135': '7 68.3 56.', '136': '2 70.7 52.', '137': '0 70.9 42.', '138': '37.', '139': '1 55.8 38.', '140': '36.', '141': '8 57.3 +F EA TS be st me dia n 50.', '142': '9 66.4 47.', '143': '8 66.4 52.', '144': '1 61.2 43.', '145': '2 60.7 56.', '146': '4 69.0 51.', '147': '5 67.3 55.', '148': '4 70.4 46.', '149': '2 61.7 64.', '150': '1 74.5 56.', '151': '5 70.1 58.', '152': '3 68.9 50.', '153': '0 57.2 43.', '154': '3 61.7 38.', '155': '5 60.6 Table 3: Multilingual Results: We report token-level one-to-one and many-to-one accuracy on a variety of languages under several experimental settings (Section 5).', '156': 'For each language and setting, we report one-to-one (11) and many- to-one (m-1) accuracies.', '157': 'For each cell, the first row corresponds to the result using the best hyperparameter choice, where best is defined by the 11 metric.', '158': 'The second row represents the performance of the median hyperparameter setting.', '159': 'Model components cascade, so the row corresponding to +FEATS also includes the PRIOR component (see Section 3).', '160': 'La ng ua ge # To ke ns # W or d Ty pe s # Ta gs E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 1 1 7 3 7 6 6 9 4 3 8 6 2 0 3 5 6 8 6 9 9 6 0 5 2 0 6 6 7 8 8 9 3 3 4 1 9 1 4 6 7 4 9 2 0 6 1 8 3 5 6 2 8 3 9 3 7 2 3 2 5 2 8 9 3 1 1 6 4 5 8 2 0 0 5 7 4 5 2 5 1 2 5 4 2 2 4 7 4 1 Table 2: Statistics for various corpora utilized in experiments.', '161': 'See Section 5.', '162': 'The English data comes from the WSJ portion of the Penn Treebank and the other languages from the training set of the CoNLL-X multilingual dependency parsing shared task.', '163': '5.1 Data Sets.', '164': 'Following the setup of Johnson (2007), we use the whole of the Penn Treebank corpus for training and evaluation on English.', '165': 'For other languages, we use the CoNLL-X multilingual dependency parsing shared task corpora (Buchholz and Marsi, 2006) which include gold POS tags (used for evaluation).', '166': 'We train and test on the CoNLL-X training set.', '167': 'Statistics for all data sets are shown in Table 2.', '168': '5.2 Setup.', '169': 'Models To assess the marginal utility of each component of the model (see Section 3), we incremen- tally increase its sophistication.', '170': 'Specifically, we (+FEATS) utilizes the tag prior as well as features (e.g., suffixes and orthographic features), discussed in Section 3, for the P (W |T , Ï\x88) component.', '171': 'Hyperparameters Our model has two Dirichlet concentration hyperparameters: Î± is the shared hyperparameter for the token-level HMM emission and transition distributions.', '172': 'Î² is the shared hyperparameter for the tag assignment prior and word feature multinomials.', '173': 'We experiment with four values for each hyperparameter resulting in 16 (Î±, Î²) combinations: Î± Î² 0.001, 0.01, 0.1, 1.0 0.01, 0.1, 1.0, 10 Iterations In each run, we performed 30 iterations of Gibbs sampling for the type assignment variables W .4 We use the final sample for evaluation.', '174': 'Evaluation Metrics We report three metrics to evaluate tagging performance.', '175': 'As is standard, we report the greedy one-to-one (Haghighi and Klein, 2006) and the many-to-one token-level accuracy obtained from mapping model states to gold POS tags.', '176': 'We also report word type level accuracy, the fraction of word types assigned their majority tag (where the mapping between model state and tag is determined by greedy one-to-one mapping discussed above).5 For each language, we aggregate results in the following way: First, for each hyperparameter setting, evaluate three variants: The first model (1TW) only 4 Typically, the performance stabilizes after only 10 itera-.', '177': 'encodes the one tag per word constraint and is uni form over type-level tag assignments.', '178': 'The second model (+PRIOR) utilizes the independent prior over type-level tag assignments P (T |Ï\x88).', '179': 'The final model tions.', '180': '5 We choose these two metrics over the Variation Information measure due to the deficiencies discussed in Gao and Johnson (2008).', '181': 'we perform five runs with different random initialization of sampling state.', '182': 'Hyperparameter settings are sorted according to the median one-to-one metric over runs.', '183': 'We report results for the best and median hyperparameter settings obtained in this way.', '184': 'Specifically, for both settings we report results on the median run for each setting.', '185': 'Tag set As is standard, for all experiments, we set the number of latent model tag states to the size of the annotated tag set.', '186': 'The original tag set for the CoNLL-X Dutch data set consists of compounded tags that are used to tag multi-word units (MWUs) resulting in a tag set of over 300 tags.', '187': 'We tokenize MWUs and their POS tags; this reduces the tag set size to 12.', '188': 'See Table 2 for the tag set size of other languages.', '189': 'With the exception of the Dutch data set, no other processing is performed on the annotated tags.', '190': '6 Results and Analysis.', '191': 'We report token- and type-level accuracy in Table 3 and 6 for all languages and system settings.', '192': 'Our analysis and comparison focuses primarily on the one-to-one accuracy since it is a stricter metric than many-to-one accuracy, but also report many-to-one for completeness.', '193': 'Comparison with state-of-the-art taggers For comparison we consider two unsupervised tag- gers: the HMM with log-linear features of Berg- Kirkpatrick et al.', '194': '(2010) and the posterior regular- ization HMM of GracÂ¸a et al.', '195': '(2009).', '196': 'The system of Berg-Kirkpatrick et al.', '197': '(2010) reports the best unsupervised results for English.', '198': 'We consider two variants of Berg-Kirkpatrick et al.', '199': '(2010)â\x80\x99s richest model: optimized via either EM or LBFGS, as their relative performance depends on the language.', '200': 'Our model outperforms theirs on four out of five languages on the best hyperparameter setting and three out of five on the median setting, yielding an average absolute difference across languages of 12.9% and 3.9% for best and median settings respectively compared to their best EM or LBFGS performance.', '201': 'While Berg-Kirkpatrick et al.', '202': '(2010) consistently outperforms ours on English, we obtain substantial gains across other languages.', '203': 'For instance, on Spanish, the absolute gap on median performance is 10%.', '204': 'Top 5 Bot to m 5 Go ld NN P NN JJ CD NN S RB S PD T # â\x80\x9d , 1T W CD W RB NN S VB N NN PR P$ W DT : MD . +P RI OR CD JJ NN S WP $ NN RR B- , $ â\x80\x9d . +F EA TS JJ NN S CD NN P UH , PR P$ # . â\x80\x9c Table 5: Type-level English POS Tag Ranking: We list the top 5 and bottom 5 POS tags in the lexicon and the predictions of our models under the best hyperparameter setting.', '205': 'Our second point of comparison is with GracÂ¸a et al.', '206': '(2009), who also incorporate a sparsity constraint, but does via altering the model objective using posterior regularization.', '207': 'We can only compare with GracÂ¸a et al.', '208': '(2009) on Portuguese (GracÂ¸a et al.', '209': '(2009) also report results on English, but on the reduced 17 tag set, which is not comparable to ours).', '210': 'Their best model yields 44.5% one-to-one accuracy, compared to our best median 56.5% result.', '211': 'However, our full model takes advantage of word features not present in GracÂ¸a et al.', '212': '(2009).', '213': 'Even without features, but still using the tag prior, our median result is 52.0%, still significantly outperforming GracÂ¸a et al.', '214': '(2009).', '215': 'Ablation Analysis We evaluate the impact of incorporating various linguistic features into our model in Table 3.', '216': 'A novel element of our model is the ability to capture type-level tag frequencies.', '217': 'For this experiment, we compare our model with the uniform tag assignment prior (1TW) with the learned prior (+PRIOR).', '218': 'Across all languages, +PRIOR consistently outperforms 1TW, reducing error on average by 9.1% and 5.9% on best and median settings respectively.', '219': 'Similar behavior is observed when adding features.', '220': 'The difference between the featureless model (+PRIOR) and our full model (+FEATS) is 13.6% and 7.7% average error reduction on best and median settings respectively.', '221': 'Overall, the difference between our most basic model (1TW) and our full model (+FEATS) is 21.2% and 13.1% for the best and median settings respectively.', '222': 'One striking example is the error reduction for Spanish, which reduces error by 36.5% and 24.7% for the best and median settings respectively.', '223': 'We observe similar trends when using another measure â\x80\x93 type-level accuracy (defined as the fraction of words correctly assigned their majority tag), according to which La ng ua ge M etr ic B K 10 E M B K 10 L B F G S G 10 F EA T S B es t F EA T S M ed ia n E ng lis h 1 1 m 1 4 8 . 3 6 8 . 1 5 6 . 0 7 5 . 5 â\x80\x93 â\x80\x93 5 0 . 9 6 6 . 4 4 7 . 8 6 6 . 4 D an is h 1 1 m 1 4 2 . 3 6 6 . 7 4 2 . 6 5 8 . 0 â\x80\x93 â\x80\x93 5 2 . 1 6 1 . 2 4 3 . 2 6 0 . 7 D ut ch 1 1 m 1 5 3 . 7 6 7 . 0 5 5 . 1 6 4 . 7 â\x80\x93 â\x80\x93 5 6 . 4 6 9 . 0 5 1 . 5 6 7 . 3 Po rtu gu es e 1 1 m 1 5 0 . 8 7 5 . 3 4 3 . 2 7 4 . 8 44 .5 69 .2 6 4 . 1 7 4 . 5 5 6 . 5 7 0 . 1 S pa ni sh 1 1 m 1 â\x80\x93 â\x80\x93 4 0 . 6 7 3 . 2 â\x80\x93 â\x80\x93 5 8 . 3 6 8 . 9 5 0 . 0 5 7 . 2 Table 4: Comparison of our method (FEATS) to state-of-the-art methods.', '224': 'Feature-based HMM Model (Berg- Kirkpatrick et al., 2010): The KM model uses a variety of orthographic features and employs the EM or LBFGS optimization algorithm; Posterior regulariation model (GracÂ¸a et al., 2009): The G10 model uses the posterior regular- ization approach to ensure tag sparsity constraint.', '225': 'La ng ua ge 1T W + P RI O R + F E A T S E ng lis h D a ni s h D u tc h G e r m a n P or tu g u e s e S p a ni s h S w e di s h 2 1.', '226': '1 1 0.', '227': '1 2 3.', '228': '8 1 2.', '229': '8 1 8.', '230': '4 7 . 3 8 . 9 2 8 . 8 2 0 . 7 3 2 . 3 3 5 . 2 2 9 . 6 2 7 . 6 1 4 . 2 4 2 . 8 4 5 . 9 4 4 . 3 6 0 . 6 6 1 . 5 4 9 . 9 3 3 . 9 Table 6: Type-level Results: Each cell report the type- level accuracy computed against the most frequent tag of each word type.', '231': 'The state-to-tag mapping is obtained from the best hyperparameter setting for 11 mapping shown in Table 3.', '232': 'our full model yields 39.3% average error reduction across languages when compared to the basic configuration (1TW).', '233': 'Table 5 provides insight into the behavior of different models in terms of the tagging lexicon they generate.', '234': 'The table shows that the lexicon tag frequency predicated by our full model are the closest to the gold standard.', '235': '7 Conclusion and Future Work.', '236': 'We have presented a method for unsupervised part- of-speech tagging that considers a word type and its allowed POS tags as a primary element of the model.', '237': 'This departure from the traditional token-based tagging approach allows us to explicitly capture type- level distributional properties of valid POS tag as signments as part of the model.', '238': 'The resulting model is compact, efficiently learnable and linguistically expressive.', '239': 'Our empirical results demonstrate that the type-based tagger rivals state-of-the-art tag-level taggers which employ more sophisticated learning mechanisms to exploit similar constraints.', '240': 'In this paper, we make a simplifying assumption of one-tag-per-word.', '241': 'This assumption, however, is not inherent to type-based tagging models.', '242': 'A promising direction for future work is to explicitly model a distribution over tags for each word type.', '243': 'We hypothesize that modeling morphological information will greatly constrain the set of possible tags, thereby further refining the representation of the tag lexicon.', '244': 'The authors acknowledge the support of the NSF (CAREER grant IIS0448168, and grant IIS 0904684).', '245': 'We are especially grateful to Taylor Berg- Kirkpatrick for running additional experiments.', '246': 'We thank members of the MIT NLP group for their suggestions and comments.', '247': 'Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.'}",['D10-1083'],['../data/summaries/D10-1083.txt'],['../data/tba/D10-1083.json']
E03-1005,An Efficient Implementation of a New DOP Model,../data/papers/E03-1005.xml,"{'0': 'An Efficient Implementation of a New DOP Model', '1': 'Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank.', '2': 'This paper proposes an integration of the two models which outperforms each of them separately.', '3': 'Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.', '4': 'The distinctive feature of the DOP approach when it was proposed in 1992 was to model sentence structures on the basis of previously observed frequencies of sentence structure fragments, without imposing any constraints on the size of these fragments.', '5': 'Fragments include, for instance, subtrees of depth 1 (corresponding to context-free rules) as well as entire trees.', '6': 'To appreciate these innovations, it should be noted that the model was radically different from all other statistical parsing models at the time.', '7': 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', '8': 'in Fujisaki et al. 1989; Black et al.', '9': '1992, 1993; Briscoe and I Thanks to Ivan Sag for this pun.', '10': 'Waegner 1992; Pereira and Schabes 1992).', '11': 'The DOP model, on the other hand, was the first model (to the best of our knowledge) that proposed not to train a predefined grammar on a corpus, but to directly use corpus fragments as a grammar.', '12': 'This approach has now gained wide usage, as exemplified by the work of Collins (1996, 1999), Charniak (1996, 1997), Johnson (1998), Chiang (2000), and many others.', '13': 'The other innovation of DOP was to take (in principle) all corpus fragments, of any size, rather than a small subset.', '14': 'This innovation has not become generally adopted yet: many approaches still work either with local trees, i.e. single level rules with limited means of information percolation, or with restricted fragments, as in Stochastic Tree-Adjoining Grammar (Schabes 1992; Chiang 2000) that do not include nonlexicalized fragments.', '15': 'However, during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.', '16': 'While the models of Collins (1996) and Eisner (1996) restricted the fragments to the locality of head-words, later models showed the importance of including context from higher nodes in the tree (Charniak 1997; Johnson 1998a).', '17': 'The importance of including nonheadwords has become uncontroversial (e.g.', '18': 'Collins 1999; Charniak 2000; Goodman 1998).', '19': 'And Collins (2000) argues for &quot;keeping track of counts of arbitrary fragments within parse trees&quot;, which has indeed been carried out in Collins and Duffy (2002) who use exactly the same set of (all) tree fragments as proposed in Bod (1992).', '20': 'Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).', '21': 'One instantiation of DOP which has received considerable interest is the model known as DOP12 (Bod 1992).', '22': 'DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).', '23': 'Bod (1993) showed how standard parsing techniques can be applied to DOP1 by converting subtrees into rules.', '24': ""However, the problem of computing the most probable parse turns out to be NP-hard (Sima'an 1996), mainly because the same parse tree can be generated by exponentially many derivations."", '25': ""Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002)."", '26': ""Sima'an (1995) gave an efficient algorithm for computing the parse tree generated by the most probable derivation, which in some cases is a reasonable approximation of the most probable parse."", '27': 'Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.', '28': ""While Goodman's method does still not allow for an efficient computation of the most probable parse in DOP1, it does efficiently compute the &quot;maximum constituents parse&quot;, i.e. the parse tree which is most likely to have the largest number of correct constituents."", '29': ""Johnson (1998b, 2002) showed that DOP1's subtree estimation method is statistically biased and inconsistent."", '30': 'Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.', '31': 'This resulted in a statistically consistent model dubbed ML-DOP.', '32': 'However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.', '33': 'Cross-validation is needed to avoid this problem.', '34': 'But even with cross-validation, ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).', '35': ""Bonnema et al. (1999) observed that another problem with DOP1's subtree-estimation method is that it provides more probability to nodes with more subtrees, and therefore more probability to larger subtrees."", '36': 'As an alternative, Bonnema et al. (1999) propose a subtree estimator which reduces the probability of a tree by a factor of two for each non-root non-terminal it contains.', '37': 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', '38': ""Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."", '39': ""Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees, reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ."", '40': ""Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction, but did not report any experiments with these reductions."", '41': ""This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ."", '42': 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', '43': 'Bod (2001, 2003).', '44': ""But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."", '45': 'In the second part of this paper, we extend our experiments with a new notion of the best parse tree.', '46': 'Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.', '47': 'We show that a combination of a probabilistic and a simplicity metric, which chooses the simplest parse from the n likeliest parses, outperforms the use of these metrics alone.', '48': 'Compared to Bod (2001), our results show an 11% improvement in terms of relative error reduction and a speedup which reduces the processing time from 220 to 3.6 seconds per WSJ sentence.', '49': 'DOP1 parses new input by combining treebanksubtrees by means of a leftmost node-subsitution operation, indicated as 0.', '50': 'The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.', '51': ""That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree."", '52': 'The probability of a parse tree T is the sum of the probabilities of its distinct derivations.', '53': 'Let tid be the i-th subtree in the derivation d that produces tree T, then the probability of T is given by Thus DOP1 considers counts of subtrees of a wide range of sizes in computing the probability of a tree: everything from counts of single-level rules to counts of entire trees.', '54': 'A disadvantage of this model is that an extremely large number of subtrees (and derivations) must be taken into account.', '55': 'Fortunately, there exists a compact PCFG-reduction of DOP1 that generates the same trees with the same probabilities, as shown by Goodman (1996, 2002).', '56': 'Here we will only sketch this PCFG-reduction, which is heavily based on Goodman (2002).', '57': 'Goodman assigns every node in every tree a unique number which is called its address.', '58': 'The notation A@k denotes the node at address k where A is the nonterminal labeling that node.', '59': 'A new nonterminal is created for each node in the training data.', '60': 'This nonterminal is called A k. Nonterminals of this form are called &quot;interior&quot; nonterminals, while the original nonterminals in the parse trees are called &quot;exterior&quot; nontermimals.', '61': 'Let aj represent the number of subtrees headed by the node A@j.', '62': 'Let a represent the number of subtrees headed by nodes with nonterminal A, that is a =Ej aj.', '63': 'Goodman (1996, 2002) further illustrates this by a node A @j of the following form: To see how many subtrees it has, Goodman first considers the possibilities of the left branch.', '64': 'There are bk non-trivial subtrees headed by B@k, and there is also the trivial case where the left node is simply B.', '65': 'Thus there are bk + 1 different possibilities on the left branch.', '66': 'Similarly, there are ci + 1 possibilities on the right branch.', '67': 'We can create a subtree by choosing any possible left subtree and any possible right subtree.', '68': 'Thus, there are aj= (bk+ 1)(ci + 1) possible subtrees headed by A @j. Goodman then gives a simple small PCFG with the following property: for every subtree in the training corpus headed by A, the grammar will generate an isomorphic subderivation with probability 1/a.', '69': 'Thus, rather than using the large, explicit DOP1 model, one can also use this small PCFG that generates isomorphic derivations, with identical probabilities.', '70': ""Goodman's construction is as follows."", '71': 'For the node in figure 1, the following eight PCFG rules are generated, where the number in parentheses following a rule is its probability.', '72': 'Goodman then shows by simple induction that subderivations headed by A with external nonterminals at the roots and leaves, internal nonterminals elsewhere have probability 1/a.', '73': 'And subderivations headed by A1 with external nonterminals only at the leaves, internal nonterminals elsewhere, have probability 1/a1 (Goodman 1996).', '74': ""Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability."", '75': 'This means that summing up over derivations of a tree in DOP yields the same probability as summing over all the isomorphic derivations in the PCFG.', '76': ""Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree."", '77': 'But Goodman shows that with his PCFG-reduction he can efficiently compute the aforementioned maximum constituents parse.', '78': ""Moreover, Goodman's PCFG reduction may also be used to estimate the most probable parse by Viterbi n-best search which computes the n most likely derivations and then sums up the probabilities of the derivations producing the same tree."", '79': ""While Bod (2001) needed to use a very large sample from the WSJ subtrees to do this, Goodman's method can do the same job with a more compact grammar."", '80': 'DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).', '81': 'The amount of probability given to two different training nodes depends on how many subtrees they have, and, given that the number of subtrees is an exponential function, this means that some training nodes could easily get hundreds or thousands of times the weight of others, even if both occur exactly once.', '82': 'Bonnema et al. (1999) show that as a consequence too much weight is given to larger subtrees, and that the parse accuracy of DOP1 deteriorates if (very) large subtrees are included.', '83': 'Although this property may not be very harmful for small corpora with relatively small trees, such as the ATIS, Bonnema et al. give evidence that it leads to severe biases for larger corpora such as the WSJ.', '84': 'There are several ways to fix this problem.', '85': 'For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).', '86': 'Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).', '87': 'Yet, his grammar contains more than 5 million subtrees and processing times of over 200 seconds per WSJ sentence are reported (Bod 2003).', '88': ""In this paper, we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data."", '89': 'Let a be the number of times nonterminals of type A occur in the training data.', '90': 'Then we slightly modify the PCFG-reduction in figure 2 as follows: We will also test the proposal by Bonnema et al. (1999) which reduces the probability of a subtree by a factor of two for each non-root nonterminal it contains.', '91': 'It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains, resulting in the PCFG reduction in figure 4.', '92': ""Tested on the OVIS corpus, Bonnema et al. 's proposal obtains results that are comparable to Sima'an (1999) -- see Bonnema et al."", '93': '(1999).', '94': 'This paper presents the first published results with this estimator on the WSJ.', '95': 'By using these PCFG-reductions we can thus parse with all subtrees in polynomial time.', '96': 'However, as mentioned above, efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.', '97': 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.', '98': ""Most DOP models, such as in Bod (1993), Goodman (1996), Bonnema et al. (1997), Sima'an (2000) and Collins & Duffy (2002), use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence."", '99': 'We will refer to these models as Likelihood-DOP models, but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', '100': 'In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.', '101': 'We will refer to this model as Simplicity-DOP.', '102': 'In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.', '103': 'That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.', '104': 'Next, the rank of each (shortest) derivation is computed as the sum of the ranks of the subtrees involved.', '105': 'The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.', '106': 'What is more important, is, that the best parse trees predicted by Simplicity-DOP are quite different from the best parse trees predicted by Likelihood-DOP.', '107': 'This suggests that a model which combines these two notions of best parse may boost the accuracy.', '108': 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', '109': 'A straightforward alternative would be to select the most probable tree from among the n simplest trees.', '110': 'We will refer to the first combination (which selects the simplest among the n likeliest trees) as Simplicity-Likelihood-DOP or SL-DOP, and to the second combination (which selects the likeliest among the n simplest trees) as Likelihood-Simplicity-DOP or LS-DOP.', '111': 'Note that for n=1, SL-DOP is equal to Likelihood-DOP, since there is only one most probable tree to select from, and LS-DOP is equal to Simplicity-DOP, since there is only one simplest tree to select from.', '112': 'Moreover, if n gets large, SL-DOP converges to Simplicity-DOP while LS-DOP converges to Likelihood-DOP.', '113': 'By varying the parameter n, we will be able to compare Likelihood-DOP, Simplicity-DOP and several instantiations of SL-DOP and LS-DOP.', '114': ""Note that Goodman's PCFG-reduction method summarized in Section 2 applies not only to Likelihood-DOP but also to Simplicity-DOP."", '115': 'The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.', '116': 'Then the shortest derivation is equal to the most probable derivation and can be computed by standard Viterbi optimization, which can be seen as follows: if each subtree has a probability p then the probability of a derivation involving n subtrees is equal to pn, and since 0<p<1, the derivation with the fewest subtrees has the greatest probability.', '117': 'For SL-DOP and LS-DOP, we first compute either n likeliest or n simplest trees by means of Viterbi optimization.', '118': 'Next, we either select the simplest tree among the n likeliest ones (for SL DOP) or the likeliest tree among the n simplest ones (for LS-DOP).', '119': 'In our experiments, n will never be larger than 1,000.', '120': 'For our experiments we used the standard division of the WSJ (Marcus et al. 1993), with sections 2 through 21 for training (approx.', '121': '40,000 sentences) and section 23 for testing (2416 sentences 100 words); section 22 was used as development set.', '122': 'As usual, all trees were stripped off their semantic tags, co-reference information and quotation marks.', '123': 'Without loss of generality, all trees were converted to binary branching (and were reconverted to n-ary trees after parsing).', '124': 'We employed the same unknown (category) word model as in Bod (2001), based on statistics on word-endings, hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).', '125': 'We used &quot;evalb&quot;4 to compute the standard PARSEVAL scores for our results (Manning & Schiitze 1999).', '126': 'We focused on the Labeled Precision (LP) and Labeled Recall (LR) scores, as these are commonly used to rank parsing systems.', '127': 'Our first experimental goal was to compare the two PCFG-reductions in Section 2.2, which we will refer to resp. as Bod01 and Bon99.', '128': 'Table 1 gives the results of these experiments and compares them with some other statistical parsers (resp.', '129': 'Collins 1996, Charniak 1997, Collins 1999 and Charniak 2000).', '130': ""While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."", '131': 'As to the processing time, the PCFG reduction parses each sentence 100 words) in 3.6 seconds average, while the parser in Bod (2001, 2003), which uses over 5 million subtrees, is reported to take about 220 seconds per sentence.', '132': 'This corresponds to a speedup of over 60 times.', '133': 'It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).', '134': 'This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.', '135': 'In the following section first results of SL-DOP and LS-DOP with a compact PCFG-reduction. we will see that our new definition of best parse tree also outperforms the best results obtained in Bod (2001).', '136': 'As our second experimental goal, we compared the models SL-DOP and LS-DOP explained in Section 3.2.', '137': 'Recall that for n=1, SL-DOP is equal to the PCFG-reduction of Bod (2001) (which we also called Likelihood-DOP) while LS-DOP is equal to Simplicity-DOP.', '138': 'Table 2 shows the results for sentences 100 words for various values of n. Note that there is an increase in accuracy for both SL-DOP and LS-DOP if the value of n increases from 1 to 12.', '139': 'But while the accuracy of SL-DOP decreases after n=14 and converges to Simplicity DOP, the accuracy of LS-DOP continues to increase and converges to Likelihood-DOP.', '140': 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', '141': 'This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.', '142': 'Compared to the reranking technique in Collins (2000), who obtained an LP of 89.9% and an LR of 89.6%, our results show a 9% relative error rate reduction.', '143': 'While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.', '144': 'While the first feature has been generally adopted in statistical NLP, the second feature has for a long time been a serious bottleneck, as it results in exponential processing time when the most probable parse tree is computed.', '145': 'This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.', '146': 'This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).'}","['E03-1005_swastika', 'E03-1005_sweta', 'E03-1005_aakansha']","['../data/summaries/E03-1005_swastika.txt', '../data/summaries/E03-1005_sweta.txt', '../data/summaries/E03-1005_aakansha.txt']","['../data/tba/E03-1005_swastika.json', '../data/tba/E03-1005_sweta.json', '../data/tba/E03-1005_aakansha.json']"
E03-1020,Discovering Corpus-Specific Word Senses,../data/papers/E03-1020.xml,"{'0': 'Discovering Corpus-Specific Word Senses', '1': 'This paper presents an unsupervised algorithm which automatically discovers word senses from text.', '2': 'The algorithm is based on a graph model representing words and relationships between them.', '3': 'Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word.', '4': 'Discrimination against previously extracted sense clusters enables us to discover new senses.', '5': 'We use the same data for both recognising and resolving ambiguity.', '6': 'This paper describes an algorithm which automatically discovers word senses from free text and maps them to the appropriate entries of existing dictionaries or taxonomies.', '7': 'Automatic word sense discovery has applications of many kinds.', '8': ""It can greatly facilitate a lexicographer's work and can be used to automatically construct corpus-based taxonomies or to tune existing ones."", '9': 'The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context (Schiitze, 1998).', '10': 'This paper is organised as follows.', '11': 'In section 2, we present the graph model from which we discover word senses.', '12': 'Section 3 describes the way we divide graphs surrounding ambiguous words into different areas corresponding to different senses, using Markov clustering (van Dongen, 2000).', '13': 'The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph.', '14': 'In section 4, we outline a word sense discovery algorithm which bypasses the problem of parameter tuning.', '15': 'We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity.', '16': 'Section 5 describes the experiment and presents a sample of the results.', '17': 'Finally, section 6 sketches applications of the algorithm and discusses future work.', '18': 'The model from which we discover distinct word senses is built automatically from the British National corpus, which is tagged for parts of speech.', '19': 'Based on the intuition that nouns which co-occur in a list are often semantically related, we extract contexts of the form Noun, Noun,... and/or Noun, e.g. ""genomic DNA from rat, mouse and dog"".', '20': 'Following the method in (Widdows and Dorow, 2002), we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1.', '21': ""Following Lin's work (1998), we are currently investigating a graph with verb-object, verb-subject and modifier-noun-collocations from which it is possible to infer more about the senses of systematically polysemous words."", '22': 'The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features.', '23': '1 Si mple cutoff functions proved unsatisfactory because of the bias they give to more frequent words.', '24': 'Instead we link each word to its top n neighbors where n can be determined by the user (cf.', '25': 'section 4)..', '26': '41=0 441=P .4161.', '27': 'sz44, CD miltrA, litrepate inovio.\x84 h,) Cik Figure 1: Local graph of the word mouse', '28': 'Ambiguous words link otherwise unrelated areas of meaning E.g. rat and printer are very different in meaning, but they are both closely related to different meanings of mouse.', '29': 'However, if we remove the mouse-node from its local graph illustrated in figure 1, the graph decomposes into two parts, one representing the electronic device meaning of mouse and the other one representing its animal sense.', '30': 'There are, of course, many more types of polysemy (cf.', '31': 'e.g.', '32': '(Kilgarriff, 1992)).', '33': 'As can be seen in figure 2, wing ""part of a bird"" is closely related to tail, as is wing ""part of a plane"".', '34': 'Therefore, even after removal of the wing-node, the two areas of meaning are still linked via tail.', '35': 'The same happens with wing ""part of a building"" and wing ""political group"" which are linked via policy.', '36': 'However, whereas there are many edges within an area of meaning, there is only a small number of (weak) links between different areas of meaning.', '37': 'To detect the different areas of meaning in our local graphs, we use a cluster algorithm for graphs (Markov clustering, MCL) developed by van Dongen (2000).', '38': 'The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters.', '39': 'The following notation and description of the MCL algorithm borrows heavily from van Dongen (2000).', '40': 'Let G\x84, denote the local graph around the ambiguous word w. The adjacency matrix MG\x84 4111) 11\x91 41 4Wit ler,1110.1/.17, cgtoserek¦Ilt Figure 2: Local graph of the word wing of a graph G\x84, is defined by setting (111G\x84) pq equal to the weight of the edge between nodes v and v q . Normalizing the columns of A/G\x84 results in the Markov Matrix Taw whose entries (Thi,)pq can be interpreted as transition probability from v q to vv . It can easily be shown that the k-th power of TG\x84 lists the probabilities (TL )pq of a path of length k starting at node vq and ending at node V. The MCL-algorithm simulates flow in Gw by iteratively recomputing the set of transition probabilities via two steps, expansion and inflation.', '41': 'The expansion step corresponds with taking the k-th power of TG\x84 as outlined above and allows nodes to see new neighbours.', '42': 'The inflation step takes each matrix entry to the r-th power and then rescales each column so that the entries sum to 1.Vi a inflation, popular neighbours are further supported at the expense of less popular ones.', '43': 'Flow within dense regions in the graph is concentrated by both expansion and inflation.', '44': 'Eventually, flow between dense regions will disappear, the matrix of transition probabilities TG\x84 will converge and the limiting matrix can be interpreted as a clustering of the graph.', '45': 'The output of the MCL-algorithm strongly depends on the inflation and expansion parameters r and k as well as the size of the local graph which serves as input to MCL.', '46': 'An appropriate choice of the inflation param 80 eter r can depend on the ambiguous word w to be clustered.', '47': 'In case of homonymy, a small inflation parameter r would be appropriate.', '48': 'However, there are ambiguous words with more closely related senses which are metaphorical or metonymic variations of one another.', '49': 'In that case, the different regions of meaning are more strongly interlinked and a small power coefficient r would lump different meanings together.', '50': 'Usually, one sense of an ambiguous word w is much more frequent than its other senses present in the corpus.', '51': ""If the local graph handed over to the MCL process is small, we might miss some of w's meanings in the corpus."", '52': 'On the other hand, if the local graph is too big, we will get a lot of noise.', '53': 'Below, we outline an algorithm which circumvents the problem of choosing the right parameters.', '54': ""In contrast to pure Markov clustering, we don't try to find a complete clustering of G into senses at once."", '55': 'Instead, in each step of the iterative process, we try to find the most disctinctive cluster c of G w (i.e. the most distinctive meaning of w) only.', '56': ""We then recompute the local graph Gw by discriminating against c's features."", '57': ""This is achieved, in a manner similar to Pantel and Lin's (2002) sense clustering approach, by removing c's features from the set of features used for finding similar words."", '58': 'The process is stopped if the similarity between w and its best neighbour under the reduced set of features is below a fixed threshold.', '59': ""Let F be the set of w's features, and let L be the output of the algorithm, i.e. a list of sense clusters initially empty."", '60': 'The algorithm consists of the following steps: 1.', '61': 'Compute a small local graph Gw around w using the set of features F. If the similarity between w and its closest neighbour is below a fixed threshold go to 6.', '62': '2. Recursively remove all nodes of degree one.', '63': 'Then remove the node corresponding with w from G. 3.', '64': 'Apply MCL to Gw with a fairly big inflation parameter r which is fixed.', '65': '4.', '66': 'Take the ""best"" cluster (the one that is most strongly connected to w in Gw before removal of w), add it to the final list of clusters L and remove/devalue its features from F. 5.', '67': 'Go back to 1 with the reduced/devalued set of features F. 6.', '68': 'Go through the final list of clusters L and assign a name to each cluster using a broad-coverage taxonomy (see below).', '69': 'Merge semantically close clusters using a taxonomy-based semantic distance measure (Budanitsky and Hirst, 2001) and assign a class-label to the newly formed cluster.', '70': '7.', '71': 'Output the list of class-labels which best represent the different senses of w in the corpus.', '72': 'The local graph in step 1 consists of w, the ni neighbours of w and the n9 neighbours of the neighbours of w. Since in each iteration we only attempt to find the ""best"" cluster, it suffices to build a relatively small graph in 1.', '73': 'Step 2 removes noisy strings of nodes pointing away from G. The removal of w from G w might already separate the different areas of meaning, but will at least significantly loosen the ties between them.', '74': 'In our simple model based on noun co-occurrences in lists, step 5 corresponds to rebuilding the graph under the restriction that the nodes in the new graph not co-occur (or at least not very often) with any of the cluster members already extracted.', '75': 'The class-labelling (step 6) is accomplished using the taxonomic structure of WordNet, using a robust algorithm developed specially for this purpose.', '76': 'The hypemym which subsumes as many cluster members as possible and does so as closely as possible in the taxonomic tree is chosen as class-label.', '77': 'The family of such algorithms is described in (Widdows, 2003).', '78': 'In this section, we describe an initial evaluation experiment and present the results.', '79': 'We will soon carry out and report on a more thorough analysis of our algorithm.', '80': 'We used the simple graph model based on co-occurrences of nouns in lists (cf.', '81': 'section 2) for our experiment.', '82': 'We gathered a list of nouns with varying degree of ambiguity, from homonymy (e.g. arms) to systematic polysemy (e.g. cherry).', '83': 'Our algorithm was applied to each word in the list (with parameters Iii = 20, n2 = 10, r = 2.0, k = 2.0) in order to extract the top two sense clusters only.', '84': 'We then determined the WordNet synsets which most adequately characterized the sense clusters.', '85': 'An extract of the results is listed in table 1.', '86': 'Word Sense clusters Class-label arms knees trousers feet biceps hips elbows backs wings body part breasts shoulders thighs bones buttocks ankles legs inches wrists shoes necks horses muskets charges weapons methods firearms weapon knives explosives bombs bases mines projectiles drugs missiles uniforms jersey israel colomho guernsey luxeinhourg denmark maim European greece belgium swede, turkey gibraltar portugal ire- country land mauritius britain cyprus netherlands norway aus tralia italy japan canada kingdom spain austria zealand england france germany switzerland finland poland a merica usa iceland holland scotland uk crucifix bow apron sweater tie anorak hose bracelet garment helmet waistcoat jacket pullover equipment cap collar suit fleece tunic shirt scarf belt head voice torso back chest face abdomen side belly groin body part spine breast bill rump midhair hat collar waist tail stomach skin throat neck speculum ceo treasurer justice chancellor principal founder pres- person ident commander deputy administrator constable li brarian secretary governor captain premier executive chief curator assistant committee patron ruler oil heat coal power water gas food wood fuel steam tax object heating kerosene fire petroleum dust sand light steel telephone timber supply drainage diesel electricity acid air insurance petrol tempera gouache watercolour poster pastel collage paint acrylic lemon bread cheese [flint butter jam cream pudding yogurt foodstuff sprinkling honey jelly toast ham chocolate pie syrup milk meat beef cake yoghurt grain hazel elder holly family virgin hawthorn shrub cherry cedar larch mahogany water sycamore lime teak ash wood hornbeam oak walnut hazel pine beech alder thorn poplar birch chestnut blackthorn spruce holly yew lau rel maple elm fir hawthorn willow bacon cream honey pie grape blackcurrant cake ha- foodstuff mama Table 1: Output of word sense clustering.', '87': 'The benefits of automatic, data-driven word sense discovery for natural language processing and lexicography would be very great.', '88': 'Here we only mention a few direct results of our work.', '89': 'Our algorithm does not only recognise ambiguity, but can also be used to resolve it, because the features shared by the members of each sense cluster provide strong indication of which reading of an ambiguous word is appropriate given a certain context.', '90': 'This gives rise to an automatic, unsupervised word sense disambiguation algorithm which is trained on the data to be disambiguated.', '91': 'The ability to map senses into a taxonomy using the class-labelling algorithm can be used to ensure that the sense-distinctions discovered correspond to recognised differences in meaning.', '92': ""This approach to disambiguation combines the benefits of both Yarowsky's (1995) and Schtitze's (1998) approaches."", '93': 'Preliminary observations show that the different neighbours in Table 1 can be used to indicate with great accuracy which of the senses is being used.', '94': 'Off-the-shelf lexical resources are rarely adequate for NLP tasks without being adapted.', '95': 'They often contain many rare senses, but not the same ones that are relevant for specific domains or corpora.', '96': 'The problem can be addressed by using word sense clustering to attune an existing resource to accurately describe the meanings used in a particular corpus.', '97': 'We prepare an evaluation of our algorithm as applied to the collocation relationships (cf.', '98': 'section 2), and we plan to evaluate the uses of our clustering algorithm for unsupervised disambiguation more thoroughly.'}",['E03-1020'],['../data/summaries/E03-1020.txt'],['../data/tba/E03-1020.json']
E09-2008,Foma: a finite-state compiler and library,../data/papers/E09-2008.xml,"{'0': 'Foma: a finite-state compiler and library', '1': 'Foma is a compiler, programming language, and C library for constructing finite-state automata and transducers for various uses.', '2': 'It has specific support for many natural language processing applications such as producing morphological and phonological analyzers.', '3': 'Foma is largely compatible with the Xerox/PARC finite-state toolkit.', '4': 'It also embraces Unicode fully and supports various different formats for specifying regular expressions: the Xerox/PARC format, a Perl-like format, and a mathematical format that takes advantage of the â\x80\x98Mathematical Operatorsâ\x80\x99 Unicode block.', '5': 'Foma is a finite-state compiler, programming language, and regular expression/finite-state library designed for multipurpose use with explicit support for automata theoretic research, constructing lexical analyzers for programming languages, and building morphological/phonological analyzers, as well as spellchecking applications.', '6': 'The compiler allows users to specify finite-state automata and transducers incrementally in a similar fashion to AT&Tâ\x80\x99s fsm (Mohri et al., 1997) and Lextools (Sproat, 2003), the Xerox/PARC finite- state toolkit (Beesley and Karttunen, 2003) and the SFST toolkit (Schmid, 2005).', '7': 'One of Fomaâ\x80\x99s design goals has been compatibility with the Xerox/PARC toolkit.', '8': 'Another goal has been to allow for the ability to work with n-tape automata and a formalism for expressing first-order logical constraints over regular languages and n-tape- transductions.', '9': 'Foma is licensed under the GNU general public license: in keeping with traditions of free software, the distribution that includes the source code comes with a user manual and a library of examples.', '10': 'The compiler and library are implemented in C and an API is available.', '11': 'The API is in many ways similar to the standard C library <regex.h>, and has similar calling conventions.', '12': 'However, all the low-level functions that operate directly on automata/transducers are also available (some 50+ functions), including regular expression primitives and extended functions as well as automata deter- minization and minimization algorithms.', '13': 'These may be useful for someone wanting to build a separate GUI or interface using just the existing low- level functions.', '14': 'The API also contains, mainly for spell-checking purposes, functionality for finding words that match most closely (but not exactly) a path in an automaton.', '15': 'This makes it straightforward to build spell-checkers from morphological transducers by simply extracting the range of the transduction and matching words approximately.', '16': 'Unicode (UTF8) is fully supported and is in fact the only encoding accepted by Foma.', '17': 'It has been successfully compiled on Linux, Mac OS X, and Win32 operating systems, and is likely to be portable to other systems without much effort.', '18': 'Retaining backwards compatibility with Xerox/PARC and at the same time extending the formalism means that one is often able to construct finite-state networks in equivalent various ways, either through ASCII-based operators or through the Unicode-based extensions.', '19': 'For example, one can either say: ContainsX = Î£* X Î£*; MyWords = {cat}|{dog}|{mouse}; MyRule = n -> m || p; ShortWords = [MyLex1]1 â\x88© Î£Ë\x86<6; or: Proceedings of the EACL 2009 Demonstrations Session, pages 29â\x80\x9332, Athens, Greece, 3 April 2009.', '20': 'Qc 2009 Association for Computational Linguistics Operators Compatibility variant Function [ ] () [ ] () grouping parentheses, optionality â\x88\x80 â\x88\x83 N/A quantifiers \\ â\x80\x98 term negation, substitution/homomorphism : : cross-product + â\x88\x97 + â\x88\x97 Kleene closures Ë\x86<n Ë\x86>n Ë\x86{m,n} Ë\x86<n Ë\x86>n Ë\x86{m,n} iterations 1 2 .1 .2 .u .l domain & range .f N/A eliminate all unification flags $ $.', '21': 'Ë\x9c $ $.', '22': 'complement, containment operators / ./.', '23': 'N/A N/A â\x80\x98ignoresâ\x80\x99, left quotient, right quotient, â\x80\x98insideâ\x80\x99 quotient â\x88\x88 â\x88\x88/ = /= N/A language membership, position equivalence â\x89º < > precedes, follows â\x88¨ â\x88ª â\x88§ â\x88© - .P. .p. | & â\x88\x92 .P. .p. union, intersection, set minus, priority unions => -> (->) @-> => -> (->) @-> context restriction, replacement rules <> shuffle (asynchronous product) Ã\x97 â\x97¦ .x. .o. cross-product, composition Table 1: The regular expressions available in Foma from highest to lower precedence.', '24': 'Horizontal lines separate precedence classes.', '25': 'define ContainsX ?* X ?*; define MyWords {cat}|{dog}|{mouse}; define MyRule n -> m || _ p; define ShortWords Mylex.i.l & ?Ë\x86<6; In addition to the basic regular expression operators shown in table 1, the formalism is extended in various ways.', '26': 'One such extension is the ability to use of a form of first-order logic to make existential statements over languages and transductions (Hulden, 2008).', '27': 'For instance, suppose we have defined an arbitrary regular language L, and want to further define a language that contains only one factor of L, we can do so by: OneL = (â\x88\x83x)(x â\x88\x88 L â\x88§ (â\x88\x83y)(y â\x88\x88 L â\x88§ (x = y))); Here, quantifiers apply to substrings, and we attribute the usual meaning to â\x88\x88 and â\x88§, and a kind of concatenative meaning to the predicate S(t1, t2).', '28': 'Hence, in the above example, OneL defines the language where there exists a string x such that x is a member of the language L and there does not exist a string y, also in L, such that y would occur in a different position than x. This kind of logical specification of regular languages can be very useful for building some languages that would be quite cumbersome to express with other regular expression operators.', '29': 'In fact, many of the internally complex operations of Foma are built through a reduction to this type of logical expressions.', '30': 'As mentioned, Foma supports reading and writing of the LEXC file format, where morphological categories are divided into so-called continuation classes.', '31': 'This practice stems back from the earliest two-level compilers (Karttunen et al., 1987).', '32': 'Below is a simple example of the format: Multichar_Symbols +Pl +Sing LEXICON Root Nouns; LEXICON Nouns cat Plural; church Plural; LEXICON Plural +Pl:%Ë\x86s #; +Sing #;', '33': 'The Foma API gives access to basic functions, such as constructing a finite-state machine from a regular expression provided as a string, performing a transduction, and exhaustively matching against a given string starting from every position.', '34': 'The following basic snippet illustrates how to use the C API instead of the main interface of Foma to construct a finite-state machine encoding the language a+b+ and check whether a string matches it: 1.', '35': 'void check_word(char *s) { 2.', '36': 'fsm_t *network; 3.', '37': 'fsm_match_result *result; 4.', '38': '5. network = fsm_regex(""a+ b+""); 6.', '39': 'result = fsm_match(fsm, s); 7.', '40': 'if (result->num_matches > 0) 8.', '41': 'printf(""Regex matches""); 9.', '42': '10 } Here, instead of calling the fsm regex() function to construct the machine from a regular expressions, we could instead have accessed the beforementioned low-level routines and built the network entirely without regular expressions by combining low-level primitives, as follows, replacing line 5 in the above: network = fsm_concat( fsm_kleene_plus( fsm_symbol(""a"")), fsm_kleene_plus( fsm_symbol(""b""))); The API is currently under active development and future functionality is likely to include conversion of networks to 8-bit letter transducers/automata for maximum speed in regular expression matching and transduction.', '43': 'educational use Foma has support for visualization of the machines it builds through the AT&T Graphviz library.', '44': 'For educational purposes and to illustrate automata construction methods, there is some support for changing the behavior of the algorithms.', '45': 'For instance, by default, for efficiency reasons, Foma determinizes and minimizes automata between nearly every incremental operation.', '46': 'Operations such as unions of automata are also constructed by default with the product construction method that directly produces deterministic automata.', '47': 'However, this on-the-fly minimization and determinization can be relaxed, and a Thompson construction method chosen in the interface so that automata remain non-deterministic and non- minimized whenever possibleâ\x80\x94non-deterministic automata naturally being easier to inspect and analyze.', '48': 'Though the main concern with Foma has not been that of efficiency, but of compatibility and extendibility, from a usefulness perspective it is important to avoid bottlenecks in the underlying algorithms that can cause compilation times to skyrocket, especially when constructing and combining large lexical transducers.', '49': 'With this in mind, some care has been taken to attempt to optimize the underlying primitive algorithms.', '50': 'Table 2 shows a comparison with some existing toolkits that build deterministic, minimized automata/transducers.', '51': 'One the whole, Foma seems to perform particularly well with pathological cases that involve exponential growth in the number of states when determinizing non- deterministic machines.', '52': 'For general usage patterns, this advantage is not quite as dramatic, and for average use Foma seems to perform comparably with e.g. the Xerox/PARC toolkit, perhaps with the exception of certain types of very large lexicon descriptions (>100,000 words).', '53': 'The Foma project is multipurpose multi-mode finite-state compiler geared toward practical construction of large-scale finite-state machines such as may be needed in natural language processing as well as providing a framework for research in finite-state automata.', '54': 'Several wide- coverage morphological analyzers specified in the LEXC/xfst format have been compiled successfully with Foma.', '55': 'Foma is free software and will remain under the GNU General Public License.', '56': 'As the source code is available, collaboration is encouraged.', '57': 'GNU AT&T Foma xfst flex fsm 4 Î£â\x88\x97aÎ£15 0.216s 16.23s 17.17s 1.884s Î£â\x88\x97aÎ£20 8.605s nf nf 153.7s North Sami 14.23s 4.264s N/A N/A 8queens 0.188s 1.200s N/A N/A sudoku2x3 5.040s 5.232s N/A N/A lexicon.lex 1.224s 1.428s N/A N/A 3sat30 0.572s 0.648s N/A N/A Table 2: A relative comparison of running a selection of regular expressions and scripts against other finite-state toolkits.', '58': 'The first and second entries are short regular expressions that exhibit exponential behavior.', '59': 'The second results in a FSM with 221 states and 222 arcs.', '60': 'The others are scripts that can be run on both Xerox/PARC and Foma.', '61': 'The file lexicon.lex is a LEXC format English dictionary with 38418 entries.', '62': 'North Sami is a large lexicon (lexc file) for the North Sami language available from http://divvun.no.'}",['E09-2008'],['../data/summaries/E09-2008.txt'],['../data/tba/E09-2008.json']
H05-1115,Using Random Walks for Question-focused Sentence Retrieval,../data/papers/H05-1115.xml,"{'0': 'Using Random Walks for Question-focused Sentence Retrieval', '1': 'We consider the problem of question-focused sentence retrieval from complexnews articles describing multi-event stories published over time.', '2': 'Annotators generated a list of questions central to understanding each story in our corpus.', '3': 'Because of the dynamic nature of the stories,many questions are time-sensitive (e.g.\x93How many victims have been found?\x94)Judges found sentences providing an answer to each question.', '4': 'To address thesentence retrieval problem, we apply astochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization.', '5': 'Currently, we present a topic-sensitive versionof our method and hypothesize that it canoutperform a competitive baseline, whichcompares the similarity of each sentenceto the input question via IDFweightedword overlap.', '6': 'In our experiments, themethod achieves a TRDR score that is significantly higher than that of the baseline.', '7': 'Recent work has motivated the need for systemsthat support \x93Information Synthesis\x94 tasks, in whicha user seeks a global understanding of a topic orstory (Amigo et al., 2004).', '8': 'In contrast to the classical question answering setting (e.g. TREC-style Q&A (Voorhees and Tice, 2000)), in which the userpresents a single question and the system returns acorresponding answer (or a set of likely answers), inthis case the user has a more complex informationneed.', '9': 'Similarly, when reading about a complex newsstory, such as an emergency situation, users mightseek answers to a set of questions in order to understand it better.', '10': 'For example, Figure 1 showsthe interface to our Web-based news summarizationsystem, which a user has queried for informationabout Hurricane Isabel.', '11': 'Understanding such storiesis challenging for a number of reasons.', '12': 'In particular,complex stories contain many sub-events (e.g. thedevastation of the hurricane, the relief effort, etc.) Inaddition, while some facts surrounding the situationdo not change (such as \x93Which area did the hurricane first hit?\x94), others may change with time (\x93Howmany people have been left homeless?\x94).', '13': 'Therefore, we are working towards developing a systemfor question answering from clusters of complex stories published over time.', '14': 'As can be seen at the bottom of Figure 1, we plan to add a component to ourcurrent system that allows users to ask questions asthey read a story.', '15': 'They may then choose to receiveeither a precise answer or a question-focused summary.', '16': 'Currently, we address the question-focused sentence retrieval task.', '17': 'While passage retrieval (PR) isclearly not a new problem (e.g.', '18': '(Robertson et al.,1992; Salton et al., 1993)), it remains important andyet often overlooked.', '19': ""As noted by (Gaizauskas et al.,2004), while PR is the crucial first step for questionanswering, Q&A research has typically not empha915 Hurricane Isabel's outer bands moving onshoreproduced on 09/18, 6:18 AM 2% SummaryThe North Carolina coast braced for a weakened but still potent Hurricane Isabel while already rain-soaked areas as faraway as Pennsylvania prepared for possibly ruinous flooding."", '20': '(2:3) A hurricane warning was in effect from CapeFear in southern North Carolina to the VirginiaMaryland line, and tropical storm warnings extended from South Carolinato New Jersey.', '21': '(2:14) While the outer edge of the hurricane approached the North Carolina coast Wednesday, the center of the storm was still400 miles south-southeast of Cape Hatteras, N.C., late Wednesday morning.', '22': '(3:10) BBC NEWS World AmericasHurricane Isabel prompts US shutdown (4:1) Ask us:What states have been affected by the hurricane so far?', '23': 'Around 200,000 people in coastal areas of North Carolina and Virginia were ordered to evacuate or risk getting trappedby flooding from storm surges up to 11 feet.', '24': '(5:8) The storm was expected to hit with its full fury today, slamming intothe North Carolina coast with 105mph winds and 45-foot wave crests, before moving through Virginia and bashing thecapital with gusts of about 60 mph.', '25': '(7:6) Figure 1: Question tracking interface to a summarization system.', '26': 'sized it.', '27': 'The specific problem we consider differsfrom the classic task of PR for a Q&A system ininteresting ways, due to the time-sensitive nature ofthe stories in our corpus.', '28': 'For example, one challengeis that the answer to a user\x92s question may be updated and reworded over time by journalists in orderto keep a running story fresh, or because the factsthemselves change.', '29': 'Therefore, there is often morethan one correct answer to a question.We aim to develop a method for sentence retrieval that goes beyond finding sentences that aresimilar to a single query.', '30': 'To this end, we propose to use a stochastic, graph-based method.', '31': 'Recently, graph-based methods have proved useful fora number of NLP and IR tasks such as documentre-ranking in ad hoc IR (Kurland and Lee, 2005)and analyzing sentiments in text (Pang and Lee,2004).', '32': 'In (Erkan and Radev, 2004), we introducedthe LexRank method and successfully applied it togeneric, multi-document summarization.', '33': 'Presently,we introduce topic-sensitive LexRank in creating asentence retrieval system.', '34': 'We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and thequestion (using IDF-weighed word overlap).', '35': 'Wedemonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline.', '36': 'Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version ofthe LexRank method.', '37': 'In contrast to previous PR systems such as Okapi (Robertson et al., 1992), which ranks documents for relevancy and then proceeds tofind paragraphs related to a question, we address thefinergrained problem of finding sentences containing answers.', '38': 'In addition, the input to our system isa set of documents relevant to the topic of the querythat the user has already identified (e.g. via a searchengine).', '39': 'Our system does not rank the input documents, nor is it restricted in terms of the number ofsentences that may be selected from the same document.', '40': 'The output of our system, a ranked list of sentences relevant to the user\x92s question, can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences.', '41': 'Alternatively, the sentences canbe returned to the user as a question-focused summary.', '42': 'This is similar to \x93snippet retrieval\x94 (Wu etal., 2004).', '43': 'However, in our system answers are extracted from a set of multiple documents rather thanon a document-by-document basis.', '44': '3.1 The LexRank method.', '45': 'In (Erkan and Radev, 2004), the concept of graph-based centrality was used to rank a set of sentences,in producing generic multi-document summaries.', '46': 'To apply LexRank, a similarity graph is producedfor the sentences in an input document set.', '47': 'In thegraph, each node represents a sentence.', '48': 'There areedges between nodes for which the cosine similarity between the respective pair of sentences exceedsa given threshold.', '49': 'The degree of a given node isan indication of how much information the respective sentence has in common with other sentences.', '50': 'Therefore, sentences that contain the most salient information in the document set should be very centralwithin the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences, using a cosine similarity threshold of 0.15.', '51': 'Once the similarity graph isconstructed, the sentences are then ranked accordingto their eigenvector centrality.', '52': 'As previously mentioned, the original LexRank method performed wellin the context of generic summarization.', '53': 'Below,we describe a topic-sensitive version of LexRank,which is more appropriate for the question-focusedsentence retrieval problem.', '54': 'In the new approach, the 916 score of a sentence is determined by a mixture modelof the relevance of the sentence to the query and thesimilarity of the sentence to other high-scoring sentences.', '55': '3.2 Relevance to the question.', '56': 'In topic-sensitive LexRank, we first stem all of thesentences in a set of articles and compute word IDFsby the following formula: idfw = log (N + 1 0.5 + sfw )(1) whereN is the total number of sentences in the cluster, and sfw is the number of sentences that the wordw appears in.', '57': 'We also stem the question and remove the stop words from it.', '58': 'Then the relevance of a sentence s tothe question q is computed by: rel(s|q) =Xw?q log(tfw,s + 1)× log(tfw,q + 1) × idfw (2) where tfw,s and tfw,q are the number of times wappears in s and q, respectively.', '59': 'This model hasproven to be successful in query-based sentence retrieval (Allan et al., 2003), and is used as our competitive baseline in this study (e.g. Tables 4, 5 and7).', '60': '3.3 The mixture model.', '61': 'The baseline system explained above does not makeuse of any inter-sentence information in a cluster.We hypothesize that a sentence that is similar tothe high scoring sentences in the cluster should alsohave a high score.', '62': 'For instance, if a sentence thatgets a high score in our baseline model is likely tocontain an answer to the question, then a related sentence, which may not be similar to the question itself, is also likely to contain an answer.This idea is captured by the following mixture model, where p(s|q), the score of a sentence s givena question q, is determined as the sum of its relevance to the question (using the same measure asthe baseline described above) and the similarity tothe other sentences in the document cluster: p(s|q) = d rel(s|q)Pz?C rel(z|q) +(1-d)Xv?C sim(s, v)Pz?C sim(z, v) p(v|q) (3) where C is the set of all sentences in the cluster.', '63': 'Thevalue of d, which we will also refer to as the \x93question bias,\x94 is a trade-off between two terms in the Vertices: Sentence IndexSentence Index SalienceSalience SentenceSentence', '64': '1 0.03614457831325301 At least two people are dead, inclu...', '65': '0 0.28454242157110576 Officials said the plane was carryin...', '66': '2 0.1973852892722677 Italian police said the plane was car..', '67': '3 0.28454242157110576 Rescue officials said that at least th...', '68': 'Graph Figure 2: LexRank example: sentence similaritygraph with a cosine threshold of 0.15.', '69': 'equation and is determined empirically.', '70': 'For highervalues of d, we give more importance to the relevance to the question compared to the similarity tothe other sentences in the cluster.', '71': 'The denominatorsin both terms are for normalization, which are described below.', '72': 'We use the cosine measure weightedby word IDFs as the similarity between two sentences in a cluster: sim(x, y) = Pw?x,y tfw,xtfw,y(idfw) 2 qPxi?x(tfxi,xidfxi ) 2 ×qP yi?y(tfyi,y idfyi )2 (4) Equation 3 can be written in matrix notation asfollows: p = [dA+ (1- d)B]Tp (5) A is the square matrix such that for a given index i,all the elements in the ith column are proportionalto rel(i|q).', '73': 'B is also a square matrix such that eachentry B(i, j) is proportional to sim(i, j).', '74': 'Both matrices are normalized so that row sums add up to 1.Note that as a result of this normalization, all rowsof the resulting square matrixQ = [dA+(1-d)B]also add up to 1.', '75': 'Such a matrix is called stochasticand defines a Markov chain.', '76': 'If we view each sentence as a state in a Markov chain, thenQ(i, j) specifies the transition probability from state i to state jin the corresponding Markov chain.', '77': 'The vector pwe are looking for in Equation 5 is the stationarydistribution of the Markov chain.', '78': 'An intuitive interpretation of the stationary distribution can be under- 917 stood by the concept of a random walk on the graphrepresentation of the Markov chain.With probability d, a transition is made from the current node (sentence) to the nodes that are similar to the query.', '79': 'With probability (1-d), a transitionis made to the nodes that are lexically similar to thecurrent node.', '80': 'Every transition is weighted accordingto the similarity distributions.', '81': 'Each element of thevector p gives the asymptotic probability of endingup at the corresponding state in the long run regardless of the starting state.', '82': 'The stationary distributionof a Markov chain can be computed by a simple iterative algorithm, called power method.1 A simpler version of Equation 5, where A is auniform matrix andB is a normalized binary matrix,is known as PageRank (Brin and Page, 1998; Pageet al., 1998) and used to rank the web pages by theGoogle search engine.', '83': 'It was also the model used torank sentences in (Erkan and Radev, 2004).', '84': '3.4 Experiments with topic-sensitive LexRank.', '85': 'We experimented with different values of d on ourtraining data.', '86': 'We also considered several thresholdvalues for inter-sentence cosine similarities, wherewe ignored the similarities between the sentencesthat are below the threshold.', '87': 'In the training phaseof the experiment, we evaluated all combinationsof LexRank with d in the range of [0, 1] (in increments of 0.10) and with a similarity threshold ranging from [0, 0.9] (in increments of 0.05).', '88': 'We thenfound all configurations that outperformed the baseline.', '89': 'These configurations were then applied to ourdevelopment/test set.', '90': 'Finally, our best sentence retrieval system was applied to our test data set andevaluated against the baseline.', '91': 'The remainder of thepaper will explain this process and the results in detail.', '92': '4 Experimental setup.', '93': '4.1 Corpus.', '94': 'We built a corpus of 20 multi-document clusters ofcomplex news stories, such as plane crashes, political controversies and natural disasters.', '95': 'The data 1The stationary distribution is unique and the power methodis guaranteed to converge provided that the Markov chain isergodic (Seneta, 1981).', '96': 'A non-ergodic Markov chain can bemade ergodic by reserving a small probability for jumping toany other state from the current state (Page et al., 1998).', '97': 'clusters and their characteristics are shown in Table 1.', '98': 'The news articles were collected from varioussources.', '99': '\x93Newstracker\x94 clusters were collected automatically by our Web-based news summarization system.', '100': 'The number of clusters randomly assignedto the training, development/test and test data setswere 11, 3 and 6, respectively.Next, we assigned each cluster of articles to an annotator, who was asked to read all articles in thecluster.', '101': 'He or she then generated a list of factualquestions key to understanding the story.', '102': 'Once wecollected the questions for each cluster, two judgesindependently annotated nine of the training clusters.', '103': 'For each sentence and question pair in a givencluster, the judges were asked to indicate whetheror not the sentence contained a complete answerto the question.', '104': 'Once an acceptable rate of inter-judge agreement was verified on the first nine clusters (Kappa (Carletta, 1996) of 0.68), the remaining11 clusters were annotated by one judge each.In some cases, the judges did not find any sentences containing the answer for a given question.Such questions were removed from the corpus.', '105': 'Thefinal number of questions annotated for answersover the entire corpus was 341, and the distributionsof questions per cluster can be found in Table 1.', '106': '4.2 Evaluation metrics and methods.', '107': 'To evaluate our sentence retrieval mechanism, weproduced extract files, which contain a list of sentences deemed to be relevant to the question, for thesystem and from human judgment.', '108': 'To compare different configurations of our system to the baselinesystem, we produced extracts at a fixed length of 20sentences.', '109': 'While evaluations of question answeringsystems are often based on a shorter list of rankedsentences, we chose to generate longer lists for several reasons.', '110': 'One is that we are developing a PRsystem, of which the output can then be input to ananswer extraction system for further processing.', '111': 'Insuch a setting, we would most likely want to generate a relatively longer list of candidate sentences.', '112': 'Aspreviously mentioned, in our corpus the questionsoften have more than one relevant answer, so ideally,our PR system would find many of the relevant sentences, sending them on to the answer componentto decide which answer(s) should be returned to theuser.', '113': 'Each system\x92s extract file lists the document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP, UPI 2 12 train What is the condition under whichthreat GIA will take its action?Milan plane MSNBC, CNN, ABC, 9 15 train How many people were in thecrash Fox, USAToday building at the time of the crash?Turkish plane BBC, ABC, 10 12 train To where was the plane headed?crash FoxNews, YahooMoscow terror UPI, AFP, AP 7 7 train How many people were killed inattack the most recent explosion?Rhode Island MSNBC, CNN, ABC, Lycos, 10 8 train Who was to blame forclub fire Fox, BBC, Ananova the fire?FBI most AFP, UPI 3 14 train How much is the State Department offeringwanted for information leading to bin Laden\x92s arrest?Russia bombing AP, AFP 2 11 train What was the cause of the blast?Bali terror CNN, FoxNews, ABC, 10 30 train What were the motivationsattack BBC, Ananova of the attackers?Washington DC FoxNews, Ha\x92aretz, BBC, 8 28 train What kinds of equipment or weaponssniper BBC, Washington Times, CBS were used in the killings?GSPC terror Newstracker 8 29 train What are the charges againstgroup the GSPC suspects?China Novelty 43 25 18 train What was the magnitude of theearthquake earthquake in Zhangjiakou?Gulfair ABC, BBC, CNN, USAToday, 11 29 dev/test How many people FoxNews, Washington Post were on board?David Beckham AFP 20 28 dev/test How long had Beckham been playing fortrade MU before he moved to RM?Miami airport Newstracker 12 15 dev/test How many concourses doesevacuation the airport have?US hurricane DUC d04a 14 14 test In which places had the hurricane landed?EgyptAir crash Novelty 4 25 29 test How many people were killed?Kursk submarine Novelty 33 25 30 test When did the Kursk sink?Hebrew University bombing Newstracker 11 27 test How many people were injured?Finland mall bombing Newstracker 9 15 test How many people were in the mall at the time of the bombing?Putin visits Newstracker 12 20 test What issue concerned BritishEngland human rights groups?', '114': 'Table 1: Corpus of complex news stories.', '115': 'and sentence numbers of the top 20 sentences.', '116': 'The\x93gold standard\x94 extracts list the sentences judged ascontaining answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR)(Voorhees and Tice, 2000) and Total ReciprocalDocument Rank (TRDR) (Radev et al., 2005).MRR, used in the TREC Q&A evaluations, is thereciprocal rank of the first correct answer (or sentence, in our case) to a given question.', '117': 'This measuregives us an idea of how far down we must look in theranked list in order to find a correct answer.', '118': 'To contrast, TRDR is the total of the reciprocal ranks of allanswers found by the system.', '119': 'In the context of answering questions from complex stories, where thereis often more than one correct answer to a question,and where answers are typically time-dependent, weshould focus on maximizing TRDR, which gives us 2For clusters annotated by two judges, all sentences chosenby at least one judge were included.', '120': 'a measure of how many of the relevant sentenceswere identified by the system.', '121': 'However, we reportboth the average MRR and TRDR over all questionsin a given data set.', '122': 'In the training phase, we searched the parameterspace for the values of d (the question bias) and thesimilarity threshold in order to optimize the resultingTRDR scores.', '123': 'For our problem, we expected that arelatively low similarity threshold pair with a highquestion bias would achieve the best results.', '124': 'Table 2shows the effect of varying the similarity threshold.3 The notation LR[a, d] is used, where a is the similarity threshold and d is the question bias.', '125': 'The optimal range for the parameter a was between 0.14 and0.20.', '126': 'This is intuitive because if the threshold is toohigh, such that only the most lexically similar sentences are represented in the graph, the method doesnot find sentences that are related but are more lex3A threshold of -1 means that no threshold was used suchthat all sentences were included in the graph.', '127': '919 System Ave. MRR Ave. TRDR LR[-1.0,0.65] 0.5270 0.8117LR[0.02,0.65] 0.5261 0.7950LR[0.16,0.65] 0.5131 0.8134LR[0.18,0.65] 0.5062 0.8020LR[0.20,0.65] 0.5091 0.7944LR[-1.0,0.80] 0.5288 0.8152LR[0.02,0.80] 0.5324 0.8043LR[0.16,0.80] 0.5184 0.8160LR[0.18,0.80] 0.5199 0.8154LR[0.20,0.80] 0.5282 0.8152 Table 2: Training phase: effect of similarity threshold (a) on Ave. MRR and TRDR.', '128': 'System Ave. MRR Ave. TRDR LR[0.02,0.65] 0.5261 0.7950LR[0.02,0.70] 0.5290 0.7997LR[0.02,0.75] 0.5299 0.8013LR[0.02,0.80] 0.5324 0.8043LR[0.02,0.85] 0.5322 0.8038LR[0.02,0.90] 0.5323 0.8077LR[0.20,0.65] 0.5091 0.7944LR[0.20,0.70] 0.5244 0.8105LR[0.20,0.75] 0.5285 0.8137LR[0.20,0.80] 0.5282 0.8152LR[0.20,0.85] 0.5317 0.8203LR[0.20,0.90] 0.5368 0.8265 Table 3: Training phase: effect of question bias (d)on Ave. MRR and TRDR.', '129': 'ically diverse (e.g. paraphrases).', '130': 'Table 3 shows theeffect of varying the question bias at two differentsimilarity thresholds (0.02 and 0.20).', '131': 'It is clear that ahigh question bias is needed.', '132': 'However, a small probability for jumping to a node that is lexically similar to the given sentence (rather than the questionitself) is needed.', '133': 'Table 4 shows the configurationsof LexRank that performed better than the baselinesystem on the training data, based on mean TRDRscores over the 184 training questions.', '134': 'We appliedall four of these configurations to our unseen development/test data, in order to see if we could furtherdifferentiate their performances.', '135': '5.1 Development/testing phase.', '136': 'The scores for the four LexRank systems and thebaseline on the development/test data are shown in System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR[0.14,0.95] 0.5267 0.8305LR[0.18,0.90] 0.5376 0.8382LR[0.18,0.95] 0.5421 0.8382LR[0.20,0.95] 0.5404 0.8311 Table 4: Training phase: systems outperforming thebaseline in terms of TRDR score.', '137': 'System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR[0.14,0.95] 0.5882 1.0469LR[0.18,0.90] 0.5820 1.0288LR[0.18,0.95] 0.5956 1.0411LR[0.20,0.95] 0.6068 1.0601 Table 5: Development testing evaluation.', '138': 'Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157 1.7028evacuation Table 6: Average scores by cluster: baseline versusLR[0.20,0.95].', '139': 'Table 5.', '140': 'This time, all four LexRank systems outperformed the baseline, both in terms of average MRRand TRDR scores.', '141': 'An analysis of the average scoresover the 72 questions within each of the three clusters for the best system, LR[0.20,0.95], is shownin Table 6.', '142': 'While LexRank outperforms the baseline system on the first two clusters both in termsof MRR and TRDR, their performances are not substantially different on the third cluster.', '143': 'Therefore,we examined properties of the questions within eachcluster in order to see what effect they might have onsystem performance.We hypothesized that the baseline system, which compares the similarity of each sentence to the question using IDF-weighted word overlap, should perform well on questions that provide many contentwords.', '144': 'To contrast, LexRank might perform better when the question provides fewer content words,since it considers both similarity to the query andinter-sentence similarity.', '145': 'Out of the 72 questions inthe development/test set, the baseline system outperformed LexRank on 22 of the questions.', '146': 'In fact, theaverage number of content words among these 22questions was slightly, but not significantly, higherthan the average on the remaining questions (3.63words per question versus 3.46).', '147': 'Given this observation, we experimented with two mixed strategies,in which the number of content words in a questiondetermined whether LexRank or the baseline systemwas used for sentence retrieval.', '148': 'We tried thresholdvalues of 4 and 6 content words, however, this didnot improve the performance over the pure strategyof system LR[0.20,0.95].', '149': 'Therefore, we applied this 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR[0.20,0.95] 0.6189 0.9906p-value na 0.0619 Table 7: Testing phase: baseline vs. LR[0.20,0.95].', '150': 'system versus the baseline to our unseen test set of134 questions.', '151': '5.2 Testing phase.', '152': 'As shown in Table 7, LR[0.20,0.95] outperformedthe baseline system on the test data both in termsof average MRR and TRDR scores.', '153': 'The improvement in average TRDR score was statistically significant with a p-value of 0.0619.', '154': 'Since we are interested in a passage retrieval mechanism that findssentences relevant to a given question, providing input to the question answering component of our system, the improvement in average TRDR score isvery promising.', '155': 'While we saw in Section 5.1 thatLR[0.20,0.95] may perform better on some questionor cluster types than others, we conclude that it beatsthe competitive baseline when one is looking to optimize mean TRDR scores over a large set of questions.', '156': 'However, in future work, we will continueto improve the performance, perhaps by developing mixed strategies using different configurationsof LexRank.', '157': 'The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candidate sentences and the inputquery, and not the similarity between the candidatesentences themselves, is likely to miss some important sentences.', '158': 'When using any metric to comparesentences and a query, there is always likely to bea tie between multiple sentences (or, similarly, theremay be cases where fewer than the number of desired sentences have similarity scores above zero).LexRank effectively provides a means to break suchties.', '159': 'An example of such a scenario is illustrated inTables 8 and 9, which show the top ranked sentencesby the baseline and LexRank, respectively for thequestion \x93What caused the Kursk to sink?\x94 from theKursk submarine cluster.', '160': 'It can be seen that all topfive sentences chosen by the baseline system have Rank Sentence Score Relevant?', '161': '1 The Russian governmental commission on the 4.2282 Naccident of the submarine Kursk sinking inthe Barents Sea on August 12 has rejected11 original explanations for the disaster,but still cannot conclude what caused the.', '162': 'tragedy indeed, Russian Deputy Premier IlyaKlebanov said here Friday.', '163': '2 There has been no final word on what caused 4.2282 Nthe submarine to sink while participatingin a major naval exercise, but DefenseMinister Igor Sergeyev said the theory.', '164': 'that Kursk may have collided with anotherobject is receiving increasingly concrete confirmation.3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y said Thursday that collision with a bigobject caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea.', '165': '4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday that collision with a big.', '166': 'object caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea.', '167': '5 President Clinton\x92s national security adviser, 4.2282 NSamuel Berger, has provided his Russian.', '168': 'counterpart with a written summary of whatU.S. naval and intelligence officials believe caused the nuclear-powered submarine Kursk tosink last month in the Barents Sea, officials said Wednesday.', '169': 'Table 8: Top ranked sentences using baseline systemon the question \x93What caused the Kursk to sink?\x94.', '170': 'the same sentence score (similarity to the query), yetthe top ranking two sentences are not actually relevant according to the judges.', '171': 'To contrast, LexRankachieved a better ranking of the sentences since it isbetter able to differentiate between them.', '172': 'It shouldbe noted that both for the LexRank and baseline systems, chronological ordering of the documents andsentences is preserved, such that in cases where twosentences have the same score, the one publishedearlier is ranked higher.', '173': 'We presented topic-sensitive LexRank and appliedit to the problem of sentence retrieval.', '174': 'In a Web-based news summarization setting, users of our system could choose to see the retrieved sentences (asin Table 9) as a question-focused summary.', '175': 'As indicated in Table 9, each of the top three sentenceswere judged by our annotators as providing a complete answer to the respective question.', '176': 'While thefirst two sentences provide the same answer (a collision caused the Kursk to sink), the third sentenceprovides a different answer (an explosion caused thedisaster).', '177': 'While the last two sentences do not provide answers according to our judges, they do provide context information about the situation.', '178': 'Alternatively, the user might prefer to see the extracted 921 Rank Sentence Score Relevant?', '179': '1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday that collision with a big.', '180': 'object caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea.', '181': '2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday that collision with a big.', '182': 'object caused the Kursk nuclear submarineto sink to the bottom of the Barents Sea.', '183': '3 The Russian navy refused to confirm this, 0.0125 Ybut officers have said an explosion in thetorpedo compartment at the front of the.', '184': 'submarine apparently caused the Kursk to sink.4 President Clinton\x92s national security adviser, 0.0124 N Samuel Berger, has provided his Russiancounterpart with a written summary of whatU.S. naval and intelligence officials believe caused the nuclear-powered submarine Kursk tosink last month in the Barents Sea, officials said Wednesday.5 There has been no final word on what caused 0.0123 N the submarine to sink while participatingin a major naval exercise, but DefenseMinister Igor Sergeyev said the theory that Kursk may have collided with anotherobject is receiving increasingly concrete confirmation.', '185': 'Table 9: Top ranked sentences using theLR[0.20,0.95] system on the question \x93What causedthe Kursk to sink?\x94 answers from the retrieved sentences.', '186': 'In this case,the sentences selected by our system would be sentto an answer identification component for furtherprocessing.', '187': 'As discussed in Section 2, our goal wasto develop a topic-sensitive version of LexRank andto use it to improve a baseline system, which hadpreviously been used successfully for query-basedsentence retrieval (Allan et al., 2003).', '188': 'In terms ofthis task, we have shown that over a large set of unaltered questions written by our annotators, LexRankcan, on average, outperform the baseline system,particularly in terms of TRDR scores.', '189': 'We would like to thank the members of the CLAIRgroup at Michigan and in particular Siwei Shen andYang Ye for their assistance with this project.'}",['H05-1115'],['../data/summaries/H05-1115.txt'],['../data/tba/H05-1115.json']
H89-2014,Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging,../data/papers/H89-2014.xml,"{'0': 'Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging', '1': 'The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.', '2': 'The model has the advantage that a pre-tagged training corpus is not required.', '3': 'Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.', '4': 'State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.', '5': 'The structure of the state chains is based on both an analysis of errors and linguistic knowledge.', '6': 'Examples show how word dependency across phrases can be modeled.', '7': 'The determination of part-of-speech categories for words is an important problem in language modeling, because both the syntactic and semantic roles of words depend on their part-of-speech category (henceforth simply termed ""category"").', '8': 'Application areas include speech recognition/synthesis and information retrieval.', '9': 'Several workers have addressed the problem of tagging text.', '10': 'Methods have ranged from locally-operating rules (Greene and Rubin, 1971), to statistical methods (Church, 1989; DeRose, 1988; Garside, Leech and Sampson, 1987; Jelinek, 1985) and back-propagation (Benello, Mackie and Anderson, 1989; Nakamura and Shikano, 1989).', '11': 'The statistical methods can be described in terms of Markov models.', '12': 'States in a model represent categories {cl...c=} (n is the number of different categories used).', '13': 'In a first order model, Ci and Ci_l are random variables denoting the categories of the words at position i and (i - 1) in a text.', '14': 'The transition probability P(Ci = cz ] Ci_~ = %) linking two states cz and cy, represents the probability of category cx following category %.', '15': 'A word at position i is represented by the random variable Wi, which ranges over the vocabulary {w~ ...wv} (v is the number of words in the vocabulary).', '16': 'State-dependent probabilities of the form P(Wi = Wa ] Ci = cz) represent the probability that word Wa is seen, given category c~.', '17': 'For instance, the word ""dog"" can be seen in the states noun and verb, and only has a nonzero probability in those states.', '18': 'A word sequence is considered as being generated from an underlying sequence of categories.', '19': 'Of all the possible category sequences from which a given word sequence can be generated, the one which maximizes the probability of the words is used.', '20': 'The Viterbi algorithm (Viterbi, 1967) will find this category sequence.', '21': 'The systems previously mentioned require a pre-tagged training corpus in order to collect word counts or to perform back-propagation.', '22': 'The Brown Corpus (Francis and Kucera, 1982) is a notable example of such a corpus, and is used by many of the systems cited above.', '23': 'An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a ""hidden"" Markov model: that is, only the words of the training text are available, their corresponding categories are not known.', '24': 'In this situation, the Baum-Welch algorithm (Baum, 1972) can be used to estimate the model parameters.', '25': 'This has the great advantage of eliminating the pre-tagged corpus.', '26': 'It minimizes the resources required, facilitates experimentation with different word categories, and is easily adapted for use with other languages.', '27': 'The work described here also makes use of a hidden Markov model.', '28': 'One aim of the work is to investigate the quality and performance of models with minimal parameter descriptions.', '29': 'In this regard, word equivalence classes were used (Kupiec, 1989).', '30': 'There it is assumed that the distribution of the use of a word depends on the set of categories it can assume, and words are partitioned accordingly.', '31': 'Thus the words ""play"" and ""touch"" are considered to behave identically, as members of the class noun-or-verb, and ""clay"" and ""zinc""are members of the class noun.', '32': 'This partitioning drastically reduces the number of parameters required in the model, and aids reliable estimation using moderate amounts of training data.', '33': 'Equivalence classes {Eqvl ...Eqvm} replace the words {wl...Wv} (m << v) and P(Eqvi I Ci) replace the parameters P(Wi I Ci).', '34': 'In the 21 category model reported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.', '35': 'In fact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling new words to be added without any modification to the model.', '36': 'Obviously, a trade-off is involved.', '37': 'For example, ""dog"" is more likely to be a noun than a verb and ""see"" is more likely to be a verb than a noun.', '38': 'However they are both members of the equivalence class noun-or-verb, and so are considered to behave identically.', '39': 'It is then local word context (embodied in the transition probabilities) which must aid disambiguation of the word.', '40': 'In practice, word context provides significant constraint, so the trade-off appears to be a remarkably favorable one.', '41': 'The Basic Model The development of the model was guided by evaluation against a simple basic model (much of the development of the model was prompted by an analysis of the errors in its hehaviour).', '42': 'The basic model contained states representing the following categories: Determiner Noun Singular Including mass nouns Noun Plural Proper Noun Pronoun Adverb Conjunction Coordinating and subordinating Preposition Adjective Including comparative and superlative Verb Uninflected Verb 3rd Pers.', '43': 'Sing.', '44': 'Auxiliary Am, is, was, has, have, should, must, can, might, etc. Present Participle Including gerund Past Participle Including past tense Question Word When, what, why, etc. Unknown Words whose stems could not be found in dictionary.', '45': 'Lisp Used to tag common symbols in the the Lisp programming language (see below:) To-inf.', '46': '""To"" acting as an infinitive marker Sentence Boundary The above states were arranged in a first-order, fully connected network, each state having a transition to every other state, allowing all possible sequences of categories.', '47': 'The training corpus was a collection of electronic mail messages concerning the design of the Common-Lisp programming language -a somewhat less than ideal representation of English.', '48': 'Many Lisp-specific words were not in the vocabulary, and thus tagged as unknown, however the lisp category was nevertheless created for frequently occurring Lisp symbols in an attempt to reduce bias in the estimation.', '49': 'It is interesting to note that the model performs very well, despite such ""noisy"" training data.', '50': 'The training was sentence-based, and the model was trained using 6,000 sentences from the corpus.', '51': 'Eight iterations of the Baum-Welch algorithm were used.', '52': 'The implementation of the hidden Markov model is based on that of Rabiner, Levinson and Sondhi (1983).', '53': 'By exploiting the fact that the matrix of probabilities P(Eqvi I Ci) is sparse, a considerable improvement can be gained over the basic training algorithm in which iterations are made over all states.', '54': 'The initial values of the model parameters are calculated from word occurrence probabilities, such that words are initially assumed to function equally probably as any of their possible categories.', '55': 'Superlative and comparative adjectives were collapsed into a single adjective category, to economize on the overall number of categories.', '56': '(If desired, after tagging the finer category can be replaced).', '57': 'In the basic model all punctuation except sentence boundaries was ignored.', '58': 'An interesting observation is worth noting with regard to words that can act both as auxiliary and main verbs.', '59': 'Modal auxiliaries were consistently tagged as auxiliary whereas the tagging for other auxiliaries (e.g. ""is .... have"" etc.) was more variable.', '60': 'This indicates that modal auxiliaries can be recognized as a natural class via their pattern of usage.', '61': 'Extending the Basic Model The basic model was used as a benchmark for successive improvements.', '62': 'The first addition was the correct treatment of all non-words in a text.', '63': 'This includes hyphenation, punctuation, numbers and abbreviations.', '64': 'New categories were added for number, abbreviation, and comma.', '65': 'All other punctuation was collapsed into the single new punctuation category.', '66': 'Refinement of Basic Categories The verb states of the basic model were found to be too coarse.', '67': 'For example, many noun/verb ambiguities in front of past participles were incorrectly tagged as verbs.', '68': 'The replacement of the auxiliary category by the following categories greatly improved this: Category Name Words included in Category Be be Been been Being being Have have Have* has, have, had, having be* is, am, are, was, were do* do, does, did modal Modal auxiliaries Unique Equivalence Classes for Common Words Common words occur often enough to be estimated reliably.', '69': 'In a ranked list of words in the corpus the most frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data is available to estimate them reliably.', '70': 'The most frequent 100 words of the corpus were assigned individually in the model, thereby enabling them to have different distributions over their categories.', '71': 'This leaves 50% of the corpus for training all the other equivalence classes.', '72': 'Editing the Transition Structure A common error in the basic model was the assignment of the word ""to"" to the to-infcategory (""to"" acting as an infinitive marker) instead of preposition before noun phrases.', '73': 'This is not surprising, because ""to"" is the only member of the to-inf category, P(Wi = ""to"" [ Ci = to-in]) = 1.0.', '74': 'In contrast, P(Wi = ""to"" I Ci = preposition) = 0.086, because many other words share the preposition state.', '75': 'Unless transition probabilities are highly constraining, the higher probability paths will tend to go through the to-infstate.', '76': 'This situation may be addressed in several ways, the simplest being to initially assign zero transition probabilities from the to-infstate to states other than verbs and the adverb state.', '77': 'ADJECTIVE DETERMINER To all states NOUN in Basic Network ""Transitions to \x95 To all states all states in in Basic Network Basic Network except NOUN and ADJECTIVE AUGMENTED NETWORK BASIC NETWORK FULLY-CONNECTED NETWORK CONTAINING ALL STATES EXCEPT DETERMINER Figure 1: Extending the Basic Model Augmenting the Model by Use of Networks The basic model consists of a first-order fully connected network.', '78': ""The lexical context available for modeling a word's category is solely the category of the preceding word (expressed via the transition probabilities P(Ci [ Ci1)."", '79': 'Such limited context does not adequately model the constraint present in local word context.', '80': 'A straightforward method of extending the context is to use second-order conditioning which takes account of the previous two word categories.', '81': 'Transition probabilities are then of the form P(Ci [ Ci1, Ci2).', '82': 'For an n category model this requires n 3 transition probabilities.', '83': 'Increasing the order of the conditioning requires exponentially more parameters.', '84': 'In practice, models have been limited to second-order, and smoothing methods are normally required to deal with the problem of estimation with limited data.', '85': 'The conditioning just described is uniform- all possible two-category contexts are modeled.', '86': 'Many of these neither contribute to the performance of the model, nor occur frequently enough to be estimated properly: e.g. P(Ci = determiner [ el1 -~ determiner, Ci2 = determiner).', '87': 'An alternative to uniformly increasing the order of the conditioning is to extend it selectively.', '88': 'Mixed higher- order context can be modeled by introducing explicit state sequences.', '89': 'In the arrangement the basic first-order network remains, permitting all possible category sequences, and modeling first-order dependency.', '90': 'The basic network is then augmented with the extra state sequences which model certain category sequences in more detail.', '91': 'The design of the augmented network has been based on linguistic considerations and also upon an analysis of tagging errors made by the basic network.', '92': 'As an example, we may consider a systematic error made by the basic model.', '93': 'It concerns the disambiguation of the equivalence class adjective-or-noun following a determiner.', '94': 'The error is exemplified by the sentence fragment ""The period of..."", where ""period"" is tagged as an adjective.', '95': 'To model the context necessary to correct the error, two extra states are used, as shown in Figure 1.', '96': 'The ""augmented network"" uniquely models all second-order dependencies of the type determiner -noun - X, and determiner -adjective -X (X ranges over {cl...cn}).', '97': 'Training a hidden Markov model having this topology corrected all nine instances of the error in the test data.', '98': 'An important point to note is that improving the model detail in this manner does not forcibly correct the error.', '99': 'The actual patterns of category usage must be distinct in the language.', '100': '95 To complete the description of the augmented model it is necessary to mention tying of the model states (Jelinek and Mercer, 1980).', '101': 'Whenever a transition is made to a state, the state-dependent probability distribution P(Eqvi I Ci) is used to obtain the probability of the observed equivalence class.', '102': 'A state is generally used in several places (E.g. in Figure 1.', '103': 'there are two noun states, and two adjective states: one of each in the augmented network, and in the basic network).', '104': 'The distributions P(Eqvi I Ci) are considered to be the same for every instance of the same state.', '105': 'Their estimates are pooled and reassigned identically after each iteration of the Baum-Welch algorithm.', '106': 'Modeling Dependencies across Phrases Linguistic considerations can be used to correct errors made by the model.', '107': 'In this section two illustrations are given, concerning simple subject/verb agreement across an intermediate prepositional phrase.', '108': 'These are exemplified by the following sentence fragments: 1.', '109': '""Temperatures in the upper mantle range apparently from...."".', '110': '2.', '111': '""The velocity of the seismic waves rises to..."".', '112': 'The basic model tagged these sentences correctly, except for- ""range"" and ""rises"" which were tagged as noun and plural-noun respectively 1.', '113': 'The basic network cannot model the dependency of the number of the verb on its subject, which precedes it by a prepositional phrase.', '114': 'To model such dependency across the phrase, the networks shown in Figure 2 can be used.', '115': 'It can be seen that only simple forms of prepositional phrase are modeled in the networks; a single noun may be optionally preceded by a single adjective and/or determiner.', '116': 'The final transitions in the networks serve to discriminate between the correct and incorrect category assignment given the selected preceding context.', '117': 'As in the previous section, the corrections are not programmed into the model.', '118': 'Only context has been supplied to aid the training procedure, and the latter is responsible for deciding which alternative is more likely, based on the training data.', '119': '(Approximately 19,000 sentences were used to train the networks used in this example).', '120': 'Discussion and Results In Figure 2, the two copies of the prepositional phrase are trained in separate contexts (preceding singu- lax/plural nouns).', '121': 'This has the disadvantage that they cannot share training data.', '122': 'This problem could be resolved by tying corresponding transitions together.', '123': 'Alternatively, investigation of a trainable grammar (Baker, 1979; Fujisaki et al., 1989) may be a fruitful way to further develop the model in terms of grammatical components.', '124': 'A model containing all of the refinements described, was tested using a magazine article containing 146 sentences (3,822 words).', '125': 'A 30,000 word dictionary was used, supplemented by inflectional analysis for words not found directly in the dictionary.', '126': 'In the document, 142 words were tagged as unknown (their possible categories were not known).', '127': 'A total of 1,526 words had ambiguous categories (i.e. 40% of the document).', '128': 'Critical examination of the tagging provided by the augmented model showed 168 word tagging errors, whereas the basic model gave 215 erroneous word tags.', '129': 'The former represents 95.6% correct word tagging on the text as a whole (ignoring unknown words), and 89% on the ambiguous words.', '130': 'The performance of a tagging program depends on the choice and number of categories used, and the correct tag assignment for words is not always obvious.', '131': 'In cases where the choice of tag was unclear (as often occurs in idioms), the tag was ruled as incorrect.', '132': 'For example, 9 errors are from 3 instances of ""... as well as ..."" that arise in the text.', '133': 'It would be appropriate to deal with idioms separately, as done by Gaxside, Leech and Sampson (1987).', '134': 'Typical errors beyond the scope of the model described here are exemplified by incorrect adverbial and prepositional assignment.', '135': '1 It is easy to construct counterexamples to the sentences presented here, where the tagging would be correct.', '136': 'However, the training procedure affirms that counterexamples occur less frequently in the corpus than the cases shown here..', '137': '96 NOUN PREPOSITION ADJECTIVE NO UN~ PLURAL NOUN PLURAL NOUN PREPOSITION A E?TIVE NO2NJC) NOUN ~ j VERB TRANSITIONS TO/FROM ~ 3RD.', '138': 'SINGULAR ALL STATES IN BASIC NETWORK NOT SHOWN Figure 2: Augmented Networks for Example of Subject/Verb Agreement For example, consider the word ""up"" in the following sentences: ""He ran up a big bill"".', '139': '""He ran up a big hill"".', '140': 'Extra information is required to assign the correct tagging.', '141': 'In these examples it is worth noting that even if a model was based on individual words, and trained on a pre-tagged corpus, the association of ""up"" (as adverb) with ""bill"" would not be captured by trigrams.', '142': '(Work on phrasal verbs, using mutual information estimates (Church et ai., 1989b) is directly relevant to this problem).', '143': 'The tagger could be extended by further category refinements (e.g. inclusion of a gerund category), and the single pronoun category currently causes erroneous tags for adjacent words.', '144': 'With respect to the problem of unknown words, alternative category assignments for them could be made by using the context embodied in transition probabilities.', '145': 'A stochastic method for assigning part-of-speech categories to unrestricted English text has been described.', '146': 'It minimizes the resources required for high performance automatic tagging.', '147': 'A pre-tagged training corpus is not required, and the tagger can cope with words not found in the training text.', '148': 'It can be trained reliably on moderate amounts of training text, and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters.', '149': 'I would like to thank Meg Withgott and Lanri Karttunen of Xerox PARC, for their helpful contributions to this work.', '150': 'I am also indebted to Sheldon Nicholl of the Univ. of Illinois, for his comments and valuable insight.', '151': 'This work was sponsored in part by the Defense Advanced Research Projects Agency (DOD), under the Information Science and Technology Office, contract #N0014086-C-8996.'}",['H89-2014'],['../data/summaries/H89-2014.txt'],['../data/tba/H89-2014.json']
I05-5011,Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs,../data/papers/I05-5011.xml,"{'0': 'Automatic Paraphrase Discovery based on Context and Keywords between NE Pairs', '1': 'Automatic paraphrase discovery is an important but challenging task.', '2': 'We propose an unsupervised method to discover paraphrases from a large untagged corpus, without requiring any seed phrase or other cue.', '3': 'We focus on phrases which connect two Named Entities (NEs), and proceed in two stages.', '4': 'The first stage identifies a keyword in each phrase and joins phrases with the same keyword into sets.', '5': 'The second stage links sets which involve the same pairs of individual NEs.', '6': 'A total of 13,976 phrases were grouped.', '7': 'The accuracy of the sets in representing paraphrase ranged from 73% to 99%, depending on the NE categories and set sizes; the accuracy of the links for two evaluated domains was 73% and 86%.', '8': 'One of the difficulties in Natural Language Processing is the fact that there are many ways to express the same thing or event.', '9': 'If the expression is a word or a short phrase (like â\x80\x9ccorporationâ\x80\x9d and â\x80\x9ccompanyâ\x80\x9d), it is called a â\x80\x9csynonymâ\x80\x9d.', '10': 'There has been a lot of research on such lexical relations, along with the creation of resources such as WordNet.', '11': 'If the expression is longer or complicated (like â\x80\x9cA buys Bâ\x80\x9d and â\x80\x9cAâ\x80\x99s purchase of Bâ\x80\x9d), it is called â\x80\x9cparaphraseâ\x80\x9d, i.e. a set of phrases which express the same thing or event.', '12': 'Recently, this topic has been getting more attention, as is evident from the Paraphrase Workshops in 2003 and 2004, driven by the needs of various NLP applications.', '13': 'For example, in Information Retrieval (IR), we have to match a userâ\x80\x99s query to the expressions in the desired documents, while in Question Answering (QA), we have to find the answer to the userâ\x80\x99s question even if the formulation of the answer in the document is different from the question.', '14': 'Also, in Information Extraction (IE), in which the system tries to extract elements of some events (e.g. date and company names of a corporate merger event), several event instances from different news articles have to be aligned even if these are expressed differently.', '15': 'We realize the importance of paraphrase; however, the major obstacle is the construction of paraphrase knowledge.', '16': 'For example, we can easily imagine that the number of paraphrases for â\x80\x9cA buys Bâ\x80\x9d is enormous and it is not possible to create a comprehensive inventory by hand.', '17': 'Also, we donâ\x80\x99t know how many such paraphrase sets are necessary to cover even some everyday things or events.', '18': 'Up to now, most IE researchers have been creating paraphrase knowledge (or IE patterns) by hand and for specific tasks.', '19': 'So, there is a limitation that IE can only be performed for a predefined task, like â\x80\x9ccorporate mergersâ\x80\x9d or â\x80\x9cmanagement successionâ\x80\x9d.', '20': 'In order to create an IE system for a new domain, one has to spend a long time to create the knowledge.', '21': 'So, it is too costly to make IE technology â\x80\x9copen- domainâ\x80\x9d or â\x80\x9con-demandâ\x80\x9d like IR or QA.', '22': 'In this paper, we will propose an unsupervised method to discover paraphrases from a large untagged corpus.', '23': 'We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.', '24': 'After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kinds of cue.', '25': '2.1 Overview.', '26': 'Before explaining our method in detail, we present a brief overview in this subsection.', '27': 'First, from a large corpus, we extract all the NE instance pairs.', '28': 'Here, an NE instance pair is any pair of NEs separated by at most 4 syntactic chunks; for example, â\x80\x9cIBM plans to acquire Lotusâ\x80\x9d.', '29': 'For each pair we also record the context, i.e. the phrase between the two NEs (Step1).', '30': 'Next, for each pair of NE categories, we collect all the contexts and find the keywords which are topical for that NE category pair.', '31': 'We use a simple TF/IDF method to measure the topicality of words.', '32': 'Hereafter, each pair of NE categories will be called a domain; e.g. the â\x80\x9cCompany â\x80\x93 Companyâ\x80\x9d domain, which we will call CC- domain (Step 2).', '33': 'For each domain, phrases which contain the same keyword are gathered to build a set of phrases (Step 3).', '34': 'Finally, we find links between sets of phrases, based on the NE instance pair data (for example, different phrases which link â\x80\x9cIBMâ\x80\x9d and â\x80\x9cLotusâ\x80\x9d) (Step 4).', '35': 'As we shall see, most of the linked sets are paraphrases.', '36': 'This overview is illustrated in Figure 1.', '37': 'Corpus Step 1 NE pair instances Step 2 Step 1.', '38': 'Extract NE instance pairs with contexts First, we extract NE pair instances with their context from the corpus.', '39': 'The sentences in the corpus were tagged by a transformation-based chunker and an NE tagger.', '40': 'The NE tagger is a rule-based system with 140 NE categories [Sekine et al. 2004].', '41': 'These 140 NE categories are designed by extending MUCâ\x80\x99s 7 NE categories with finer sub-categories (such as Company, Institute, and Political Party for Organization; and Country, Province, and City for Location) and adding some new types of NE categories (Position Title, Product, Event, and Natural Object).', '42': 'All the NE pair instances which co-occur separated by at most 4 chunks are collected along with information about their NE types and the phrase between the NEs (the â\x80\x98contextâ\x80\x99).', '43': 'Figure 2 shows examples of extracted NE pair instances and their contexts.', '44': 'The data is sorted based on the frequency of the context (â\x80\x9ca unit ofâ\x80\x9d appeared 314 times in the corpus) and the NE pair instances appearing with that context are shown with their frequency (e.g. â\x80\x9cNBCâ\x80\x9d and â\x80\x9cGeneral Electric Co.â\x80\x9d appeared 10 times with the context â\x80\x9ca unit ofâ\x80\x9d).', '45': 'Step 2.', '46': 'Find keywords for each NE pair When we look at the contexts for each domain, we noticed that there is one or a few important words which indicate the relation between the NEs (for example, the word â\x80\x9cunitâ\x80\x9d for the phrase â\x80\x9ca unit ofâ\x80\x9d).', '47': 'Once we figure out the important word (e.g. keyword), we believe we can capture the meaning of the phrase by the keyword.', '48': 'We used the TF/ITF metric to identify keywords.', '49': 'keywords Step 3 Sets of phrases based on keywords Step 4 Links between sets of phrases All the contexts collected for a given domain are gathered in a bag and the TF/ITF scores are calculated for all the words except stopwords in the bag.', '50': 'Here, the term frequency (TF) is the frequency of a word in the bag and the inverse term frequency (ITF) is the inverse of the log of the frequency in the entire corpus.', '51': 'Figure 3 Figure 1.', '52': 'Overview of the method 2.2 Step by Step Algorithm.', '53': 'In this section, we will explain the algorithm step by step with examples.', '54': 'Because of their size, the examples (Figures 2 to 4) appear at the end of the paper.', '55': 'shows some keywords with their scores.', '56': 'Step 3.', '57': 'Gather phrases using keywords Next, we select a keyword for each phrase â\x80\x93 the top-ranked word based on the TF/IDF metric.', '58': '(If the TF/IDF score of that word is below a threshold, the phrase is discarded.)', '59': 'We then gather all phrases with the same keyword.', '60': 'Figure 4 shows some such phrase sets based on keywords in the CC-domain.', '61': 'Step 4.', '62': 'Cluster phrases based on Links We now have a set of phrases which share a keyword.', '63': 'However, there are phrases which express the same meanings even though they do not share the same keyword.', '64': 'For example, in Figure 3, we can see that the phrases in the â\x80\x9cbuyâ\x80\x9d, â\x80\x9cacquireâ\x80\x9d and â\x80\x9cpurchaseâ\x80\x9d sets are mostly paraphrases.', '65': 'At this step, we will try to link those sets, and put them into a single cluster.', '66': 'Our clue is the NE instance pairs.', '67': 'If the same pair of NE instances is used with different phrases, these phrases are likely to be paraphrases.', '68': 'For example, the two NEs â\x80\x9cEastern Group Plcâ\x80\x9d and â\x80\x9cHanson Plcâ\x80\x9d have the following contexts.', '69': 'Here, â\x80\x9cEGâ\x80\x9d represents â\x80\x9cEastern Group Plcâ\x80\x9d.', '70': 'and â\x80\x9cHâ\x80\x9d represents â\x80\x9cHanson Plcâ\x80\x9d.', '71': 'x EG, has agreed to be bought by H x EG, now owned by H x H to acquire EG x Hâ\x80\x99s agreement to buy EG Three of those phrases are actually paraphrases, but sometime there could be some noise; such as the second phrase above.', '72': 'So, we set a threshold that at least two examples are required to build a link.', '73': 'More examples are shown in Figure 5.', '74': 'Notice that the CC-domain is a special case.', '75': 'As the two NE categories are the same, we canâ\x80\x99t differentiate phrases with different orders of par ticipants â\x80\x93 whether the buying company or the to-be-bought company comes first.', '76': 'The links can solve the problem.', '77': 'As can be seen in the example, the first two phrases have a different order of NE names from the last two, so we can determine that the last two phrases represent a reversed relation.', '78': 'In figure 4, reverse relations are indicated by `*â\x80\x99 next to the frequency.', '79': 'Now we have sets of phrases which share a keyword and we have links between those sets.', '80': '3.1 Corpora.', '81': 'For the experiments, we used four newswire corpora, the Los Angeles Times/Washington Post, The New York Times, Reuters and the Wall Street Journal, all published in 1995.', '82': 'They contain about 200M words (25M, 110M, 40M and 19M words, respectively).', '83': 'All the sentences have been analyzed by our chunker and NE tag- ger.', '84': 'The procedure using the tagged sentences to discover paraphrases takes about one hour on a 2GHz Pentium 4 PC with 1GB of memory.', '85': '3.2 Results.', '86': 'In this subsection, we will report the results of the experiment, in terms of the number of words, phrases or clusters.', '87': 'We will report the evaluation results in the next subsection.', '88': 'Step 1.', '89': 'Extract NE pair instances with contexts From the four years of newspaper corpus, we extracted 1.9 million pairs of NE instances.', '90': 'The most frequent NE category pairs are â\x80\x9cPerson - Person (209,236), followed by â\x80\x9cCountry - Coun- tryâ\x80\x9d (95,123) and â\x80\x9cPerson - Countryâ\x80\x9d (75,509).', '91': 'The frequency of the Company â\x80\x93 Company domain ranks 11th with 35,567 examples.', '92': 'As lower frequency examples include noise, we set a threshold that an NE category pair should appear at least 5 times to be considered and an NE instance pair should appear at least twice to be considered.', '93': 'This limits the number of NE category pairs to 2,000 and the number of NE pair instances to 0.63 million.', '94': 'Step 2.', '95': 'Find keywords for each NE pair The keywords are found for each NE category pair.', '96': 'For example, in the CC-domain, 96 keywords are found which have TF/ITF scores above a threshold; some of them are shown in Figure 3.', '97': 'It is natural that the larger the data in the domain, the more keywords are found.', '98': 'In the â\x80\x9cPerson â\x80\x93 Personâ\x80\x9d domain, 618 keywords are found, and in the â\x80\x9cCountry â\x80\x93 Countryâ\x80\x9d domain, 303 keywords are found.', '99': 'In total, for the 2,000 NE category pairs, 5,184 keywords are found.', '100': 'Step 3.', '101': 'Gather phrases using keywords Now, the keyword with the top TF/ITF score is selected for each phrase.', '102': 'If a phrase does not contain any keywords, the phrase is discarded.', '103': 'For example, out of 905 phrases in the CC- domain, 211 phrases contain keywords found in step 2.', '104': 'In total, across all domains, we kept 13,976 phrases with keywords.', '105': 'Step 4.', '106': 'Link phrases based on instance pairs Using NE instance pairs as a clue, we find links between sets of phrases.', '107': 'In the CC-domain, there are 32 sets of phrases which contain more than 2 phrases.', '108': 'We concentrate on those sets.', '109': 'Among these 32 sets, we found the following pairs of sets which have two or more links.', '110': 'Here a set is represented by the keyword and the number in parentheses indicates the number of shared NE pair instances.', '111': 'buy - acquire (5) buy - agree (2) buy - purchase (5) buy - acquisition (7) buy - pay (2)* buy - buyout (3) buy - bid (2) acquire - purchase (2) acquire - acquisition (2) acquire - pay (2)* purchase - acquisition (4) purchase - stake (2)* acquisition - stake (2)* unit - subsidiary (2) unit - parent (5) It is clear that these links form two clusters which are mostly correct.', '112': 'We will describe the evaluation of such clusters in the next subsection.', '113': '3.3 Evaluation Results.', '114': 'We evaluated the results based on two metrics.', '115': 'One is the accuracy within a set of phrases which share the same keyword; the other is the accuracy of links.', '116': 'We picked two domains, the CC-domain and the â\x80\x9cPerson â\x80\x93 Companyâ\x80\x9d domain (PC-domain), for the evaluation, as the entire system output was too large to evaluate.', '117': 'It is not easy to make a clear definition of â\x80\x9cparaphraseâ\x80\x9d.', '118': 'Sometimes extracted phrases by themselves are not meaningful to consider without context, but we set the following criteria.', '119': 'If two phrases can be used to express the same relationship within an information extraction application (â\x80\x9cscenarioâ\x80\x9d), these two phrases are paraphrases.', '120': 'Although this is not a precise criterion, most cases we evaluated were relatively clear-cut.', '121': 'In general, different modalities (â\x80\x9cplanned to buyâ\x80\x9d, â\x80\x9cagreed to buyâ\x80\x9d, â\x80\x9cboughtâ\x80\x9d) were considered to express the same relationship within an extraction setting.', '122': 'We did have a problem classifying some modified noun phrases where the modified phrase does not represent a qualified or restricted form of the head, like â\x80\x9cchairmanâ\x80\x9d and â\x80\x9cvice chairmanâ\x80\x9d, as these are both represented by the keyword â\x80\x9cchairmanâ\x80\x9d.', '123': 'In this specific case, as these two titles could fill the same column of an IE table, we regarded them as paraphrases for the evaluation.', '124': 'Evaluation within a set The evaluation of paraphrases within a set of phrases which share a keyword is illustrated in Figure 4.', '125': 'For each set, the phrases with bracketed frequencies are considered not paraphrases in the set.', '126': ""For example, the phrase â\x80\x9c's New York-based trust unit,â\x80\x9d is not a paraphrase of the other phrases in the â\x80\x9cunitâ\x80\x9d set."", '127': 'As you can see in the figure, the accuracy for the domain is quite high except for the â\x80\x9cagreeâ\x80\x9d set, which contains various expressions representing different relationships for an IE application.', '128': 'The accuracy is calculated as the ratio of the number of paraphrases to the total number of phrases in the set.', '129': 'The results, along with the total number of phrases, are shown in Table 1.', '130': 'D o m ai n # of ph ras es t o t a l p h r a s e s ac cu ra cy C C 7 o r m o r e 1 0 5 8 7 . 6 % 6 o r l e s s 1 0 6 6 7 . 0 % P C 7 o r m o r e 3 5 9 9 9 . 2 % 6 o r l e s s 2 5 5 6 5 . 1 % Table 1.', '131': 'Evaluation results within sets Table 1 shows the evaluation result based on the number of phrases in a set.', '132': 'The larger sets are more accurate than the small sets.', '133': 'We can make several observations on the cause of errors.', '134': 'One is that smaller sets sometime have meaningless keywords, like â\x80\x9cstrengthâ\x80\x9d or â\x80\x9caddâ\x80\x9d in the CC-domain, or â\x80\x9ccompareâ\x80\x9d in the PC-domain.', '135': 'Eight out of the thirteen errors in the high frequency phrases in the CC-domain are the phrases in â\x80\x9cagreeâ\x80\x9d.', '136': 'As can be seen in Figure 3, the phrases in the â\x80\x9cagreeâ\x80\x9d set include completely different relationships, which are not paraphrases.', '137': 'Other errors include NE tagging errors and errors due to a phrase which includes other NEs.', '138': 'For example, in the phrase â\x80\x9cCompany-A last week purchased rival Marshalls from Company-Bâ\x80\x9d, the purchased company is Marshalls, not Company-B.', '139': 'Also there are cases where one of the two NEs belong to a phrase outside of the relation.', '140': 'For example, from the sentence â\x80\x9cMr.', '141': 'Smith estimates Lotus will make a profit this quarterâ\x80¦â\x80\x9d, our system extracts â\x80\x9cSmith esti mates Lotusâ\x80\x9d as an instance.', '142': 'Obviously â\x80\x9cLotusâ\x80\x9d is part of the following clause rather than being the object of â\x80\x9cestimatesâ\x80\x9d and the extracted instance makes no sense.', '143': 'We will return to these issues in the discussion section.', '144': 'Evaluation of links A link between two sets is considered correct if the majority of phrases in both sets have the same meaning, i.e. if the link indicates paraphrase.', '145': 'All the links in the â\x80\x9cCC-domain are shown in Step 4 in subsection 3.2.', '146': 'Out of those 15 links, 4 are errors, namely â\x80\x9cbuy - payâ\x80\x9d, â\x80\x9cacquire - payâ\x80\x9d, â\x80\x9cpurchase - stakeâ\x80\x9d â\x80\x9cacquisition - stakeâ\x80\x9d.', '147': 'When a company buys another company, a paying event can occur, but these two phrases do not indicate the same event.', '148': 'The similar explanation applies to the link to the â\x80\x9cstakeâ\x80\x9d set.', '149': 'We checked whether the discovered links are listed in WordNet.', '150': 'Only 2 link in the CC- domain (buy-purchase, acquire-acquisition) and 2 links (trader-dealer and head-chief) in the PC- domain are found in the same synset of Word- Net 2.1 (http://wordnet.princeton.edu/).', '151': 'This result suggests the benefit of using the automatic discovery method.', '152': 'D o m ai n Li n k ac cu ra cy W N c o v e r a g e C C 7 3 . 3 % 2 / 1 1 P C 8 8 . 9 % 2 / 8 Table 2.', '153': 'Evaluation results for links', '154': 'The work reported here is closely related to [Ha- segawa et al. 04].', '155': 'First, we will describe their method and compare it with our method.', '156': 'They first collect the NE instance pairs and contexts, just like our method.', '157': 'However, the next step is clearly different.', '158': 'They cluster NE instance pairs based on the words in the contexts using a bag- of-words method.', '159': 'In order to create good-sized vectors for similarity calculation, they had to set a high frequency threshold, 30.', '160': 'Because of this threshold, very few NE instance pairs could be used and hence the variety of phrases was also limited.', '161': 'Instead, we focused on phrases and set the frequency threshold to 2, and so were able to utilize a lot of phrases while minimizing noise.', '162': '[Hasegawa et al. 04] reported only on relation discovery, but one could easily acquire para phrases from the results.', '163': 'The number of NE instance pairs used in their experiment is less than half of our method.', '164': 'There have been other kinds of efforts to discover paraphrase automatically from corpora.', '165': 'One of such approaches uses comparable documents, which are sets of documents whose content are found/known to be almost the same, such as different newspaper stories about the same event [Shinyama and Sekine 03] or different translations of the same story [Barzilay 01].', '166': 'The availability of comparable corpora is limited, which is a significant limitation on the approach.', '167': 'Another approach to finding paraphrases is to find phrases which take similar subjects and objects in large corpora by using mutual information of word distribution [Lin and Pantel 01].', '168': 'This approach needs a phrase as an initial seed and thus the possible relationships to be extracted are naturally limited.', '169': 'There has also been work using a bootstrap- ping approach [Brin 98; Agichtein and Gravano 00; Ravichandran and Hovy 02].', '170': 'The basic strategy is, for a given pair of entity types, to start with some examples, like several famous book title and author pairs; and find expressions which contains those names; then using the found expressions, find more author and book title pairs.', '171': 'This can be repeated several times to collect a list of author / book title pairs and expressions.', '172': 'However, those methods need initial seeds, so the relation between entities has to be known in advance.', '173': 'This limitation is the obstacle to making the technology â\x80\x9copen domainâ\x80\x9d.', '174': 'Keywords with more than one word In the evaluation, we explained that â\x80\x9cchairmanâ\x80\x9d and â\x80\x9cvice chairmanâ\x80\x9d are considered paraphrases.', '175': 'However, it is desirable if we can separate them.', '176': 'This problem arises because our keywords consist of only one word.', '177': 'Sometime, multiple words are needed, like â\x80\x9cvice chairmanâ\x80\x9d, â\x80\x9cprime ministerâ\x80\x9d or â\x80\x9cpay forâ\x80\x9d (â\x80\x9cpayâ\x80\x9d and â\x80\x9cpay forâ\x80\x9d are different senses in the CC-domain).', '178': 'One possibility is to use n-grams based on mutual information.', '179': 'If there is a frequent multi-word sequence in a domain, we could use it as a keyword candidate.', '180': 'Keyword detection error Even if a keyword consists of a single word, there are words which are not desirable as keywords for a domain.', '181': 'As was explained in the results section, â\x80\x9cstrengthâ\x80\x9d or â\x80\x9caddâ\x80\x9d are not desirable keywords in the CC-domain.', '182': 'In our experiment, we set the threshold of the TF/ITF score empirically using a small development corpus; a finer adjustment of the threshold could reduce the number of such keywords.', '183': 'Also, â\x80\x9cagreeâ\x80\x9d in the CC-domain is not a desirable keyword.', '184': 'It is a relatively frequent word in the domain, but it can be used in different extraction scenarios.', '185': 'In this domain the major scenarios involve the things they agreed on, rather than the mere fact that they agreed.', '186': 'â\x80\x9cAgreeâ\x80\x9d is a subject control verb, which dominates another verb whose subject is the same as that of â\x80\x9cagreeâ\x80\x9d; the latter verb is generally the one of interest for extraction.', '187': 'We have checked if there are similar verbs in other major domains, but this was the only one.', '188': 'Using structural information As was explained in the results section, we extracted examples like â\x80\x9cSmith estimates Lotusâ\x80\x9d, from a sentence like â\x80\x9cMr.', '189': 'Smith estimates Lotus will make profit this quarterâ\x80¦â\x80\x9d.', '190': 'In order to solve this problem, a parse tree is needed to understand that â\x80\x9cLotusâ\x80\x9d is not the object of â\x80\x9cestimatesâ\x80\x9d.', '191': 'Chunking is not enough to find such relationships.', '192': 'This remains as future work.', '193': 'Limitations There are several limitations in the methods.', '194': 'The phrases have to be the expressions of length less than 5 chunks, appear between two NEs.', '195': 'Also, the method of using keywords rules out phrases which donâ\x80\x99t contain popular words in the domain.', '196': 'We are not claiming that this method is almighty.', '197': 'Rather we believe several methods have to be developed using different heuristics to discover wider variety of paraphrases.', '198': 'Applications The discovered paraphrases have multiple applications.', '199': 'One obvious application is information extraction.', '200': 'In IE, creating the patterns which express the requested scenario, e.g. â\x80\x9cmanagement successionâ\x80\x9d or â\x80\x9ccorporate merger and acquisitionâ\x80\x9d is regarded as the hardest task.', '201': 'The discovered paraphrases can be a big help to reduce human labor and create a more comprehensive pattern set.', '202': 'Also, expanding on the techniques for the automatic generation of extraction patterns (Riloff 96; Sudo 03) using our method, the extraction patterns which have the same meaning can be automatically linked, enabling us to produce the final table fully automatically.', '203': 'While there are other obstacles to completing this idea, we believe automatic paraphrase discovery is an important component for building a fully automatic information extraction system.', '204': 'We proposed an unsupervised method to discover paraphrases from a large untagged corpus.', '205': 'We are focusing on phrases which have two Named Entities (NEs), as those types of phrases are very important for IE applications.', '206': 'After tagging a large corpus with an automatic NE tagger, the method tries to find sets of paraphrases automatically without being given a seed phrase or any kind of cue.', '207': 'In total 13,976 phrases are assigned to sets of phrases, and the accuracy on our evaluation data ranges from 65 to 99%, depending on the domain and the size of the sets.', '208': 'The accuracies for link were 73% and 86% on two evaluated domains.', '209': 'These results are promising and there are several avenues for improving on these results.', '210': 'This research was supported in part by the Defense Advanced Research Projects Agency as part of the Translingual Information Detection, Extraction and Summarization (TIDES) program, under Grant N66001001-18917 from the Space and Naval Warfare Systems Center, San Diego, and by the National Science Foundation under Grant IIS00325657.', '211': 'This paper does not necessarily reflect the position of the U.S. Government.', '212': 'We would like to thank Prof. Ralph Grish- man, Mr. Takaaki Hasegawa and Mr. Yusuke Shinyama for useful comments, discussion and evaluation.'}",['I05-5011'],['../data/summaries/I05-5011.txt'],['../data/tba/I05-5011.json']
J01-2004,Probabilistic Top-Down Parsing and Language Modeling,../data/papers/J01-2004.xml,"{'0': 'Probabilistic Top-Down Parsing and Language Modeling', '1': 'This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition.', '2': 'The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.', '3': 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', '4': 'A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.', '5': 'Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.', '6': 'A small recognition experiment also demonstrates the utility of the model.', '7': 'This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition.', '8': 'The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling.', '9': 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', '10': 'A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.', '11': 'Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.', '12': 'A small recognition experiment also demonstrates the utility of the model.', '13': 'With certain exceptions, computational linguists have in the past generally formed a separate research community from speech recognition researchers, despite some obvious overlap of interest.', '14': 'Perhaps one reason for this is that, until relatively recently, few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.', '15': 'In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.', '16': 'While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems, there is reason to hope that better language models can and will be developed by computational linguists for this task.', '17': 'This paper will examine language modeling for speech recognition from a natural language processing point of view.', '18': 'Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.', '19': 'A new language model, based on probabilistic top-down parsing, will be outlined and compared with the previous literature, and extensive empirical results will be presented which demonstrate its utility.', '20': 'Two features of our top-down parsing approach will emerge as key to its success.', '21': 'First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.', '22': 'A left-toright parser whose derivations are not rooted, i.e., with derivations that can consist of disconnected tree fragments, such as an LR or shift-reduce parser, cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar, because their derivations include probability mass from unrooted structures.', '23': 'Only at the point when their derivations become rooted (at the end of the string) can generative string probabilities be calculated from the grammar.', '24': 'These parsers can calculate word probabilities based upon the parser state—as in Chelba and Jelinek (1998a)—but such a distribution is not generative from the probabilistic grammar.', '25': 'A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.', '26': 'For example, suppose that there are two possible verbs that could be the head of a sentence.', '27': ""For a head-first parser, some derivations will have the first verb as the head of the sentence, and the second verb will be generated after the first; hence the second verb's probability will be conditioned on the first verb."", '28': ""Other derivations will have the second verb as the head of the sentence, and the first verb's probability will be conditioned on the second verb."", '29': 'In such a scenario, there is no way to decompose the joint probability calculated from the set of derivations into the product of conditional probabilities using the chain rule.', '30': 'Of course, the joint probability can be used as a language model, but it cannot be interpolated on a word-by-word basis with, say, a trigram model, which we will demonstrate is a useful thing to do.', '31': 'Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).', '32': 'A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.', '33': 'Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.', '34': 'In contrast, an Earley or left-corner parser will underspecify certain connections between constituents in the left context, and if some of the underspecified information is used in the conditional probability model, it will have to become specified.', '35': 'Of course, this can be done, but at the expense of search efficiency; the more that this is done, the less benefit there is from the underspecification.', '36': 'A top-down parser will, in contrast, derive an efficiency benefit from precisely the information that is underspecified in these other approaches.', '37': 'Thus, our top-down parser makes it very easy to condition the probabilistic grammar on an arbitrary number of values extracted from the rooted, fully specified derivation.', '38': 'This has lead us to a formulation of the conditional probability model in terms of values returned from tree-walking functions that themselves are contextually sensitive.', '39': 'The top-down guidance that is provided makes this approach quite efficient in practice.', '40': 'The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.', '41': 'There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.', '42': 'Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.', '43': ""This section will introduce probabilistic (or stochastic) context-free grammars (PCFGs), as well as such notions as complete and partial parse trees, which will be important in defining our language model later in the paper.'"", '44': 'In addition, we will explain some simple grammar transformations that will be used.', '45': 'Finally, we will explain the notion of c-command, which will be used extensively later as well.', '46': 'PCFGs model the syntactic combinatorics of a language by extending conventional context-free grammars (CFGs).', '47': 'A CFG G = (V,T,P, St), consists of a set of nonterminal symbols V, a set of terminal symbols T, a start symbol St E V, and a set of rule productions P of the form: A a, where a E (VU T)*.', '48': 'These context-free rules can be interpreted as saying that a nonterminal symbol A expands into one or more either nonterminal or terminal symbols, a X0 ... Xk.2 A sequence of context-free rule expansions can be represented in a tree, with parents expanding into one or more children below them in the tree.', '49': 'Each of the individual local expansions in the tree is a rule in the CFG.', '50': 'Nodes in the tree with no children are called leaves.', '51': 'A tree whose leaves consist entirely of terminal symbols is complete.', '52': 'Consider, for example, the parse tree shown in (a) in Figure 1: the start symbol is St, which expands into an S. The S node expands into an NP followed by a VP.', '53': 'These nonterminal nodes each in turn expand, and this process of expansion continues until the tree generates the terminal string, &quot;Spot chased the ball&quot;, as leaves.', '54': 'A CFG G defines a language LG, which is a subset of the set of strings of terminal symbols, including only those that are leaves of complete trees rooted at St, built with rules from the grammar G. We will denote strings either as w or as wowi .', '55': '• • wn, where wn is understood to be the last terminal symbol in the string.', '56': 'For simplicity in displaying equations, from this point forward let w/ be the substring wj.', '57': 'Let Twg be the set of all complete trees rooted at the start symbol, with the string of terminals zug as leaves.', '58': 'We call Tzq the set of complete parses of wg.', '59': 'A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.', '60': 'The probability of a parse tree is the product of the probabilities of each rule in the tree.', '61': ""Provided a PCFG is consistent (or tight), which it always will be in the approach we will be advocating, this defines a proper probability distribution over completed trees.'"", '62': 'A PCFG also defines a probability distribution over strings of words (terminals) in the following way: The intuition behind Equation 1 is that, if a string is generated by the PCFG, then it will be produced if and only if one of the trees in the set Tzq generated it.', '63': ""Thus the probability of the string is the probability of the set Twg, i.e., the sum of its members' probabilities."", '64': 'Up to this point, we have been discussing strings of words without specifying whether they are &quot;complete&quot; strings or not.', '65': 'We will adopt the convention that an explicit beginning of string symbol, (s), and an explicit end symbol, (/s), are part of the vocabulary, and a string wg is a complete string if and only if Tao is (s) and tv, is (/s).', '66': 'Since the beginning of string symbol is not predicted by language models, but rather is axiomatic in the same way that St is for a parser, we can safely omit it from the current discussion, and simply assume that it is there.', '67': 'See Figure 1(b) for the explicit representation.', '68': 'While a complete string of words must contain the end symbol as its final word, a string prefix does not have this restriction.', '69': 'For example, &quot;Spot chased the ball (/s)&quot; is a complete string, and the following is the set of prefix strings of this complete string: &quot;Spot&quot;; &quot;Spot chased&quot;; &quot;Spot chased the&quot;; &quot;Spot chased the ball&quot;; and &quot;Spot chased the ball Us}&quot;.', '70': 'A PCFG also defines a probability distribution over string prefixes, and we will present this in terms of partial derivations.', '71': ""A partial derivation (or parse) d is defined with respect to a prefix string w as follows: it is the leftmost derivation of the string, with wj on the right-hand side of the last expansion in the derivation.'"", '72': 'Let Dw, be the set of all partial derivations for a prefix string 4.', '73': ""Then We left-factor the PCFG, so that all productions are binary, except those with a single terminal on the right-hand side and epsilon productions.'"", '74': 'We do this because it delays predictions about what nonterminals we expect later in the string until we have seen more of the string.', '75': 'In effect, this is an underspecification of some of the predictions that our top-down parser is making about the rest of the string.', '76': 'The leftfactorization transform that we use is identical to what is called right binarization in Roark and Johnson (1999).', '77': 'See that paper for more discussion of the benefits of Two parse trees: (a) a complete left-factored parse tree with epsilon productions and an explicit stop symbol; and (b) a partial left-factored parse tree. factorization for top-down and left-corner parsing.', '78': 'For a grammar G, we define a factored grammar Gf as follows: We can see the effect of this transform on our example parse trees in Figure 2.', '79': 'This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.', '80': 'It also brings words further downstream into the look-ahead at the point of specification.', '81': 'Note that partial trees are defined in exactly the same way (Figure 2b), but that the nonterminal yields are made up exclusively of the composite nonterminals introduced by the grammar transform.', '82': 'This transform has a couple of very nice properties.', '83': 'First, it is easily reversible, i.e., every parse tree built with Gf corresponds to a unique parse tree built with G. Second, if we use the relative frequency estimator for our production probabilities, the probability of a tree built with Gf is identical to the probability of the corresponding tree built with G. Finally, let us introduce the term c-command.', '84': 'We will use this notion in our conditional probability model, and it is also useful for understanding some of the previous work in this area.', '85': ""The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e., non-unary node) that dominates A also dominates B.'"", '86': 'Thus in Figure 1(a), the subject NP and the VP each c-command the other, because neither dominates the other and the lowest branching node above both (the S) dominates the other.', '87': 'Notice that the subject NP c-commands the object NP, but not vice versa, since the lowest branching node that dominates the object NP is the VP, which does not dominate the subject NP.', '88': ""This section will briefly introduce language modeling for statistical speech recognition.'"", '89': 'In language modeling, we assign probabilities to strings of words.', '90': 'To assign a probability, the chain rule is generally invoked.', '91': 'The chain rule states, for a string of k+1 words: A Markov language model of order n truncates the conditioning information in the chain rule to include only the previous n words.', '92': ""These models are commonly called n-gram models.'"", '93': 'The standard language model used in many speech recognition systems is the trigram model, i.e., a Markov model of order 2, which can be characterized by the following equation: To smooth the trigram models that are used in this paper, we interpolate the probability estimates of higher-order Markov models with lower-order Markov models (Jelinek and Mercer 1980).', '94': 'The idea behind interpolation is simple, and it has been shown to be very effective.', '95': 'For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency, and An is a function from Vn to [0, 1].', '96': 'This interpolation is recursively applied to the smaller-order n-grams until the bigram is finally interpolated with the unigram, i.e., Ao = 1.', '97': 'There have been attempts to jump over adjacent words to words farther back in the left context, without the use of dependency links or syntactic structure, for example Saul and Pereira (1997) and Rosenfeld (1996, 1997).', '98': 'We will focus our very brief review, however, on those that use grammars or parsing for their language models.', '99': 'These can be divided into two rough groups: those that use the grammar as a language model, and those that use a parser to uncover phrasal heads standing in an important relation (c-command) to the current word.', '100': 'The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.', '101': 'As mentioned in Section 2.1, a PCFG defines a probability distribution over strings of words.', '102': 'One approach to syntactic language modeling is to use this distribution directly as a language model.', '103': 'There are efficient algorithms in the literature (Jelinek and Lafferty 1991; Stolcke 1995) for calculating exact string prefix probabilities given a PCFG.', '104': 'The algorithms both utilize a left-corner matrix, which can be calculated in closed form through matrix inversion.', '105': 'They are limited, therefore, to grammars where the nonterminal set is small enough to permit inversion.', '106': 'String prefix probabilities can be straightforwardly used to compute conditional word probabilities by definition: Stolcke and Segal (1994) and Jurafsky et al. (1995) used these basic ideas to estimate bigram probabilities from hand-written PCFGs, which were then used in language models.', '107': 'Interpolating the observed bigram probabilities with these calculated bigrams led, in both cases, to improvements in word error rate over using the observed bigrams alone, demonstrating that there is some benefit to using these syntactic language models to generalize beyond observed n-grams.', '108': 'Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &quot;surface&quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.', '109': 'A shift-reduce parser operates from left to right using a stack and a pointer to the next word in the input string.9 Each stack entry consists minimally of a nonterminal label.', '110': 'The parser performs two basic operations: (i) shifting, which involves pushing the POS label of the next word onto the stack and moving the pointer to the following word in the input string; and (ii) reducing, which takes the top k stack entries and replaces them with a single new entry, the nonterminal label of which is the left-hand side of a rule in the grammar that has the k top stack entry labels on the right-hand side.', '111': 'For example, if there is a rule NP --> DT NN, and the top two stack entries are NN and DT, then those two entries can be popped off of the stack and an entry with the label NP pushed onto the stack.', '112': 'Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state, and conditioning on those entries in a way similar to an n-gram.', '113': 'In empirical trials, Goddeau used the top two stack entries to condition the word probability.', '114': 'He was able to reduce both sentence and word error rates on the ATIS corpus using this method.', '115': 'The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.', '116': 'The SLM is like a trigram, except that the conditioning words are taken from the tops of the stacks of candidate parses in the beam, rather than from the linear order of the string.', '117': 'Their parser functions in three stages.', '118': 'The first stage assigns a probability to the word given the left context (represented by the stack state).', '119': 'The second stage predicts the POS given the word and the left context.', '120': 'The last stage performs all possible parser operations (reducing stack entries and shifting the new word).', '121': 'When there is no more parser work to be done (or, in their case, when the beam is full), the following word is predicted.', '122': 'And so on until the end of the string.', '123': 'Each different POS assignment or parser operation is a step in a derivation.', '124': 'Each distinct derivation path within the beam has a probability and a stack state associated with it.', '125': 'Every stack entry has a nonterminal node label and a designated headword of the constituent.', '126': 'When all of the parser operations have finished at a particular point in the string, the next word is predicted as follows: For each derivation in the beam, the headwords of the two topmost stack entries form a trigram with the conditioned word.', '127': ""This interpolated trigram probability is then multiplied by the normalized probability of the derivation, to provide that derivation's contribution to the probability of the word."", '128': 'More precisely, for a beam of derivations D, where hod and hid are the lexical heads of the top two entries on the stack of d. Figure 3 gives a partial tree representation of a potential derivation state for the string &quot;the dog chased the cat with spots&quot;, at the point when the word &quot;with&quot; is to be predicted.', '129': 'The shift-reduce parser will have, perhaps, built the structure shown, and the stack state will have an NP entry with the head &quot;cat&quot; at the top of the stack, and a VBD entry with the head &quot;chased&quot; second on the stack.', '130': 'In the Chelba and Jelinek model, the probability of &quot;with&quot; is conditioned on these two headwords, for this derivation.', '131': 'Since the specific results of the SLM will be compared in detail with our model when the empirical results are presented, at this point we will simply state that they have achieved a reduction in both perplexity and word error rate over a standard trigram using this model.', '132': 'The rest of this paper will present our parsing model, its application to language modeling for speech recognition, and empirical results.', '133': 'Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.', '134': ""The parsers with the highest published broad-coverage parsing accuracy, which include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997), all utilize simple and straightforward statistically based search heuristics, pruning the search-space quite dramatically!'"", '135': 'Such methods are nearly always used in conjunction with some form of dynamic programming (henceforth DP).', '136': 'That is, search efficiency for these parsers is improved by both statistical search heuristics and DP.', '137': 'Here we will present a parser that uses simple search heuristics of this sort without DP.', '138': 'Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.', '139': 'This on-line characteristic allows our language model to be interpolated on a word-by-word basis with other models, such as the trigram, yielding further improvements.', '140': 'Next we will outline our conditional probability model over rules in the PCFG, followed by a presentation of the top-down parsing algorithm.', '141': 'We will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the Chelba and Jelinek results mentioned above.', '142': 'A simple PCFG conditions rule probabilities on the left-hand side of the rule.', '143': 'It has been shown repeatedly—e.g., Briscoe and Carroll (1993), Charniak (1997), Collins (1997), Inui et al. (1997), Johnson (1998)—that conditioning the probabilities of structures on the context within which they appear, for example on the lexical head of a constituent (Charniak 1997; Collins 1997), on the label of its parent nonterminal (Johnson 1998), or, ideally, on both and many other things besides, leads to a much better parsing model and results in higher parsing accuracies.', '144': 'One way of thinking about conditioning the probabilities of productions on contextual information (e.g., the label of the parent of a constituent or the lexical heads of constituents), is as annotating the extra conditioning information onto the labels in the context-free rules.', '145': 'Examples of this are bilexical grammars—such as Eisner and Satta (1999), Charniak (1997), Collins (1997)—where the lexical heads of each constituent are annotated on both the right- and left-hand sides of the context-free rules, under the constraint that every constituent inherits the lexical head from exactly one of its children, and the lexical head of a POS is its terminal item.', '146': 'Thus the rule S NP VP becomes, for instance, S [barks] NP [dog] VP[barks].', '147': 'One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).', '148': 'This procedure yields conditional probability distributions of constituents on the right-hand side with their lexical heads, given the left-hand side constituent and its lexical head.', '149': 'The same procedure works if we annotate parent information onto constituents.', '150': 'This is how Johnson (1998) conditioned the probabilities of productions: the left-hand side is no longer, for example, S, but rather SI SBAR, i.e., an S with SBAR as parent.', '151': 'Notice, however, that in this case the annotations on the righthand side are predictable from the annotation on the left-hand side (unlike, for example, bilexical grammars), so that the relative frequency estimator yields conditional probability distributions of the original rules, given the parent of the left-hand side.', '152': 'All of the conditioning information that we will be considering will be of this latter sort: the only novel predictions being made by rule expansions are the node labels of the constituents on the right-hand side.', '153': 'Everything else is already specified by the left context.', '154': 'We use the relative frequency estimator, and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.', '155': 'This perspective on conditioning production probabilities makes it easy to see that, in essence, by conditioning these probabilities, we are growing the state space.', '156': 'That is, the number of distinct nonterminals grows to include the composite labels; so does the number of distinct productions in the grammar.', '157': 'In a top-down parser, each rule expansion is made for a particular candidate parse, which carries with it the entire rooted derivation to that point; in a sense, the left-hand side of the rule is annotated with the entire left context, and the rule probabilities can be conditioned on any aspect of this derivation.', '158': 'We do not use the entire left context to condition the rule probabilities, but rather &quot;pick-and-choose&quot; which events in the left context we would like to condition on.', '159': 'One can think of the conditioning events as functions, which take the partial tree structure as an argument and return a value, upon which the rule probability can be conditioned.', '160': 'Each of these functions is an algorithm for walking the provided tree and returning a value.', '161': 'For example, suppose that we want to condition the probability of the rule A —> a.', '162': 'We might write a function that takes the partial tree, finds the parent of the left-hand side of the rule and returns its node label.', '163': 'If the left-hand side has no parent (i.e., it is at the root of the tree), the function returns the null value (NULL).', '164': 'We might write another function that returns the nonterminal label of the closest sibling to the left of A, and NULL if no such node exists.', '165': 'We can then condition the probability of the production on the values that were returned by the set of functions.', '166': 'Recall that we are working with a factored grammar, so some of the nodes in the factored tree have nonterminal labels that were created by the factorization, and may not be precisely what we want for conditioning purposes.', '167': 'In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version, we introduce the function constituent (A) for every nonterminal in the factored grammar Gf, which is simply the label of the constituent whose factorization results in A.', '168': 'For example, in Figure 2, constituent (NP-DT-NN) is simply NP.', '169': 'Note that a function can return different values depending upon the location in the tree of the nonterminal that is being expanded.', '170': 'For example, suppose that we have a function that returns the label of the closest sibling to the left of constituent (A) or NULL if no such node exists.', '171': 'Then a subsequent function could be defined as follows: return the parent of the parent (the grandparent) of constituent (A) only if constituent (A) has no sibling to the left—in other words, if the previous function returns NULL; otherwise return the second closest sibling to the left of constituent (A), or, as always, NULL if no such node exists.', '172': 'If the function returns, for example, NP, this could either mean that the grandparent is NP or the second closest sibling is Conditional probability model represented as a decision tree, identifying the location in the partial parse tree of the conditioning information.', '173': 'NP; yet there is no ambiguity in the meaning of the function, since the result of the previous function disambiguates between the two possibilities.', '174': 'The functions that were used for the present study to condition the probability of the rule, A a, are presented in Figure 4, in a tree structure.', '175': 'This is a sort of decision tree for a tree-walking algorithm to decide what value to return, for a given partial tree and a given depth.', '176': 'For example, if the algorithm is asked for the value at level 0, it will return A, the left-hand side of the rule being expanded.&quot; Suppose the algorithm is asked for the value at level 4.', '177': 'After level 2 there is a branch in the decision tree.', '178': 'If the left-hand side of the rule is a POS, and there is no sibling to the left of constituent (A) in the derivation, then the algorithm takes the right branch of the decision tree to decide what value to return; otherwise the left branch.', '179': 'Suppose it takes the left branch.', '180': 'Then after level 3, there is another branch in the decision tree.', '181': 'If the left-hand side of the production is a POS, then the algorithm takes the right branch of the decision tree, and returns (at level 4) the POS of the closest c-commanding lexical head to A, which it finds by walking the parse tree; if the left-hand side of the rule is not a POS, then the algorithm returns (at level 4) the closest sibling to the left of the parent of constituent (A).', '182': 'The functions that we have chosen for this paper follow from the intuition (and experience) that what helps parsing is different depending on the constituent that is being expanded.', '183': 'POS nodes have lexical items on the right-hand side, and hence can bring into the model some of the head-head dependencies that have been shown to be so effective.', '184': 'If the POS is leftmost within its constituent, then very often the lexical item is sensitive to the governing category to which it is attaching.', '185': 'For example, if the POS is a preposition, then its probability of expanding to a particular word is very different if it is attaching to a noun phrase than if it is attaching to a verb phrase, and perhaps quite different depending on the head of the constituent to which it is attaching.', '186': 'Subsequent POSs within a constituent are likely to be open-class words, and less dependent on these sorts of attachment preferences.', '187': 'Conditioning on parents and siblings of the left-hand side has proven to be very useful.', '188': 'To understand why this is the case, one need merely to think of VP expansions.', '189': ""If the parent of a VP is another VP (i.e., if an auxiliary or modal verb is used), then the distribution over productions is different than if the parent is an S. Conditioning on head information, both POS of the head and the lexical item itself, has proven useful as well, although given our parser's left-to-right orientation, in many cases the head has not been encountered within the particular constituent."", '190': 'In such a case, the head of the last child within the constituent is used as a proxy for the constituent head.', '191': 'All of our conditioning functions, with one exception, return either parent or sibling node labels at some specific distance from the left-hand side, or head information from ccommanding constituents.', '192': 'The exception is the function at level 5 along the left branch of the tree in Figure 4.', '193': 'Suppose that the node being expanded is being conjoined with another node, which we can tell by the presence or absence of a CC node.', '194': 'In that case, we want to condition the expansion on how the conjoining constituent expanded.', '195': 'In other words, this attempts to capture a certain amount of parallelism between the expansions of conjoined categories.', '196': 'In presenting the parsing results, we will systematically vary the amount of conditioning information, so as to get an idea of the behavior of the parser.', '197': 'We will refer to the amount of conditioning by specifying the deepest level from which a value is returned for each branching path in the decision tree, from left to right in Figure 4: the first number is for left contexts where the left branch of the decision tree is always followed (non-POS nonterminals on the left-hand side); the second number is for a left branch followed by a right branch (POS nodes that are leftmost within their constituent); and the third number is for the contexts where the right branch is always followed (POS nodes that are not leftmost within their constituent).', '198': 'For example, (4,3,2) would represent a conditional probability model that (i) returns NULL for all functions below level 4 in all contexts; (ii) returns NULL for all functions below level 3 if the left-hand side is a POS; and (iii) returns NULL for all functions below level 2 for nonleftmost POS expansions.', '199': 'Table 1 gives a breakdown of the different levels of conditioning information used in the empirical trials, with a mnemonic label that will be used when presenting results.', '200': 'These different levels were chosen as somewhat natural points at which to observe how much of an effect increasing the conditioning information has.', '201': 'We first include structural information from the context, namely, node labels from constituents in the left context.', '202': 'Then we add lexical information, first for non-POS expansions, then for leftmost POS expansions, then for all expansions.', '203': 'All of the conditional probabilities are linearly interpolated.', '204': 'For example, the probability of a rule conditioned on six events is the linear interpolation of two probabilities: (i) the empirically observed relative frequency of the rule when the six events co-occur; and (ii) the probability of the rule conditioned on the first five events (which is in turn interpolated).', '205': 'The interpolation coefficients are a function of the frequency of the set of conditioning events, and are estimated by iteratively adjusting the coefficients so as to maximize the likelihood of a held-out corpus.', '206': 'This was an outline of the conditional probability model that we used for the PCFG.', '207': 'The model allows us to assign probabilities to derivations, which can be used by the parsing algorithm to decide heuristically which candidates are promising and should be expanded, and which are less promising and should be pruned.', '208': 'We now outline the top-down parsing algorithm.', '209': 'This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).', '210': 'It uses a PCFG with a conditional probability model of the sort defined in the previous section.', '211': 'We will first define candidate analysis (i.e., a partial parse), and then a derives relation between candidate analyses.', '212': 'We will then present the algorithm in terms of this relation.', '213': 'The parser takes an input string 4, a PCFG G, and a priority queue of candidate analyses.', '214': 'A candidate analysis C = (D, 8, PD, F, wn consists of a derivation D, a stack S. a derivation probability PD, a figure of merit F, and a string w7 remaining to be parsed.', '215': 'The first word in the string remaining to be parsed, w1, we will call the look-ahead word.', '216': 'The derivation D consists of a sequence of rules used from G. The stack S contains a sequence of nonterminal symbols, and an end-of-stack marker $ at the bottom.', '217': 'The probability PD is the product of the probabilities of all rules in the derivation D. F is the product of PD and a look-ahead probability, LAP(S,w,), which is a measure of the likelihood of the stack S rewriting with w, at its left corner.', '218': 'We can define a derives relation, denoted between two candidate analyses as follows.', '219': ""(D, S, PD, F, w'n = (D' , S' , Pp, F' w) if and only if12 The parse begins with a single candidate analysis on the priority queue: ), st$, 1, 1, 4)."", '220': 'Next, the top ranked candidate analysis, C = (D, 5, PD, F, wr,l), is popped from the priority queue.', '221': 'If S = $ and w, = Vs), then the analysis is complete.', '222': ""Otherwise, all C' such that C C' are pushed onto the priority queue."", '223': 'We implement this as a beam search.', '224': 'For each word position i, we have a separate priority queue H, of analyses with look-ahead w,.', '225': 'When there are &quot;enough&quot; analyses by some criteria (which we will discuss below) on priority queue H,+1, all candidate analyses remaining on H, are discarded.', '226': 'Since w„ = ( /s), all parses that are pushed onto 1-4+1 are complete.', '227': 'The parse on H,i+i with the highest probability is returned for evaluation.', '228': 'In the case that no complete parse is found, a partial parse is returned and evaluated.', '229': 'The LAP is the probability of a particular terminal being the next left-corner of a particular analysis.', '230': 'The terminal may be the left corner of the topmost nonterminal on the stack of the analysis or it might be the left corner of the nth nonterminal, after the top n - 1 nonterminals have rewritten to E. Of course, we cannot expect to have adequate statistics for each nonterminal/word pair that we encounter, so we smooth to the POS.', '231': 'Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).', '232': 'The same empirical probability, P(A, Xoe), is collected for every preterminal X as well.', '233': 'The LAP approximation for a given stack state and look-ahead terminal is: where PG(Aj Wia) AAjP(Ai Wice) + (1 - AA,) E Po; xcoti(x wi) (11) XEV The lambdas are a function of the frequency of the nonterminal A1, in the standard way (Jelinek and Mercer 1980).', '234': 'The beam threshold at word w, is a function of the probability of the top-ranked candidate analysis on priority queue kiwi, and the number of candidates on H1+1.', '235': 'The basic idea is that we want the beam to be very wide if there are few analyses that have been advanced, but relatively narrow if many analyses have been advanced.', '236': 'If p is the probability of the highest-ranked analysis on H1±1, then another analysis is discarded if its probability falls below pf(-y, IH,+11), where -y is an initial parameter, which we call the base beam factor.', '237': ""For the current study, ,y was 10-11, unless otherwise noted, and f ('-y,1H,+11) = -y11-11+113."", '238': 'Thus, if 100 analyses have already been pushed onto then a candidate analysis must have a probability above 10-5/3 to avoid being pruned.', '239': 'After 1,000 candidates, the beam has narrowed to 10-2p.', '240': 'There is also a maximum number of allowed analyses on H„ in case the parse fails to advance an analysis to H1±1.', '241': 'This was typically 10,000.', '242': 'As mentioned in Section 2.1, we left-factor the grammar, so that all productions are binary, except those with a single terminal on the right-hand side and epsilon productions.', '243': 'The only c-productions are those introduced by left-factorization.', '244': 'Our factored grammar was produced by factoring the trees in the training corpus before grammar induction, which proceeded in the standard way, by counting rule frequencies.', '245': 'The empirical results will be presented in three stages: (i) trials to examine the accuracy and efficiency of the parser; (ii) trials to examine its effect on test corpus perplexity and recognition performance; and (iii) trials to examine the effect of beam variation on these performance measures.', '246': 'Before presenting the results, we will introduce the methods of evaluation.', '247': 'Perplexity is a standard measure within the speech recognition community for comparing language models.', '248': 'In principle, if two models are tested on the same test corpus, the model that assigns the lower perplexity to the test corpus is the model closest to the true distribution of the language, and thus better as a prior model for speech recognition.', '249': 'Perplexity is the exponential of the cross entropy, which we will define next.', '250': 'Given a random variable X with distribution p and a probability model q, the cross entropy, H(p, q) is defined as follows: Let p be the true distribution of the language.', '251': ""Then, under certain assumptions, given a large enough sample, the sample mean of the negative log probability of a model will converge to its cross entropy with the true mode1.14 That is where w'ol is a string of the language L. In practice, one takes a large sample of the language, and calculates the negative log probability of the sample, normalized by its size.15 The lower the cross entropy (i.e., the higher the probability the model assigns to the sample), the better the model."", '252': ""Usually this is reported in terms of perplexity, which we will do as well.'"", '253': 'Some of the trials discussed below will report results in terms of word and/or sentence error rate, which are obtained when the language model is embedded in a speech recognition system.', '254': 'Word error rate is the number of deletion, insertion, or substitution errors per 100 words.', '255': 'Sentence error rate is the number of sentences with one or more errors per 100 sentences.', '256': 'Statistical parsers are typically evaluated for accuracy at the constituent level, rather than simply whether or not the parse that the parser found is completely correct or not.', '257': 'A constituent for evaluation purposes consists of a label (e.g., NP) and a span (beginning and ending word positions).', '258': 'For example, in Figure 1(a), there is a VP that spans the words &quot;chased the ball&quot;.', '259': 'Evaluation is carried out on a hand-parsed test corpus, and the manual parses are treated as correct.', '260': 'We will call the manual parse GOLD and the parse that the parser returns TEST.', '261': 'Precision is the number of common constituents in GOLD and TEST divided by the number of constituents in TEST.', '262': 'Recall is the number of common constituents in GOLD and TEST divided by the number of constituents in GOLD.', '263': 'Following standard practice, we will be reporting scores only for non-part-of-speech constituents, which are called labeled recall (LR) and labeled precision (LP).', '264': 'Sometimes in figures we will plot their average, and also what can be termed the parse error, which is one minus their average.', '265': 'LR and LP are part of the standard set of PARSEVAL measures of parser quality (Black et al. 1991).', '266': 'From this set of measures, we will also include the crossing bracket scores: average crossing brackets (CB), percentage of sentences with no crossing brackets (0 CB), and the percentage of sentences with two crossing brackets or fewer (< 2 CB).', '267': 'In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.', '268': 'This is an incremental parser with a pruning strategy and no backtracking.', '269': 'In such a model, it is possible to commit to a set of partial analyses at a particular point that cannot be completed given the rest of the input string (i.e., the parser can &quot;garden path&quot;).', '270': 'In such a case, the parser fails to return a complete parse.', '271': 'In the event that no complete parse is found, the highest initially ranked parse on the last nonempty priority queue is returned.', '272': 'All unattached words are then attached at the highest level in the tree.', '273': 'In such a way we predict no new constituents and all incomplete constituents are closed.', '274': 'This structure is evaluated for precision and recall, which is entirely appropriate for these incomplete as well as complete parses.', '275': 'If we fail to identify nodes later in the parse, recall will suffer, and if our early predictions were bad, both precision and recall will suffer.', '276': 'Of course, the percentage of these failures are reported as well.', '277': 'The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989,860 words, 39,832 sentences) of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) served as the training data, Section 24 (34,199 words, 1,346 sentences) as the held-out data for parameter estimation, and Section 23 (59,100 words, 2,416 sentences) as the test data.', '278': 'Section 22 (41,817 words, 1,700 sentences) served as the development corpus, on which the parser was tested until stable versions were ready to run on the test data, to avoid developing the parser to fit the specific test data.', '279': 'Table 2 shows trials with increasing amounts of conditioning information from the left context.', '280': 'There are a couple of things to notice from these results.', '281': 'First, and least surprising, is that the accuracy of the parses improved as we conditioned on more and more information.', '282': 'Like the nonlexicalized parser in Roark and Johnson (1999), we found that the search efficiency, in terms of number of rule expansions considered or number of analyses advanced, also improved as we increased the amount of conditioning.', '283': 'Unlike the Roark and Johnson parser, however, our coverage did not substantially drop as the amount of conditioning information increased, and in some cases, coverage improved slightly.', '284': 'They did not smooth their conditional probability estimates, and blamed sparse data for their decrease in coverage as they increased the conditioning information.', '285': 'These results appear to support this, since our smoothed model showed no such tendency.', '286': 'Figure 5 shows the reduction in parser error, 1 LR---LP , and the reduction in rule expansions considered as the conditioning information increased.', '287': 'The bulk of the improvement comes from simply conditioning on the labels of the parent and the closest sibling to the node being expanded.', '288': 'Interestingly, conditioning all POS expansions on two c-commanding heads made no difference in accuracy compared to conditioning only leftmost POS expansions on a single c-commanding head; but it did improve the efficiency.', '289': ""These results, achieved using very straightforward conditioning events and considering only the left context, are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank, with the full conditional probability model and beam of 10-11, using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'"", '290': 'Of the 2,416 sentences in the section, 728 had the totally correct parse, 30.1 percent tree accuracy.', '291': 'Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.', '292': 'The parser, thus, could be used as a front end to some other model, with the hopes of selecting a more accurate parse from among the final candidates.', '293': 'While we have shown that the conditioning information improves the efficiency in terms of rule expansions considered and analyses advanced, what does the efficiency of such a parser look like in practice?', '294': 'Figure 6 shows the observed time at our standard base beam of 10-11 with the full conditioning regimen, alongside an approximation of the reported observed (linear) time in Ratnaparkhi (1997).', '295': 'Our observed times look polynomial, which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis, the more time will be spent working on these competitors; and the farther along in the sentence, the more chance for ambiguities that can lead to such a situation.', '296': 'While our observed times are not linear, and are clearly slower than his times (even with a faster machine), they are quite respectably fast.', '297': 'The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.', '298': 'What is perhaps surprising is that the difference is not greater.', '299': 'Furthermore, this is quite a large beam (see discussion below), so that very large improvements in efficiency can be had at the expense of the number of analyses that are retained.', '300': 'The next set of results will highlight what recommends this approach most: the ease with which one can estimate string probabilities in a single pass from left to right across the string.', '301': ""By definition, a PCFG's estimate of a string's probability is the sum of the probabilities of all trees that produce the string as terminal leaves (see Equation 1)."", '302': ""In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds."", '303': 'Since this is not an exhaustive search, the parses that are returned will be a subset of the total set of trees that would be used in the exact PCFG estimate; hence the estimate thus arrived at will be bounded above by the probability that would be generated from an exhaustive search.', '304': 'The hope is that a large amount of the probability mass will be accounted for by the parses in the beam.', '305': 'The method cannot overestimate the probability of the string.', '306': 'Recall the discussion of the grammar models above, and our definition of the set of partial derivations Dw, with respect to a prefix string wil) (see Equations 2 and 7).', '307': 'By definition, Note that the numerator at word wj is the denominator at word w1+1, so that the product of all of the word probabilities is the numerator at the final word, namely, the string prefix probability.', '308': 'We can make a consistent estimate of the string probability by similarly summing over all of the trees within our beam.', '309': 'Let Ht be the priority queue H, before any processing has begun with word w, in the look-ahead.', '310': ""This is a subset of the possible leftmost partial derivations with respect to the prefix string W. Since RV is produced by expanding only analyses on priority queue H;', the set of complete trees consistent with the partial derivations on priority queue Ht is a subset of the set of complete trees consistent with the partial derivations on priority queue HT'', that is, the total probability mass represented by the priority queues is monotonically decreasing."", '311': 'Thus conditional word probabilities defined in a way consistent with Equation 14 will always be between zero and one.', '312': 'Our conditional word probabilities are calculated as follows: As mentioned above, the model cannot overestimate the probability of a string, because the string probability is simply the sum over the beam, which is a subset of the possible derivations.', '313': 'By utilizing a figure of merit to identify promising analyses, we are simply focusing our attention on those parses that are likely to have a high probability, and thus we are increasing the amount of probability mass that we do capture, of the total possible.', '314': 'It is not part of the probability model itself.', '315': 'Since each word is (almost certainly, because of our pruning strategy) losing some probability mass, the probability model is not &quot;proper &quot;—the sum of the probabilities over the vocabulary is less than one.', '316': 'In order to have a proper probability distribution, we would need to renormalize by dividing by some factor.', '317': ""Note, however, that this renormalization factor is necessarily less than one, and thus would uniformly increase each word's probability under the model, that is, any perplexity results reported below will be higher than the &quot;true&quot; perplexity that would be assigned with a properly normalized distribution."", '318': 'In other words, renormalizing would make our perplexity measure lower still.', '319': 'The hope, however, is that the improved parsing model provided by our conditional probability model will cause the distribution over structures to be more peaked, thus enabling us to capture more of the total probability mass, and making this a fairly snug upper bound on the perplexity.', '320': 'One final note on assigning probabilities to strings: because this parser does garden path on a small percentage of sentences, this must be interpolated with another estimate, to ensure that every word receives a probability estimate.', '321': 'In our trials, we used the unigram, with a very small mixing coefficient: following words since the denominator is zero.', '322': 'Thus, Chelba and Jelinek (1998a, 1998b) also used a parser to help assign word probabilities, via the structured language model outlined in Section 3.2.', '323': 'They trained and tested the SLM on a modified, more &quot;speech-like&quot; version of the Penn Treebank.', '324': 'Their modifications included: (i) removing orthographic cues to structure (e.g., punctuation); (ii) replacing all numbers with the single token N; and (iii) closing the vocabulary at 10,000, replacing all other words with the UNK token.', '325': 'They used Sections 00-20 (929,564 words) as the development set, Sections 21-22 (73,760 words) as the check set (for interpolation coefficient estimation), and tested on Sections 23-24 (82,430 words).', '326': 'We obtained the training and testing corpora from them (which we will denote C&J corpus), and also created intermediate corpora, upon which only the first two modifications were carried out (which we will denote no punct).', '327': 'Differences in performance will give an indication of the impact on parser performance of the different modifications to the corpora.', '328': 'All trials in this section used Sections 00-20 for counts, held out 21-22, and tested on 23-24.', '329': 'Table 3 shows several things.', '330': 'First, it shows relative performance for unmodified, no punct, and C&J corpora with the full set of conditioning information.', '331': 'We can see that removing the punctuation causes (unsurprisingly) a dramatic drop in the accuracy and efficiency of the parser.', '332': 'Interestingly, it also causes coverage to become nearly total, with failure on just two sentences per thousand on average.', '333': 'We see the familiar pattern, in the C&J corpus results, of improving performance as the amount of conditioning information grows.', '334': 'In this case we have perplexity results as well, and Figure 7 shows the reduction in parser error, rule expansions, and perplexity as the amount of conditioning information grows.', '335': 'While all three seem to be similarly improved by the addition of structural context (e.g., parents and siblings), the addition of c-commanding heads has only a moderate effect on the parser accuracy, but a very large effect on the perplexity.', '336': 'The fact that the efficiency was improved more than the accuracy in this case (as was also seen in Figure 5), seems to indicate that this additional information is causing the distribution to become more peaked, so that fewer analyses are making it into the beam.', '337': 'Reduction in average precision/recall error, number of rule expansions, and perplexity as conditioning increases.', '338': 'Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a, 1998b) on the same training and testing corpora.', '339': ""We built an interpolated trigram model to serve as a baseline (as they did), and also interpolated our model's perplexity with the trigram, using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'"", '340': 'The trigram model was also trained on Sections 00-20 of the C&J corpus.', '341': 'Trigrams and bigrams were binned by the total count of the conditioning words in the training corpus, and maximum likelihood mixing coefficients were calculated for each bin, to mix the trigram with bigram and unigram estimates.', '342': 'Our trigram model performs at almost exactly the same level as theirs does, which is what we would expect.', '343': ""Our parsing model's perplexity improves upon their first result fairly substantially, but is only slightly better than their second result.'"", '344': 'However, when we interpolate with the trigram, we see that the additional improvement is greater than the one they experienced.', '345': 'This is not surprising, since our conditioning information is in many ways orthogonal to that of the trigram, insofar as it includes the probability mass of the derivations; in contrast, their model in some instances is very close to the trigram, by conditioning on two words in the prefix string, which may happen to be the two adjacent words.', '346': 'These results are particularly remarkable, given that we did not build our model as a language model per se, but rather as a parsing model.', '347': 'The perplexity improvement was achieved by simply taking the existing parsing model and applying it, with no extra training beyond that done for parsing.', '348': 'The hope was expressed above that our reported perplexity would be fairly close to the &quot;true&quot; perplexity that we would achieve if the model were properly normalized, i.e., that the amount of probability mass that we lose by pruning is small.', '349': ""One way to test this is the following: at each point in the sentence, calculate the conditional probability of each word in the vocabulary given the previous words, and sum them.'"", '350': 'If there is little loss of probability mass, the sum should be close to one.', '351': 'We did this for the first 10 sentences in the test corpus, a total of 213 words (including the end-of-sentence markers).', '352': 'One of the sentences was a failure, so that 12 of the word probabilities (all of the words after the point of the failure) were not estimated by our model.', '353': 'Of the remaining 201 words, the average sum of the probabilities over the 10,000-word vocabulary was 0.9821, with a minimum of 0.7960 and a maximum of 0.9997.', '354': 'Interestingly, at the word where the failure occurred, the sum of the probabilities was 0.9301.', '355': 'In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.', '356': ""The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal, a total of 3,446 words."", '357': 'The corpus comes with a baseline trigram model, using a 20,000-word open vocabulary, and trained on approximately 40 million words.', '358': ""We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice, along with the acoustic and trigram scores.'"", '359': 'Given the idealized circumstances of the production (text read in a lab), the lattices are relatively sparse, and in many cases 50 distinct string hypotheses were not found in a lattice.', '360': 'We reranked an average of 22.9 hypotheses with our language model per utterance.', '361': 'One complicating issue has to do with the tokenization in the Penn Treebank versus that in the HUB1 lattices.', '362': ""In particular, contractions (e.g., he are split in the Penn Treebank (he 's) but not in the HUB1 lattices."", '363': 'Splitting of the contractions is critical for parsing, since the two parts oftentimes (as in the previous example) fall in different constituents.', '364': 'We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.', '365': ""If we are to interpolate our model with the lattice trigram, we must wait until we have our model's estimate for the probability of both parts of the contraction; their product can then be interpolated with the trigram estimate."", '366': 'In fact, interpolation in these trials made no improvement over the better of the uninterpolated models, but simply resulted in performance somewhere between the better and the worse of the two models, so we will not present interpolated trials here.', '367': 'Table 5 reports the word and sentence error rates for five different models: (i) the trigram model that comes with the lattices, trained on approximately 40M words, with a vocabulary of 20,000; (ii) the best-performing model from Chelba (2000), which was interpolated with the lattice trigram at A -= 0.4; (iii) our parsing model, with the same training and vocabulary as the perplexity trials above; (iv) a trigram model with the same training and vocabulary as the parsing model; and (v) no language model at all.', '368': 'This last model shows the performance from the acoustic model alone, without the influence of the language model.', '369': 'The log of the language model score is multiplied by the language model (LM) weight when summing the logs of the language and acoustic scores, as a way of increasing the relative contribution of the language model to the composite score.', '370': 'We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.', '371': 'For our model and the Treebank trigram model, the LM weight that resulted in the lowest error rates is given.', '372': 'The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.', '373': 'What is more informative is the difference between our model and the trigram trained on the same amount of data.', '374': 'We achieved an 8.5 percent relative improvement in word error rate, and an 8.3 percent relative improvement in sentence error rate over the Treebank trigram.', '375': 'Interestingly, as mentioned above, interpolating two models together gave no improvement over the better of the two, whether our model was interpolated with the lattice or the Treebank trigram.', '376': 'This contrasts with our perplexity results reported above, as well as with the recognition experiments in Chelba (2000), where the best results resulted from interpolated models.', '377': 'The point of this small experiment was to see if our parsing model could provide useful information even in the case that recognition errors occur, as opposed to the (generally) fully grammatical strings upon which the perplexity results were obtained.', '378': 'As one reviewer pointed out, given that our model relies so heavily on context, it may have difficulty recovering from even one recognition error, perhaps more difficulty than a more locally oriented trigram.', '379': 'While the improvements over the trigram model in these trials are modest, they do indicate that our model is robust enough to provide good information even in the face of noisy input.', '380': 'Future work will include more substantial word recognition experiments.', '381': 'The last set of results that we will present addresses the question of how wide the beam must be for adequate results.', '382': ""The base beam factor that we have used to this point is 10', which is quite wide."", '383': 'It was selected with the goal of high parser accuracy; but in this new domain, parser accuracy is a secondary measure of performance.', '384': 'To determine the effect on perplexity, we varied the base beam factor in trials on the Chelba and Jelinek corpora, keeping the level of conditioning information constant, and Table 6 shows the results across a variety of factors.', '385': 'The parser error, parser coverage, and the uninterpolated model perplexity (A = 1) all suffered substantially from a narrower search, but the interpolated perplexity remained quite good even at the extremes.', '386': 'Figure 8 plots the percentage increase in parser error, model perplexity, interpolated perplexity, and efficiency (i.e., decrease in rule expansions per word) as the base beam factor decreased.', '387': 'Note that the model perplexity and parser accuracy are quite similarly affected, but that the interpolated perplexity remained far below the trigram baseline, even with extremely narrow beams.', '388': 'The empirical results presented above are quite encouraging, and the potential of this kind of approach both for parsing and language modeling seems very promising.', '389': 'Increase in average precision/recall error, model perplexity, interpolated perplexity, and efficiency (i.e., decrease in rule expansions per word) as base beam factor decreases.', '390': 'With a simple conditional probability model, and simple statistical search heuristics, we were able to find very accurate parses efficiently, and, as a side effect, were able to assign word probabilities that yield a perplexity improvement over previous results.', '391': 'These perplexity improvements are particularly promising, because the parser is providing information that is, in some sense, orthogonal to the information provided by a trigram model, as evidenced by the robust improvements to the baseline trigram when the two models are interpolated.', '392': 'There are several important future directions that will be taken in this area.', '393': 'First, there is reason to believe that some of the conditioning information is not uniformly useful, and we would benefit from finer distinctions.', '394': 'For example, the probability of a preposition is presumably more dependent on a c-commanding head than the probability of a determiner is.', '395': 'Yet in the current model they are both conditioned on that head, as leftmost constituents of their respective phrases.', '396': 'Second, there are advantages to top-down parsing that have not been examined to date, e.g., empty categories.', '397': 'A top-down parser, in contrast to a standard bottom-up chart parser, has enough information to predict empty categories only where they are likely to occur.', '398': 'By including these nodes (which are in the original annotation of the Penn Treebank), we may be able to bring certain long-distance dependencies into a local focus.', '399': 'In addition, as mentioned above, we would like to further test our language model in speech recognition tasks, to see if the perplexity improvement that we have seen can lead to significant reductions in word error rate.', '400': 'Other parsing approaches might also be used in the way that we have used a topdown parser.', '401': 'Earley and left-corner parsers, as mentioned in the introduction, also have rooted derivations that can be used to calculated generative string prefix probabilities incrementally.', '402': 'In fact, left-corner parsing can be simulated by a top-down parser by transforming the grammar, as was done in Roark and Johnson (1999), and so an approach very similar to the one outlined here could be used in that case.', '403': 'Perhaps some compromise between the fully connected structures and extreme underspecification will yield an efficiency improvement.', '404': 'Also, the advantages of head-driven parsers may outweigh their inability to interpolate with a trigram, and lead to better off-line language models than those that we have presented here.', '405': 'Does a parsing model capture exactly what we need for informed language modeling?', '406': 'The answer to that is no.', '407': 'Some information is simply not structural in nature (e.g., topic), and we might expect other kinds of models to be able to better handle nonstructural dependencies.', '408': 'The improvement that we derived from interpolating the different models above indicates that using multiple models may be the most fruitful path in the future.', '409': 'In any case, a parsing model of the sort that we have presented here should be viewed as an important potential source of key information for speech recognition.', '410': 'Future research will show if this early promise can be fully realized.', '411': 'The author wishes to thank Mark Johnson for invaluable discussion, guidance, and moral support over the course of this project.', '412': 'Many thanks also to Eugene Charniak for the use of certain grammar training routines, and for an enthusiastic interest in the project.', '413': 'Thanks also to four anonymous reviewers for valuable and insightful comments, and to Ciprian Chelba, Sanjeev Khudanpur, and Frederick Jelinek for comments and suggestions.', '414': 'Finally, the author would like to express his appreciation to the participants of discussions during meetings of the Brown'}","['J01-2004_aakansha', 'J01-2004_swastika', 'J01-2004_sweta']","['../data/summaries/J01-2004_aakansha.txt', '../data/summaries/J01-2004_swastika.txt', '../data/summaries/J01-2004_sweta.txt']","['../data/tba/J01-2004_aakansha.json', '../data/tba/J01-2004_swastika.json', '../data/tba/J01-2004_sweta.json']"
J96-3004,A Stochastic Finite-State Word-Segmentation Algorithm for Chinese,../data/papers/J96-3004.xml,"{'0': 'A Stochastic Finite-State Word-Segmentation Algorithm for Chinese', '': ' We evaluate the system\'s performance by comparing its segmentation \'Tudgments"" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.', '1': 'Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TIS).', '2': 'An initial step of any textÂ\xad analysis task is the tokenization of the input into words.', '3': 'For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.', '4': ""Thus in an English sentence such as I'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces."", '5': ""A moment's reflection will reveal that things are not quite that simple."", '6': ""There are clearly eight orthographic words in the example given, but if one were doing syntactic analysis one would probably want to consider I'm to consist of two syntactic words, namely I and am."", '7': 'If one is interested in translation, one would probably want to consider show up as a single dictionary word since its semantic interpretation is not trivially derivable from the meanings of show and up.', '8': ""And if one is interested in TIS, one would probably consider the single orthographic word ACL to consist of three phonological words-lei s'i d/-corresponding to the pronunciation of each of the letters in the acronym."", '9': 'Space- or punctuation-delimited * 700 Mountain Avenue, 2d451, Murray Hill, NJ 07974, USA.', '10': 'Email: rlls@bell-labs.', '11': 'com t 700 Mountain Avenue, 2d451, Murray Hill, NJ 07974, USA.', '12': 'Email: cls@bell-labs.', '13': 'com t 600 Mountain Avenue, 2c278, Murray Hill, NJ 07974, USA.', '14': 'Email: gale@research.', '15': 'att.', '16': ""com Â§Cambridge, UK Email: nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries."", '17': 'In (b) is a plausible segmentation for this sentence; in (c) is an implausible segmentation.', '18': 'orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words.', '19': 'Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion ""orthographic word"" is not universal.', '20': 'Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ\xad ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.', '21': 'In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion ""word"" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ\xad thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).', '22': 'Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.', '23': 'All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..', '24': ""2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji.."", '25': '3 Throughout this paper we shall give Chinese examples in traditional orthography, followed.', '26': 'immediately by a Romanization into the pinyin transliteration scheme; numerals following each pinyin syllable represent tones.', '27': 'Examples will usually be accompanied by a translation, plus a morpheme-by-morpheme gloss given in parentheses whenever the translation does not adequately serve this purpose.', '28': 'In the pinyin transliterations a dash(-) separates syllables that may be considered part of the same phonological word; spaces are used to separate plausible phonological words; and a plus sign (+) is used, where relevant, to indicate morpheme boundaries of interest.', '29': ""raphy: A ren2 'person' is a fairly uncontroversial case of a monographemic word, and rplil zhong1guo2 (middle country) 'China' a fairly uncontroversial case of a diÂ\xad graphernic word."", '30': ""The relevance of the distinction between, say, phonological words and, say, dictionary words is shown by an example like rpftl_A :;!:Hfllil zhong1hua2 ren2min2 gong4he2-guo2 (China people republic) 'People's Republic of China.'"", '31': 'Arguably this consists of about three phonological words.', '32': 'On the other hand, in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English.', '33': 'Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.', '34': 'For example, suppose one is building a ITS system for Mandarin Chinese.', '35': 'For that application, at a minimum, one would want to know the phonological word boundaries.', '36': 'Now, for this application one might be tempted to simply bypass the segmentation problem and pronounce the text character-by-character.', '37': 'However, there are several reasons why this approach will not in general work: 1.', '38': 'Many hanzi have more than one pronunciation, where the correct.', '39': ""pronunciation depends upon word affiliation: tfJ is pronounced deO when it is a prenominal modification marker, but di4 in the word Â§tfJ mu4di4 'goal'; fl; is normally ganl 'dry,' but qian2 in a person's given name."", '40': 'including Third Tone Sandhi (Shih 1986), which changes a 3 (low) tone into a 2 (rising) tone before another 3 tone: \'j"";gil, xiao3 [lao3 shu3] \'little rat,\' becomes xiao3 { lao2shu3 ], rather than xiao2 { lao2shu3 ], because the rule first applies within the word lao3shu3 \'rat,\' blocking its phrasal application.', '41': '3.', '42': 'In various dialects of Mandarin certain phonetic rules apply at the word.', '43': 'level.', '44': ""For example, in Northern dialects (such as Beijing), a full tone (1, 2, 3, or 4) is changed to a neutral tone (0) in the final syllable of many words: Jll donglgual 'winter melon' is often pronounced donglguaO."", '45': 'The high 1 tone of J1l would not normally neutralize in this fashion if it were functioning as a word on its own.', '46': '4.', '47': 'TIS systems in general need to do more than simply compute the.', '48': 'pronunciations of individual words; they also need to compute intonational phrase boundaries in long utterances and assign relative prominence to words in those utterances.', '49': 'It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.', '50': 'Given that part-of-speech labels are properties of words rather than morphemes, it follows that one cannot do part-of-speech assignment without having access to word-boundary information.', '51': 'Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.', '52': ""The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval."", '53': 'There are thus some very good reasons why segmentation into words is an important task.', '54': 'A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.', '55': 'For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words.', '56': 'Among these are words derived by various productive processes, including: 1.', '57': 'Morphologically derived words such as, xue2shengl+men0.', '58': ""(student+plural) 'students,' which is derived by the affixation of the plural affix f, menD to the nounxue2shengl."", '59': '2.', '60': ""Personal names such as 00, 3R; zhoulenl-lai2 'Zhou Enlai.'"", '61': 'Of course, we.', '62': ""can expect famous names like Zhou Enlai's to be in many dictionaries, but names such as :fi lf;f; shi2jil-lin2, the name of the second author of this paper, will not be found in any dictionary."", '63': ""'Malaysia.'"", '64': ""Again, famous place names will most likely be found in the dictionary, but less well-known names, such as 1PMÂ± R; bu4lang3-shi4wei2-ke4 'Brunswick' (as in the New Jersey town name 'New Brunswick') will not generally be found."", '65': 'In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.', '66': ""The segmenter handles the grouping of hanzi into words and outputs word pronunciations, with default pronunciations for hanzi it cannot group; we focus here primarily on the system's ability to segment text appropriately (rather than on its pronunciation abilities)."", '67': 'The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.', '68': 'It also incorporates the Good-Turing method (Baayen 1989; Church and Gale 1991) in estimating the likelihoods of previously unseen conÂ\xad structions, including morphological derivatives and personal names.', '69': 'We will evaluate various specific aspects of the segmentation, as well as the overall segmentation perÂ\xad formance.', '70': 'This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.', '71': 'Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context.', '72': '2.', '73': 'A Brief Introduction to the Chinese Writing System Most readers will undoubtedly be at least somewhat familiar with the nature of the Chinese writing system, but there are enough common misunderstandings that it is as well to spend a few paragraphs on properties of the Chinese script that will be relevant to topics discussed in this paper.', '74': 'The first point we need to address is what type of linguistic object a hanzi repreÂ\xad sents.', '75': 'Much confusion has been sown about Chinese writing by the use of the term ideograph, suggesting that hanzi somehow directly represent ideas.', '76': 'The most accurate characterization of Chinese writing is that it is morphosyllabic (DeFrancis 1984): each hanzi represents one morpheme lexically and semantically, and one syllable phonologiÂ\xad cally.', '77': ""Thus in a two-hanzi word like lflli?J zhong1guo2 (middle country) 'China' there are two syllables, and at the same time two morphemes."", '78': ""Of course, since the number of attested (phonemic) Mandarin syllables (roughly 1400, including tonal distinctions) is far smaller than the number of morphemes, it follows that a given syllable could in principle be written with any of several different hanzi, depending upon which morpheme is intended: the syllable zhongl could be lfl 'middle,''clock,''end,' or ,'loyal.'"", '79': 'A morpheme, on the other hand, usually corresponds to a unique hanzi, though there are a few cases where variant forms are found.', '80': 'Finally, quite a few hanzi are homographs, meaning that they may be pronounced in several different ways, and in extreme cases apparently represent different morphemes: The prenominal modifiÂ\xad cation marker eg deO is presumably a different morpheme from the second morpheme of Â§eg mu4di4, even though they are written the same way.4 The second point, which will be relevant in the discussion of personal names in Section 4.4, relates to the internal structure of hanzi.', '81': ""Following the system devised under the Qing emperor Kang Xi, hanzi have traditionally been classified according to a set of approximately 200 semantic radicals; members of a radical class share a particular structural component, and often also share a common meaning (hence the term 'semantic')."", '82': ""For example, hanzi containing the INSECT radical !R tend to denote insects and other crawling animals; examples include tr wal 'frog,' feng1 'wasp,' and !Itt she2 'snake.'"", '83': ""Similarly, hanzi sharing the GHOST radical _m tend to denote spirits and demons, such as _m gui3 'ghost' itself, II: mo2 'demon,' and yan3 'nightmare.'"", '84': 'While the semantic aspect of radicals is by no means completely predictive, the semantic homogeneity of many classes is quite striking: for example 254 out of the 263 examples (97%) of the INSECT class listed by Wieger (1965, 77376) denote crawling or invertebrate animals; similarly 21 out of the 22 examples (95%) of the GHOST class (page 808) denote ghosts or spirits.', '85': 'As we shall argue, the semantic class affiliation of a hanzi constitutes useful information in predicting its properties.', '86': '3.', '87': 'Previous Work.', '88': 'There is a sizable literature on Chinese word segmentation: recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).', '89': 'Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÂ\xad cal rule-based approaches, and approaches that combine lexical information with staÂ\xad tistical information.', '90': 'The present proposal falls into the last group.', '91': 'Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.', '92': 'In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.', '93': 'Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.', '94': 'A related point is that mutual information is helpful in augmenting existing electronic dictionaries, (cf.', '95': '4 To be sure, it is not always true that a hanzi represents a syllable or that it represents a morpheme.', '96': 'For.', '97': ""example, in Northern Mandarin dialects there is a morpheme -r that attaches mostly to nouns, and which is phonologically incorporated into the syllable to which it attaches: thus men2+r (door+R) 'door' is realized as mer2."", '98': 'This is orthographically represented as 7C.', '99': ""so that 'door' would be and in this case the hanzi 7C, does not represent a syllable."", '100': ""Similarly, there is no compelling evidence that either of the syllables of f.ifflll binllang2 'betelnut' represents a morpheme, since neither can occur in any context without the other: more likely fjfflll binllang2 is a disyllabic morpheme."", '101': '(See Sproat and Shih 1995.)', '102': 'However, the characterization given in the main body of the text is correct sufficiently often to be useful.', '103': 'Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.', '104': 'Nonstochastic lexical-knowledge-based approaches have been much more numerÂ\xad ous.', '105': 'Two issues distinguish the various proposals.', '106': 'The first concerns how to deal with ambiguities in segmentation.', '107': 'The second concerns the methods used (if any) to exÂ\xad tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.', '108': 'The most popular approach to dealing with segÂ\xad mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.', '109': 'This method, one instance of which we term the ""greedy algorithm"" in our evaluation of our own system in Section 5, involves starting at the beginning (or end) of the sentence, finding the longest word starting (ending) at that point, and then repeating the process starting at the next (previous) hanzi until the end (beginÂ\xad ning) of the sentence is reached.', '110': 'Papers that use this method or minor variants thereof include Liang (1986), Li et al.', '111': '(1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).', '112': 'The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.', '113': 'Methods that allow multiple segmentations must provide criteria for choosing the best segmentation.', '114': 'Some approaches depend upon some form of conÂ\xad straint satisfaction based on syntactic or semantic features (e.g., Yeh and Lee [1991], which uses a unification-based approach).', '115': 'Others depend upon various lexical heurisÂ\xad tics: for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.', '116': 'Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).', '117': 'Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ\xad based scoring mechanism.', '118': 'Approaches differ in the algorithms used for scoring and selecting the best path, as well as in the amount of contextual information used in the scoring process.', '119': 'The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993).', '120': 'More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.', '121': 'Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.', '122': 'Several papers report the use of part-of-speech information to rank segmentations (Lin, Chiang, and Su 1993; Peng and Chang 1993; Chang and Chen 1993); typically, the probability of a segmentation is multiplied by the probability of the tagging(s) for that segmentation to yield an estimate of the total probability for the analysis.', '123': 'Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular seÂ\xad quence of hanzi might be a name, but that it is likely to be a name with some probabilÂ\xad ity.', '124': 'Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).', '125': 'Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ\xad tually tag the words as belonging to one or another class of expression.', '126': 'This is not ideal for some applications, however.', '127': 'For instance, for TTS it is necessary to know that a particular sequence of hanzi is of a particular category because that knowlÂ\xad edge could affect the pronunciation; consider, for example the issues surrounding the pronunciation of ganl I qian2 discussed in Section 1.', '128': 'Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.', '129': 'However, it is almost universally the case that no clear definition of what constitutes a ""correct"" segmentation is given, so these performance measures are hard to evaluate.', '130': 'Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.', '131': 'In a few cases, the criteria for correctness are made more explicit.', '132': 'For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.', '133': 'Besides the lack of a clear definition of what constitutes a correct segmentation for a given Chinese sentence, there is the more general issue that the test corpora used in these evaluations differ from system to system, so meaningful comparison between systems is rendered even more difficult.', '134': 'The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.', '135': 'The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.', '136': 'Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.', '137': 'Chinese word segmentation can be viewed as a stochastic transduction problem.', '138': 'More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ\xad ducer (WFST) (Pereira, Riley, and Sproat 1994).', '139': 'Let H be the set of hanzi, p be the set of pinyin syllables with tone marks, and P be the set of grammatical part-of-speech labels.', '140': 'Then each arc of D maps either from an element of H to an element of p, or from E-i.e., the empty string-to an element of P. More specifically, each word is represented in the dictionary as a sequence of arcs, starting from the initial state of D and labeled with an element 5 of Hxp, which is terminated with a weighted arc labeled with an element of Ex P. The weight represents the estimated cost (negative log probability) of the word.', '141': 'Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).', '142': 'We can 5 Recall that precision is defined to be the number of correct hits divided by the total number of items.', '143': 'selected; and that recall is defined to be the number of correct hits divided by the number of items that should have been selected.', '144': 'then define the best segmentation to be the cheapest or best path in Id(I) o D* (i.e., Id(I) composed with the transitive closure of 0).6 Consider the abstract example illustrated in Figure 2.', '145': 'In this example there are four ""input characters,"" A, B, C and D, and these map respectively to four ""pronunciations"" a, b, c and d. Furthermore, there are four ""words"" represented in the dictionary.', '146': 'These are shown, with their associated costs, as follows: ABj nc 4.0 AB C/jj 6.0 CD /vb 5.', '147': '0 D/ nc 5.0 The minimal dictionary encoding this information is represented by the WFST in Figure 2(a).', '148': 'An input ABCD can be represented as an FSA as shown in Figure 2(b).', '149': 'This FSA I can be segmented into words by composing Id(I) with D*, to form the WFST shown in Figure 2(c), then selecting the best path through this WFST to produce the WFST in Figure 2(d).', '150': 'This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.', '151': 'Since the segmentation corresponds to the sequence of words that has the lowest summed unigram cost, the segmenter under discussion here is a zeroth-order model.', '152': 'It is important to bear in mind, though, that this is not an inherent limitation of the model.', '153': 'For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)', '154': 'In Section 6 we disÂ\xad cuss other issues relating to how higher-order language models could be incorporated into the model.', '155': '4.1 Dictionary Representation.', '156': 'As we have seen, the lexicon of basic words and stems is represented as a WFST; most arcs in this WFST represent mappings between hanzi and pronunciations, and are costless.', '157': 'Each word is terminated by an arc that represents the transduction between f and the part of speech of that word, weighted with an estimated cost for that word.', '158': 'The cost is computed as follows, where N is the corpus size and f is the frequency: (1) Besides actual words from the base dictionary, the lexicon contains all hanzi in the Big 5 Chinese code/ with their pronunciation(s), plus entries for other characters that can be found in Chinese text, such as Roman letters, numerals, and special symbols.', '159': 'Note that hanzi that are not grouped into dictionary words (and are not identified as singleÂ\xad hanzi words), or into one of the other categories of words discussed in this paper, are left unattached and tagged as unknown words.', '160': 'Other strategies could readily 6 As a reviewer has pointed out, it should be made clear that the function for computing the best path is. an instance of the Viterbi algorithm.', '161': '7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong.', '162': 'It is. based on the traditional character set rather than the simplified character set used in Singapore and Mainland China.', '163': '(a) IDictionary D I D:d/0.000 B:b/0.000 B:b/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cps:nd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm.', '164': 'The transitive closure of the dictionary in (a) is composed with Id(input) (b) to form the WFST (c).', '165': 'The segmentation chosen is the best path through the WFST, shown in (d).', '166': '(In this figure eps is c) be implemented, though, such as a maximal-grouping strategy (as suggested by one reviewer of this paper); or a pairwise-grouping strategy, whereby long sequences of unattached hanzi are grouped into two-hanzi words (which may have some prosodic motivation).', '167': 'We have not to date explored these various options.', '168': 'Word frequencies are estimated by a re-estimation procedure that involves applyÂ\xad ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.', '169': 'newspaper material, but also including kungfu fiction, Buddhist tracts, and scientific material.', '170': 'This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used: in other words, derived words not in the base dictionary and personal and foreign names were not used.', '171': 'The best analysis of the corpus is taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated until it converges.', '172': 'Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods: in particÂ\xad ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.', '173': 'In any event, to date, we have not compared different methods for deriving the set of initial frequency estimates.', '174': 'Note also that the costs currently used in the system are actually string costs, rather than word costs.', '175': ""This is because our corpus is not annotated, and hence does not distinguish between the various words represented by homographs, such as, which could be /adv jiangl 'be about to' orInc jiang4 '(military) general'-as in 1j\\xiao3jiang4 'little general.'"", '176': 'In such cases we assign all of the estimated probability mass to the form with the most likely pronunciation (determined by inspection), and assign a very small probability (a very high cost, arbitrarily chosen to be 40) to all other variants.', '177': 'In the case of, the most common usage is as an adverb with the pronunciation jiangl, so that variant is assigned the estimated cost of 5.98, and a high cost is assigned to nominal usage with the pronunciation jiang4.', '178': 'The less favored reading may be selected in certain contexts, however; in the case of , for example, the nominal reading jiang4 will be selected if there is morphological information, such as a following plural affix ir, menD that renders the nominal reading likely, as we shall see in Section 4.3.', '179': ""Figure 3 shows a small fragment of the WFST encoding the dictionary, containing both entries forjust discussed, g:tÂ¥ zhonglhua2 min2guo2 (China Republic) 'Republic of China,' and iÂ¥inl."", '180': ""nan2gual 'pumpkin.'"", '181': ""4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X:Â¥ .:.S:P:l 'How do you say octopus in Japanese?' previously shown in Figure 1."", '182': ""As noted, this sentence consists of four words, namely B X ri4wen2 'Japanese,' :Â¥, zhanglyu2 'octopus/ :&P:l zen3me0 'how,' and IDt shuol 'say.'"", '183': ""As indicated in Figure 1(c), apart from this correct analysis, there is also the analysis taking B ri4 as a word (e.g., a common abbreviation for Japan), along with X:Â¥ wen2zhangl 'essay/ and f!!."", '184': ""yu2 'fish.'"", '185': 'Both of these analyses are shown in Figure 4; fortunately, the correct analysis is also the one with the lowest cost, so it is this analysis that is chosen.', '186': '4.3 Morphological Analysis.', '187': 'The method just described segments dictionary words, but as noted in Section 1, there are several classes of words that should be handled that are not found in a standard dictionary.', '188': 'One class comprises words derived by productive morphologiÂ\xad cal processes, such as plural noun formation using the suffix ir, menD.', '189': '(Other classes handled by the current system are discussed in Section 5.)', '190': 'The morphological analÂ\xadysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.', '191': 'each word in the lexicon whether or not each string is actually an instance of the word in question.', '192': 'Â£ : _ADV: 5.88 If:!', '193': "":zhong1 : 0.0 tjl :huo2 :0.0 (R:spub:/ic of Ch:ina) + .,_,...I : jlong4 :0.0 (mUifaty genG181) 0 Â£: _NC: 40.0 Figure 3 Partial Chinese Lexicon (NC = noun; NP = proper noun).c=- - I â\x80¢=- :il: .;ss:;zhangt â\x80¢ '-:."", '194': 'I â\x80¢ JAPANS :rl4 .Â·Â·Â·Â·Â·Â·Â·Â·Â·""\\)Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·""oÂ·\'Â·Â·Â·Â·Â·Â·Â·""\\:JÂ·Â·Â·Â·Â·Â·Â·Â·Â· Â·Â·Â·Â·Â·Â·Â·Â·Â·\'\\; . \'.:: ..........0 6.51 9.51 : jj / JAPANESE OCTOPUS 10Â·28iÂ£ :_nc HOW SAY f B :rl4 :il: :wen2 t \'- â\x80¢ :zhang!', '195': '!!:\\ :yu2 e:_nc [::!!:zen3 l!f :moO t:_adv il!:shuot ,:_vb i i i 1 â\x80¢ 10.03 13...', '196': '7.96 5.55 1 l...................................................................................................................................................................................................J..', '197': ""Figure 4 Input lattice (top) and two segmentations (bottom) of the sentence 'How do you say octopus in Japanese?'."", '198': 'A non-optimal analysis is shown with dotted lines in the bottom frame.', '199': 'ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t:-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.', '200': 'However, for our purposes it is not sufficient to repreÂ\xad sent the morphological decomposition of, say, plural nouns: we also need an estimate of the cost of the resulting word.', '201': 'For derived words that occur in our corpus we can estimate these costs as we would the costs for an underived dictionary entry.', '202': ""So, 1: f, xue2shengl+men0 (student+PL) 'students' occurs and we estimate its cost at 11.43; similarly we estimate the cost of f, jiang4+men0 (general+PL) 'generals' (as in 'J' f, xiao3jiang4+men0 'little generals'), at 15.02."", '203': ""But we also need an estimate of the probability for a non-occurring though possible plural form like iÂ¥JJ1l.f, nan2gua1-men0 'pumpkins.'"", '204': '10 Here we use the Good-Turing estimate (Baayen 1989; Church and Gale 1991), whereby the aggregate probability of previously unseen instances of a construction is estimated as ni/N, where N is the total number of observed tokens and n1 is the number of types observed only once.', '205': 'Let us notate the set of previously unseen, or novel, members of a category X as unseen(X); thus, novel members of the set of words derived in f, menO will be deÂ\xad noted unseen(f,).', '206': 'For irt the Good-Turing estimate just discussed gives us an estimate of p(unseen(f,) I f,)-the probability of observing a previously unseen instance of a construction in ft given that we know that we have a construction in f,.', '207': 'This GoodÂ\xad Turing estimate of p(unseen(f,) If,) can then be used in the normal way to define the probability of finding a novel instance of a construction in ir, in a text: p(unseen(f,)) = p(unseen(f,) I f,) p(fn Here p(ir,) is just the probability of any construction in ft as estimated from the frequency of such constructions in the corpus.', '208': 'Finally, asÂ\xad suming a simple bigram backoff model, we can derive the probability estimate for the particular unseen word iÂ¥1J1l.', '209': 'irL as the product of the probability estimate for iÂ¥JJ1l., and the probability estimate just derived for unseen plurals in ir,: p(iÂ¥1J1l.ir,) p(iÂ¥1J1l.)p(unseen(f,)).', '210': 'The cost estimate, cost(iÂ¥JJ1l.fn is computed in the obvious way by summing the negative log probabilities of iÂ¥JJ1l.', '211': 'and f,.', '212': 'Figure 5 shows how this model is implemented as part of the dictionary WFST.', '213': 'There is a (costless) transition between the NC node and f,.', '214': 'The transition from f, to a final state transduces c to the grammatical tag PL with cost cost(unseen(f,)): cost(iÂ¥JJ1l.ir,) == cost(iÂ¥JJ1l.)', '215': '+ cost(unseen(fm, as desired.', '216': ""For the seen word ir, 'genÂ\xad erals,' there is an c:NC transduction from to the node preceding ir,; this arc has cost cost( f,) - cost(unseen(f,)), so that the cost of the whole path is the desired cost( f,)."", '217': 'This representation gives ir, an appropriate morphological decomposition, preÂ\xad serving information that would be lost by simply listing ir, as an unanalyzed form.', '218': 'Note that the backoff model assumes that there is a positive correlation between the frequency of a singular noun and its plural.', '219': 'An analysis of nouns that occur in both the singular and the plural in our database reveals that there is indeed a slight but significant positive correlation-R2 = 0.20, p < 0.005; see Figure 6.', '220': 'This suggests that the backoff model is as reasonable a model as we can use in the absence of further information about the expected cost of a plural form.', '221': '10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.', '222': 'attaching to terms denoting human beings.', '223': ""However, it is possible to personify any noun, so in children's stories or fables, iÂ¥JJ1l."", '224': ""f, nan2gual+men0 'pumpkins' is by no means impossible."", '225': 'J:j:l :zhongl :0.0 ;m,Jlong4 :0.0 (mHHaryg9tltHBI) Â£: _ADV: 5.98 Â¥ :hua2:o.o E :_NC: 4.41 :mln2:o.o mm : guo2 : 0.0 (RopubllcofChlna) .....,.', '226': '0 Figure 5 An example of affixation: the plural affix.', '227': '4.4 Chinese Personal Names.', '228': 'Full Chinese personal names are in one respect simple: they are always of the form family+given.', '229': 'The family name set is restricted: there are a few hundred single-hanzi family names, and about ten double-hanzi ones.', '230': 'Given names are most commonly two hanzi long, occasionally one hanzi long: there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following: 1.', '231': 'wo rd => na m e 2.', '232': 'na me =>1 ha nzi fa mi ly 2 ha nzi gi ve n 3.', '233': 'na me =>1 ha nzi fa mi ly 1 ha nzi gi ve n 4.', '234': 'na me =>2 ha nzi fa mi ly 2 ha nzi gi ve n 5.', '235': 'na me =>2 ha nzi fa mi ly 1 ha nzi gi ve n 6.1 ha nzi fa mi ly => ha nz ii 7.2 ha nzi fa mi ly => ha nzi i ha nz ij 8.1 ha nzi gi ve n => ha nz ii 9.2 ha nzi giv en => ha nzi i ha nz ij The difficulty is that given names can consist, in principle, of any hanzi or pair of hanzi, so the possible given names are limited only by the total number of hanzi, though some hanzi are certainly far more likely than others.', '236': 'For a sequence of hanzi that is a possible name, we wish to assign a probability to that sequence qua name.', '237': 'We can model this probability straightforwardly enough with a probabilistic version of the grammar just given, which would assign probabilities to the individual rules.', '238': 'For example, given a sequence F1G1G2, where F1 is a legal single-hanzi family name, and Plural Nouns X g 0 g ""\' X X 0 T!i c""\'.', '239': '0 X u} ""\' o; .2 X X><X X XX X X X X X X x X X X X X x X V X X X X .;t\'*- XXX:OX X X X X X X 9 x X X XX XX X X X X X X X XXX:< X X>O<XX>!KXX XI<>< Â»C X X XX :X: X X ""\' X X XX >OO<X>D<XIK X X X X X X --XXÂ»: XXX X XÂ»C X XÂ«X...C:XXX X Xll< X X ><XX>IIC:liiC:oiiiiCI--8!X:liiOC!I!S8K X X X 10 100 1000 10000 log(F)_base: R""2=0.20 (p < 0.005) X 100000 Figure 6 Plot of log frequency of base noun, against log frequency of plural nouns.', '240': 'G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of: â\x80¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â\x80¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â\x80¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â\x80¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.', '241': '(1992).', '242': ""The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on."", '243': 'This model is easily incorporated into the segmenter by building a WFST restrictÂ\xad ing the names to the four licit types, with costs on the arcs for any particular name summing to an estimate of the cost of that name.', '244': ""This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model."", '245': ""There are two weaknesses in Chang et al.'s model, which we improve upon."", '246': 'First, the model assumes independence between the first and second hanzi of a double given name.', '247': ""Yet, some hanzi are far more probable in women's names than they are in men's names, and there is a similar list of male-oriented hanzi: mixing hanzi from these two lists is generally less likely than would be predicted by the independence model."", '248': 'As a partial solution, for pairs of hanzi that co-occur sufficiently often in our namelists, we use the estimated bigram cost, rather than the independence-based cost.', '249': 'The second weakness is purely conceptual, and probably does not affect the perÂ\xad formance of the model.', '250': 'For previously unseen hanzi in given names, Chang et al. assign a uniform small cost; but we know that some unseen hanzi are merely acciÂ\xad dentally missing, whereas others are missing for a reason-for example, because they have a bad connotation.', '251': 'As we have noted in Section 2, the general semantic class to which a hanzi belongs is often predictable from its semantic radical.', '252': 'Not surprisingly some semantic classes are better for names than others: in our corpora, many names are picked from the GRASS class but very few from the SICKNESS class.', '253': 'Other good classes include JADE and GOLD; other bad classes are DEATH and RAT.', '254': 'We can better predict the probability of an unseen hanzi occurring in a name by computing a within-class Good-Turing estimate for each radical class.', '255': ""Assuming unseen objects within each class are equiprobable, their probabilities are given by the Good-Turing theorem as: cis E( n'J.ls) Po oc N * E(N8ls) (2) where p815 is the probability of one unseen hanzi in class cls, E(n'J.15 ) is the expected number of hanzi in cls seen once, N is the total number of hanzi, and E(N(/ 5 ) is the expected number of unseen hanzi in class cls."", '256': 'The use of the Good-Turing equation presumes suitable estimates of the unknown expectations it requires.', '257': 'In the denomi 11 We have two such lists, one containing about 17,000 full names, and another containing frequencies of.', '258': 'hanzi in the various name positions, derived from a million names.', '259': ""12 One class of full personal names that this characterization does not cover are married women's names."", '260': ""where the husband's family name is optionally prepended to the woman's full name; thus ;f:*lf#i xu3lin2-yan2hai3 would represent the name that Ms. Lin Yanhai would take if she married someone named Xu."", '261': 'This style of naming is never required and seems to be losing currency.', '262': 'It is formally straightforward to extend the grammar to include these names, though it does increase the likelihood of overgeneration and we are unaware of any working systems that incorporate this type of name.', '263': 'We of course also fail to identify, by the methods just described, given names used without their associated family name.', '264': 'This is in general very difficult, given the extremely free manner in which Chinese given names are formed, and given that in these cases we lack even a family name to give the model confidence that it is identifying a name.', '265': 'Table 1 The cost as a novel given name (second position) for hanzi from various radical classes.', '266': 'JA DE G O L D G R AS S SI C K NE SS DE AT H R A T 14.', '267': '98 15.', '268': '52 15.', '269': '76 16.', '270': '25 16.', '271': '30 16.', '272': '42 nator, the N31s can be measured well by counting, and we replace the expectation by the observation.', '273': 'In the numerator, however, the counts of ni1s are quite irregular, inÂ\xad cluding several zeros (e.g., RAT, none of whose members were seen).', '274': 'However, there is a strong relationship between ni1s and the number of hanzi in the class.', '275': 'For E(ni1s), then, we substitute a smooth S against the number of class elements.', '276': 'This smooth guarantees that there are no zeroes estimated.', '277': 'The final estimating equation is then: (3) Since the total of all these class estimates was about 10% off from the Turing estimate n1/N for the probability of all unseen hanzi, we renormalized the estimates so that they would sum to n 1jN.', '278': 'This class-based model gives reasonable results: for six radical classes, Table 1 gives the estimated cost for an unseen hanzi in the class occurring as the second hanzi in a double GIVEN name.', '279': 'Note that the good classes JADE, GOLD and GRASS have lower costs than the bad classes SICKNESS, DEATH and RAT, as desired, so the trend observed for the results of this method is in the right direction.', '280': '4.5 Transliterations of Foreign Words.', '281': 'Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.', '282': 'Since foreign names can be of any length, and since their original pronunciation is effectively unlimited, the identiÂ\xad fication of such names is tricky.', '283': ""Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign: even a name like !:i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ\xad nese personal name, retains a foreign flavor because of liM."", '284': 'As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ\xad ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.', '285': 'As with personal names, we also derive an estimate from text of the probability of finding a transliterated name of any kind (PTN).', '286': 'Finally, we model the probability of a new transliterated name as the product of PTN and PTN(hanzi;) for each hanzi; in the putative name.13 The foreign name model is implemented as an WFST, which is then summed with the WFST implementing the dictionary, morpho 13 The current model is too simplistic in several respects.', '287': 'For instance, the common ""suffixes,"" -nia (e.g.,.', '288': 'Virginia) and -sia are normally transliterated as fbSi!', '289': 'ni2ya3 and @5:2 xilya3, respectively.', '290': 'The interdependence between fb or 1/!i, and 5:2 is not captured by our model, but this could easily be remedied.', '291': 'logical rules, and personal names; the transitive closure of the resulting machine is then computed.', '292': 'In this section we present a partial evaluation of the current system, in three parts.', '293': ""The first is an evaluation of the system's ability to mimic humans at the task of segmenting text into word-sized units; the second evaluates the proper-name identification; the third measures the performance on morphological analysis."", '294': 'To date we have not done a separate evaluation of foreign-name recognition.', '295': 'Evaluation of the Segmentation as a Whole.', '296': 'Previous reports on Chinese segmentation have invariably cited performance either in terms of a single percent-correct score, or else a single precision-recall pair.', '297': 'The problem with these styles of evaluation is that, as we shall demonstrate, even human judges do not agree perfectly on how to segment a given text.', '298': 'Thus, rather than give a single evaluative score, we prefer to compare the performance of our method with the judgments of several human subjects.', '299': 'To this end, we picked 100 sentences at random containing 4,372 total hanzi from a test corpus.14 (There were 487 marks of punctuation in the test sentences, including the sentence-final periods, meaning that the average inter-punctuation distance was about 9 hanzi.)', '300': 'We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.', '301': 'Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part, the instructions were simple: subjects were to mark all places they might plausibly pause if they were reading the text aloud.', '302': ""An examination of the subjects' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units."", '303': '(See also Wu and Fung [1994].)', '304': 'Various segmentation approaches were then compared with human performance: 1.', '305': 'A greedy algorithm (or maximum-matching algorithm), GR: proceed through the sentence, taking the longest match with a dictionary entry at each point.', '306': '2.', '307': 'An anti-greedy algorithm, AG: instead of the longest match, take the.', '308': 'shortest match at each point.', '309': '3.', '310': 'The method being described-henceforth ST..', '311': 'Two measures that can be used to compare judgments are: 1.', '312': 'Precision.', '313': 'For each pair of judges consider one judge as the standard,.', '314': ""computing the precision of the other's judgments relative to this standard."", '315': '2.', '316': 'Recall.', '317': 'For each pair of judges, consider one judge as the standard,.', '318': ""computing the recall of the other's judgments relative to this standard."", '319': 'Clearly, for judges h and h taking h as standard and computing the precision and recall for Jz yields the same results as taking h as the standard, and computing for h, 14 All evaluation materials, with the exception of those used for evaluating personal names were drawn.', '320': 'from the subset of the United Informatics corpus not used in the training of the models.', '321': 'Table 2 Similarity matrix for segmentation judgments.', '322': 'Jud ges A G G R ST M 1 M 2 M 3 T1 T2 T3 AG 0.7 0 0.7 0 0 . 4 3 0.4 2 0.6 0 0.6 0 0.6 2 0.5 9 GR 0.9 9 0 . 6 2 0.6 4 0.7 9 0.8 2 0.8 1 0.7 2 ST 0 . 6 4 0.6 7 0.8 0 0.8 4 0.8 2 0.7 4 M1 0.7 7 0.6 9 0.7 1 0.6 9 0.7 0 M2 0.7 2 0.7 3 0.7 1 0.7 0 M3 0.8 9 0.8 7 0.8 0 T1 0.8 8 0.8 2 T2 0.7 8 respectively, the recall and precision.', '323': 'We therefore used the arithmetic mean of each interjudge precision-recall pair as a single measure of interjudge similarity.', '324': 'Table 2 shows these similarity measures.', '325': 'The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ\xad tance matrix, and plotting the first two most significant dimensions.', '326': 'The result of this is shown in Figure 7.', '327': 'The horizontal axis in this plot represents the most significant dimension, which explains 62% of the variation.', '328': 'In addition to the automatic methods, AG, GR, and ST, just discussed, we also added to the plot the values for the current algorithm using only dictionary entries (i.e., no productively derived words or names).', '329': 'This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.', '330': 'As can be seen, GR and this ""pared-down"" statistical method perform quite similarly, though the statistical method is still slightly better.16 AG clearly performs much less like humans than these methods, whereas the full statistical algorithm, including morphological derivatives and names, performs most closely to humans among the automatic methods.', '331': 'It can also be seen clearly in this plot that two of the Taiwan speakers cluster very closely together, and the third TaiÂ\xad wan speaker is also close in the most significant dimension (the x axis).', '332': 'Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers.', '333': 'The breakdown of the different types of words found by ST in the test corpus is given in Table 3.', '334': 'Clearly the percentage of productively formed words is quite small (for this particular corpus), meaning that dictionary entries are covering most of the 15 GR is .73 or 96%..', '335': '16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.', '336': 'That is, given a choice between segmenting a sequence abc into abc and ab, c, the former will always be picked so long as its cost does not exceed the summed costs of ab and c: while; it is possible for abc to be so costly as to preclude the larger grouping, this will certainly not usually be the case.', '337': 'In this way, the method reported on here will necessarily be similar to a greedy method, though of course not identical.', '338': 'As the reviewer also points out, this is a problem that is shared by, e.g., probabilistic context-free parsers, which tend to pick trees with fewer nodes.', '339': 'The question is how to normalize the probabilities in such a way that smaller groupings have a better shot at winning.', '340': 'This is an issue that we have not addressed at the current stage of our research.', '341': 'i..f,..', '342': '""c\' 0 + 0 ""0 \' â\x80¢ + a n t i g r e e d y x g r e e d y < > c u r r e n t m e t h o d o d i e t . o n l y â\x80¢ Taiwan 0 Â·;; 0 c CD E i5 0""\' 9 9 â\x80¢ Mainland â\x80¢ â\x80¢ â\x80¢ â\x80¢ -0.30.20.1 0.0 0.1 0.2 Dimension 1 (62%) Figure 7 Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions.', '343': 'The percentage scores on the axis labels represent the amount of variation in the data explained by the dimension in question.', '344': 'Table 3 Classes of words found by ST for the test corpus.', '345': 'Word type N % Dic tion ary entr ies 2 , 5 4 3 9 7 . 4 7 Mor pho logi call y deri ved wor ds 3 0 . 1 1 Fore ign tran slite rati ons 9 0 . 3 4 Per son al na mes 5 4 2 . 0 7 cases.', '346': 'Nonetheless, the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words.', '347': 'It may seem surprising to some readers that the interhuman agreement scores reported here are so low.', '348': 'However, this result is consistent with the results of exÂ\xad periments discussed in Wu and Fung (1994).', '349': 'Wu and Fung introduce an evaluation method they call nk-blind.', '350': 'Under this scheme, n human judges are asked independently to segment a text.', '351': 'Their results are then compared with the results of an automatic segmenter.', '352': 'For a given ""word"" in the automatic segmentation, if at least k of the huÂ\xad man judges agree that this is a word, then that word is considered to be correct.', '353': 'For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.', '354': 'Proper-Name Identification.', '355': 'To evaluate proper-name identification, we randomly seÂ\xad lected 186 sentences containing 12,000 hanzi from our test corpus and segmented the text automatically, tagging personal names; note that for names, there is always a sinÂ\xad gle unambiguous answer, unlike the more general question of which segmentation is correct.', '356': 'The performance was 80.99% recall and 61.83% precision.', '357': 'Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus: seemingly, our system finds as many names as their system, but with four times as many false hits.', '358': ""However, we have reason to doubt Chang et al.'s performance claims."", '359': 'Without using the same test corpus, direct comparison is obviously difficult; fortunately, Chang et al. include a list of about 60 sentence fragments that exemplify various categories of performance for their system.', '360': 'The performance of our system on those sentences apÂ\xad peared rather better than theirs.', '361': 'On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.', '362': 'However, they list two sets, one consisting of 28 fragments and the other of 22 fragments, in which they had 0% recall and precision.', '363': 'On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.', '364': 'Note that it is in precision that our overÂ\xad all performance would appear to be poorer than the reported performance of Chang et al., yet based on their published examples, our system appears to be doing better precisionwise.', '365': 'Thus we have some confidence that our own performance is at least as good as that of Chang et al.', '366': '(1992).', '367': ""In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system."", '368': 'Fortunately, we were able to obtain a copy of the full set of sentences from Chang et al. on which Wang, Li, and Chang tested their system, along with the output of their system.18 In what follows we will discuss all cases from this set where our performance on names differs from that of Wang, Li, and Chang.', '369': 'Examples are given in Table 4.', '370': 'In these examples, the names identified by the two systems (if any) are underlined; the sentence with the correct segmentation is boxed.19 The differences in performance between the two systems relate directly to three issues, which can be seen as differences in the tuning of the models, rather than repreÂ\xad senting differences in the capabilities of the model per se.', '371': 'The first issue relates to the completeness of the base lexicon.', '372': ""The Wang, Li, and Chang system fails on fragment (b) because their system lacks the word youlyoul 'soberly' and misinterpreted the thus isolated first youl as being the final hanzi of the preceding name; similarly our system failed in fragment (h) since it is missing the abbreviation i:lJI!"", '373': ""tai2du2 'Taiwan Independence.'"", '374': 'This is a rather important source of errors in name identifiÂ\xad cation, and it is not really possible to objectively evaluate a name recognition system without considering the main lexicon with which it is used.', '375': ""17 They also provide a set of title-driven rules to identify names when they occur before titles such as $t. 1: xianlshengl 'Mr.' or i:l:itr!J tai2bei3 shi4zhang3 'Taipei Mayor.'"", '376': 'Obviously, the presence of a title after a potential name N increases the probability that N is in fact a name.', '377': 'Our system does not currently make use of titles, but it would be straightforward to do so within the finite-state framework that we propose.', '378': '18 We are grateful to ChaoHuang Chang for providing us with this set.', '379': ""Note that Wang, Li, and Chang's."", '380': 'set was based on an earlier version of the Chang et a!.', '381': 'paper, and is missing 6 examples from the A set.', '382': ""19 We note that it is not always clear in Wang, Li, and Chang's examples which segmented words."", '383': 'constitute names, since we have only their segmentation, not the actual classification of the segmented words.', '384': 'Therefore in cases where the segmentation is identical between the two systems we assume that tagging is also identical.', '385': 'Table 4 Differences in performance between our system and Wang, Li, and Chang (1992).', '386': 'Our System Wang, Li, and Chang a. 1\\!f!IP Eflltii /1\\!f!J:P $1til I b. agm: I a m: c. 5 Bf is Bf 1 d. ""*:t: w _t ff 1 ""* :t: w_tff 1 g., , Transliteration/Translation chen2zhongl-shenl qu3 \'music by Chen Zhongshen \' huang2rong2 youlyoul de dao4 \'Huang Rong said soberly\' zhangl qun2 Zhang Qun xian4zhang3 you2qingl shang4ren2 hou4 \'after the county president You Qing had assumed the position\' lin2 quan2 \'Lin Quan\' wang2jian4 \'Wang Jian\' oulyang2-ke4 \'Ouyang Ke\' yinl qi2 bu4 ke2neng2 rong2xu3 tai2du2 er2 \'because it cannot permit Taiwan Independence so\' silfa3-yuan4zhang3 lin2yang2-gang3 \'president of the Judicial Yuan, Lin Yanggang\' lin2zhangl-hu2 jiangl zuo4 xian4chang3 jie3shuol \'Lin Zhanghu will give an exÂ\xad planation live\' jin4/iang3 nian2 nei4 sa3 xia4 de jinlqian2 hui4 ting2zhi3 \'in two years the distributed money will stop\' gaoltangl da4chi2 ye1zi0 fen3 \'chicken stock, a tablespoon of coconut flakes\' you2qingl ru4zhu3 xian4fu3 lwu4 \'after You Qing headed the county government\' Table 5 Performance on morphological analysis.', '387': 'Affix Pron Base category N found N missed (recall) N correct (precision) t,-,7 The second issue is that rare family names can be responsible for overgeneration, especially if these names are otherwise common as single-hanzi words.', '388': ""For example, the Wang, Li, and Chang system fails on the sequence 1:f:p:]nian2 nei4 sa3 in (k) since 1F nian2 is a possible, but rare, family name, which also happens to be written the same as the very common word meaning 'year.'"", '389': 'Our system fails in (a) because of$ shenl, a rare family name; the system identifies it as a family name, whereas it should be analyzed as part of the given name.', '390': 'Finally, the statistical method fails to correctly group hanzi in cases where the individual hanzi comprising the name are listed in the dictionary as being relatively high-frequency single-hanzi words.', '391': 'An example is in (i), where the system fails to group t;,f;?""$?t!: lin2yang2gang3 as a name, because all three hanzi can in principle be separate words (t;,f; lin2 \'wood\';?""$ yang2 \'ocean\'; ?t!; gang3 \'harbor\').', '392': 'In many cases these failures in recall would be fixed by having better estimates of the actual probÂ\xad abilities of single-hanzi words, since our estimates are often inflated.', '393': ""A totally nonÂ\xad stochastic rule-based system such as Wang, Li, and Chang's will generally succeed in such cases, but of course runs the risk of overgeneration wherever the single-hanzi word is really intended."", '394': 'Evaluation of Morphological Analysis.', '395': 'In Table 5 we present results from small test corÂ\xad pora for the productive affixes handled by the current version of the system; as with names, the segmentation of morphologically derived words is generally either right or wrong.', '396': ""The first four affixes are so-called resultative affixes: they denote some propÂ\xad erty of the resultant state of a verb, as in E7 wang4bu4-liao3 (forget-not-attain) 'cannot forget.'"", '397': 'The last affix in the list is the nominal plural f, men0.20 In the table are the (typical) classes of words to which the affix attaches, the number found in the test corpus by the method, the number correct (with a precision measure), and the number missed (with a recall measure).', '398': 'In this paper we have argued that Chinese word segmentation can be modeled efÂ\xad fectively using weighted finite-state transducers.', '399': 'This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.', '400': 'Other kinds of productive word classes, such as company names, abbreviations (termed fijsuolxie3 in Mandarin), and place names can easily be 20 Note that 7 in E 7 is normally pronounced as leO, but as part of a resultative it is liao3..', '401': 'handled given appropriate models.', '402': '(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)', '403': 'We have argued that the proposed method performs well.', '404': 'However, some caveats are in order in comparing this method (or any method) with other approaches to segÂ\xad mentation reported in the literature.', '405': 'First of all, most previous articles report perforÂ\xad mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall.', '406': 'What both of these approaches presume is that there is a sinÂ\xad gle correct segmentation for a sentence, against which an automatic algorithm can be compared.', '407': 'We have shown that, at least given independent human judgments, this is not the case, and that therefore such simplistic measures should be mistrusted.', '408': 'This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca.', '409': 'May 1995).', '410': 'However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion.', '411': 'Second, comparisons of different methods are not meaningful unless one can evalÂ\xad uate them on the same corpus.', '412': 'Unfortunately, there is no standard corpus of Chinese texts, tagged with either single or multiple human judgments, with which one can compare performance of various methods.', '413': 'One hopes that such a corpus will be forthÂ\xad coming.', '414': 'Finally, we wish to reiterate an important point.', '415': 'The major problem for our segÂ\xad menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).', '416': 'We have provided methods for handling certain classes of unknown words, and models for other classes could be provided, as we have noted.', '417': 'However, there will remain a large number of words that are not readily adduced to any producÂ\xad tive pattern and that would simply have to be added to the dictionary.', '418': 'This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.', '419': 'The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model: the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.', '420': 'However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.', '421': 'For example, as Gan (1994) has noted, one can construct examples where the segmenÂ\xad tation is locally ambiguous but can be determined on the basis of sentential or even discourse context.', '422': ""Two sets of examples from Gan are given in (1) and (2) (:::::: Gan's Appendix B, exx."", '423': 'lla/llb and 14a/14b respectively).', '424': 'In (1) the sequencema3lu4 cannot be resolved locally, but depends instead upon broader context; similarly in (2), the sequence :::tcai2neng2 cannot be resolved locally: 1.', '425': ""(a) 1 Â§ . ;m t 7 leO z h e 4 pil m a 3 lu 4 sh an g4 bi ng 4 t h i s CL (assi fier) horse w ay on sic k A SP (ec t) 'This horse got sick on the way' (b) 1Â§: . til y zhe4 tiao2 ma3lu4 hen3 shao3 this CL road very few 'Very few cars pass by this road' :$ chel jinglguo4 car pass by 2."", '426': ""(a) I f f fi * fi :1 }'l ij 1Â§: {1M m m s h e n 3 m e 0 shi2 ho u4 wo 3 cai2 ne ng 2 ke4 fu 2 zh e4 ge 4 ku n4 w h a t ti m e I just be abl e ov er co m e thi s C L dif fic 'When will I be able to overcome this difficulty?'"", '427': ""(b) 89 :1 t& tal de cai2neng2 hen3 he DE talent very 'He has great talent' f.b ga ol hig h While the current algorithm correctly handles the (b) sentences, it fails to handle the (a) sentences, since it does not have enough information to know not to group the sequences.ma3lu4 and?]cai2neng2 respectively."", '428': ""Gan's solution depends upon a fairly sophisticated language model that attempts to find valid syntactic, semantic, and lexical relations between objects of various linguistic types (hanzi, words, phrases)."", '429': 'An example of a fairly low-level relation is the affix relation, which holds between a stem morpheme and an affix morpheme, such as f1 -menD (PL).', '430': 'A high-level relation is agent, which relates an animate nominal to a predicate.', '431': 'Particular instances of relations are associated with goodness scores.', '432': 'Particular relations are also consistent with particular hypotheses about the segmentation of a given sentence, and the scores for particular relations can be incremented or decremented depending upon whether the segmentations with which they are consistent are ""popular"" or not.', '433': ""While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable."", '434': 'Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper.', '435': 'For the examples given in (1) and (2) this certainly seems possible.', '436': 'Consider first the examples in (2).', '437': ""The segmenter will give both analyses :1 cai2 neng2 'just be able,' and ?]cai2neng2 'talent,' but the latter analysis is preferred since splitting these two morphemes is generally more costly than grouping them."", '438': ""In (2a), we want to split the two morphemes since the correct analysis is that we have the adverb :1 cai2 'just,' the modal verb neng2 'be able' and the main verb R: Hke4fu2 'overcome'; the competing analysis is, of course, that we have the noun :1 cai2neng2 'talent,' followed by }'lijke4fu2 'overcome.'"", '439': 'Clearly it is possible to write a rule that states that if an analysis Modal+ Verb is available, then that is to be preferred over Noun+ Verb: such a rule could be stated in terms of (finite-state) local grammars in the sense of Mohri (1993).', '440': ""Turning now to (1), we have the similar problem that splitting.into.ma3 'horse' andlu4 'way' is more costly than retaining this as one word .ma3lu4 'road.'"", '441': ""However, there is again local grammatical information that should favor the split in the case of (1a): both .ma3 'horse' and .ma3 lu4 are nouns, but only .ma3 is consistent with the classifier pil, the classifier for horses.21 By a similar argument, the preference for not splitting , lm could be strengthened in (lb) by the observation that the classifier 1'1* tiao2 is consistent with long or winding objects like , lm ma3lu4 'road' but not with,ma3 'horse.'"", '442': 'Note that the sets of possible classifiers for a given noun can easily be encoded on that noun by grammatical features, which can be referred to by finite-state grammatical rules.', '443': ""Thus, we feel fairly confident that for the examples we have considered from Gan's study a solution can be incorporated, or at least approximated, within a finite-state framework."", '444': 'With regard to purely morphological phenomena, certain processes are not hanÂ\xad dled elegantly within the current framework Any process involving reduplication, for instance, does not lend itself to modeling by finite-state techniques, since there is no way that finite-state networks can directly implement the copying operations required.', '445': 'Mandarin exhibits several such processes, including A-not-A question formation, ilÂ\xad lustrated in (3a), and adverbial reduplication, illustrated in (3b): 3.', '446': ""(a) ;IE shi4 'be' => ;IE;IE shi4bu2-shi4 (be-not-be) 'is it?'"", '447': 'JI!', '448': ""gaolxing4 'happy' => F.i'JF.i'J Jl!"", '449': ""gaolbu4-gaolxing4 (hap-not-happy) 'happy?'"", '450': ""(b) F.i'JJI!"", '451': ""gaolxing4 'happy'=> F.i'JF.i'JJI!JI!"", '452': ""gaolgaolxing4xing4 'happily' In the particular form of A-not-A reduplication illustrated in (3a), the first syllable of the verb is copied, and the negative markerbu4 'not' is inserted between the copy and the full verb."", '453': 'In the case of adverbial reduplication illustrated in (3b) an adjective of the form AB is reduplicated as AABB.', '454': 'The only way to handle such phenomena within the framework described here is simply to expand out the reduplicated forms beforehand, and incorporate the expanded forms into the lexical transducer.', '455': 'Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.', '456': 'The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way.', '457': 'The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system: presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.', '458': 'As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.', '459': 'Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).', '460': 'Since the transducers are built from human-readable descriptions using a lexical toolkit (Sproat 1995), the system is easily maintained and extended.', '461': 'While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.', '462': '21 In Chinese, numerals and demonstratives cannot modify nouns directly, and must be accompanied by.', '463': 'a classifier.', '464': 'The particular classifier used depends upon the noun.', '465': 'Mohri [1995]) shows promise for improving this situation.', '466': 'The model described here thus demonstrates great potential for use in widespread applications.', '467': 'This flexibility, along with the simplicity of implementation and expansion, makes this framework an attractive base for continued research.', '468': ""We thank United Informatics for providing us with our corpus of Chinese text, and BDC for the 'Behavior ChineseEnglish Electronic Dictionary.'"", '469': 'We further thank Dr. J.-S.', '470': 'Chang of Tsinghua University, Taiwan, R.O.C., for kindly providing us with the name corpora.', '471': 'We also thank ChaoHuang Chang, reviewers for the 1994 ACL conference, and four anonymous reviewers for Computational Linguistics for useful comments.'}",['J96-3004'],['../data/summaries/J96-3004.txt'],['../data/tba/J96-3004.json']
N01-1011,A Decision Tree of Bigrams is an Accurate Predictor of Word Sense,../data/papers/N01-1011.xml,"{'0': 'A Decision Tree of Bigrams is an Accurate Predictor of Word Sense', '1': 'This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.', '2': 'This approach is evaluated using the sense-tagged corpora from the 1998 SENSEVAL word sense disambiguation exercise.', '3': 'It is more accurate than the average results reported for 30 of 36 words, and is more accurate than the best results for 19 of 36 words.', '4': 'Word sense disambiguation is the process of selecting the most appropriate meaning for a word, based on the context in which it occurs.', '5': 'For our purposes it is assumed that the set of possible meanings, i.e., the sense inventory, has already been determined.', '6': 'For example, suppose bill has the following set of possible meanings: a piece of currency, pending legislation, or a bird jaw.', '7': 'When used in the context of The Senate bill is under consideration, a human reader immediately understands that bill is being used in the legislative sense.', '8': 'However, a computer program attempting to perform the same task faces a diÆcult problem since it does not have the bene?t of innate common{sense or linguistic knowledge.', '9': 'Rather than attempting to provide computer programs with real{world knowledge comparable to that of humans, natural language processing has turned to corpus{based methods.', '10': 'These approaches use techniques from statistics and machine learning to induce models of language usage from large samples of text.', '11': 'These models are trained to perform particular tasks, usually via supervised learning.', '12': 'This paper describes an approach where a decision tree is learned from some number of sentences where each instance of an ambiguous word has been manually annotated with a sense{tag that denotes the most appropriate sense for that context.', '13': 'Prior to learning, the sense{tagged corpus must be converted into a more regular form suitable for automatic processing.', '14': 'Each sense{tagged occurrence of an ambiguous word is converted into a feature vector, where each feature represents some property of the surrounding text that is considered to be relevant to the disambiguation process.', '15': 'Given the exibility and complexity of human language, there is potentially an in?nite set of features that could be utilized.', '16': 'However, in corpus{based approaches features usually consist of information that can be readily iden- ti?ed in the text, without relying on extensive external knowledge sources.', '17': 'These typically include the part{of{speech of surrounding words, the presence of certain key words within some window of context, and various syntactic properties of the sentence and the ambiguous word.', '18': 'The approach in this paper relies upon a feature set made up of bigrams, two word sequences that occur in a text.', '19': 'The context in which an ambiguous word occurs is represented by some number of binary features that indicate whether or not a particular bigram has occurred within approximately 50 words to the left or right of the word being disambiguated.', '20': 'We take this approach since surface lexical features like bigrams, collocations, and co{occurrences often contribute a great deal to disambiguation accuracy.', '21': 'It is not clear how much disambiguation accuracy is improved through the use of features that are identi?ed by more complex pre{processing such as part{of{speech tagging, parsing, or anaphora resolution.', '22': 'One of our objectives is to establish a clear upper bounds on the accuracy of disambiguation using feature sets that do not impose substantial pre{ processing requirements.', '23': 'This paper continues with a discussion of our methods for identifying the bigrams that should be included in the feature set for learning.', '24': 'Then the decision tree learning algorithm is described, as are some benchmark learning algorithms that are included for purposes of comparison.', '25': 'The experimental data is discussed, and then the empirical results are presented.', '26': 'We close with an analysis of our ?ndings and a discussion of related work.', '27': 'We have developed an approach to word sense disambiguation that represents text entirely in terms of the occurrence of bigrams, which we de?ne to be two cat :cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 :big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1: Representation of Bigram Counts consecutive words that occur in a text.', '28': 'The distributional characteristics of bigrams are fairly consistent across corpora; a majority of them only occur one time.', '29': 'Given the sparse and skewed nature of this data, the statistical methods used to select interesting bigrams must be carefully chosen.', '30': 'We explore two alternatives, the power divergence family of goodness of ?t statistics and the Dice CoeÆcient, an information theoretic measure related to point- wise Mutual Information.', '31': 'Figure 1 summarizes the notation for word and bigram counts used in this paper by way of a 2 ? 2 contingency table.', '32': 'The value of n 11 shows how many times the bigram big cat occurs in the corpus.', '33': 'The value of n 12 shows how often bigrams occur where big is the ?rst word and cat is not the second.', '34': 'The counts in n +1 and n 1+ indicate how often words big and cat occur as the ?rst and second words of any bigram in the corpus.', '35': 'The total number of bigrams in the corpus is represented by n ++ . 2.1 The Power Divergence Family.', '36': '(Cressie and Read, 1984) introduce the power divergence family of goodness of ?t statistics.', '37': ""A number of well known statistics belong to this family, including the likelihood ratio statisticG 2 and Pearson'sX 2 statistic."", '38': 'These measure the divergence of the observed (n ij ) and expected (m ij ) bigram counts, where m ij is estimated based on the assumption that the component words in the bigram occur together strictly by chance.', '39': '(Dunning, 1993) argues in favor of G2 over X2, especially when dealing with very sparse and skewed data distributions.', '40': ""However, (Cressie and Read, 1984) suggest that there are cases where Pearson's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other."", '41': ""In light of this, (Pedersen, 1996) presents Fisher's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson's test and the likelihood ratio."", '42': 'Unfortunately it is usually not clear which test is most appropriate for a particular sample of data.', '43': 'We take the following approach, based on the observation that all tests should assign approximately the same measure of statistical signi?cance when the bi- gram counts in the contingency table do not violate any of the distributional assumptions that underly the goodness of ?t statistics.', '44': ""We perform tests using X 2 , G 2 , and Fisher's exact test for each bigram."", '45': 'If the resulting measures of statistical signi?cance di?er, then the distribution of the bigram counts is causing at least one of the tests to become unreliable.', '46': ""When this occurs we rely upon the value from Fisher's exact test since it makes fewer assumptions about the underlying distribution of data."", '47': 'For the experiments in this paper, we identi?ed the top 100 ranked bigrams that occur more than 5 times in the training corpus associated with a word.', '48': ""There were no cases where rankings produced by G 2 , X 2 , and Fisher's exact test disagreed, which is not altogether surprising given that low frequency bigrams were excluded."", '49': 'Since all of these statistics produced the same rankings, hereafter we make no distinction among them and simply refer to them generically as the power divergence statistic.', '50': '2.2 Dice CoeÆcient.', '51': 'The Dice CoeÆcient is a descriptive statistic that provides a measure of association among two words in a corpus.', '52': 'It is similar to pointwise Mutual Information, a widely used measure that was ?rst introduced for identifying lexical relationships in (Church and Hanks, 1990).', '53': 'Pointwise Mutual Information can be de?ned as follows: MI(w 1 ; w 2 ) = log 2 n 11 ? n ++ n +1 ? n 1+ where w 1 and w 2 represent the two words that make up the bigram.', '54': 'Pointwise Mutual Information quanti?es how often two words occur together in a bigram (the numerator) relative to how often they occur overall in the corpus (the denominator).', '55': 'However, there is a curious limitation to pointwise Mutual Information.', '56': 'A bigram w 1 w 2 that occurs n 11 times in the corpus, and whose component words w 1 and w 2 only occur as a part of that bigram, will result in increasingly strong measures of association as the value of n 11 decreases.', '57': 'Thus, the maximum pointwise Mutual Information in a given corpus will be assigned to bi- grams that occur one time, and whose component words never occur outside that bigram.', '58': 'These are usually not the bigrams that prove most useful for disambiguation, yet they will dominate a ranked list as determined by pointwise Mutual Information.', '59': 'The Dice CoeÆcient overcomes this limitation, and can be de?ned as follows: Dice(w 1 ; w 2 ) = 2 ? n 11 n +1 + n 1+ When n 11 = n 1+ = n +1 the value of Dice(w 1 ; w 2 ) will be 1 for all values n 11 . When the value of n. 11 is less than either of the marginal totals (the more typical case) the rankings produced by the Dice Co- eÆcient are similar to those of Mutual Information.', '60': 'The relationship between pointwise Mutual Information and the Dice CoeÆcient is also discussed in (Smadja et al., 1996).', '61': 'We have developed the Bigram Statistics Package to produce ranked lists of bigrams using a range of tests.', '62': 'This software is written in Perl and is freely available from www.d.umn.edu/~tpederse.', '63': 'Decision trees are among the most widely used machine learning algorithms.', '64': 'They perform a general to speci?c search of a feature space, adding the most informative features to a tree structure as the search proceeds.', '65': 'The objective is to select a minimal set of features that eÆciently partitions the feature space into classes of observations and assemble them into a tree.', '66': 'In our case, the observations are manually sense{tagged examples of an ambiguous word in context and the partitions correspond to the di?erent possible senses.', '67': 'Each feature selected during the search process is represented by a node in the learned decision tree.', '68': 'Each node represents a choice point between a number of di?erent possible values for a feature.', '69': 'Learning continues until all the training examples are accounted for by the decision tree.', '70': 'In general, such a tree will be overly speci?c to the training data and not generalize well to new examples.', '71': 'Therefore learning is followed by a pruning step where some nodes are eliminated or reorganized to produce a tree that can generalize to new circumstances.', '72': 'Test instances are disambiguated by ?nding a path through the learned decision tree from the root to a leaf node that corresponds with the observed features.', '73': 'An instance of an ambiguous word is dis- ambiguated by passing it through a series of tests, where each test asks if a particular bigram occurs in the available window of context.', '74': 'We also include three benchmark learning algorithms in this study: the majority classi?er, the decision stump, and the Naive Bayesian classi?er.', '75': 'The majority classi?er assigns the most common sense in the training data to every instance in the test data.', '76': 'A decision stump is a one node decision tree(Holte, 1993) that is created by stopping the decision tree learner after the single most informative feature is added to the tree.', '77': 'The Naive Bayesian classi?er (Duda and Hart, 1973) is based on certain blanket assumptions about the interactions among features in a corpus.', '78': 'There is no search of the feature space performed to build a representative model as is the case with decision trees.', '79': 'Instead, all features are included in the classi- ?er and assumed to be relevant to the task at hand.', '80': 'There is a further assumption that each feature is conditionally independent of all other features, given the sense of the ambiguous word.', '81': 'It is most often used with a bag of words feature set, where every word in the training sample is represented by a binary feature that indicates whether or not it occurs in the window of context surrounding the ambiguous word.', '82': 'We use the Weka (Witten and Frank, 2000) implementations of the C4.5 decision tree learner (known as J48), the decision stump, and the Naive Bayesian classi?er.', '83': 'Weka is written in Java and is freely available from www.cs.waikato.ac.nz/~ml.', '84': 'Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems.', '85': 'Ten teams participated in the supervised learning portion of this event.', '86': 'Additional details about the exercise, including the data and results referred to in this paper, can be found at the SENSEVAL web site (www.itri.bton.ac.uk/events/senseval/) and in (Kilgarri?', '87': 'and Palmer, 2000).', '88': 'We included all 36 tasks from SENSEVAL for which training and test data were provided.', '89': 'Each task requires that the occurrences of a particular word in the test data be disambiguated based on a model learned from the sense{tagged instances in the training data.', '90': 'Some words were used in multiple tasks as di?erent parts of speech.', '91': 'For example, there were two tasks associated with bet, one for its use as a noun and the other as a verb.', '92': 'Thus, there are 36 tasks involving the disambiguation of 29 di?erent words.', '93': 'The words and part of speech associated with each task are shown in Table 1 in column 1.', '94': 'Note that the parts of speech are encoded as n for noun, a for adjective, v for verb, and p for words where the part of speech was not provided.', '95': 'The number of test and training instances for each task are shown in columns 2 and 4.', '96': 'Each instance consists of the sentence in which the ambiguous word occurs as well as one or two surrounding sentences.', '97': 'In general the total context available for each ambiguous word is less than 100 surrounding words.', '98': 'The number of distinct senses in the test data for each task is shown in column 3.', '99': 'The following process is repeated for each task.', '100': 'Capitalization and punctuation are removed from the training and test data.', '101': 'Two feature sets are selected from the training data based on the top 100 ranked bigrams according to the power divergence statistic and the Dice CoeÆcient.', '102': 'The bigram must have occurred 5 or more times to be included as a feature.', '103': 'This step ?lters out a large number of possible bi- grams and allows the decision tree learner to focus on a small number of candidate bigrams that are likely to be helpful in the disambiguation process.', '104': 'The training and test data are converted to feature vectors where each feature represents the occurrence of one of the bigrams that belong in the feature set.', '105': 'This representation of the training data is the actual input to the learning algorithms.', '106': 'Decision tree and decision stump learning is performed twice, once using the feature set determined by the power divergence statistic and again using the feature set identi?ed by the Dice CoeÆcient.', '107': 'The majority classi?er simply determines the most frequent sense in the training data and assigns that to all instances in the test data.', '108': 'The Naive Bayesian classi?er is based on a feature set where every word that occurs 5 or more times in the training data is included as a feature.', '109': 'All of these learned models are used to disambiguate the test data.', '110': 'The test data is kept separate until this stage.', '111': 'We employ a ?ne grained scoring method, where a word is counted as correctly disambiguated only when the assigned sense tag exactly matches the true sense tag.', '112': 'No partial credit is assigned for near misses.', '113': 'The accuracy attained by each of the learning algorithms is shown in Table 1.', '114': 'Column 5 reports the accuracy of the majority classi?er, columns 6 and 7 show the best and average accuracy reported by the 10 participating SENSEVAL teams.', '115': 'The evaluation at SENSEVAL was based on precision and recall, so we converted those scores to accuracy by taking their product.', '116': 'However, the best precision and recall may have come from di?erent teams, so the best accuracy shown in column 6 may actually be higher than that of any single participating SENSEVAL system.', '117': 'The average accuracy in column 7 is the product of the average precision and recall reported for the participating SENSEVAL teams.', '118': 'Column 8 shows the accuracy of the decision tree using the J48 learning algorithm and the features identi?ed by a power divergence statistic.', '119': 'Column 10 shows the accuracy of the decision tree when the Dice CoeÆcient selects the features.', '120': 'Columns 9 and 11 show the accuracy of the decision stump based on the power divergence statistic and the Dice CoeÆcient respectively.', '121': 'Finally, column 13 shows the accuracy of the Naive Bayesian classi?er based on a bag of words feature set.', '122': 'The most accurate method is the decision tree based on a feature set determined by the power divergence statistic.', '123': 'The last line of Table 1 shows the win-tie-loss score of the decision tree/power divergence method relative to every other method.', '124': 'A win shows it was more accurate than the method in the column, a loss means it was less accurate, and a tie means it was equally accurate.', '125': 'The decision tree/power divergence method was more accurate than the best reported SENSEVAL results for 19 of the 36 tasks, and more accurate for 30 of the 36 tasks when compared to the average reported accuracy.', '126': 'The decision stumps also fared well, proving to be more accurate than the best SENSEVAL results for 14 of the 36 tasks.', '127': 'In general the feature sets selected by the power divergence statistic result in more accurate decision trees than those selected by the Dice CoeÆcient.', '128': 'The power divergence tests prove to be more reliable since they account for all possible events surrounding two words w 1 and w 2 ; when they occur as bigram w 1 w 2 , when w 1 or w 2 occurs in a bigram without the other, and when a bigram consists of neither.', '129': 'The Dice CoeÆcient is based strictly on the event where w 1 and w 2 occur together in a bigram.', '130': 'There are 6 tasks where the decision tree / power divergence approach is less accurate than the SENSEVAL average; promise-n, scrap-n, shirt-n, amaze- v, bitter-p, and sanction-p.', '131': 'The most dramatic difference occurred with amaze-v, where the SENSE- VAL average was 92.4% and the decision tree accuracy was 58.6%.', '132': 'However, this was an unusual task where every instance in the test data belonged to a single sense that was a minority sense in the training data.', '133': 'The characteristics of the decision trees and decision stumps learned for each word are shown in Table 2.', '134': 'Column 1 shows the word and part of speech.', '135': 'Columns 2, 3, and 4 are based on the feature set selected by the power divergence statistic while columns 5, 6, and 7 are based on the Dice CoeÆ- cient.', '136': 'Columns 2 and 5 show the node selected to serve as the decision stump.', '137': 'Columns 3 and 6 show the number of leaf nodes in the learned decision tree relative to the number of total nodes.', '138': 'Columns 4 and 7 show the number of bigram features selected Table 1: Experimental Results.', '139': 'This table shows that there is little di?erence in the decision stump nodes selected from feature sets determined by the power divergence statistics versus the Dice CoeÆcient.', '140': 'This is to be expected since the top ranked bigrams for each measure are consistent, and the decision stump node is generally chosen from among those.', '141': 'However, there are di?erences between the feature sets selected by the power divergence statistics and the Dice CoeÆcient.', '142': 'These are re ected in the different sized trees that are learned based on these feature sets.', '143': 'The number of leaf nodes and the total number of nodes for each learned tree is shown in columns 3 and 6.', '144': 'The number of internal nodes is simply the di?erence between the total nodes and the leaf nodes.', '145': 'Each leaf node represents the end of a path through the decision tree that makes a sense distinction.', '146': 'Since a bigram feature can only appear once in the decision tree, the number of inter- Table 2: Decision Tree and Stump Characteristics power divergence dice coeÆcient (1) (2) (3) (4) (5) (6) (7) word-pos stump node leaf/total features stump node leaf/total features accident-n by accident 8/15 101 by accident 12/23 112 behaviour-n best behaviour 2/3 100 best behaviour 2/3 104 bet-n betting shop 20/39 50 betting shop 20/39 50 excess-n in excess 13/25 104 in excess 11/21 102 oat-n the oat 7/13 13 the oat 7/13 13 giant-n the giants 16/31 103 the giants 14/27 78 knee-n knee injury 23/45 102 knee injury 20/39 104 onion-n in the 1/1 7 in the 1/1 7 promise-n promise of 95/189 100 a promising 49/97 107 sack-n the sack 5/9 31 the sack 5/9 31 scrap-n scrap of 7/13 8 scrap of 7/13 8 shirt-n shirt and 38/75 101 shirt and 55/109 101 amaze-v amazed at 11/21 102 amazed at 11/21 102 bet-v i bet 4/7 10 i bet 4/7 10 bother-v be bothered 19/37 101 be bothered 20/39 106 bury-v buried in 28/55 103 buried in 32/63 103 calculate-v calculated to 5/9 103 calculated to 5/9 103 consume-v on the 4/7 20 on the 4/7 20 derive-v derived from 10/19 104 derived from 10/19 104 oat-v oated on 24/47 80 oated on 24/47 80 invade-v to invade 55/109 107 to invade 66/127 108 promise-v promise to 3/5 100 promise you 5/9 106 sack-v return to 1/1 91 return to 1/1 91 scrap-v of the 1/1 7 of the 1/1 7 seize-v to seize 26/51 104 to seize 57/113 104 brilliant-a a brilliant 26/51 101 a brilliant 42/83 103 oating-a in the 7/13 10 in the 7/13 10 generous-a a generous 57/113 103 a generous 56/111 102 giant-a the giant 2/3 102 a giant 1/1 101 modest-a a modest 14/27 101 a modest 10/19 105 slight-a the slightest 2/3 105 the slightest 2/3 105 wooden-a wooden spoon 2/3 104 wooden spoon 2/3 101 band-p band of 14/27 100 the band 21/41 117 bitter-p a bitter 22/43 54 a bitter 22/43 54 sanction-p south africa 12/23 52 south africa 12/23 52 shake-p his head 90/179 100 his head 81/161 105 nal nodes represents the number of bigram features selected by the decision tree learner.', '147': 'One of our original hypotheses was that accurate decision trees of bigrams will include a relatively small number of features.', '148': 'This was motivated by the success of decision stumps in performing disambiguation based on a single bigram feature.', '149': 'In these experiments, there were no decision trees that used all of the bigram features identi?ed by the ?ltering step, and for many words the decision tree learner went on to eliminate most of the candidate features.', '150': 'This can be seen by comparing the number of internal nodes with the number of candidate features as shown in columns 4 or 7.', '151': '1 It is also noteworthy that the bigrams ultimately selected by the decision tree learner for inclusion in the tree do not always include those bigrams ranked most highly by the power divergence statistic or the Dice CoeÆcient.', '152': 'This is to be expected, since the selection of the bigrams from raw text is only mea1 For most words the 100 top ranked bigrams form the set of candidate features presented to the decision tree learner.', '153': 'If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bi- grams that occurred more than 5 times then all such bigrams are included in the feature set.', '154': 'suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses.', '155': 'In particular, the decision tree learner makes decisions as to what bigram to include as nodes in the tree using the gain ratio, a measure based on the overall Mutual Information between the bigram and a particular word sense.', '156': 'Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods.', '157': 'A decision tree with 1 leaf node and no internal nodes (1/1) acts as a majority classi?er.', '158': 'A decision tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump.', '159': 'One of our long-term objectives is to identify a core set of features that will be useful for disambiguating a wide class of words using both supervised and unsupervised methodologies.', '160': 'We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classi?ers, each based on co{ occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line.', '161': 'While the accuracy of this approach was as good as any previously published results, the learned models were complex and diÆcult to interpret, in e?ect acting as very accurate black boxes.', '162': 'Our experience has been that variations in learning algorithms are far less signi?cant contributors to disambiguation accuracy than are variations in the feature set.', '163': 'In other words, an informative feature set will result in accurate disambiguation when used with a wide range of learning algorithms, but there is no learning algorithm that can perform well given an uninformative or misleading set of features.', '164': 'Therefore, our focus is on developing and discovering feature sets that make distinctions among word senses.', '165': 'Our learning algorithms must not only produce accurate models, but they should also shed new light on the relationships among features and allow us to continue re?ning and understanding our feature sets.', '166': 'We believe that decision trees meet these criteria.', '167': 'A wide range of implementations are available, and they are known to be robust and accurate across a range of domains.', '168': 'Most important, their structure is easy to interpret and may provide insights into the relationships that exist among features and more general rules of disambiguation.', '169': 'Bigrams have been used as features for word sense disambiguation, particularly in the form of collocations where the ambiguous word is one component of the bigram (e.g., (Bruce and Wiebe, 1994), (Ng and Lee, 1996), (Yarowsky, 1995)).', '170': 'While some of the bigrams we identify are collocations that include the word being disambiguated, there is no requirement that this be the case.', '171': 'Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)).', '172': 'In the former they were used with the bag of word feature sets and in the latter they were used with a mixed feature set that included the part-of-speech of neighboring words, three collocations, and the morphology of the ambiguous word.', '173': 'We believe that the approach in this paper is the ?rst time that decision trees based strictly on bigram features have been employed.', '174': 'The decision list is a closely related approach that has also been applied to word sense disambiguation (e.g., (Yarowsky, 1994), (Wilks and Stevenson, 1998), (Yarowsky, 2000)).', '175': 'Rather than building and traversing a tree to perform disambiguation, a list is employed.', '176': 'In the general case a decision list may suffer from less fragmentation during learning than decision trees; as a practical matter this means that the decision list is less likely to be over{trained.', '177': 'However, we believe that fragmentation also re ects on the feature set used for learning.', '178': 'Ours consists of at most approximately 100 binary features.', '179': 'This results in a relatively small feature space that is not as likely to su?er from fragmentation as are larger spaces.', '180': 'There are a number of immediate extensions to this work.', '181': 'The ?rst is to ease the requirement that bi- grams be made up of two consecutive words.', '182': 'Rather, we will search for bigrams where the component words may be separated by other words in the text.', '183': 'The second is to eliminate the ?ltering step by which candidate bigrams are selected by a power divergence statistic.', '184': 'Instead, the decision tree learner would consider all possible bigrams.', '185': 'Despite increasing the danger of fragmentation, this is an interesting issue since the bigrams judged most informative by the decision tree learner are not always ranked highly in the ?ltering step.', '186': 'In particular, we will determine if the ?ltering process ever eliminates bi- grams that could be signi?cant sources of disambiguation information.', '187': 'In the longer term, we hope to adapt this approach to unsupervised learning, where disambiguation is performed without the bene?t of sense tagged text.', '188': 'We are optimistic that this is viable, since bigram features are easy to identify in raw text.', '189': 'This paper shows that the combination of a simple feature set made up of bigrams and a standard decision tree learning algorithm results in accurate word sense disambiguation.', '190': 'The results of this approach are compared with those from the 1998 SENSEVAL word sense disambiguation exercise and show that the bigram based decision tree approach is more accurate than the best SENSEVAL results for 19 of 36 words.', '191': 'The Bigram Statistics Package has been implemented by Satanjeev Banerjee, who is supported by a Grant{in{Aid of Research, Artistry and Scholarship from the OÆce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota.', '192': 'We would like to thank the SENSEVAL organizers for making the data and results from the 1998 event freely available.', '193': 'The comments of three anonymous reviewers were very helpful in preparing the ?nal version of this paper.', '194': 'A preliminary version of this paper appears in (Pedersen, 2001).'}",['N01-1011'],['../data/summaries/N01-1011.txt'],['../data/tba/N01-1011.json']
N04-1038,Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution,../data/papers/N04-1038.xml,"{'0': 'Unsupervised Learning of Contextual Role Knowledge for Coreference Resolution', '1': 'We present a coreference resolver called BABAR that uses contextual role knowledge to evaluate possible antecedents for an anaphor.', '2': 'BABAR uses information extraction patterns to identify contextual roles and creates four contextual role knowledge sources using unsupervised learning.', '3': 'These knowledge sources determine whether the contexts surrounding an anaphor and antecedent are compatible.', '4': 'BABAR applies a DempsterShafer probabilistic model to make resolutions based on evidence from the contextual role knowledge sources as well as general knowledge sources.', '5': 'Experiments in two domains showed that the contextual role knowledge improved coreference performance, especially on pronouns.', '6': 'The problem of coreference resolution has received considerable attention, including theoretical discourse models (e.g., (Grosz et al., 1995; Grosz and Sidner, 1998)), syntactic algorithms (e.g., (Hobbs, 1978; Lappin and Le- ass, 1994)), and supervised machine learning systems (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).', '7': 'Most computational models for coreference resolution rely on properties of the anaphor and candidate antecedent, such as lexical matching, grammatical and syntactic features, semantic agreement, and positional information.', '8': 'The focus of our work is on the use of contextual role knowledge for coreference resolution.', '9': 'A contextual role represents the role that a noun phrase plays in an event or relationship.', '10': 'Our work is motivated by the observation that contextual roles can be critically important in determining the referent of a noun phrase.', '11': 'Consider the following sentences: (a) Jose Maria Martinez, Roberto Lisandy, and Dino Rossy, who were staying at a Tecun Uman hotel, were kidnapped by armed men who took them to an unknown place.', '12': '(b) After they were released...', '13': '(c) After they blindfolded the men...', '14': 'In (b) â\x80\x9ctheyâ\x80\x9d refers to the kidnapping victims, but in (c) â\x80\x9ctheyâ\x80\x9d refers to the armed men.', '15': 'The role that each noun phrase plays in the kidnapping event is key to distinguishing these cases.', '16': 'The correct resolution in sentence (b) comes from knowledge that people who are kidnapped are often subsequently released.', '17': 'The correct resolution in sentence (c) depends on knowledge that kidnappers frequently blindfold their victims.', '18': 'We have developed a coreference resolver called BABAR that uses contextual role knowledge to make coreference decisions.', '19': 'BABAR employs information extraction techniques to represent and learn role relationships.', '20': 'Each pattern represents the role that a noun phrase plays in the surrounding context.', '21': 'BABAR uses unsupervised learning to acquire this knowledge from plain text without the need for annotated training data.', '22': 'Training examples are generated automatically by identifying noun phrases that can be easily resolved with their antecedents using lexical and syntactic heuristics.', '23': 'BABAR then computes statistics over the training examples measuring the frequency with which extraction patterns and noun phrases co-occur in coreference resolutions.', '24': 'In this paper, Section 2 begins by explaining how contextual role knowledge is represented and learned.', '25': 'Section 3 describes the complete coreference resolution model, which uses the contextual role knowledge as well as more traditional coreference features.', '26': 'Our coreference resolver also incorporates an existential noun phrase recognizer and a DempsterShafer probabilistic model to make resolution decisions.', '27': 'Section 4 presents experimen tal results on two corpora: the MUC4 terrorism corpus, and Reuters texts about natural disasters.', '28': 'Our results show that BABAR achieves good performance in both domains, and that the contextual role knowledge improves performance, especially on pronouns.', '29': 'Finally, Section 5 explains how BABAR relates to previous work, and Section 6 summarizes our conclusions.', '30': 'In this section, we describe how contextual role knowledge is represented and learned.', '31': 'Section 2.1 describes how BABAR generates training examples to use in the learning process.', '32': 'We refer to this process as Reliable Case Resolution because it involves finding cases of anaphora that can be easily resolved with their antecedents.', '33': 'Section 2.2 then describes our representation for contextual roles and four types of contextual role knowledge that are learned from the training examples.', '34': '2.1 Reliable Case Resolutions.', '35': 'The first step in the learning process is to generate training examples consisting of anaphor/antecedent resolutions.', '36': 'BABAR uses two methods to identify anaphors that can be easily and reliably resolved with their antecedent: lexical seeding and syntactic seeding.', '37': '2.1.1 Lexical Seeding It is generally not safe to assume that multiple occurrences of a noun phrase refer to the same entity.', '38': 'For example, the company may refer to Company X in one paragraph and Company Y in another.', '39': 'However, lexically similar NPs usually refer to the same entity in two cases: proper names and existential noun phrases.', '40': 'BABAR uses a named entity recognizer to identify proper names that refer to people and companies.', '41': 'Proper names are assumed to be coreferent if they match exactly, or if they closely match based on a few heuristics.', '42': 'For example, a personâ\x80\x99s full name will match with just their last name (e.g., â\x80\x9cGeorge Bushâ\x80\x9d and â\x80\x9cBushâ\x80\x9d), and a company name will match with and without a corporate suffix (e.g., â\x80\x9cIBM Corp.â\x80\x9d and â\x80\x9cIBMâ\x80\x9d).', '43': 'Proper names that match are resolved with each other.', '44': 'The second case involves existential noun phrases (Allen, 1995), which are noun phrases that uniquely specify an object or concept and therefore do not need a prior referent in the discourse.', '45': 'In previous work (Bean and Riloff, 1999), we developed an unsupervised learning algorithm that automatically recognizes definite NPs that are existential without syntactic modification because their meaning is universally understood.', '46': 'For example, a story can mention â\x80\x9cthe FBIâ\x80\x9d, â\x80\x9cthe White Houseâ\x80\x9d, or â\x80\x9cthe weatherâ\x80\x9d without any prior referent in the story.', '47': 'Although these existential NPs do not need a prior referent, they may occur multiple times in a document.', '48': 'By definition, each existential NP uniquely specifies an object or concept, so we can infer that all instances of the same existential NP are coreferent (e.g., â\x80\x9cthe FBIâ\x80\x9d always refers to the same entity).', '49': 'Using this heuristic, BABAR identifies existential definite NPs in the training corpus using our previous learning algorithm (Bean and Riloff, 1999) and resolves all occurrences of the same existential NP with each another.1 2.1.2 Syntactic Seeding BABAR also uses syntactic heuristics to identify anaphors and antecedents that can be easily resolved.', '50': 'Table 1 briefly describes the seven syntactic heuristics used by BABAR to resolve noun phrases.', '51': 'Words and punctuation that appear in brackets are considered optional.', '52': 'The anaphor and antecedent appear in boldface.', '53': '1.', '54': 'Reflexive pronouns with only 1 NP in scope..', '55': 'Ex: The regime gives itself the right...', '56': '2.', '57': 'Relative pronouns with only 1 NP in scope..', '58': 'Ex: The brigade, which attacked ...', '59': 'Ex: Mr. Cristiani is the president ...', '60': 'Ex: The government said it ...', '61': 'Ex: He was found in San Jose, where ...', '62': 'Ex: Mr. Cristiani, president of the country ...', '63': 'Ex: Mr. Bush disclosed the policy by reading it...', '64': 'Table 1: Syntactic Seeding Heuristics BABARâ\x80\x99s reliable case resolution heuristics produced a substantial set of anaphor/antecedent resolutions that will be the training data used to learn contextual role knowledge.', '65': 'For terrorism, BABAR generated 5,078 resolutions: 2,386 from lexical seeding and 2,692 from syntactic seeding.', '66': 'For natural disasters, BABAR generated 20,479 resolutions: 11,652 from lexical seeding and 8,827 from syntactic seeding.', '67': '2.2 Contextual Role Knowledge.', '68': 'Our representation of contextual roles is based on information extraction patterns that are converted into simple caseframes.', '69': 'First, we describe how the caseframes are represented and learned.', '70': 'Next, we describe four contextual role knowledge sources that are created from the training examples and the caseframes.', '71': '2.2.1 The Caseframe Representation Information extraction (IE) systems use extraction patterns to identify noun phrases that play a specific role in 1 Our implementation only resolves NPs that occur in the same document, but in retrospect, one could probably resolve instances of the same existential NP in different documents too.', '72': 'an event.', '73': 'For IE, the system must be able to distinguish between semantically similar noun phrases that play different roles in an event.', '74': 'For example, management succession systems must distinguish between a person who is fired and a person who is hired.', '75': 'Terrorism systems must distinguish between people who perpetrate a crime and people who are victims of a crime.', '76': 'We applied the AutoSlog system (Riloff, 1996) to our unannotated training texts to generate a set of extraction patterns for each domain.', '77': 'Each extraction pattern represents a linguistic expression and a syntactic position indicating where a role filler can be found.', '78': 'For example, kidnapping victims should be extracted from the subject of the verb â\x80\x9ckidnappedâ\x80\x9d when it occurs in the passive voice (the shorthand representation of this pattern would be â\x80\x9c<subject> were kidnappedâ\x80\x9d).', '79': 'The types of patterns produced by AutoSlog are outlined in (Riloff, 1996).', '80': 'Ideally weâ\x80\x99d like to know the thematic role of each extracted noun phrase, but AutoSlog does not generate thematic roles.', '81': 'As a (crude) approximation, we normalize the extraction patterns with respect to active and passive voice and label those extractions as agents or patients.', '82': 'For example, the passive voice pattern â\x80\x9c<subject> were kidnappedâ\x80\x9d and the active voice pattern â\x80\x9ckidnapped <direct object>â\x80\x9d are merged into a single normalized pattern â\x80\x9ckidnapped <patient>â\x80\x9d.2 For the sake of sim plicity, we will refer to these normalized extraction patterns as caseframes.3 These caseframes can capture two types of contextual role information: (1) thematic roles corresponding to events (e.g, â\x80\x9c<agent> kidnappedâ\x80\x9d or â\x80\x9ckidnapped <patient>â\x80\x9d), and (2) predicate-argument relations associated with both verbs and nouns (e.g., â\x80\x9ckidnapped for <np>â\x80\x9d or â\x80\x9cvehicle with <np>â\x80\x9d).', '83': 'We generate these caseframes automatically by running AutoSlog over the training corpus exhaustively so that it literally generates a pattern to extract every noun phrase in the corpus.', '84': 'The learned patterns are then normalized and applied to the corpus.', '85': 'This process produces a large set of caseframes coupled with a list of the noun phrases that they extracted.', '86': 'The contextual role knowledge that BABAR uses for coreference resolution is derived from this caseframe data.', '87': '2.2.2 The Caseframe Network The first type of contextual role knowledge that BABAR learns is the Caseframe Network (CFNet), which identifies caseframes that co-occur in anaphor/antecedent resolutions.', '88': 'Our assumption is that caseframes that co-occur in resolutions often have a 2 This normalization is performed syntactically without semantics, so the agent and patient roles are not guaranteed to hold, but they usually do in practice.', '89': '3 These are not full case frames in the traditional sense, but they approximate a simple case frame with a single slot.', '90': 'conceptual relationship in the discourse.', '91': 'For example, co-occurring caseframes may reflect synonymy (e.g., â\x80\x9c<patient> kidnappedâ\x80\x9d and â\x80\x9c<patient> abductedâ\x80\x9d) or related events (e.g., â\x80\x9c<patient> kidnappedâ\x80\x9d and â\x80\x9c<patient> releasedâ\x80\x9d).', '92': 'We do not attempt to identify the types of relationships that are found.', '93': 'BABAR merely identifies caseframes that frequently co-occur in coreference resolutions.', '94': 'Te rro ris m Na tur al Dis ast ers mu rde r of < NP > kill ed <p atie nt > <a ge nt > da ma ged wa s inj ure d in < NP > <a ge nt > rep ort ed <a ge nt > add ed <a ge nt > occ urr ed cau se of < NP > <a ge nt > stat ed <a ge nt > add ed <a ge nt > wr eak ed <a ge nt > cro sse d per pet rat ed <p atie nt > con de mn ed <p atie nt > dri ver of < NP > <a ge nt > car ryi ng Figure 1: Caseframe Network Examples Figure 1 shows examples of caseframes that co-occur in resolutions, both in the terrorism and natural disaster domains.', '95': 'The terrorism examples reflect fairly obvious relationships: people who are murdered are killed; agents that â\x80\x9creportâ\x80\x9d things also â\x80\x9caddâ\x80\x9d and â\x80\x9cstateâ\x80\x9d things; crimes that are â\x80\x9cperpetratedâ\x80\x9d are often later â\x80\x9ccondemnedâ\x80\x9d.', '96': 'In the natural disasters domain, agents are often forces of nature, such as hurricanes or wildfires.', '97': 'Figure 1 reveals that an event that â\x80\x9cdamagedâ\x80\x9d objects may also cause injuries; a disaster that â\x80\x9coccurredâ\x80\x9d may be investigated to find its â\x80\x9ccauseâ\x80\x9d; a disaster may â\x80\x9cwreakâ\x80\x9d havoc as it â\x80\x9ccrossesâ\x80\x9d geographic regions; and vehicles that have a â\x80\x9cdriverâ\x80\x9d may also â\x80\x9ccarryâ\x80\x9d items.', '98': 'During coreference resolution, the caseframe network provides evidence that an anaphor and prior noun phrase might be coreferent.', '99': 'Given an anaphor, BABAR identifies the caseframe that would extract it from its sentence.', '100': 'For each candidate antecedent, BABAR identifies the caseframe that would extract the candidate, pairs it with the anaphorâ\x80\x99s caseframe, and consults the CF Network to see if this pair of caseframes has co-occurred in previous resolutions.', '101': 'If so, the CF Network reports that the anaphor and candidate may be coreferent.', '102': '2.2.3 Lexical Caseframe Expectations The second type of contextual role knowledge learned by BABAR is Lexical Caseframe Expectations, which are used by the CFLex knowledge source.', '103': 'For each case- frame, BABAR collects the head nouns of noun phrases that were extracted by the caseframe in the training corpus.', '104': 'For each resolution in the training data, BABAR also associates the co-referring expression of an NP with the NPâ\x80\x99s caseframe.', '105': 'For example, if X and Y are coreferent, then both X and Y are considered to co-occur with the caseframe that extracts X as well as the caseframe that extracts Y. We will refer to the set of nouns that co-occur with a caseframe as the lexical expectations of the case- frame.', '106': 'Figure 2 shows examples of lexical expectations that were learned for both domains.', '107': 'collected too.', '108': 'We will refer to the semantic classes that co-occur with a caseframe as the semantic expectations of the caseframe.', '109': 'Figure 3 shows examples of semantic expectations that were learned.', '110': 'For example, BABAR learned that agents that â\x80\x9cassassinateâ\x80\x9d or â\x80\x9cinvestigate a causeâ\x80\x9d are usually humans or groups (i.e., organizations).', '111': 'T e r r o r i s m Ca sef ra me Semantic Classes <a ge nt > ass ass ina ted group, human inv esti gat ion int o < N P> event exp lod ed out sid e < N P> building N a t u r a l D i s a s t e r s Ca sef ra me Semantic Classes <a ge nt > inv esti gat ing cau se group, human sur viv or of < N P> event, natphenom hit wit h < N P> attribute, natphenom Figure 3: Semantic Caseframe Expectations Figure 2: Lexical Caseframe Expectations To illustrate how lexical expectations are used, suppose we want to determine whether noun phrase X is the antecedent for noun phrase Y. If they are coreferent, then X and Y should be substitutable for one another in the story.4 Consider these sentences: (S1) Fred was killed by a masked man with a revolver.', '112': '(S2) The burglar fired the gun three times and fled.', '113': 'â\x80\x9cThe gunâ\x80\x9d will be extracted by the caseframe â\x80\x9cfired <patient>â\x80\x9d.', '114': 'Its correct antecedent is â\x80\x9ca revolverâ\x80\x9d, which is extracted by the caseframe â\x80\x9ckilled with <NP>â\x80\x9d.', '115': 'If â\x80\x9cgunâ\x80\x9d and â\x80\x9crevolverâ\x80\x9d refer to the same object, then it should also be acceptable to say that Fred was â\x80\x9ckilled with a gunâ\x80\x9d and that the burglar â\x80\x9cfireda revolverâ\x80\x9d.', '116': 'During coreference resolution, BABAR checks (1) whether the anaphor is among the lexical expectations for the caseframe that extracts the candidate antecedent, and (2) whether the candidate is among the lexical expectations for the caseframe that extracts the anaphor.', '117': 'If either case is true, then CFLex reports that the anaphor and candidate might be coreferent.', '118': '2.2.4 Semantic Caseframe Expectations The third type of contextual role knowledge learned by BABAR is Semantic Caseframe Expectations.', '119': 'Semantic expectations are analogous to lexical expectations except that they represent semantic classes rather than nouns.', '120': 'For each caseframe, BABAR collects the semantic classes associated with the head nouns of NPs that were extracted by the caseframe.', '121': 'As with lexical expections, the semantic classes of co-referring expressions are 4 They may not be perfectly substitutable, for example one NP may be more specific (e.g., â\x80\x9cheâ\x80\x9d vs. â\x80\x9cJohn F. Kennedyâ\x80\x9d).', '122': 'But in most cases they can be used interchangably.', '123': 'For each domain, we created a semantic dictionary by doing two things.', '124': 'First, we parsed the training corpus, collected all the noun phrases, and looked up each head noun in WordNet (Miller, 1990).', '125': 'We tagged each noun with the top-level semantic classes assigned to it in Word- Net.', '126': 'Second, we identified the 100 most frequent nouns in the training corpus and manually labeled them with semantic tags.', '127': 'This step ensures that the most frequent terms for each domain are labeled (in case some of them are not in WordNet) and labeled with the sense most appropriate for the domain.', '128': 'Initially, we planned to compare the semantic classes of an anaphor and a candidate and infer that they might be coreferent if their semantic classes intersected.', '129': 'However, using the top-level semantic classes of WordNet proved to be problematic because the class distinctions are too coarse.', '130': 'For example, both a chair and a truck would be labeled as artifacts, but this does not at all suggest that they are coreferent.', '131': 'So we decided to use semantic class information only to rule out candidates.', '132': 'If two nouns have mutually exclusive semantic classes, then they cannot be coreferent.', '133': 'This solution also obviates the need to perform word sense disambiguation.', '134': 'Each word is simply tagged with the semantic classes corresponding to all of its senses.', '135': 'If these sets do not overlap, then the words cannot be coreferent.', '136': 'The semantic caseframe expectations are used in two ways.', '137': 'One knowledge source, called WordSemCFSem, is analogous to CFLex: it checks whether the anaphor and candidate antecedent are substitutable for one another, but based on their semantic classes instead of the words themselves.', '138': 'Given an anaphor and candidate, BABAR checks (1) whether the semantic classes of the anaphor intersect with the semantic expectations of the caseframe that extracts the candidate, and (2) whether the semantic classes of the candidate intersect with the semantic ex pectations of the caseframe that extracts the anaphor.', '139': 'If one of these checks fails then this knowledge source reports that the candidate is not a viable antecedent for the anaphor.', '140': 'A different knowledge source, called CFSemCFSem, compares the semantic expectations of the caseframe that extracts the anaphor with the semantic expectations of the caseframe that extracts the candidate.', '141': 'If the semantic expectations do not intersect, then we know that the case- frames extract mutually exclusive types of noun phrases.', '142': 'In this case, this knowledge source reports that the candidate is not a viable antecedent for the anaphor.', '143': '2.3 Assigning Evidence Values.', '144': 'Contextual role knowledge provides evidence as to whether a candidate is a plausible antecedent for an anaphor.', '145': 'The two knowledge sources that use semantic expectations, WordSemCFSem and CFSemCFSem, always return values of -1 or 0.', '146': '-1 means that an NP should be ruled out as a possible antecedent, and 0 means that the knowledge source remains neutral (i.e., it has no reason to believe that they cannot be coreferent).', '147': 'The CFLex and CFNet knowledge sources provide positive evidence that a candidate NP and anaphor might be coreferent.', '148': 'They return a value in the range [0,1], where 0 indicates neutrality and 1 indicates the strongest belief that the candidate and anaphor are coreferent.', '149': 'BABAR uses the log-likelihood statistic (Dunning, 1993) to evaluate the strength of a co-occurrence relationship.', '150': 'For each co-occurrence relation (noun/caseframe for CFLex, and caseframe/caseframe for CFNet), BABAR computes its log-likelihood value and looks it up in the Ï\x872 table to obtain a confidence level.', '151': 'The confidence level is then used as the belief value for the knowledge source.', '152': 'For example, if CFLex determines that the log- likelihood statistic for the co-occurrence of a particular noun and caseframe corresponds to the 90% confidence level, then CFLex returns .90 as its belief that the anaphor and candidate are coreferent.', '153': '3 The Coreference Resolution Model.', '154': 'Given a document to process, BABAR uses four modules to perform coreference resolution.', '155': 'First, a non-anaphoric NP classifier identifies definite noun phrases that are existential, using both syntactic rules and our learned existential NP recognizer (Bean and Riloff, 1999), and removes them from the resolution process.', '156': 'Second, BABAR performs reliable case resolution to identify anaphora that can be easily resolved using the lexical and syntactic heuristics described in Section 2.1.', '157': 'Third, all remaining anaphora are evaluated by 11 different knowledge sources: the four contextual role knowledge sources just described and seven general knowledge sources.', '158': 'Finally, a DempsterShafer probabilistic model evaluates the evidence provided by the knowledge sources for all candidate antecedents and makes the final resolution decision.', '159': 'In this section, we describe the seven general knowledge sources and explain how the DempsterShafer model makes resolutions.', '160': '3.1 General Knowledge Sources.', '161': 'Figure 4 shows the seven general knowledge sources (KSs) that represent features commonly used for coreference resolution.', '162': 'The gender, number, and scoping KSs eliminate candidates from consideration.', '163': 'The scoping heuristics are based on the anaphor type: for reflexive pronouns the scope is the current clause, for relative pronouns it is the prior clause following its VP, for personal pronouns it is the anaphorâ\x80\x99s sentence and two preceding sentences, and for definite NPs it is the anaphorâ\x80\x99s sentence and eight preceding sentences.', '164': 'The semantic agreement KS eliminates some candidates, but also provides positive evidence in one case: if the candidate and anaphor both have semantic tags human, company, date, or location that were assigned via NER or the manually labeled dictionary entries.', '165': 'The rationale for treating these semantic labels differently is that they are specific and reliable (as opposed to the WordNet classes, which are more coarse and more noisy due to polysemy).', '166': 'KS Function Ge nde r filters candidate if gender doesnâ\x80\x99t agree.', '167': 'Nu mb er filters candidate if number doesnâ\x80\x99t agree.', '168': 'Sc opi ng filters candidate if outside the anaphorâ\x80\x99s scope.', '169': 'Se ma nti c (a) filters candidate if its semantic tags d o n â\x80\x99 t i n t e r s e c t w i t h t h o s e o f t h e a n a p h o r .', '170': '( b ) s u p p o r t s c a n d i d a t e i f s e l e c t e d s e m a n t i c t a g s m a t c h t h o s e o f t h e a n a p h o r . Le xic al computes degree of lexical overlap b e t w e e n t h e c a n d i d a t e a n d t h e a n a p h o r . Re cen cy computes the relative distance between the c a n d i d a t e a n d t h e a n a p h o r . Sy nR ole computes relative frequency with which the c a n d i d a t e â\x80\x99 s s y n t a c t i c r o l e o c c u r s i n r e s o l u t i o n s . Figure 4: General Knowledge Sources The Lexical KS returns 1 if the candidate and anaphor are identical, 0.5 if their head nouns match, and 0 otherwise.', '171': 'The Recency KS computes the distance between the candidate and the anaphor relative to its scope.', '172': 'The SynRole KS computes the relative frequency with which the candidatesâ\x80\x99 syntactic role (subject, direct object, PP object) appeared in resolutions in the training set.', '173': 'During development, we sensed that the Recency and Syn- role KSs did not deserve to be on equal footing with the other KSs because their knowledge was so general.', '174': 'Consequently, we cut their evidence values in half to lessen their influence.', '175': '3.2 The DempsterShafer Decision Model.', '176': 'BABAR uses a DempsterShafer decision model (Stefik, 1995) to combine the evidence provided by the knowledge sources.', '177': 'Our motivation for using DempsterShafer is that it provides a well-principled framework for combining evidence from multiple sources with respect to competing hypotheses.', '178': 'In our situation, the competing hypotheses are the possible antecedents for an anaphor.', '179': 'An important aspect of the DempsterShafer model is that it operates on sets of hypotheses.', '180': 'If evidence indicates that hypotheses C and D are less likely than hypotheses A and B, then probabilities are redistributed to reflect the fact that {A, B} is more likely to contain the answer than {C, D}.', '181': 'The ability to redistribute belief values across sets rather than individual hypotheses is key.', '182': 'The evidence may not say anything about whether A is more likely than B, only that C and D are not likely.', '183': 'Each set is assigned two values: belief and plausibility.', '184': 'Initially, the DempsterShafer model assumes that all hypotheses are equally likely, so it creates a set called Î¸ that includes all hypotheses.', '185': 'Î¸ has a belief value of 1.0, indicating complete certainty that the correct hypothesis is included in the set, and a plausibility value of 1.0, indicating that there is no evidence for competing hypotheses.5 As evidence is collected and the likely hypotheses are whittled down, belief is redistributed to subsets of Î¸.', '186': 'Formally, the DempsterShafer theory defines a probability density function m(S), where S is a set of hypotheses.', '187': 'm(S) represents the belief that the correct hypothesis is included in S. The model assumes that evidence also arrives as a probability density function (pdf) over sets of hypotheses.6 Integrating new evidence into the existing model is therefore simply a matter of defining a function to merge pdfs, one representing the current belief system and one representing the beliefs of the new evidence.', '188': 'The DempsterShafer rule for combining pdfs is: to {C}, meaning that it is 70% sure the correct hypothesis is C. The intersection of these sets is the null set because these beliefs are contradictory.', '189': 'The belief value that would have been assigned to the intersection of these sets is .60*.70=.42, but this belief has nowhere to go because the null set is not permissible in the model.7 So this probability mass (.42) has to be redistributed.', '190': 'DempsterShafer handles this by re-normalizing all the belief values with respect to only the non-null sets (this is the purpose of the denominator in Equation 1).', '191': 'In our coreference resolver, we define Î¸ to be the set of all candidate antecedents for an anaphor.', '192': 'Each knowledge source then assigns a probability estimate to each candidate, which represents its belief that the candidate is the antecedent for the anaphor.', '193': 'The probabilities are incorporated into the DempsterShafer model using Equation 1.', '194': 'To resolve the anaphor, we survey the final belief values assigned to each candidateâ\x80\x99s singleton set.', '195': 'If a candidate has a belief value â\x89¥ .50, then we select that candidate as the antecedent for the anaphor.', '196': 'If no candidate satisfies this condition (which is often the case), then the anaphor is left unresolved.', '197': 'One of the strengths of the DempsterShafer model is its natural ability to recognize when several credible hypotheses are still in play.', '198': 'In this situation, BABAR takes the conservative approach and declines to make a resolution.', '199': '4 Evaluation Results.', '200': '4.1 Corpora.', '201': 'We evaluated BABAR on two domains: terrorism and natural disasters.', '202': 'We used the MUC4 terrorism corpus (MUC4 Proceedings, 1992) and news articles from the Reuterâ\x80\x99s text collection8 that had a subject code corresponding to natural disasters.', '203': 'For each domain, we created a blind test set by manually annotating 40 doc uments with anaphoric chains, which represent sets of m3 (S) = ) X â\x88©Y =S 1 â\x88\x92 ) m1 (X ) â\x88\x97 m2 (Y ) m1 (X ) â\x88\x97 m2 (Y ) (1) noun phrases that are coreferent (as done for MUC6 (MUC6 Proceedings, 1995)).', '204': 'In the terrorism domain, 1600 texts were used for training and the 40 test docu X â\x88©Y =â\x88\x85 All sets of hypotheses (and their corresponding belief values) in the current model are crossed with the sets of hypotheses (and belief values) provided by the new evidence.', '205': 'Sometimes, however, these beliefs can be contradictory.', '206': 'For example, suppose the current model assigns a belief value of .60 to {A, B}, meaning that it is 60% sure that the correct hypothesis is either A or B. Then new evidence arrives with a belief value of .70 assigned 5 Initially there are no competing hypotheses because all hypotheses are included in Î¸ by definition.', '207': '6 Our knowledge sources return some sort of probability estimate, although in some cases this estimate is not especially well-principled (e.g., the Recency KS).', '208': 'ments contained 322 anaphoric links.', '209': 'For the disasters domain, 8245 texts were used for training and the 40 test documents contained 447 anaphoric links.', '210': 'In recent years, coreference resolvers have been evaluated as part of MUC6 and MUC7 (MUC7 Proceedings, 1998).', '211': 'We considered using the MUC6 and MUC7 data sets, but their training sets were far too small to learn reliable co-occurrence statistics for a large set of contextual role relationships.', '212': 'Therefore we opted to use the much 7 The DempsterShafer theory assumes that one of the hypotheses in Î¸ is correct, so eliminating all of the hypotheses violates this assumption.', '213': '8 Volume 1, English language, 19961997, Format version 1, correction level 0 An ap ho r T e r r o r i s m R e c Pr F D i s a s t e r s R e c Pr F De f. NP s Pro no uns .43 .79 .55 .50 .72 .59 .42 .91 .58 .42 .82 .56 Tot al .46 .76 .57 .42 .87 .57 Table 2: General Knowledge Sources Table 4: Individual Performance of KSs for Terrorism Table 3: General + Contextual Role Knowledge Sources larger MUC4 and Reuters corpora.9 4.2 Experiments.', '214': 'We adopted the MUC6 guidelines for evaluating coreference relationships based on transitivity in anaphoric chains.', '215': 'For example, if {N P1, N P2, N P3} are all coreferent, then each NP must be linked to one of the other two NPs.', '216': 'First, we evaluated BABAR using only the seven general knowledge sources.', '217': 'Table 2 shows BABARâ\x80\x99s performance.', '218': 'We measured recall (Rec), precision (Pr), and the F-measure (F) with recall and precision equally weighted.', '219': 'BABAR achieved recall in the 4250% range for both domains, with 76% precision overall for terrorism and 87% precision for natural disasters.', '220': 'We suspect that the higher precision in the disasters domain may be due to its substantially larger training corpus.', '221': 'Table 3 shows BABARâ\x80\x99s performance when the four contextual role knowledge sources are added.', '222': 'The F- measure score increased for both domains, reflecting a substantial increase in recall with a small decrease in precision.', '223': 'The contextual role knowledge had the greatest impact on pronouns: +13% recall for terrorism and +15% recall for disasters, with a +1% precision gain in terrorism and a small precision drop of -3% in disasters.', '224': 'The difference in performance between pronouns and definite noun phrases surprised us.', '225': 'Analysis of the data revealed that the contextual role knowledge is especially helpful for resolving pronouns because, in general, they are semantically weaker than definite NPs.', '226': 'Since pronouns carry little semantics of their own, resolving them depends almost entirely on context.', '227': 'In contrast, even though context can be helpful for resolving definite NPs, context can be trumped by the semantics of the nouns themselves.', '228': 'For example, even if the contexts surrounding an anaphor and candidate match exactly, they are not coreferent if they have substantially different meanings 9 We would be happy to make our manually annotated test data available to others who also want to evaluate their coreference resolver on the MUC4 or Reuters collections.', '229': 'Table 5: Individual Performance of KSs for Disasters (e.g., â\x80\x9cthe mayorâ\x80\x9d vs. â\x80\x9cthe journalistâ\x80\x9d).', '230': 'We also performed experiments to evaluate the impact of each type of contextual role knowledge separately.', '231': 'Tables 4 and 5 show BABARâ\x80\x99s performance when just one contextual role knowledge source is used at a time.', '232': 'For definite NPs, the results are a mixed bag: some knowledge sources increased recall a little, but at the expense of some precision.', '233': 'For pronouns, however, all of the knowledge sources increased recall, often substantially, and with little if any decrease in precision.', '234': 'This result suggests that all of contextual role KSs can provide useful information for resolving anaphora.', '235': 'Tables 4 and 5 also show that putting all of the contextual role KSs in play at the same time produces the greatest performance gain.', '236': 'There are two possible reasons: (1) the knowledge sources are resolving different cases of anaphora, and (2) the knowledge sources provide multiple pieces of evidence in support of (or against) a candidate, thereby acting synergistically to push the DempsterShafer model over the belief threshold in favor of a single candidate.', '237': '5 Related Work.', '238': 'Many researchers have developed coreference resolvers, so we will only discuss the methods that are most closely related to BABAR.', '239': 'Dagan and Itai (Dagan and Itai, 1990) experimented with co-occurrence statistics that are similar to our lexical caseframe expectations.', '240': 'Their work used subject-verb, verb-object, and adjective-noun relations to compare the contexts surrounding an anaphor and candidate.', '241': 'However their work did not consider other types of lexical expectations (e.g., PP arguments), semantic expectations, or context comparisons like our case- frame network.(Niyu et al., 1998) used unsupervised learning to ac quire gender, number, and animacy information from resolutions produced by a statistical pronoun resolver.', '242': 'The learned information was recycled back into the resolver to improve its performance.', '243': 'This approach is similar to BABAR in that they both acquire knowledge from earlier resolutions.', '244': '(Kehler, 1997) also used a DempsterShafer model to merge evidence from different sources for template-level coreference.', '245': 'Several coreference resolvers have used supervised learning techniques, such as decision trees and rule learners (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Ng and Cardie, 2002; Soon et al., 2001).', '246': 'These systems rely on a training corpus that has been manually annotated with coreference links.', '247': '6 Conclusions.', '248': 'The goal of our research was to explore the use of contextual role knowledge for coreference resolution.', '249': 'We identified three ways that contextual roles can be exploited: (1) by identifying caseframes that co-occur in resolutions, (2) by identifying nouns that co-occur with case- frames and using them to crosscheck anaphor/candidate compatibility, (3) by identifying semantic classes that co- occur with caseframes and using them to crosscheck anaphor/candidate compatability.', '250': 'We combined evidence from four contextual role knowledge sources with evidence from seven general knowledge sources using a DempsterShafer probabilistic model.', '251': 'Our coreference resolver performed well in two domains, and experiments showed that each contextual role knowledge source contributed valuable information.', '252': 'We found that contextual role knowledge was more beneficial for pronouns than for definite noun phrases.', '253': 'This suggests that different types of anaphora may warrant different treatment: definite NP resolution may depend more on lexical semantics, while pronoun resolution may depend more on contextual semantics.', '254': 'In future work, we plan to follow-up on this approach and investigate other ways that contextual role knowledge can be used.', '255': '7 Acknowledgements.', '256': 'This work was supported in part by the National Science Foundation under grant IRI9704240.', '257': 'The inventions disclosed herein are the subject of a patent application owned by the University of Utah and licensed on an exclusive basis to Attensity Corporation.'}",['N04-1038'],['../data/summaries/N04-1038.txt'],['../data/tba/N04-1038.json']
N06-2049,Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation,../data/papers/N06-2049.xml,"{'0': 'Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation', '1': 'We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.', '2': 'We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.', '3': 'In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.', '4': 'By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.', '5': 'The character-based â\x80\x9cIOBâ\x80\x9d tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).', '6': 'Under the scheme, each character of a word is labeled as â\x80\x98Bâ\x80\x99 if it is the first character of a multiple-character word, or â\x80\x98Oâ\x80\x99 if the character functions as an independent word, or â\x80\x98Iâ\x80\x99 otherwise.â\x80\x9d For example, â\x80\x9d (whole) (Beijing city)â\x80\x9d is labeled as â\x80\x9d (whole)/O (north)/B (capital)/I (city)/Iâ\x80\x9d.', '7': 'We found that so far all the existing implementations were using character-based IOB tagging.', '8': 'In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.', '9': 'If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.', '10': 'Taking the same example mentioned above, â\x80\x9c (whole) (Beijing city)â\x80\x9d is labeled as â\x80\x9d (whole)/O (Beijing)/B (city)/Iâ\x80\x9d in the subword-based tagging, where â\x80\x9d (Beijing)/Bâ\x80\x9d is labeled as one unit.', '11': 'We will give a detailed description of this approach in Section 2.', '12': 'â\x88\x97 Now the second author is affiliated with NTT.', '13': 'In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov).', '14': 'In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.', '15': 'While OOV recognition is very important in word segmentation, a higher IV rate is also desired.', '16': 'In this work we propose a confidence measure approach to lessen the weakness.', '17': 'By this approach we can change R-oovs and R-ivs and find an optimal tradeoff.', '18': 'This approach will be described in Section 2.2.', '19': 'In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.', '20': 'Section 3 presents our experimental results.', '21': 'Section 4 describes current state- of-the-art methods for Chinese word segmentation, with which our results were compared.', '22': 'Section 5 provides the concluding remarks.', '23': 'Our word segmentation process is illustrated in Fig.', '24': '1.', '25': 'It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.', '26': 'An example exhibiting each stepâ\x80\x99s results is also given in the figure.', '27': 'Since the dictionary-based approach is a well-known method, we skip its technical descriptions.', '28': 'However, keep in mind that the dictionary-based approach can produce a higher R-iv rate.', '29': 'We will use this advantage in the confidence measure approach.', '30': '2.1 Subword-based IOB tagging using CRFs.', '31': 'There are several steps to train a subword-based IOB tag- ger.', '32': 'First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193â\x80\x93196, New York, June 2006.', '33': 'Qc 2006 Association for Computational Linguistics input å\x92\x98ã£\x85á¯¹Ô£à³¼à£«Ò\x80á\x8f\x96 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Dictionary-based word segmentation å\x92\x98 ã£\x85 á¯¹ Ô£ à³¼ à£«Ò\x80á\x8f\x96 +XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJFLW\\ Subword-based IOB tagging å\x92\x98/% ã£\x85/, á¯¹/, Ô£/2 à³¼/2 à£«Ò\x80/% á\x8f\x96/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, Confidence-based segmentation å\x92\x98/% ã£\x85/, á¯¹/, Ô£/2 à³¼/2 à£«Ò\x80/% á\x8f\x96/, +XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\\/, output å\x92\x98ã£\x85á¯¹ Ô£ à³¼ à£«Ò\x80á\x8f\x96 +XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJFLW\\ Figure 1: Outline of word segmentation process data.', '34': 'We chose all the single characters and the top multi- character words as a lexicon subset for the IOB tagging.', '35': 'If the subset consists of Chinese characters only, it is a character-based IOB tagger.', '36': 'We regard the words in the subset as the subwords for the IOB tagging.', '37': 'Second, we re-segmented the words in the training data into subwords belonging to the subset, and assigned IOB tags to them.', '38': 'For a character-based IOB tagger, there is only one possibility of re-segmentation.', '39': 'However, there are multiple choices for a subword-based IOB tagger.', '40': 'For example, â\x80\x9c (Beijing-city)â\x80\x9d can be segmented as â\x80\x9c (Beijing-city)/O,â\x80\x9d or â\x80\x9c (Beijing)/B (city)/I,â\x80\x9d or â\x80\x9d (north)/B (capital)/I (city)/I.â\x80\x9d In this work we used forward maximal match (FMM) for disambiguation.', '41': 'Of course, backward maximal match (BMM) or other approaches are also applicable.', '42': 'We did not conduct comparative experiments because trivial differences of these approaches may not result in significant consequences to the subword-based ap proach.', '43': 'In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.', '44': 'We downloaded and used the package â\x80\x9cCRF++â\x80\x9d from the site â\x80\x9chttp://www.chasen.org/Ë\x9ctaku/software.â\x80\x9d According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 Â· Â· Â· tM , given the word sequence, W = w0 w1 Â· Â· Â· wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively.', '45': 'The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.', '46': 'In order to overcome overfitting, a gaussian prior was imposed in the training.', '47': 'The types of unigram features used in our experiments included the following types: w0 , wâ\x88\x921 , w1 , wâ\x88\x922 , w2 , w0 wâ\x88\x921 , w0 w1 , wâ\x88\x921 w1 , wâ\x88\x922 wâ\x88\x921 , w2 w0 where w stands for word.', '48': 'The subscripts are position indicators.', '49': '0 means the current word; â\x88\x921, â\x88\x922, the first or second word to the left; 1, 2, the first or second word to the right.', '50': 'For the bigram features, we only used the previous and the current observations, tâ\x88\x921 t0 . As to feature selection, we simply used absolute counts for each feature in the training data.', '51': 'We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff.', '52': 'A forward-backward algorithm was used in the training and viterbi algorithm was used in the decoding.', '53': '2.2 Confidence-dependent word segmentation.', '54': 'Before moving to this step in Figure 1, we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging.', '55': 'However, neither was perfect.', '56': 'The dictionary-based segmentation produced results with higher R-ivs but lower R-oovs while the IOB tagging yielded the contrary results.', '57': 'In this section we introduce a confidence measure approach to combine the two results.', '58': 'We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.', '59': 'The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.', '60': 'Its calculation is defined as: C M(tiob |w) = Î±C Miob (tiob |w) + (1 â\x88\x92 Î±)Î´(tw , tiob )ng (2) where tiob is the word wâ\x80\x99s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.', '61': 'After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.', '62': ""Each subword is given a prior IOB tag, tw . C Miob (t|w), a ï£« M ï£«ï£¶ï£¶ confidence probability derived in the process of IOB tag exp ï£¬)' ï£¬)' Î»k fk (tiâ\x88\x921 , ti , W ) + )' Âµk gk (ti , W )ï£·ï£· /Z, ï£¬ï£\xad i=1 ï£¬ï£\xad k k ï£·ï£¸ ï£·ï£¸ (1) ging, is defined as Z = )' T =t0 t1 Â·Â·Â·tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 Â·Â·Â·tM ,ti =t P(T |W, wi ) T =t 0 t1 Â·Â·Â· tM P ( T | W ) where we call fk (tiâ\x88\x921 , ti , W ) bigram feature functions because the features trigger the previous observation tiâ\x88\x921 where the numerator is a sum of all the observation sequences with word wi labeled as t. Î´(tw , tiob )ng denotes the contribution of the dictionary- based segmentation."", '63': 'It is a Kronecker delta function defined as Î´(tw , tiob )ng = { 1 if tw = tiob 0 otherwise In Eq. 2, Î± is a weighting between the IOB tagging and the dictionary-based word segmentation.', '64': 'We found the value 0.7 for Î±, empirically.', '65': 'By Eq. 2 the results of IOB tagging were reevaluated.', '66': 'A confidence measure threshold, t, was defined for making a decision based on the value.', '67': 'If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.', '68': 'A new OOV was thus created.', '69': 'For the two extreme cases, t = 0 is the case of the IOB tagging while t = 1 is that of the dictionary-based approach.', '70': 'In a real application, a satisfactory tradeoff between R- ivs and R-oovs could find through tuning the confidence threshold.', '71': 'In Section 3.2 we will present the experimental segmentation results of the confidence measure approach.', '72': 'We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.', '73': 'The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR).', '74': 'Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.', '75': 'Five metrics were used to evaluate segmentation results: recall(R), precision(P), F-score(F), OOV rate(R-oov) and IV rate(R-iv).', '76': 'For detailed info.', '77': 'of the corpora and these scores, refer to (Emerson, 2005).', '78': 'For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.', '79': 'Tri- gram LMs were generated using the SRI LM toolkit for disambiguation.', '80': 'Table 1 shows the performance of the dictionary-based segmentation.', '81': 'Since there were some single-character words present in the test data but not in the training data, the R-oov rates were not zero in this experiment.', '82': 'In fact, there were no OOV recognition.', '83': 'Hence, this approach produced lower F-scores.', '84': 'However, the R-ivs were very high.', '85': '3.1 Effects of the Character-based and the.', '86': 'subword-based tagger The main difference between the character-based and the word-based is the contents of the lexicon subset used for re-segmentation.', '87': 'For the character-based tagging, we used all the Chinese characters.', '88': 'For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.', '89': 'The segmentation results of the dictionary-based were re-segmented Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied.', '90': 'R P FR oo vR iv A S 0.9 51 0.9 53 0.9 42 0.9 40 0.9 47 0.9 47 0.', '91': '67 8 0.', '92': '64 7 0.9 64 0.9 67 CI TY U 0.9 39 0.9 50 0.9 43 0.9 42 0.9 41 0.9 46 0.', '93': '70 0 0.', '94': '73 6 0.9 58 0.9 67 P K U 0.9 40 0.9 43 0.9 50 0.9 46 0.9 45 0.9 45 0.', '95': '78 3 0.', '96': '75 4 0.9 49 0.9 55 M S R 0.9 57 0.9 65 0.9 60 0.9 63 0.9 59 0.9 64 0.', '97': '71 0 0.', '98': '71 6 0.9 64 0.9 72 Table 2: Segmentation results by a pure subword-based IOB tagging.', '99': 'The upper numbers are of the character- based and the lower ones, the subword-based.', '100': 'using the FMM, and then labeled with â\x80\x9cIOBâ\x80\x9d tags by the CRFs.', '101': 'The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.', '102': 'We found that the proposed subword-based approaches were effective in CITYU and MSR corpora, raising the F-scores from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for MSR corpus.', '103': 'There were no F-score changes for AS and PKU corpora, but the recall rates were improved.', '104': 'Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.', '105': 'However, the R-iv rates were getting worse in return for higher R-oov rates.', '106': 'We will tackle this problem by the confidence measure approach.', '107': '3.2 Effect of the confidence measure.', '108': 'In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation.', '109': 'The effect of the confidence measure is shown in Table 3, where we used Î± = 0.7 and confidence threshold t = 0.8.', '110': 'In each slot, the numbers on the top were of the character-based approach while the numbers on the bottom were the subword-based.', '111': 'We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.', '112': 'The act of confidence measure made a tradeoff between R-ivs and R- oovs, yielding higher R-oovs than Table 1 and higher R R P FR oo vR iv A S 0.9 53 0.9 56 0.9 44 0.9 47 0.9 48 0.9 51 0.', '113': '60 7 0.', '114': '64 9 0.9 69 0.9 69 CI TY U 0.9 43 0.9 52 0.9 48 0.9 49 0.9 46 0.9 51 0.', '115': '68 2 0.', '116': '74 1 0.9 64 0.9 69 P K U 0.9 42 0.9 47 0.9 57 0.9 55 0.9 49 0.9 51 0.', '117': '77 5 0.', '118': '74 8 0.9 52 0.9 59 M S R 0.9 60 0.9 72 0.9 66 0.9 69 0.9 63 0.9 71 0.', '119': '67 4 0.', '120': '71 2 0.9 67 0.9 76 Table 3: Effects of combination using the confidence measure.', '121': 'The upper numbers and the lower numbers are of the character-based and the subword-based, respectively A S CI T Y U M SR P K U Ba ke off be st 0.', '122': '95 2 0.', '123': '9 4 3 0.', '124': '96 4 0.', '125': '95 0 O u r s 0.', '126': '95 1 0.', '127': '9 5 1 0.', '128': '97 1 0.', '129': '95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.', '130': 'Even with the use of confidence measure, the word- based IOB tagging still outperformed the character-based IOB tagging.', '131': 'It proves the proposed word-based IOB tagging was very effective.', '132': 'The IOB tagging approach adopted in this work is not a new idea.', '133': 'It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used.', '134': 'Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).', '135': 'Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based.', '136': 'We proved the new approach enhanced the word segmentation significantly.', '137': 'Our results are listed together with the best results from Bakeoff 2005 in Table 4 in terms of F-scores.', '138': 'We achieved the highest F-scores in CITYU, PKU and MSR corpora.', '139': 'We think our proposed subword- based tagging played an important role for the good results.', '140': 'Since it was a closed test, some information such as Arabic and Chinese number and alphabetical letters cannot be used.', '141': 'We could yield a better results than those shown in Table 4 using such information.', '142': 'For example, inconsistent errors of foreign names can be fixed if alphabetical characters are known.', '143': 'For AS corpus, â\x80\x9cAdam Smithâ\x80\x9d are two words in the training but become a one- word in the test, â\x80\x9cAdamSmithâ\x80\x9d.', '144': 'Our approaches produced wrong segmentations for labeling inconsistency.', '145': 'Another advantage of the word-based IOB tagging over the character-based is its speed.', '146': 'The subword-based approach is faster because fewer words than characters were labeled.', '147': 'We found a speed up both in training and test.', '148': 'The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs.', '149': 'In this work we used it more delicately.', '150': 'By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.', '151': 'In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.', '152': 'Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.', '153': 'We also successfully employed the confidence measure to make a confidence-dependent word segmentation.', '154': 'This approach is effective for performing desired segmentation based on usersâ\x80\x99 requirements to R-oov and R-iv.', '155': 'The authors appreciate the reviewersâ\x80\x99 effort and good advice for improving the paper.'}",['N06-2049'],['../data/summaries/N06-2049.txt'],['../data/tba/N06-2049.json']
P04-1036,Finding Predominant Word Senses in Untagged Text,../data/papers/P04-1036.xml,"{'0': 'Finding Predominant Word Senses in Untagged Text', '1': 'word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', '2': 'The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data.', '3': 'Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration.', '4': 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', '5': 'The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task.', '6': 'This is a very promising result given that our method does not require any hand-tagged text, such as SemCor.', '7': 'Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.', '8': 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.', '9': 'This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al., 1998) in figure 1 below, where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al., 2001).', '10': 'The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al., 1993).', '11': 'Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred.', '12': 'The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor, from those that do not (without HTD).', '13': 'The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.', '14': 'Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al., 2001).', '15': 'Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.', '16': 'SemCor comprises a relatively small sample of 250,000 words.', '17': 'There are words where the first sense in WordNet is counter-intuitive, because of the size of the corpus, and because where the frequency data does not indicate a first sense, the ordering is arbitrary.', '18': 'For example the first sense of tiger in WordNet is audacious person whereas one might expect that carnivorous animal is a more common usage.', '19': 'There are only a couple of instances of tiger within SemCor.', '20': 'Another example is embryo, which does not occur at all in SemCor and the first sense is listed as rudimentary plant rather than the anticipated fertilised egg meaning.', '21': 'We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson, 1998; Hoste et al., 2001) and for systems that use it in lexical acquisition (McCarthy, 1997; Merlo and Leybold, 2001; Korhonen, 2002) because of the limited size of hand-tagged resources.', '22': 'More importantly, when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand.', '23': 'The first sense of star in SemCor is celestial body, however, if one were disambiguating popular news celebrity would be preferred.', '24': 'Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts.', '25': 'However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples (Yarowsky and Florian, 2002) available.', '26': 'We the WordNet hierarchy.', '27': 'We use WordNet as our sense inventory for this work.', '28': 'The paper is structured as follows.', '29': 'We discuss our method in the following section.', '30': 'Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.', '31': 'In section 5 we present results of the method on two domain specific sections of the Reuters corpus for a sample of words.', '32': 'We describe some related work in section 6 and conclude in section 7. are therefore investigating a method of automatically ranking WordNet senses from raw text.', '33': 'Many researchers are developing thesauruses from automatically parsed data.', '34': 'In these each target word is entered with an ordered list of “nearest neighbours”.', '35': 'The neighbours are words ordered in terms of the “distributional similarity” that they have with the target.', '36': 'Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.', '37': 'From inspection, one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word.', '38': 'For example, the neighbours of star in a dependency-based thesaurus provided by Lin 1 has the ordered list of neighbours: superstar, player, teammate, actor early in the list, but one can also see words that are related to another sense of star e.g. galaxy, sun, world and planet further down the list.', '39': 'We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.', '40': 'This is because there will be more relational data for the more prevalent senses compared to the less frequent senses.', '41': 'In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.', '42': 'The neighbours for a word in a thesaurus are words themselves, rather than senses.', '43': 'In order to associate the neighbours with senses we make use of another notion of similarity, “semantic similarity”, which exists between senses, rather than words.', '44': 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen, 2003) which aim to capture semantic relatedness within', '45': 'In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).', '46': 'This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.', '47': 'We then use the WordNet similarity package (Patwardhan and Pedersen, 2003) to give us a semantic similarity measure (hereafter referred to as the WordNet similarity measure) to weight the contribution that each neighbour makes to the various senses of the target word.', '48': 'To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.', '49': 'Let be the ordered set of the top scoring neighbours of from the thesaurus with associated distributional similarity scores The thesaurus was acquired using the method described by Lin (1998).', '50': 'For input we used grammatical relation data extracted using an automatic where: .', '51': 'Let be the set of senses of .', '52': 'For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.', '53': 'This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score, divided by the sum of all such WordNet similarity scores for and .', '54': 'Thus we rank each sense using: parser (Briscoe and Carroll, 2002).', '55': 'For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.', '56': 'For each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.', '57': 'We could easily extend the set of relations in the future.', '58': 'A noun, , is thus described by a set of co-occurrence triples and associated frequencies, where is a grammatical relation and is a possible cooccurrence with in that relation.', '59': 'For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998).', '60': 'If is the set of co-occurrence types such that is positive then the similarity between two nouns, and , can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .', '61': 'We use the WordNet Similarity Package 0.05 and WordNet version 1.6.', '62': '2 The WordNet Similarity package supports a range of WordNet similarity scores.', '63': 'We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.', '64': 'We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).', '65': 'The measures provide a similarity score between two WordNet senses ( and ), these being synsets within WordNet. lesk (Banerjee and Pedersen, 2002) This score maximises the number of overlapping words in the gloss, or definition, of the senses.', '66': 'It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.', '67': 'Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately.', '68': 'We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.', '69': 'The frequency data is used to calculate the “information content” (IC) of a class .', '70': 'Jiang and Conrath specify a distance measure: , where the third class ( ) is the most informative, or most specific, superordinate synset of the two senses and .', '71': 'This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:', '72': 'In order to evaluate our method we use the data in SemCor as a gold-standard.', '73': 'This is not ideal since we expect that the sense frequency distributions within SemCor will differ from those in the BNC, from which we obtain our thesaurus.', '74': 'Nevertheless, since many systems performed well on the English all-words task for SENSEVAL-2 by using the frequency information in SemCor this is a reasonable approach for evaluation.', '75': 'We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.', '76': 'The jcn measure uses corpus data for the calculation of IC.', '77': 'We experimented with counts obtained from the BNC and the Brown corpus.', '78': 'The variation in counts had negligible affect on the results.', '79': '3 The experimental results reported here are obtained using IC counts from the BNC corpus.', '80': 'All the results shown here are those with the size of thesaurus entries ( ) set to 50.', '81': '4 We calculate the accuracy of finding the predominant sense, when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).', '82': 'We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).', '83': 'The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.', '84': 'The random baseline for choosing the predominant sense over all these words ( ) is 32%.', '85': 'Both WordNet similarity measures beat this baseline.', '86': 'The random baseline for ( ) is 24%.', '87': 'Again, the automatic ranking outperforms this by a large margin.', '88': 'The first sense in SemCor provides an upperbound for this task of 67%.', '89': 'Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.', '90': 'From manual analysis, there are cases where the acquired first sense disagrees with SemCor, yet is intuitively plausible.', '91': 'This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.', '92': 'For example, in WordNet the first listed sense ofpipe is tobacco pipe, and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water, oil or gas etc....', '93': 'The automatic ranking from the BNC data lists the latter tube sense first.', '94': 'This seems quite reasonable given the nearest neighbours: tube, cable, wire, tank, hole, cylinder, fitting, tap, cistern, plate....', '95': 'Since SemCor is derived from the Brown corpus, which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6, the high ranking for the tobacco pipe sense according to SemCor seems plausible.', '96': 'Another example where the ranking is intuitive, is soil.', '97': 'The first ranked sense according to SemCor is the filth, stain: state of being unclean sense whereas the automatic ranking lists dirt, ground, earth as the first sense, which is the second ranked sense according to SemCor.', '98': 'This seems intuitive given our expected relative usage of these senses in modern British English.', '99': 'Even given the difference in text type between SemCor and the BNC the results are encouraging, especially given that our results are for polysemous nouns.', '100': 'In the English all-words SENSEVAL-2, 25% of the noun data was monosemous.', '101': 'Thus, if we used the sense ranking as a heuristic for an “all nouns” task we would expect to get precision in the region of 60%.', '102': 'We test this below on the SENSEVAL-2 English all-words data.', '103': 'In order to see how well the automatically acquired predominant sense performs on a WSD task from which the WordNet sense ordering has not been taken, we use the SENSEVAL-2 all-words data (Palmer et al., 2001).', '104': '7 This is a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II.', '105': 'We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.', '106': 'We do not assume that the predominant sense is a method of WSD in itself.', '107': 'To disambiguate senses a system should take context into account.', '108': 'However, it is important to know the performance of this heuristic for any systems that use it.', '109': 'We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above.', '110': 'We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English allwords task.', '111': 'We give the results for this WSD task in table 2.', '112': 'We compare results using the first sense listed in SemCor, and the first sense according to the SENSEVAL-2 English all-words test data itself.', '113': 'For the latter, we only take a first-sense where there is more than one occurrence of the noun in the test data and one sense has occurred more times than any of the others.', '114': 'We trivially labelled all monosemous items.', '115': 'Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.', '116': 'The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task.', '117': 'The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed.', '118': 'Two such words, today and one, each occurred 5 times in the test data.', '119': 'Extending the grammatical relations used for building the thesaurus should improve the coverage.', '120': 'There were a similar number of words that were not covered by a predominant sense in SemCor.', '121': 'For these one would need to obtain more sense-tagged text in order to use this heuristic.', '122': 'Our automatic ranking gave 67% precision on these items.', '123': 'This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.', '124': 'A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains.', '125': 'In order to test this we applied our method to two specific sections of the Reuters corpus.', '126': 'We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.', '127': 'We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus.', '128': 'The Reuters corpus (Rose et al., 2002) is a collection of about 810,000 Reuters, English Language News stories.', '129': 'Many of the articles are economy related, but several other topics are included too.', '130': 'We selected documents from the SPORTS domain (topic code: GSPO) and a limited number of documents from the FINANCE domain (topic codes: ECAT and MCAT).', '131': 'The SPORTS corpus consists of 35317 documents (about 9.1 million words).', '132': 'The FINANCE corpus consists of 117734 documents (about 32.5 million words).', '133': 'We acquired thesauruses for these corpora using the procedure described in section 2.1.', '134': 'There is no existing sense-tagged data for these domains that we could use for evaluation.', '135': 'We therefore decided to select a limited number of words and to evaluate these words qualitatively.', '136': 'The words included in this experiment are not a random sample, since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', '137': 'Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.', '138': 'The SFC contains an economy label and a sports label.', '139': 'For this domain label experiment we selected all the words in WordNet that have at least one synset labelled economy and at least one synset labelled sports.', '140': 'The resulting set consisted of 38 words.', '141': 'We contrast the distribution of domain labels for these words in the 2 domain specific corpora.', '142': 'The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.', '143': 'The results are promising.', '144': 'Most words show the change in predominant sense (PS) that we anticipated.', '145': 'It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC, but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).', '146': 'Moreover, the chosen senses of the word tie proved to be a textbook example of the behaviour we expected.', '147': 'The word share is among the words whose predominant sense remained the same for all three corpora.', '148': 'We anticipated that the stock certificate sense would be chosen for the FINANCE domain, but this did not happen.', '149': 'However, that particular sense ended up higher in the ranking for the FINANCE domain.', '150': 'Figure 2 displays the results of the second experiment with the domain specific corpora.', '151': 'This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora.', '152': 'We see that both domains have a similarly high percentage of factotum (domain independent) labels, but as we would expect, the other peaks correspond to the economy label for the FINANCE corpus, and the sports label for the SPORTS corpus. inant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus.', '153': 'Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', '154': 'In contrast, our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one, and because handtagged data is not always available.', '155': 'A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.', '156': 'Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset, and those related by hyponymy, and a term relevance measure taken from information retrieval.', '157': 'Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.', '158': 'We have evaluated our method using publically available resources, both for balanced and domain specific text.', '159': 'Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.', '160': 'Identification of these domain labels for word senses was semiautomatic and required a considerable amount of hand-labelling.', '161': 'Our approach is complementary to this.', '162': 'It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.', '163': 'Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.', '164': 'They used syntactic evidence to find a prior distribution for verb classes, based on (Levin, 1993), and incorporate this in a WSD system.', '165': 'Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.', '166': 'There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).', '167': 'In this work the lists of neighbours are themselves clustered to bring out the various senses of the word.', '168': 'They evaluate using the lin measure described above in section 2.2 to determine the precision and recall of these discovered classes with respect to WordNet synsets.', '169': 'This method obtains precision of 61% and recall 51%.', '170': 'If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found.', '171': 'In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.', '172': 'We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.', '173': 'It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets, as Ciaramita and Johnson (2003) do with unknown nouns.', '174': 'We have restricted ourselves to nouns in this work, since this PoS is perhaps most affected by domain.', '175': 'We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.', '176': 'The lesk measure can be used when ranking adjectives, and adverbs as well as nouns and verbs (which can also be ranked using jcn).', '177': 'Another major advantage that lesk has is that it is applicable to lexical resources which do not have the hierarchical structure that WordNet does, but do have definitions associated with word senses.', '178': 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.', '179': 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', '180': 'The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.', '181': 'This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.', '182': 'In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.', '183': 'Indeed, the merit of our technique is the very possibility of obtaining predominant senses from the data at hand.', '184': 'We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns.', '185': 'In the future, we will perform a large scale evaluation on domain specific corpora.', '186': 'In particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora.', '187': 'There is plenty of scope for further work.', '188': 'We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al., 2004).', '189': 'Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.', '190': 'Whilst we have used WordNet as our sense inventory, it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbours and the senses.', '191': 'The lesk measure for example, can be used with definitions in any standard machine readable dictionary.', '192': 'We would like to thank Siddharth Patwardhan and Ted Pedersen for making the WN Similarity package publically available.', '193': 'This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies, UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.'}","['P04-1036_sweta', 'P04-1036_vardha', 'P04-1036_swastika', 'P04-1036_aakansha']","['../data/summaries/P04-1036_sweta.txt', '../data/summaries/P04-1036_vardha.txt', '../data/summaries/P04-1036_swastika.txt', '../data/summaries/P04-1036_aakansha.txt']","['../data/tba/P04-1036_sweta.json', '../data/tba/P04-1036_vardha.json', '../data/tba/P04-1036_swastika.json', '../data/tba/P04-1036_aakansha.json']"
P05-1004,Supersense Tagging of Unknown Nouns using Semantic Similarity,../data/papers/P05-1004.xml,"{'0': 'Supersense Tagging of Unknown Nouns using Semantic Similarity', '1': 'The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.', '2': 'Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.', '3': 'Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.', '4': 'We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.', '5': 'We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.', '6': 'Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).', '7': 'In particular, WORDNET (Fellbaum, 1998) has significantly influenced research in NLP.', '8': 'Unfortunately, these resource are extremely time- consuming and labour-intensive to manually develop and maintain, requiring considerable linguistic and domain expertise.', '9': 'Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular.', '10': 'Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary.', '11': 'Bur- gun and Bodenreider (2001) compared an alignment of WORDNET with the UMLS medical resource and found only a very small degree of overlap.', '12': 'Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics.', '13': 'Some specialist topics are better covered in WORD- NET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.', '14': 'Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.', '15': 'By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult.', '16': 'consistency when classifying similar words into categories.', '17': 'For instance, the WORDNET lexicographer file for ionosphere (location) is different to exo- sphere and stratosphere (object), two other layers of the earthâ\x80\x99s atmosphere.', '18': 'These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources.', '19': 'Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.', '20': 'Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNETâ\x80\x99s hierarchical structure to create many annotated training instances from the synset glosses.', '21': 'This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.', '22': 'Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.', '23': 'The supersenses of these synonyms are then combined to determine the supersense.', '24': 'This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1.', '25': '26 Proceedings of the 43rd Annual Meeting of the ACL, pages 26â\x80\x9333, Ann Arbor, June 2005.', '26': 'Qc 2005 Association for Computational Linguistics L E X -FI L E D E S C R I P T I O N act acts or actions animal animals artifact man-made objects attribute attributes of people and objects body body parts cognition cognitive processes and contents communication communicative processes and contents event natural events feeling feelings and emotions food foods and drinks group groupings of people or objects location spatial position motive goals object natural objects (not man-made) person people phenomenon natural phenomena plant plants possession possession and transfer of possession process natural processes quantity quantities and units of measure relation relations between people/things/ideas shape two and three dimensional shapes state stable states of affairs substance substances time time and temporal relations Table 1: 25 noun lexicographer files in WORDNET', '27': 'There are 26 broad semantic classes employed by lexicographers in the initial phase of inserting words into the WORDNET hierarchy, called lexicographer files (lex- files).', '28': 'For the noun hierarchy, there are 25 lex-files and a file containing the top level nodes in the hierarchy called Tops.', '29': 'Other syntactic classes are also organised using lex-files: 15 for verbs, 3 for adjectives and 1 for adverbs.', '30': 'Lex-files form a set of coarse-grained sense distinctions within WORDNET.', '31': 'For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.', '32': 'The names and descriptions of the noun lex-files are shown in Table 1.', '33': 'Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30).', '34': 'For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time.', '35': 'Ciaramita and Johnson (2003) call the noun lex-file classes supersenses.', '36': 'There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses.', '37': 'Ciaramita (2002) has produced a mini- WORDNET by manually reducing the WORDNET hierarchy to 106 broad categories.', '38': 'Ciaramita et al.', '39': '(2003) describe how the lex-files can be used as root nodes in a two level hierarchy with the WORDNET synsets appear ing directly underneath.', '40': 'Other alternative sets of supersenses can be created by an arbitrary cut through the WORDNET hierarchy near the top, or by using topics from a thesaurus such as Rogetâ\x80\x99s (Yarowsky, 1992).', '41': 'These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.', '42': 'Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.', '43': 'They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g. location and person) for labelling predominantly unseen terms.', '44': 'Supersense tagging can provide automated or semi- automated assistance to lexicographers adding words to the WORDNET hierarchy.', '45': 'Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself.', '46': 'Clearly, this is the ultimate goal, to be able to insert new terms into lexical resources, extending the structure where necessary.', '47': 'Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.', '48': 'A considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORD- NET and the construction of new wordnets using the concept structure from English.', '49': 'For lexical FreeNet, Beefer- man (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information.', '50': 'The co-occurrence window was 500 words which was designed to approximate average document length.', '51': 'Caraballo and Charniak (1999) have explored determining noun specificity from raw text.', '52': 'They find that simple frequency counts are the most effective way of determining the parent-child ordering, achieving 83% accuracy over types of vehicle, food and occupation.', '53': 'The other measure they found to be successful was the entropy of the conditional distribution of surrounding words given the noun.', '54': 'Specificity ordering is a necessary step for building a noun hierarchy.', '55': 'However, this approach clearly cannot build a hierarchy alone.', '56': 'For instance, entity is less frequent than many concepts it subsumes.', '57': 'This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners.', '58': 'Hearst and SchuÂ¨ tze (1993) flatten WORDNET into 726 categories using an algorithm which attempts to minimise the variance in category size.', '59': 'These categories are used to label paragraphs with topics, effectively repeating Yarowskyâ\x80\x99s (1992) experiments using the their categories rather than Rogetâ\x80\x99s thesaurus.', '60': 'SchuÂ¨ tzeâ\x80\x99s (1992) WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem).', '61': 'Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word.', '62': 'Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy.', '63': 'He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms.', '64': 'This same technique as is used in our approach to supersense tagging.', '65': 'Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.', '66': 'Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy.', '67': 'They developed an efficient algorithm for estimating the model over hierarchical training data.', '68': 'Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.', '69': 'They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.', '70': 'Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.', '71': 'Our evaluation will use exactly the same test sets as Ciaramita and Johnson (2003).', '72': 'The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense.', '73': 'The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).', '74': 'They have kindly supplied us with the WORDNET 1.7.1 test set and one cross-validation run of the WORDNET 1.6 test set.', '75': 'Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORD- NET 1.7.1 test set.', '76': 'Some examples from the test sets are given in Table 2 with their supersenses.', '77': 'We have developed a 2 billion word corpus, shallow- parsed with a statistical NLP pipeline, which is by far the Table 2: Example nouns and their supersenses largest NLP processed corpus described in published re search.', '78': 'The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortiumâ\x80\x99s news text collected since 1987: Continuous Speech Recognition III (CSRIII); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus.', '79': 'The components and their sizes including punctuation are given in Table 3.', '80': 'The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above.', '81': 'C O R P U S D O C S . S E N T S . WO R D S B N C 4 1 2 4 6 . 2 M 1 1 4 M R C V1 8 0 6 7 9 1 8 . 1 M 2 0 7 M C S R -I I I 4 9 1 3 4 9 9 . 3 M 2 2 6 M NA N T C 9 3 0 3 6 7 2 3.', '82': '2 M 5 5 9 M NA N T S 9 4 2 1 6 7 2 5.', '83': '2 M 5 0 7 M AC QU A I N T 1 03 3 46 1 2 1.', '84': '3 M 4 9 1 M Table 3: 2 billion word corpus statistics We have tokenized the text using the Grok OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997).', '85': 'Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise.', '86': 'The rest of the pipeline is described in the next section.', '87': 'Similarity Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.', '88': 'This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in.', '89': 'In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in.', '90': 'The key parameters are the context extraction method and the similarity measure used to compare context vectors.', '91': 'Our approach to vector-space similarity is based on the SEXTANT system described in Grefenstette (1994).', '92': 'Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.', '93': 'SEXTANT extracts relation tuples (w, r, wt ) for each noun, where w is the headword, r is the relation type and wt is the other word.', '94': 'The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible.', '95': 'We describe the shallow pipeline in detail below.', '96': 'Curran and Moens (2002a) compared several different similarity measures and found that Grefenstetteâ\x80\x99s weighted JACCARD measure performed the best: R E L AT I O N D E S C R I P T I O N adj nounâ\x80\x93adjectival modifier relation dobj verbâ\x80\x93direct object relation iobj verbâ\x80\x93indirect object relation nn nounâ\x80\x93noun modifier relation nnprep nounâ\x80\x93prepositional head relation subj verbâ\x80\x93subject relation Table 4: Grammatical relations from SEXTANT against the CELEX lexical database (Minnen et al., 2001) â\x80\x93 and is very efficient, analysing over 80 000 words per second.', '97': 'morpha often maintains sense distinctions between singular and plural nouns; for instance: spectacles is not reduced to spectacle, but fails to do so in other cases: glasses is converted to glass.', '98': 'This inconsis L min(wgt(w1 , â\x88\x97r , â\x88\x97wI ), wgt(w2 , â\x88\x97r , â\x88\x97wI )) L max(wgt(w1 , â\x88\x97r , â\x88\x97wI ), wgt(w2 , â\x88\x97r , â\x88\x97wI )) (1) tency is problematic when using morphological analysis to smooth vector-space models.', '99': 'However, morphological smoothing still produces better results in practice.', '100': 'where wgt(w, r, wt ) is the weight function for relation (w, r, wt ).', '101': 'Curran and Moens (2002a) introduced the TTEST weight function, which is used in collocation extraction.', '102': 'Here, the t-test compares the joint and product probability distributions of the headword and context: 6.3 Grammatical Relation Extraction.', '103': 'After the raw text has been POS tagged and chunked, the grammatical relation extraction algorithm is run over the chunks.', '104': 'This consists of five passes over each sentence that first identify noun and verb phrase heads and p(w, r, wt ) â\x88\x92 p(â\x88\x97, r, wt )p(w, â\x88\x97, â\x88\x97) p(â\x88\x97, r, wt )p(w, â\x88\x97, â\x88\x97) (2) then collect grammatical relations between each common noun and its modifiers and verbs.', '105': 'A global list of grammatical relations generated by each pass is maintained where â\x88\x97 indicates a global sum over that element of the relation tuple.', '106': 'JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moenâ\x80\x99s configuration for our super- sense tagging experiments.', '107': '6.1 Part of Speech Tagging and Chunking.', '108': 'Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).', '109': 'The only similar performing tool is the Trigrams â\x80\x98nâ\x80\x99 Tags tagger (Brants, 2000) which uses a much simpler statistical model.', '110': 'Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script.', '111': 'Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstetteâ\x80\x99s table-based results, i.e. the SEXTANT always prefers noun attachment.', '112': '6.2 Morphological Analysis.', '113': 'Our implementation uses morpha, the Sussex morphological analyser (Minnen et al., 2001), which is implemented using lex grammars for both affix splitting and generation.', '114': 'morpha has wide coverage â\x80\x93 nearly 100% across the passes.', '115': 'The global list is used to determine if a word is already attached.', '116': 'Once all five passes have been completed this association list contains all of the noun- modifier/verb pairs which have been extracted from the sentence.', '117': 'The types of grammatical relation extracted by SEXTANT are shown in Table 4.', '118': 'For relations between nouns (nn and nnprep), we also create inverse relations (wt , rt , w) representing the fact that wt can modify w. The 5 passes are described below.', '119': 'Pass 1: Noun Pre-modifiers This pass scans NPs, left to right, creating adjectival (adj) and nominal (nn) pre-modifier grammatical relations (GRs) with every noun to the pre-modifierâ\x80\x99s right, up to a preposition or the phrase end.', '120': 'This corresponds to assuming right-branching noun compounds.', '121': 'Within each NP only the NP and PP heads remain unattached.', '122': 'Pass 2: Noun Post-modifiers This pass scans NPs, right to left, creating post-modifier GRs between the unattached heads of NPs and PPs.', '123': 'If a preposition is encountered between the noun heads, a prepositional noun (nnprep) GR is created, otherwise an appositional noun (nn) GR is created.', '124': 'This corresponds to assuming right-branching PP attachment.', '125': 'After this phrase only the NP head remains unattached.', '126': 'Tense Determination The rightmost verb in each VP is considered the head.', '127': 'A VP is initially categorised as active.', '128': 'If the head verb is a form of be then the VP becomes attributive.', '129': 'Otherwise, the algorithm scans the VP from right to left: if an auxiliary verb form of be is encountered the VP becomes passive; if a progressive verb (except being) is encountered the VP becomes active.', '130': 'Only the noun heads on either side of VPs remain unattached.', '131': 'The remaining three passes attach these to the verb heads as either subjects or objects depending on the voice of the VP.', '132': 'Pass 3: Verb Pre-Attachment This pass scans sentences, right to left, associating the first NP head to the left of the VP with its head.', '133': 'If the VP is active, a subject (subj) relation is created; otherwise, a direct object (dobj) relation is created.', '134': 'For example, antigen is the subject of represent.', '135': 'Pass 4: Verb Post-Attachment This pass scans sentences, left to right, associating the first NP or PP head to the right of the VP with its head.', '136': 'If the VP was classed as active and the phrase is an NP then a direct object (dobj) relation is created.', '137': 'If the VP was classed as passive and the phrase is an NP then a subject (subj) relation is created.', '138': 'If the following phrase is a PP then an indirect object (iobj) relation is created.', '139': 'The interaction between the head verb and the preposition determine whether the noun is an indirect object of a ditransitive verb or alternatively the head of a PP that is modifying the verb.', '140': 'However, SEXTANT always attaches the PP to the previous phrase.', '141': 'Pass 5: Verb Progressive Participles The final step of the process is to attach progressive verbs to subjects and objects (without concern for whether they are already attached).', '142': 'Progressive verbs can function as nouns, verbs and adjectives and once again a naÂ¨Ä±ve approximation to the correct attachment is made.', '143': 'Any progressive verb which appears after a determiner or quantifier is considered a noun.', '144': 'Otherwise, it is a verb and passes 3 and 4 are repeated to attach subjects and objects.', '145': 'Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation.', '146': 'Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manu ally checking 60 sentences.', '147': 'Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.', '148': 'This technique is similar to Hearst and SchuÂ¨ tze (1993) and Widdows (2003).', '149': 'However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.', '150': 'In these cases, our SUFFIX EXAMPLE SUPERSENSEness remoteness attribute -tion, -ment annulment act -ist, -man statesman person -ing, -ion bowling act -ity viscosity attribute -ics, -ism electronics cognition -ene, -ane, -ine arsine substance -er, -or, -ic, -ee, -an mariner person -gy entomology cognition Table 5: Hand-coded rules for supersense guessing fall-back method is a simple hand-coded classifier which examines the unknown noun and makes a guess based on simple morphological analysis of the suffix.', '151': 'These rules were created by inspecting the suffixes of rare nouns in WORDNET 1.6.', '152': 'The supersense guessing rules are given in Table 5.', '153': 'If none of the rules match, then the default supersense artifact is assigned.', '154': 'The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection.', '155': 'Each extracted synonym votes for its one or more supersenses that appear in WORDNET 1.6.', '156': 'There are many parameters to consider: â\x80¢ how many extracted synonyms to use; â\x80¢ how to weight each synonymâ\x80\x99s vote; â\x80¢ whether unreliable synonyms should be filtered out; â\x80¢ how to deal with polysemous synonyms.', '157': 'The experiments described below consider a range of options for these parameters.', '158': 'In fact, these experiments are so quick to run we have been able to exhaustively test many combinations of these parameters.', '159': 'We have experimented with up to 200 voting extracted synonyms.', '160': 'There are several ways to weight each synonymâ\x80\x99s contribution.', '161': 'The simplest approach would be to give each synonym the same weight.', '162': 'Another approach is to use the scores returned by the similarity system.', '163': 'Alternatively, the weights can use the ranking of the extracted synonyms.', '164': 'Again these options have been considered below.', '165': 'A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.', '166': 'The final issue is how to deal with polysemy.', '167': 'Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?', '168': 'Another alternative is to only consider unambiguous synonyms with a single supersense in WORDNET.', '169': 'A disadvantage of this similarity approach is that it requires full synonym extraction, which compares the unknown word against a large number of words when, in S Y S T E M W N 1.6 W N 1.7 .1 Cia ra mit a an d Joh nso n bas eli ne 2 1 % 2 8 % Cia ra mit a an d Joh nso n per cep tro n 5 3 % 5 3 % Si mil arit y bas ed res ult s 6 8 % 6 3 % Table 6: Summary of supersense tagging accuracies fact, we want to calculate the similarity to a small number of supersenses.', '170': 'This inefficiency could be reduced significantly if we consider only very high frequency words, but even this is still expensive.', '171': 'We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003).', '172': 'The experiments were performed by considering all possible configurations of the parameters described above.', '173': 'The following voting options were considered for each supersense of each extracted synonym: the initial voting weight for a supersense could either be a constant (IDENTITY) or the similarity score (SCORE) of the synonym.', '174': 'The initial weight could then be divided by the number of supersenses to share out the weight (SHARED).', '175': 'The weight could also be divided by the rank (RANK) to penalise supersenses further down the list.', '176': 'The best performance on the 1.6 test set was achieved with the SCORE voting, without sharing or ranking penalties.', '177': 'The extracted synonyms are filtered before contributing to the vote with their supersense(s).', '178': 'This filtering involves checking that the synonymâ\x80\x99s frequency and number of contexts are large enough to ensure it is reliable.', '179': 'We have experimented with a wide range of cutoffs and the best performance on the 1.6 test set was achieved using a minimum cutoff of 5 for the synonymâ\x80\x99s frequency and the number of contexts it appears in.', '180': 'The next question is how many synonyms are considered.', '181': 'We considered using just the nearest unambiguous synonym, and the top 5, 10, 20, 50, 100 and 200 synonyms.', '182': 'All of the top performing configurations used 50 synonyms.', '183': 'We have also experimented with filtering out highly polysemous nouns by eliminating words with two, three or more synonyms.', '184': 'However, such a filter turned out to make little difference.', '185': 'Finally, we need to decide when to use the similarity measure and when to fall-back to the guessing rules.', '186': 'This is determined by looking at the frequency and number of attributes for the unknown word.', '187': 'Not surprisingly, the similarity system works better than the guessing rules if it has any information at all.', '188': 'The results are summarised in Table 6.', '189': 'The accuracy of the best-performing configurations was 68% on the Table 7: Breakdown of results by supersense WORDNET 1.6 test set with several other parameter combinations described above performing nearly as well.', '190': 'On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set.', '191': 'By optimising the parameters on the 1.7.1 test set we can increase that to 64%, indicating that we have not excessively over-tuned on the 1.6 test set.', '192': 'Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised.', '193': 'The large difference between our 1.6 and 1.7.1 test set accuracy demonstrates that the 1.7.1 set is much harder.', '194': 'Table 7 shows the breakdown in performance for each supersense.', '195': 'The columns show the number of instances of each supersense with the precision, recall and f-score measures as percentages.', '196': 'The most frequent supersenses in both test sets were person, attribute and act.', '197': 'Of the frequent categories, person is the easiest supersense to get correct in both the 1.6 and 1.7.1 test sets, followed by food, artifact and substance.', '198': 'This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.', '199': 'These factors are conducive for extracting reliable synonyms.', '200': 'These results also support Ciaramita and Johnsonâ\x80\x99s view that abstract concepts like communication, cognition and state are much harder.', '201': 'We would expect the location supersense to perform well since it is quite concrete, but unfortunately our synonym extraction system does not incorporate proper nouns, so many of these words were classified using the hand-built classifier.', '202': 'Also, in the data from Ciaramita and Johnson all of the words are in lower case, so no sensible guessing rules could help.', '203': 'An alternative approach worth exploring is to create context vectors for the supersense categories themselves and compare these against the words.', '204': 'This has the advantage of producing a much smaller number of vectors to compare against.', '205': 'In the current system, we must compare a word against the entire vocabulary (over 500 000 headwords), which is much less efficient than a comparison against only 26 supersense context vectors.', '206': 'The question now becomes how to construct vectors of supersenses.', '207': 'The most obvious solution is to sum the context vectors across the words which have each supersense.', '208': 'However, our early experiments suggest that this produces extremely large vectors which do not match well against the much smaller vectors of each unseen word.', '209': 'Also, the same questions arise in the construction of these vectors.', '210': 'How are words with multiple supersenses handled?', '211': 'Our preliminary experiments suggest that only combining the vectors for unambiguous words produces the best results.', '212': 'One solution would be to take the intersection between vectors across words for each supersense (i.e. to find the common contexts that these words appear in).', '213': 'However, given the sparseness of the data this may not leave very large context vectors.', '214': 'A final solution would be to consider a large set of the canonical attributes (Curran and Moens, 2002a) to represent each supersense.', '215': 'Canonical attributes summarise the key contexts for each headword and are used to improve the efficiency of the similarity comparisons.', '216': 'There are a number of problems our system does not currently handle.', '217': 'Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate).', '218': 'Further, our similarity system does not currently incorporate multi-word terms.', '219': 'We overcome this by using the synonyms of the last word in the multi-word term.', '220': 'However, there are 174 multi-word terms (23%) in the WORDNET 1.7.1 test set which we could probably tag more accurately with synonyms for the whole multi-word term.', '221': 'Finally, we plan to implement a supervised machine learner to replace the fall- back method, which currently has an accuracy of 37% on the WORDNET 1.7.1 test set.', '222': 'We intend to extend our experiments beyond the Ciaramita and Johnson (2003) set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results.', '223': 'We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis.', '224': 'Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors.', '225': 'Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze (1993) and Widdows (2003).', '226': 'To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6.', '227': 'We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability.', '228': 'Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor.', '229': 'Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003).', '230': 'This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus.', '231': 'Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.', '232': 'This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available.', '233': 'We would like to thank Massi Ciaramita for supplying his original data for these experiments and answering our queries, and to Stephen Clark and the anonymous reviewers for their helpful feedback and corrections.', '234': 'This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.'}",['P05-1004'],['../data/summaries/P05-1004.txt'],['../data/tba/P05-1004.json']
P05-1013,Pseudo-Projective Dependency Parsing,../data/papers/P05-1013.xml,"{'0': 'Pseudo-Projective Dependency Parsing', '1': 'In order to realize the full potential of dependency-based syntactic parsing, it is desirable to allow non-projective dependency structures.', '2': 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', '3': 'Experiments using data from the Prague Dependency Treebank show that the combined system can handle nonprojective constructions with a precision sufficient to yield a significant improvement in overall parsing accuracy.', '4': 'This leads to the best reported performance for robust non-projective parsing of Czech.', '5': 'It is sometimes claimed that one of the advantages of dependency grammar over approaches based on constituency is that it allows a more adequate treatment of languages with variable word order, where discontinuous syntactic constructions are more common than in languages like English (Mel’ˇcuk, 1988; Covington, 1990).', '6': 'However, this argument is only plausible if the formal framework allows non-projective dependency structures, i.e. structures where a head and its dependents may correspond to a discontinuous constituent.', '7': 'From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', '8': 'Thus, most broad-coverage parsers based on dependency grammar have been restricted to projective structures.', '9': 'This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).', '10': 'It is also true of the adaptation of the Collins parser for Czech (Collins et al., 1999) and the finite-state dependency parser for Turkish by Oflazer (2003).', '11': 'This is in contrast to dependency treebanks, e.g.', '12': 'Prague Dependency Treebank (Hajiˇc et al., 2001b), Danish Dependency Treebank (Kromann, 2003), and the METU Treebank of Turkish (Oflazer et al., 2003), which generally allow annotations with nonprojective dependency structures.', '13': 'The fact that projective dependency parsers can never exactly reproduce the analyses found in non-projective treebanks is often neglected because of the relative scarcity of problematic constructions.', '14': 'While the proportion of sentences containing non-projective dependencies is often 15–25%, the total proportion of non-projective arcs is normally only 1–2%.', '15': 'As long as the main evaluation metric is dependency accuracy per word, with state-of-the-art accuracy mostly below 90%, the penalty for not handling non-projective constructions is almost negligible.', '16': 'Still, from a theoretical point of view, projective parsing of non-projective structures has the drawback that it rules out perfect accuracy even as an asymptotic goal.', '17': 'There exist a few robust broad-coverage parsers that produce non-projective dependency structures, notably Tapanainen and J¨arvinen (1997) and Wang and Harper (2004) for English, Foth et al. (2004) for German, and Holan (2004) for Czech.', '18': 'In addition, there are several approaches to non-projective dependency parsing that are still to be evaluated in the large (Covington, 1990; Kahane et al., 1998; Duchier and Debusmann, 2001; Holan et al., 2001; Hellwig, 2003).', '19': 'Finally, since non-projective constructions often involve long-distance dependencies, the problem is closely related to the recovery of empty categories and non-local dependencies in constituency-based parsing (Johnson, 2002; Dienes and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill et al., 2004; Levy and Manning, 2004; Campbell, 2004).', '20': 'In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.', '21': 'First, the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels.', '22': 'When the parser is trained on the transformed data, it will ideally learn not only to construct projective dependency structures but also to assign arc labels that encode information about lifts.', '23': 'By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.', '24': 'We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).', '25': 'The rest of the paper is structured as follows.', '26': 'In section 2 we introduce the graph transformation techniques used to projectivize and deprojectivize dependency graphs, and in section 3 we describe the data-driven dependency parser that is the core of our system.', '27': 'We then evaluate the approach in two steps.', '28': 'First, in section 4, we evaluate the graph transformation techniques in themselves, with data from the Prague Dependency Treebank and the Danish Dependency Treebank.', '29': 'In section 5, we then evaluate the entire parsing system by training and evaluating on data from the Prague Dependency Treebank.', '30': 'We assume that the goal in dependency parsing is to construct a labeled dependency graph of the kind depicted in Figure 1.', '31': 'Formally, we define dependency graphs as follows: 3.', '32': 'A graph D = (W, A) is well-formed iff it is acyclic and connected.', '33': 'If (wi, r, wj) E A, we say that wi is the head of wj and wj a dependent of wi.', '34': 'In the following, we use the notation wi wj to mean that (wi, r, wj) E A; r we also use wi wj to denote an arc with unspecified label and wi —*∗ wj for the reflexive and transitive closure of the (unlabeled) arc relation.', '35': 'The dependency graph in Figure 1 satisfies all the defining conditions above, but it fails to satisfy the condition ofprojectivity (Kahane et al., 1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is), which is not dominated by jedna.', '36': 'As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.', '37': 'Here we use a slightly different notion of lift, applying to individual arcs and moving their head upwards one step at a time: Intuitively, lifting an arc makes the word wk dependent on the head wi of its original head wj (which is unique in a well-formed dependency graph), unless wj is a root in which case the operation is undefined (but then wj —* wk is necessarily projective if the dependency graph is well-formed).', '38': 'Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.', '39': 'However, since we want to preserve as much of the original structure as possible, we are interested in finding a transformation that involves a minimal number of lifts.', '40': 'Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).', '41': 'Applying the function PROJECTIVIZE to the graph in Figure 1 yields the graph in Figure 2, where the problematic arc pointing to Z has been lifted from the original head jedna to the ancestor je.', '42': 'Using the terminology of Kahane et al. (1998), we say that jedna is the syntactic head of Z, while je is its linear head in the projectivized representation.', '43': 'Unlike Kahane et al. (1998), we do not regard a projectivized representation as the final target of the parsing process.', '44': 'Instead, we want to apply an inverse transformation to recover the underlying (nonprojective) dependency graph.', '45': 'In order to facilitate this task, we extend the set of arc labels to encode information about lifting operations.', '46': 'In principle, it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head, but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.', '47': 'In practice, we can therefore expect a trade-off such that increasing the amount of information encoded in arc labels will cause an increase in the accuracy of the inverse transformation but a decrease in the accuracy with which the parser can construct the labeled representations.', '48': 'To explore this tradeoff, we have performed experiments with three different encoding schemes (plus a baseline), which are described schematically in Table 1.', '49': 'The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d↑h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.', '50': 'Using this encoding scheme, the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).', '51': 'In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.', '52': 'Thus, the arc from je to jedna will be labeled 5b↓ (to indicate that there is a syntactic head below it).', '53': 'In the third and final scheme, denoted Path, we keep the extra infor2Note that this is a baseline for the parsing experiment only (Experiment 2).', '54': 'For Experiment 1 it is meaningless as a baseline, since it would result in 0% accuracy. mation on path labels but drop the information about the syntactic head of the lifted arc, using the label d↑ instead of d↑h (AuxP↑ instead of AuxP↑Sb).', '55': 'As can be seen from the last column in Table 1, both Head and Head+Path may theoretically lead to a quadratic increase in the number of distinct arc labels (Head+Path being worse than Head only by a constant factor), while the increase is only linear in the case of Path.', '56': 'On the other hand, we can expect Head+Path to be the most useful representation for reconstructing the underlying non-projective dependency graph.', '57': 'In approaching this problem, a variety of different methods are conceivable, including a more or less sophisticated use of machine learning.', '58': 'In the present study, we limit ourselves to an algorithmic approach, using a deterministic breadthfirst search.', '59': ""The details of the transformation procedure are slightly different depending on the encoding schemes: d↑h let the linear head be the syntactic head). target arc must have the form wl −→ wm; if no target arc is found, Head is used as backoff. must have the form wl −→ wm and no outgoing arcs of the form wm p'↓ −→ wo; no backoff."", '60': 'In section 4 we evaluate these transformations with respect to projectivized dependency treebanks, and in section 5 they are applied to parser output.', '61': 'Before we turn to the evaluation, however, we need to introduce the data-driven dependency parser used in the latter experiments.', '62': 'In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).', '63': 'The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.', '64': 'At each point during the derivation, the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped.', '65': 'More details on the parsing algorithm can be found in Nivre (2003).', '66': 'The choice between different actions is in general nondeterministic, and the parser relies on a memorybased classifier, trained on treebank data, to predict the next action based on features of the current parser configuration.', '67': 'Table 2 shows the features used in the current version of the parser.', '68': 'At each point during the derivation, the prediction is based on six word tokens, the two topmost tokens on the stack, and the next four input tokens.', '69': 'For each token, three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token – the arc from its head and the arcs to its leftmost and rightmost dependent, respectively.', '70': 'Except for the left3The graphs satisfy all the well-formedness conditions given in section 2 except (possibly) connectedness.', '71': 'For robustness reasons, the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token, dependency type features are limited to tokens on the stack.', '72': 'The prediction based on these features is a knearest neighbor classification, using the IB1 algorithm and k = 5, the modified value difference metric (MVDM) and class voting with inverse distance weighting, as implemented in the TiMBL software package (Daelemans et al., 2003).', '73': 'More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).', '74': 'The first experiment uses data from two dependency treebanks.', '75': 'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text, annotated on three levels, the morphological, analytical and tectogrammatical levels (Hajiˇc, 1998).', '76': 'Our experiments all concern the analytical annotation, and the first experiment is based only on the training part.', '77': 'The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus, with annotation of primary and secondary dependencies (Kromann, 2003).', '78': 'The entire treebank is used in the experiment, but only primary dependencies are considered.4 In all experiments, punctuation tokens are included in the data but omitted in evaluation scores.', '79': 'In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.', '80': 'As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.', '81': 'However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.', '82': 'The last four columns in Table 3 show the distribution of nonprojective arcs with respect to the number of lifts required.', '83': 'It is worth noting that, although nonprojective constructions are less frequent in DDT than in PDT, they seem to be more deeply nested, since only about 80% can be projectivized with a single lift, while almost 95% of the non-projective arcs in PDT only require a single lift.', '84': 'In the second part of the experiment, we applied the inverse transformation based on breadth-first search under the three different encoding schemes.', '85': 'The results are given in Table 4.', '86': 'As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.', '87': 'However, it can be noted that the results for the least informative encoding, Path, are almost comparable, while the third encoding, Head, gives substantially worse results for both data sets.', '88': 'We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.', '89': 'The increase is generally higher for PDT than for DDT, which indicates a greater diversity in non-projective constructions.', '90': 'The second experiment is limited to data from PDT.5 The training part of the treebank was projectivized under different encoding schemes and used to train memory-based dependency parsers, which were run on the test part of the treebank, consisting of 7,507 sentences and 125,713 tokens.6 The inverse transformation was applied to the output of the parsers and the result compared to the gold standard test set.', '91': 'Table 5 shows the overall parsing accuracy attained with the three different encoding schemes, compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.', '92': 'Evaluation metrics used are Attachment Score (AS), i.e. the proportion of tokens that are attached to the correct head, and Exact Match (EM), i.e. the proportion of sentences for which the dependency graph exactly matches the gold standard.', '93': 'In the labeled version of these metrics (L) both heads and arc labels must be correct, while the unlabeled version (U) only considers heads.', '94': 'The first thing to note is that projectivizing helps in itself, even if no encoding is used, as seen from the fact that the projective baseline outperforms the non-projective training condition by more than half a percentage point on attachment score, although the gain is much smaller with respect to exact match.', '95': 'The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.', '96': 'With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.', '97': 'All improvements over the baseline are statistically significant beyond the 0.01 level (McNemar’s test).', '98': 'By contrast, when we turn to a comparison of the three encoding schemes it is hard to find any significant differences, and the overall impression is that it makes little or no difference which encoding scheme is used, as long as there is some indication of which words are assigned their linear head instead of their syntactic head by the projective parser.', '99': 'This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.', '100': 'It is likely that the more complex cases, where path information could make a difference, are beyond the reach of the parser in most cases.', '101': 'However, if we consider precision, recall and Fmeasure on non-projective dependencies only, as shown in Table 6, some differences begin to emerge.', '102': 'The most informative scheme, Head+Path, gives the highest scores, although with respect to Head the difference is not statistically significant, while the least informative scheme, Path – with almost the same performance on treebank transformation – is significantly lower (p < 0.01).', '103': 'On the other hand, given that all schemes have similar parsing accuracy overall, this means that the Path scheme is the least likely to introduce errors on projective arcs.', '104': 'The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.', '105': 'Although the best published results for the Collins parser is 80% UAS (Collins, 1999), this parser reaches 82% when trained on the entire training data set, and an adapted version of Charniak’s parser (Charniak, 2000) performs at 84% (Jan Hajiˇc, pers. comm.).', '106': 'However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).', '107': 'Compared to related work on the recovery of long-distance dependencies in constituency-based parsing, our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process, via an extension of the set of syntactic categories, whereas most other approaches rely on postprocessing only.', '108': 'However, while Dienes and Dubey recognize empty categories in a pre-processing step and only let the parser find their antecedents, we use the parser both to detect dislocated dependents and to predict either the type or the location of their syntactic head (or both) and use post-processing only to transform the graph in accordance with the parser’s analysis.', '109': 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.', '110': 'The main result is that the combined system can recover non-projective dependencies with a precision sufficient to give a significant improvement in overall parsing accuracy, especially with respect to the exact match criterion, leading to the best reported performance for robust non-projective parsing of Czech.', '111': 'This work was supported in part by the Swedish Research Council (621-2002-4207).', '112': 'Memory-based classifiers for the experiments were created using TiMBL (Daelemans et al., 2003).', '113': 'Special thanks to Jan Hajiˇc and Matthias Trautner Kromann for assistance with the Czech and Danish data, respectively, and to Jan Hajiˇc, Tom´aˇs Holan, Dan Zeman and three anonymous reviewers for valuable comments on a preliminary version of the paper.'}","['P05-1013_swastika', 'P05-1013_aakansha', 'P05-1013_vardha']","['../data/summaries/P05-1013_swastika.txt', '../data/summaries/P05-1013_aakansha.txt', '../data/summaries/P05-1013_vardha.txt']","['../data/tba/P05-1013_swastika.json', '../data/tba/P05-1013_aakansha.json', '../data/tba/P05-1013_vardha.json']"
P06-2124,BiTAM: Bilingual Topic AdMixture Models forWord Alignment,../data/papers/P06-2124.xml,"{'0': 'BiTAM: Bilingual Topic AdMixture Models forWord Alignment', '1': 'We propose a novel bilingual topical admixture (BiTAM) formalism for word alignment in statistical machine translation.', '2': 'Under this formalism, the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics; each word-pair follows a topic-specific bilingual translation model.', '3': 'Three BiTAM models are proposed to capture topic sharing at different levels of linguistic granularity (i.e., at the sentence or word levels).', '4': 'These models enable word- alignment process to leverage topical contents of document-pairs.', '5': 'Efficient variational approximation algorithms are designed for inference and parameter estimation.', '6': 'With the inferred latent topics, BiTAM models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects.', '7': 'Our preliminary experiments show that the proposed models improve word alignment accuracy, and lead to better translation quality.', '8': 'Parallel data has been treated as sets of unrelated sentence-pairs in state-of-the-art statistical machine translation (SMT) models.', '9': 'Most current approaches emphasize within-sentence dependencies such as the distortion in (Brown et al., 1993), the dependency of alignment in HMM (Vogel et al., 1996), and syntax mappings in (Yamada and Knight, 2001).', '10': 'Beyond the sentence-level, corpus- level word-correlation and contextual-level topical information may help to disambiguate translation candidates and word-alignment choices.', '11': 'For example, the most frequent source words (e.g., functional words) are likely to be translated into words which are also frequent on the target side; words of the same topic generally bear correlations and similar translations.', '12': 'Extended contextual information is especially useful when translation models are vague due to their reliance solely on word-pair co- occurrence statistics.', '13': 'For example, the word shot in â\x80\x9cIt was a nice shot.â\x80\x9d should be translated differently depending on the context of the sentence: a goal in the context of sports, or a photo within the context of sightseeing.', '14': 'Nida (1964) stated that sentence-pairs are tied by the logic-flow in a document-pair; in other words, the document-pair should be word-aligned as one entity instead of being uncorrelated instances.', '15': 'In this paper, we propose a probabilistic admixture model to capture latent topics underlying the context of document- pairs.', '16': 'With such topical information, the translation models are expected to be sharper and the word-alignment process less ambiguous.', '17': 'Previous works on topical translation models concern mainly explicit logical representations of semantics for machine translation.', '18': 'This include knowledge-based (Nyberg and Mitamura, 1992) and interlingua-based (Dorr and Habash, 2002) approaches.', '19': 'These approaches can be expensive, and they do not emphasize stochastic translation aspects.', '20': 'Recent investigations along this line includes using word-disambiguation schemes (Carpua and Wu, 2005) and non-overlapping bilingual word-clusters (Wang et al., 1996; Och, 1999; Zhao et al., 2005) with particular translation models, which showed various degrees of success.', '21': 'We propose a new statistical formalism: Bilingual Topic AdMixture model, or BiTAM, to facilitate topic-based word alignment in SMT.', '22': 'Variants of admixture models have appeared in population genetics (Pritchard et al., 2000) and text modeling (Blei et al., 2003).', '23': 'Statistically, an object is said to be derived from an admixture if it consists of a bag of elements, each sampled independently or coupled in some way, from a mixture model.', '24': 'In a typical SMT setting, each document- pair corresponds to an object; depending on a chosen modeling granularity, all sentence-pairs or word-pairs in the document-pair correspond to the elements constituting the object.', '25': 'Correspondingly, a latent topic is sampled for each pair from a prior topic distribution to induce topic-specific translations; and the resulting sentence-pairs and word- pairs are marginally dependent.', '26': 'Generatively, this admixture formalism enables word translations to be instantiated by topic-specific bilingual models 969 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 969â\x80\x93976, Sydney, July 2006.', '27': 'Qc 2006 Association for Computational Linguistics and/or monolingual models, depending on their contexts.', '28': 'In this paper we investigate three instances of the BiTAM model, They are data-driven and do not need handcrafted knowledge engineering.', '29': 'The remainder of the paper is as follows: in section 2, we introduce notations and baselines; in section 3, we propose the topic admixture models; in section 4, we present the learning and inference algorithms; and in section 5 we show experiments of our models.', '30': 'We conclude with a brief discussion in section 6.', '31': 'In statistical machine translation, one typically uses parallel data to identify entities such as â\x80\x9cword-pairâ\x80\x9d, â\x80\x9csentence-pairâ\x80\x9d, and â\x80\x9cdocument- pairâ\x80\x9d.', '32': 'Formally, we define the following terms1: â\x80¢ A word-pair (fj , ei) is the basic unit for word alignment, where fj is a French word and ei is an English word; j and i are the position indices in the corresponding French sentence f and English sentence e. â\x80¢ A sentence-pair (f , e) contains the source sentence f of a sentence length of J ; a target sentence e of length I . The two sentences f and e are translations of each other.â\x80¢ A document-pair (F, E) refers to two doc uments which are translations of each other.', '33': 'Assuming sentences are one-to-one correspondent, a document-pair has a sequence of N parallel sentence-pairs {(fn, en)}, where (fn, en) is the ntth parallel sentence-pair.', '34': 'â\x80¢ A parallel corpus C is a collection of M parallel document-pairs: {(Fd, Ed)}.', '35': '2.1 Baseline: IBM Model-1.', '36': 'The translation process can be viewed as operations of word substitutions, permutations, and insertions/deletions (Brown et al., 1993) in noisy- channel modeling scheme at parallel sentence-pair level.', '37': 'The translation lexicon p(f |e) is the key component in this generative process.', '38': 'An efficient way to learn p(f |e) is IBM1: IBM1 has global optimum; it is efficient and easily scalable to large training data; it is one of the most informative components for re-ranking translations (Och et al., 2004).', '39': 'We start from IBM1 as our baseline model, while higher-order alignment models can be embedded similarly within the proposed framework.', '40': 'Now we describe the BiTAM formalism that captures the latent topical structure and generalizes word alignments and translations beyond sentence-level via topic sharing across sentence- pairs: Eâ\x88\x97 = arg max p(F|E)p(E), (2) {E} where p(F|E) is a document-level translation model, generating the document F as one entity.', '41': 'In a BiTAM model, a document-pair (F, E) is treated as an admixture of topics, which is induced by random draws of a topic, from a pool of topics, for each sentence-pair.', '42': 'A unique normalized and real-valued vector Î¸, referred to as a topic-weight vector, which captures contributions of different topics, are instantiated for each document-pair, so that the sentence-pairs with their alignments are generated from topics mixed according to these common proportions.', '43': 'Marginally, a sentence- pair is word-aligned according to a unique bilingual model governed by the hidden topical assignments.', '44': 'Therefore, the sentence-level translations are coupled, rather than being independent as assumed in the IBM models and their extensions.', '45': 'Because of this coupling of sentence-pairs (via topic sharing across sentence-pairs according to a common topic-weight vector), BiTAM is likely to improve the coherency of translations by treating the document as a whole entity, instead of uncorrelated segments that have to be independently aligned and then assembled.', '46': 'There are at least two levels at which the hidden topics can be sampled for a document-pair, namely: the sentence- pair and the word-pair levels.', '47': 'We propose three variants of the BiTAM model to capture the latent topics of bilingual documents at different levels.', '48': 'J I 3.1 BiTAM1: The Frameworks p(f |e) = n ) p(fj |ei ) Â· p(ei |e).', '49': '(1) j=1 i=1 1 We follow the notations in (Brown et al., 1993) for.', '50': 'English-French, i.e., e â\x86\x94 f , although our models are tested,in this paper, for EnglishChinese.', '51': 'We use the end-user ter minology for source and target languages.', '52': 'In the first BiTAM model, we assume that topics are sampled at the sentence-level.', '53': 'Each document- pair is represented as a random mixture of latent topics.', '54': 'Each topic, topic-k, is presented by a topic-specific word-translation table: Bk , which is e I e I Î² e I a Î± Î¸ z f J B N M Î± Î¸ z a a f J B Î± Î¸ z N M f J B N M (a) (b) (c) Figure 1: BiTAM models for Bilingual document- and sentence-pairs.', '55': 'A node in the graph represents a random variable, and a hexagon denotes a parameter.', '56': 'Un-shaded nodes are hidden variables.', '57': 'All the plates represent replicates.', '58': 'The outmost plate (M -plate) represents M bilingual document-pairs, while the inner N -plate represents the N repeated choice of topics for each sentence-pairs in the document; the inner J -plate represents J word-pairs within each sentence-pair.', '59': '(a) BiTAM1 samples one topic (denoted by z) per sentence-pair; (b) BiTAM2 utilizes the sentence-level topics for both the translation model (i.e., p(f |e, z)) and the monolingual word distribution (i.e., p(e|z)); (c) BiTAM3 samples one topic per word-pair.', '60': 'a translation lexicon: Bi,j,k =p(f =fj |e=ei, z=k), where z is an indicator variable to denote the choice of a topic.', '61': 'Given a specific topic-weight vector Î¸d for a document-pair, each sentence-pair draws its conditionally independent topics from a mixture of topics.', '62': 'This generative process, for a document-pair (Fd, Ed), is summarized as below: 1.', '63': 'Sample sentence-number N from a Poisson(Î³)..', '64': '2.', '65': 'Sample topic-weight vector Î¸d from a Dirichlet(Î±)..', '66': '3.', '67': 'For each sentence-pair (fn , en ) in the dtth doc-pair ,.', '68': '(a) Sample sentence-length Jn from Poisson(Î´); (b) Sample a topic zdn from a Multinomial(Î¸d ); (c) Sample ej from a monolingual model p(ej );(d) Sample each word alignment link aj from a uni form model p(aj ) (or an HMM); (e) Sample each fj according to a topic-specific graphical model representation for the BiTAM generative scheme discussed so far.', '69': 'Note that, the sentence-pairs are now connected by the node Î¸d. Therefore, marginally, the sentence-pairs are not independent of each other as in traditional SMT models, instead they are conditionally independent given the topic-weight vector Î¸d. Specifically, BiTAM1 assumes that each sentence-pair has one single topic.', '70': 'Thus, the word-pairs within this sentence-pair are conditionally independent of each other given the hidden topic index z of the sentence-pair.', '71': 'The last two sub-steps (3.d and 3.e) in the BiTam sampling scheme define a translation model, in which an alignment link aj is proposed translation lexicon p(fj |e, aj , zn , B).', '72': 'and an observation of fj is generated accordingWe assume that, in our model, there are K pos sible topics that a document-pair can bear.', '73': 'For each document-pair, a K -dimensional Dirichlet random variable Î¸d, referred to as the topic-weight vector of the document, can take values in the (K â\x88\x921)-simplex following a probability density: to the proposed distributions.', '74': 'We simplify alignment model of a, as in IBM1, by assuming that aj is sampled uniformly at random.', '75': 'Given the parameters Î±, B, and the English part E, the joint conditional distribution of the topic-weight vector Î¸, the topic indicators z, the alignment vectors A, and the document F can be written as: Î\x93( K Î±k ) p(Î¸|Î±) = k=1 Î¸Î±1 â\x88\x921 Â· Â· Â· Î¸Î±K â\x88\x921 , (3) p(F,A, Î¸, z|E, Î±, B) = k=1 Î\x93(Î±k ) N (4) where the hyperparameter Î± is a K -dimension vector with each component Î±k >0, and Î\x93(x) is the Gamma function.', '76': 'The alignment is represented by a J -dimension vector a = {a1, a2, Â· Â· Â· , aJ }; for each French word fj at the position j, an position variable aj maps it to anEnglish word eaj at the position aj in English sen p(Î¸ | Î±) n p(zn |Î¸)p(fn , an |en , Î±, Bzn), n=1 where N is the number of the sentence-pair.', '77': 'Marginalizing out Î¸ and z, we can obtain the marginal conditional probability of generating F from E for each document-pair: p(F, A|E, Î±, Bzn ) = tence.', '78': 'The word level translation lexicon probabil- r ( (5) ities are topic-specific, and they are parameterized by the matrix B = {Bk }.', '79': 'p(Î¸|Î±) n) p(zn |Î¸)p(fn , an |en , Bzn ) dÎ¸, n=1 zn For simplicity, in our current models we omit the modelings of the sentence-number N and the sentence-length Jn, and focus only on the bilingual translation model.', '80': 'Figure 1 (a) shows the where p(fn, an|en, Bzn ) is a topic-specific sentence-level translation model.', '81': 'For simplicity, we assume that the French words fj â\x80\x99s are conditionally independent of each other; the alignment variables aj â\x80\x99s are independent of other variables and are uniformly distributed a priori.', '82': 'Therefore, the distribution for each sentence-pair is: p(fn , an |en , Bzn) = p(fn |en , an , Bzn)p(an |en , Bzn) Jn â\x80\x9cNullâ\x80\x9d is attached to every target sentence to align the source words which miss their translations.', '83': 'Specifically, the latent Dirichlet allocation (LDA) in (Blei et al., 2003) can be viewed as a special case of the BiTAM3, in which the target sentence 1 n p(f n n j=1 |eanj , Bzn ).', '84': '(6) contains only one word: â\x80\x9cNullâ\x80\x9d, and the alignment link a is no longer a hidden variable.', '85': 'Thus, the conditional likelihood for the entire parallel corpus is given by taking the product of the marginal probabilities of each individual document-pair in Eqn.', '86': '5.', '87': '3.2 BiTAM2: Monolingual Admixture.', '88': 'In general, the monolingual model for English can also be a rich topic-mixture.', '89': 'This is realized by using the same topic-weight vector Î¸d and the same topic indicator zdn sampled according to Î¸d, as described in Â§3.1, to introduce not onlytopic-dependent translation lexicon, but also topic dependent monolingual model of the source language, English in this case, for generating each sentence-pair (Figure 1 (b)).', '90': 'Now e is generated', '91': 'Due to the hybrid nature of the BiTAM models, exact posterior inference of the hidden variables A, z and Î¸ is intractable.', '92': 'A variational inference is used to approximate the true posteriors of these hidden variables.', '93': 'The inference scheme is presented for BiTAM1; the algorithms for BiTAM2 and BiTAM3 are straight forward extensions and are omitted.', '94': '4.1 Variational Approximation.', '95': 'To approximate: p(Î¸, z, A|E, F, Î±, B), the joint posterior, we use the fully factorized distribution over the same set of hidden variables: q(Î¸,z, A) â\x88\x9d q(Î¸|Î³, Î±)Â· from a topic-based language model Î², instead of a N Jn (7) uniform distribution in BiTAM1.', '96': 'We refer to this n q(zn |Ï\x86n ) n q(anj , fnj |Ï\x95nj , en , B), model as BiTAM2.', '97': 'n=1 j=1 Unlike BiTAM1, where the information observed in ei is indirectly passed to z via the node of fj and the hidden variable aj , in BiTAM2, the topics of corresponding English and French sentences are also strictly aligned so that the information observed in ei can be directly passed to z, in the hope of finding more accurate topics.', '98': 'The topics are inferred more directly from the observed bilingual data, and as a result, improve alignment.', '99': '3.3 BiTAM3: Word-level Admixture.', '100': 'where the Dirichlet parameter Î³, the multinomial parameters (Ï\x861, Â· Â· Â· , Ï\x86n), and the parameters (Ï\x95n1, Â· Â· Â· , Ï\x95nJn ) are known as variational param eters, and can be optimized with respect to the KullbackLeibler divergence from q(Â·) to the original p(Â·) via an iterative fixed-point algorithm.', '101': 'It can be shown that the fixed-point equations for the variational parameters in BiTAM1 are as follows: Nd Î³k = Î±k + ) Ï\x86dnk (8) n=1 K It is straightforward to extend the sentence-level BiTAM1 to a word-level admixture model, by Ï\x86dnk â\x88\x9d exp (Î¨(Î³k ) â\x88\x92 Î¨( Jdn Idn ) kt =1 Î³kt ) Â· sampling topic indicator zn,j for each word-pair (fj , eaj ) in the ntth sentence-pair, rather than once for all (words) in the sentence (Figure 1 (c)).', '102': 'exp ( ) ) Ï\x95dnji log Bf ,e ,k (9) j i j=1 i=1 K ( This gives rise to our BiTAM3.', '103': 'The conditional Ï\x95dnji â\x88\x9d exp ) Ï\x86dnk log Bf ,e ,k , (10) k=1 likelihood functions can be obtained by extending where Î¨(Â·) is a digamma function.', '104': 'Note that inthe formulas in Â§3.1 to move the variable zn,j in side the same loop over each of the fn,j . the above formulas Ï\x86 dnkis the variational param 3.4 Incorporation of Word â\x80\x9cNullâ\x80\x9d.', '105': 'Similar to IBM models, â\x80\x9cNullâ\x80\x9d word is used for the source words which have no translation counterparts in the target language.', '106': 'For example, Chinese words â\x80\x9cdeâ\x80\x9d (ffl) , â\x80\x9cbaâ\x80\x9d (I\\) and â\x80\x9cbeiâ\x80\x9d (%i) generally do not have translations in English.', '107': 'eter underlying the topic indicator zdn of the nth sentence-pair in document d, and it can be used to predict the topic distribution of that sentence-pair.', '108': 'Following a variational EM scheme (Beal and Ghahramani, 2002), we estimate the model parameters Î± and B in an unsupervised fashion.', '109': 'Essentially, Eqs.', '110': '(810) above constitute the E-step, where the posterior estimations of the latent variables are obtained.', '111': 'In the M-step, we update Î± and B so that they improve a lower bound of the log-likelihood defined bellow: L(Î³, Ï\x86, Ï\x95; Î±, B) = Eq [log p(Î¸|Î±)]+Eq [log p(z|Î¸)] +Eq [log p(a)]+Eq [log p(f |z, a, B)]â\x88\x92Eq [log q(Î¸)] â\x88\x92Eq [log q(z)]â\x88\x92Eq [log q(a)].', '112': '(11) The close-form iterative updating formula B is: BDA selects iteratively, for each f , the best aligned e, such that the word-pair (f, e) is the maximum of both row and column, or its neighbors have more aligned pairs than the other combpeting candidates.A close check of {Ï\x95dnji} in Eqn.', '113': '10 re veals that it is essentially an exponential model: weighted log probabilities from individual topic- specific translation lexicons; or it can be viewed as weighted geometric mean of the individual lex M Nd Jdn Idn Bf,e,k â\x88\x9d ) ) ) ) Î´(f, fj )Î´(e, ei )Ï\x86dnk Ï\x95dnji (12) d n=1 j=1 i=1 For Î±, close-form update is not available, and we resort to gradient accent as in (SjoÂ¨ lander et al., 1996) with restarts to ensure each updated Î±k >0.', '114': '4.2 Data Sparseness and Smoothing.', '115': 'The translation lexicons Bf,e,k have a potential size of V 2K , assuming the vocabulary sizes for both languages are V . The data sparsity (i.e., lack of large volume of document-pairs) poses a more serious problem in estimating Bf,e,k than the monolingual case, for instance, in (Blei et al., 2003).', '116': 'To reduce the data sparsity problem, we introduce two remedies in our models.', '117': 'First: Laplace smoothing.', '118': 'In this approach, the matrix set B, whose columns correspond to parameters of conditional multinomial distributions, is treated as a collection of random vectors all under a symmetric Dirichlet prior; the posterior expectation of these multinomial parameter vectors can be estimated using Bayesian theory.', '119': 'Second: interpolation smoothing.', '120': 'Empirically, we can employ a linear interpolation with IBM1 to avoid overfitting: Bf,e,k = Î»Bf,e,k +(1â\x88\x92Î»)p(f |e).', '121': '(13) As in Eqn.', '122': '1, p(f |e) is learned via IBM1; Î» is estimated via EM on held out data.', '123': '4.3 Retrieving Word Alignments.', '124': 'Two word-alignment retrieval schemes are designed for BiTAMs: the uni-direction alignment (UDA) and the bi-direction alignment (BDA).', '125': 'Both use the posterior mean of the alignment indicators adnji, captured by what we call the poste rior alignment matrix Ï\x95 â\x89¡ {Ï\x95dnji}.', '126': 'UDA uses a French word fdnj (at the jtth position of ntth sentence in the dtth document) to query Ï\x95 to get the best aligned English word (by taking the maximum point in a row of Ï\x95): adnj = arg max Ï\x95dnji .', '127': '(14) iâ\x88\x88[1,Idn ] iconâ\x80\x99s strength.', '128': 'We evaluate BiTAM models on the word alignment accuracy and the translation quality.', '129': 'For word alignment accuracy, F-measure is reported, i.e., the harmonic mean of precision and recall against a gold-standard reference set; for translation quality, Bleu (Papineni et al., 2002) and its variation of NIST scores are reported.', '130': 'Table 1: Training and Test Data Statistics Tra in #D oc.', '131': '#S ent . #T ok en s En gli sh Ch ine se Tr ee b a n k F B IS . B J Si n or a m a Xi nH ua 31 6 6,1 11 2,3 73 19, 14 0 41 72 10 5K 10 3K 11 5K 13 3K 4.1 8M 3.8 1M 3.8 5M 10 5K 3.5 4M 3.6 0M 3.9 3M Tes t 95 62 7 25, 50 0 19, 72 6 We have two training data settings with different sizes (see Table 1).', '132': 'The small one consists of 316 document-pairs from Tree- bank (LDC2002E17).', '133': 'For the large training data setting, we collected additional document- pairs from FBIS (LDC2003E14, Beijing part), Sinorama (LDC2002E58), and Xinhua News (LDC2002E18, document boundaries are kept in our sentence-aligner (Zhao and Vogel, 2002)).', '134': 'There are 27,940 document-pairs, containing 327K sentence-pairs or 12 million (12M) English tokens and 11M Chinese tokens.', '135': 'To evaluate word alignment, we hand-labeled 627 sentence-pairs from 95 document-pairs sampled from TIDESâ\x80\x9901 dryrun data.', '136': 'It contains 14,769 alignment-links.', '137': 'To evaluate translation quality, TIDESâ\x80\x9902 Eval.', '138': 'test is used as development set, and TIDESâ\x80\x9903 Eval.', '139': 'test is used as the unseen test data.', '140': '5.1 Model Settings.', '141': 'First, we explore the effects of Null word and smoothing strategies.', '142': 'Empirically, we find that adding â\x80\x9cNullâ\x80\x9d word is always beneficial to all models regardless of number of topics selected.', '143': 'To pics Le xic ons To pic1 To pic2 To pic3 Co oc.', '144': 'IBM 1 H M M IBM 4 p( Ch ao Xi an (Ji!', '145': '$) |K ore an) 0.', '146': '06 12 0.', '147': '21 38 0.', '148': '22 54 3 8 0.2 19 8 0.2 15 7 0.2 10 4 p( Ha nG uo (li!', '149': 'ï¿½ )|K ore an) 0.', '150': '83 79 0.', '151': '61 16 0.', '152': '02 43 4 6 0.5 61 9 0.4 72 3 0.4 99 3 Table 2: Topic-specific translation lexicons are learned by a 3-topic BiTAM1.', '153': 'The third lexicon (Topic-3) prefers to translate the word Korean into ChaoXian (Ji!$:North Korean).', '154': 'The co-occurrence (Cooc), IBM1&4 and HMM only prefer to translate into HanGuo (li!ï¿½:South Korean).', '155': 'The two candidate translations may both fade out in the learned translation lexicons.', '156': 'Uni gram rank 1 2 3 4 5 6 7 8 9 1 0 Topi c A. fo rei gn c h i n a u . s . dev elop men t trad e ente rpri ses tech nolo gy cou ntri es y e a r eco nom ic Topi c B. cho ngqi ng com pani es take over s co m pa ny cit y bi lli o n m o r e eco nom ic re a c h e d y u a n Topi c C. sp or ts dis abl ed te a m p e o p l e caus e w at e r na tio na l ga m es han dica ppe d me mb ers Table 3: Three most distinctive topics are displayed.', '157': 'The English words for each topic are ranked according to p(e|z) estimated from the topic-specific English sentences weighted by {Ï\x86dnk }.', '158': '33 functional words were removed to highlight the main content of each topic.', '159': 'Topic A is about Us-China economic relationships; Topic B relates to Chinese companiesâ\x80\x99 merging; Topic C shows the sports of handicapped people.The interpolation smoothing in Â§4.2 is effec tive, and it gives slightly better performance than Laplace smoothing over different number of topics for BiTAM1.', '160': 'However, the interpolation leverages the competing baseline lexicon, and this can blur the evaluations of BiTAMâ\x80\x99s contributions.', '161': 'Laplace smoothing is chosen to emphasize more on BiTAMâ\x80\x99s strength.', '162': 'Without any smoothing, F- measure drops very quickly over two topics.', '163': 'In all our following experiments, we use both Null word and Laplace smoothing for the BiTAM models.', '164': 'We train, for comparison, IBM1&4 and HMM models with 8 iterations of IBM1, 7 for HMM and 3 for IBM4 (18h743) with Null word and a maximum fertility of 3 for ChineseEnglish.', '165': 'Choosing the number of topics is a model selection problem.', '166': 'We performed a tenfold cross- validation, and a setting of three-topic is chosen for both the small and the large training data sets.', '167': 'The overall computation complexity of the BiTAM is linear to the number of hidden topics.', '168': '5.2 Variational Inference.', '169': 'Under a non-symmetric Dirichlet prior, hyperparameter Î± is initialized randomly; B (K translation lexicons) are initialized uniformly as did in IBM1.', '170': 'Better initialization of B can help to avoid local optimal as shown in Â§ 5.5.', '171': 'With the learned B and Î± fixed, the variational parameters to be computed in Eqn.', '172': '(810) are initialized randomly; the fixed-point iterative updates stop when the change of the likelihood is smaller than 10â\x88\x925.', '173': 'The convergent variational parameters, corresponding to the highest likelihood from 20 random restarts, are used for retrieving the word alignment for unseen document-pairs.', '174': 'To estimate B, Î² (for BiTAM2) and Î±, at most eight variational EM iterations are run on the training data.', '175': 'Figure 2 shows absolute 2â\x88¼3% better F-measure over iterations of variational EM using two and three topics of BiTAM1 comparing with IBM1.', '176': 'BiTam with Null and Laplace Smoothing Over Var.', '177': 'EM Iterations 41 40 39 38 37 36 35 BiTamâ\x88\x921, Topic #=3 34 BiTamâ\x88\x921, Topic #=2.', '178': 'IB M â\x88\x921 33 32 3 3.5 4 4.5 5 5.5 6 6.5 7 7.5 8 Number of EM/Variational EM Iterations for IBMâ\x88\x921 and BiTamâ\x88\x921 Figure 2: performances over eight Variational EM iterations of BiTAM1 using both the â\x80\x9cNullâ\x80\x9d word and the laplace smoothing; IBM1 is shown over eight EM iterations for comparison.', '179': '5.3 Topic-Specific Translation.', '180': 'Lexicons The topic-specific lexicons Bk are smaller in size than IBM1, and, typically, they contain topic trends.', '181': 'For example, in our training data, North Korean is usually related to politics and translated into â\x80\x9cChaoXianâ\x80\x9d (Ji!', '182': '$); South Korean occurs more often with economics and is translated as â\x80\x9cHanGuoâ\x80\x9d(li!', '183': 'ï¿½).', '184': 'BiTAMs discriminate the two by considering the topics of the context.', '185': 'Table 2 shows the lexicon entries for â\x80\x9cKoreanâ\x80\x9d learned by a 3-topic BiTAM1.', '186': 'The values are relatively sharper, and each clearly favors one of the candidates.', '187': 'The co-occurrence count, however, only favors â\x80\x9cHanGuoâ\x80\x9d, and this can easily dominate the decisions of IBM and HMM models due to their ignorance of the topical context.', '188': 'Monolingual topics learned by BiTAMs are, roughly speaking, fuzzy especially when the number of topics is small.', '189': 'With proper filtering, we find that BiTAMs do capture some topics as illustrated in Table 3.', '190': '5.4 Evaluating Word.', '191': 'Alignments We evaluate word alignment accuracies in various settings.', '192': 'Notably, BiTAM allows to test alignments in two directions: English-to Chinese (EC) and Chinese-to-English (CE).', '193': 'Additional heuristics are applied to further improve the accuracies.', '194': 'Inter takes the intersection of the two directions and generates high-precision alignments; the SE T TI N G IBM 1 H M M IBM 4 B I T A M 1 U D A BDA B I T A M 2 U D A BDA B I T A M 3 U D A BDA C E ( % ) E C ( % ) 36 .2 7 32 .9 4 43 .0 0 44 .2 6 45 .0 0 45 .9 6 40 .13 48.26 36 .52 46.61 40 .26 48.63 37 .35 46.30 40 .47 49.02 37 .54 46.62 R E FI N E D ( % ) U N I O N ( % ) IN TE R (% ) 41 .7 1 32 .1 8 39 .8 6 44 .4 0 42 .9 4 44 .8 7 48 .4 2 43 .7 5 48 .6 5 45 .06 49.02 35 .87 48.66 43 .65 43.85 47 .20 47.61 36 .07 48.99 44 .91 45.18 47 .46 48.18 36 .26 49.35 45 .13 45.48 N I S T B L E U 6.', '195': '45 8 15 .7 0 6.', '196': '82 2 17 .7 0 6.', '197': '92 6 18 .2 5 6.', '198': '93 7 6.954 17 .93 18.14 6.', '199': '90 4 6.976 18 .13 18.05 6.', '200': '96 7 6.962 18 .11 18.25 Table 4: Word Alignment Accuracy (F-measure) and Machine Translation Quality for BiTAM Models, comparing with IBM Models, and HMMs with a training scheme of 18 h7 43 on the Treebank data listed in Table 1.', '201': 'For each column, the highlighted alignment (the best one under that model setting) is picked up to further evaluate the translation quality.', '202': 'Union of two directions gives high-recall; Refined grows the intersection with the neighboring word- pairs seen in the union, and yields high-precision and high-recall alignments.', '203': 'As shown in Table 4, the baseline IBM1 gives its best performance of 36.27% in the CE direc tion; the UDA alignments from BiTAM1â\x88¼3 give 40.13%, 40.26%, and 40.47%, respectively, which are significantly better than IBM1.', '204': 'A close look at the three BiTAMs does not yield significant difference.', '205': 'BiTAM3 is slightly better in most settings; BiTAM1 is slightly worse than the other two, because the topics sampled at the sentence level are not very concentrated.', '206': 'The BDA align ments of BiTAM1â\x88¼3 yield 48.26%, 48.63% and 49.02%, which are even better than HMM and IBM4 â\x80\x94 their best performances are at 44.26% and 45.96%, respectively.', '207': 'This is because BDA partially utilizes similar heuristics on the approximated posterior matrix {Ï\x95dnji} instead of di rect operations on alignments of two directions in the heuristics of Refined.', '208': 'Practically, we also apply BDA together with heuristics for IBM1, HMM and IBM4, and the best achieved performances are at 40.56%, 46.52% and 49.18%, respectively.', '209': 'Overall, BiTAM models achieve performances close to or higher than HMM, using only a very simple IBM1 style alignment model.', '210': 'Similar improvements over IBM models and HMM are preserved after applying the three kinds of heuristics in the above.', '211': 'As expected, since BDA already encodes some heuristics, it is only slightly improved with the Union heuristic; UDA, similar to the viterbi style alignment in IBM and HMM, is improved better by the Refined heuristic.', '212': 'We also test BiTAM3 on large training data, and similar improvements are observed over those of the baseline models (see Table.', '213': '5).', '214': '5.5 Boosting BiTAM Models.', '215': 'The translation lexicons of Bf,e,k are initialized uniformly in our previous experiments.', '216': 'Better ini tializations can potentially lead to better performances because it can help to avoid the undesirable local optima in variational EM iterations.', '217': 'We use the lexicons from IBM Model-4 to initialize Bf,e,k to boost the BiTAM models.', '218': 'This is one way of applying the proposed BiTAM models into current state-of-the-art SMT systems for further improvement.', '219': 'The boosted alignments are denoted as BUDA and BBDA in Table.', '220': '5, corresponding to the uni-direction and bi-direction alignments, respectively.', '221': 'We see an improvement in alignment quality.', '222': '5.6 Evaluating Translations.', '223': 'To further evaluate our BiTAM models, word alignments are used in a phrase-based decoder for evaluating translation qualities.', '224': 'Similar to the Pharoah package (Koehn, 2004), we extract phrase-pairs directly from word alignment together with coherence constraints (Fox, 2002) to remove noisy ones.', '225': 'We use TIDES Evalâ\x80\x9902 CE test set as development data to tune the decoder parameters; the Evalâ\x80\x9903 data (919 sentences) is the unseen data.', '226': 'A trigram language model is built using 180 million English words.', '227': 'Across all the reported comparative settings, the key difference is the bilingual ngram-identity of the phrase-pair, which is collected directly from the underlying word alignment.', '228': 'Shown in Table 4 are results for the small- data track; the large-data track results are in Table 5.', '229': 'For the small-data track, the baseline Bleu scores for IBM1, HMM and IBM4 are 15.70, 17.70 and 18.25, respectively.', '230': 'The UDA alignment of BiTAM1 gives an improvement over the baseline IBM1 from 15.70 to 17.93, and it is close to HMMâ\x80\x99s performance, even though BiTAM doesnâ\x80\x99t exploit any sequential structures of words.', '231': 'The proposed BiTAM2 and BiTAM 3 are slightly better than BiTAM1.', '232': 'Similar improvements are observed for the large-data track (see Table 5).', '233': 'Note that, the boosted BiTAM3 us SE T TI N G IBM 1 H M M IBM 4 B I T A M 3 U D A BDA BUDA B BDA C E ( % ) E C ( % ) 46 .7 3 44 .3 3 49 .1 2 54 .5 6 54 .1 7 55 .0 8 50 .55 56.27 55.80 57.02 51 .59 55.18 54.76 58.76 R E FI N E D ( % ) U N I O N ( % ) I N T E R ( % ) 54 .6 4 42 .4 7 52 .2 4 56 .3 9 51 .5 9 54 .6 9 58 .4 7 52 .6 7 57 .7 4 56 .45 54.57 58.26 56.23 50 .23 57.81 56.19 58.66 52 .44 52.71 54.70 55.35 N I S T B L E U 7.', '234': '5 9 19 .1 9 7.', '235': '7 7 21 .9 9 7.', '236': '8 3 23 .1 8 7.', '237': '64 7.68 8.10 8.23 21 .20 21.43 22.97 24.07 Table 5: Evaluating Word Alignment Accuracies and Machine Translation Qualities for BiTAM Models, IBM Models, HMMs, and boosted BiTAMs using all the training data listed in Table.', '238': '1.', '239': 'Other experimental conditions are similar to Table.', '240': '4.', '241': 'ing IBM4 as the seed lexicon, outperform the Refined IBM4: from 23.18 to 24.07 on Bleu score, and from 7.83 to 8.23 on NIST.', '242': 'This result suggests a straightforward way to leverage BiTAMs to improve statistical machine translations.', '243': 'In this paper, we proposed novel formalism for statistical word alignment based on bilingual admixture (BiTAM) models.', '244': 'Three BiTAM models were proposed and evaluated on word alignment and translation qualities against state-of- the-art translation models.', '245': 'The proposed models significantly improve the alignment accuracy and lead to better translation qualities.', '246': 'Incorporation of within-sentence dependencies such as the alignment-jumps and distortions, and a better treatment of the source monolingual model worth further investigations.'}",['P06-2124'],['../data/summaries/P06-2124.txt'],['../data/tba/P06-2124.json']
P08-1028,Vector-based Models of Semantic Composition,../data/papers/P08-1028.xml,"{'0': 'Vector-based Models of Semantic Composition', '1': 'This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', '2': 'Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.', '3': 'Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.', '4': 'Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.', '5': 'Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science.', '6': 'The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968).', '7': 'A variety of NLP tasks have made good use of vector-based models.', '8': 'Examples include automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998) and disambiguation (McCarthy et al., 2004), collocation extraction (Schone and Jurafsky, 2001), text segmentation (Choi et al., 2001) , and notably information retrieval (Salton et al., 1975).', '9': 'In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998).', '10': 'Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004).', '11': 'Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.', '12': 'In fact, the commonest method for combining the vectors is to average them.', '13': 'Vector averaging is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary.', '14': 'This is illustrated in the example below taken from Landauer et al. (1997).', '15': 'Sentences (1-a) and (1-b) contain exactly the same set of words but their meaning is entirely different.', '16': '(1) a.', '17': 'It was not the sales manager who hit the bottle that day, but the office worker with the serious drinking problem. b.', '18': 'That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.', '19': 'While vector addition has been effective in some applications such as essay grading (Landauer and Dumais, 1997) and coherence assessment (Foltz et al., 1998), there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing (Neville et al., 1991; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994).', '20': 'Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences.', '21': 'Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.', '22': 'Here, semantic analysis is guided by syntactic structure, and therefore sentences (1-a) and (1-b) receive distinct representations.', '23': 'The downside of this approach is that differences in meaning are qualitative rather than quantitative, and degrees of similarity cannot be expressed easily.', '24': 'In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', '25': 'We present a general framework for vector-based composition which allows us to consider different classes of models.', '26': 'Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.', '27': 'Our results show that the multiplicative models are superior and correlate significantly with behavioral data.', '28': 'The problem of vector composition has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Fodor and Pylyshyn, 1988).', '29': 'While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.', '30': 'For the hierarchical structure of natural language this binding problem becomes particularly acute.', '31': 'For example, simplistic approaches to handling sentences such as John loves Mary and Mary loves John typically fail to make valid representations in one of two ways.', '32': 'Either there is a failure to distinguish between these two structures, because the network fails to keep track of the fact that John is subject in one and object in the other, or there is a failure to recognize that both structures involve the same participants, because John as a subject has a distinct representation from John as an object.', '33': 'In contrast, symbolic representations can naturally handle the binding of constituents to their roles, in a systematic manner that avoids both these problems.', '34': 'Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.', '35': 'The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely, the tensor product has dimensionality m x n).', '36': 'To overcome this problem, other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.', '37': 'Holographic reduced representations (Plate, 1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.', '38': 'The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.', '39': 'The compression is achieved by summing along the transdiagonal elements of the tensor product.', '40': 'Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.', '41': 'The success of circular correlation crucially depends on the components of the n-dimensional vectors u and v being randomly distributed with mean 0 and variance 1n.', '42': 'This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.', '43': 'Vector addition is by far the most common method for representing the meaning of linguistic sequences.', '44': 'For example, assuming that individual words are represented by vectors, we can compute the meaning of a sentence by taking their mean (Foltz et al., 1998; Landauer and Dumais, 1997).', '45': 'Vector addition does not increase the dimensionality of the resulting vector.', '46': 'However, since it is order independent, it cannot capture meaning differences that are modulated by differences in syntactic structure.', '47': 'Kintsch (2001) proposes a variation on the vector addition theme in an attempt to model how the meaning of a predicate (e.g., run) varies depending on the arguments it operates upon (e.g, the horse ran vs. the color ran).', '48': 'The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.', '49': 'The neighbors, Kintsch argues, can ‘strengthen features of the predicate that are appropriate for the argument of the predication’. animal stable village gallop jokey horse 0 6 2 10 4 run 1 8 4 4 0 Unfortunately, comparisons across vector composition models have been few and far between in the literature.', '50': 'The merits of different approaches are illustrated with a few hand picked examples and parameter values and large scale evaluations are uniformly absent (see Frank et al. (2007) for a criticism of Kintsch’s (2001) evaluation standards).', '51': 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', '52': 'Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.', '53': 'We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.', '54': 'The construction of the semantic space depends on the definition of linguistic context (e.g., neighbouring words can be documents or collocations), the number of components used (e.g., the k most frequent words in a corpus), and their values (e.g., as raw co-occurrence frequencies or ratios of probabilities).', '55': 'A hypothetical semantic space is illustrated in Figure 1.', '56': 'Here, the space has only five dimensions, and the matrix cells denote the co-occurrence of the target words (horse and run) with the context words animal, stable, and so on.', '57': 'Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.', '58': 'We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v, as is the case for tensor products.', '59': 'It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence, via the argument R, on syntax.', '60': 'To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.', '61': 'One particularly useful constraint is to hold R fixed by focusing on a single well defined linguistic structure, for example the verb-subject relation.', '62': 'Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge.', '63': 'This reduces the class of models to: However, this still leaves the particular form of the function f unspecified.', '64': 'Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.', '65': 'So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.', '66': 'To give a concrete example, circular convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example, according to (5), the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].', '67': 'Whereas their product, as given by (6), is horse · run = [0 48 8 40 0].', '68': 'Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.', '69': 'Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.', '70': 'Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.', '71': 'In effect, this is what model (6) achieves.', '72': 'As a result of the assumption of symmetry, both these models are ‘bag of words’ models and word order insensitive.', '73': 'Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.', '74': 'As an example if we set α to 0.4 and β to 0.6, then horse= [0 2.4 0.8 4 1.6] and run = [0.6 4.8 2.4 2.4 0], and their sum horse + run = [0.6 5.6 3.2 6.4 1.6].', '75': 'An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.', '76': 'The models considered so far assume that components do not ‘interfere’ with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.', '77': 'For additive models, a natural way to achieve this is to include further vectors into the summation.', '78': 'These vectors are not arbitrary and ideally they must exhibit some relation to the words of the construction under consideration.', '79': 'When modeling predicate-argument structures, Kintsch (2001) proposes including one or more distributional neighbors, n, of the predicate: Note that considerable latitude is allowed in selecting the appropriate neighbors.', '80': 'Kintsch (2001) considers only the m most similar neighbors to the predicate, from which he subsequently selects k, those most similar to its argument.', '81': 'So, if in the composition of horse with run, the chosen neighbor is ride, ride = [2 15 7 9 1], then this produces the representation horse + run + ride = [3 29 13 23 5].', '82': 'In contrast to the simple additive model, this extended model is sensitive to syntactic structure, since n is chosen from among the neighbors of the predicate, distinguishing it from the argument.', '83': 'Although we have presented multiplicative and additive models separately, there is nothing inherent in our formulation that disallows their combination.', '84': 'The proposal is not merely notational.', '85': 'One potential drawback of multiplicative models is the effect of components with value zero.', '86': 'Since the product of zero with any number is itself zero, the presence of zeroes in either of the vectors leads to information being essentially thrown away.', '87': 'Combining the multiplicative model with an additive model, which does not suffer from this problem, could mitigate this problem: pi = αui +βvi +γuivi (11) where α, β, and γ are weighting constants.', '88': 'We evaluated the models presented in Section 3 on a sentence similarity task initially proposed by Kintsch (2001).', '89': 'In his study, Kintsch builds a model of how a verb’s meaning is modified in the context of its subject.', '90': 'He argues that the subjects of ran in The color ran and The horse ran select different senses of ran.', '91': 'This change in the verb’s sense is equated to a shift in its position in semantic space.', '92': 'To quantify this shift, Kintsch proposes measuring similarity relative to other verbs acting as landmarks, for example gallop and dissolve.', '93': 'The idea here is that an appropriate composition model when applied to horse and ran will yield a vector closer to the landmark gallop than dissolve.', '94': 'Conversely, when color is combined with ran, the resulting vector will be closer to dissolve than gallop.', '95': 'Focusing on a single compositional structure, namely intransitive verbs and their subjects, is a good point of departure for studying vector combination.', '96': 'Any adequate model of composition must be able to represent argument-verb meaning.', '97': 'Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.', '98': 'Unfortunately, Kintsch (2001) demonstrates how his own composition algorithm works intuitively on a few hand selected examples but does not provide a comprehensive test set.', '99': 'In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.', '100': 'In the following we describe our data collection procedure and give details on how our composition models were constructed and evaluated.', '101': 'Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.', '102': 'We first compiled a list of intransitive verbs from CELEX2.', '103': 'All occurrences of these verbs with a subject noun were next extracted from a RASP parsed (Briscoe and Carroll, 2002) version of the British National Corpus (BNC).', '104': 'Verbs and nouns that were attested less than fifty times in the BNC were removed as they would result in unreliable vectors.', '105': 'Each reference subject-verb tuple (e.g., horse ran) was paired with two landmarks, each a synonym of the verb.', '106': 'The landmarks were chosen so as to represent distinct verb senses, one compatible with the reference (e.g., horse galloped) and one incompatible (e.g., horse dissolved).', '107': 'Landmarks were taken from WordNet (Fellbaum, 1998).', '108': 'Specifically, they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs, each paired with 10 nouns, and 2 landmarks (400 pairs of sentences in total).', '109': 'These were further pretested to allow the selection of a subset of items showing clear variations in sense as we wanted to have a balanced set of similar and dissimilar sentences.', '110': 'In the pretest, subjects saw a reference sentence containing a subject-verb tuple and its landmarks and were asked to choose which landmark was most similar to the reference or neither.', '111': 'Our items were converted into simple sentences (all in past tense) by adding articles where appropriate.', '112': 'The stimuli were administered to four separate groups; each group saw one set of 100 sentences.', '113': 'The pretest was completed by 53 participants.', '114': 'For each reference verb, the subjects’ responses were entered into a contingency table, whose rows corresponded to nouns and columns to each possible answer (i.e., one of the two landmarks).', '115': 'Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.', '116': 'We used Fisher’s exact test to determine which verbs and nouns showed the greatest variation in landmark preference and items with p-values greater than 0.001 were discarded.', '117': 'This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs, each with 4 nouns, and 2 landmarks.', '118': 'Procedure and Subjects Participants first saw a set of instructions that explained the sentence similarity task and provided several examples.', '119': 'Then the experimental items were presented; each contained two sentences, one with the reference verb and one with its landmark.', '120': 'Examples of our items are given in Table 1.', '121': 'Here, burn is a high similarity landmark (High) for the reference The fire glowed, whereas beam is a low similarity landmark (Low).', '122': 'The opposite is the case for the reference The face glowed.', '123': 'Sentence pairs were presented serially in random order.', '124': 'Participants were asked to rate how similar the two sentences were on a scale of one to seven.', '125': 'The study was conducted remotely over the Internet using Webexp4, a software package designed for conducting psycholinguistic studies over the web.', '126': '49 unpaid volunteers completed the experiment, all native speakers of English.', '127': 'Analysis of Similarity Ratings The reliability of the collected judgments is important for our evaluation experiments; we therefore performed several tests to validate the quality of the ratings.', '128': 'First, we examined whether participants gave high ratings to high similarity sentence pairs and low ratings to low similarity ones.', '129': 'Figure 2 presents a box-and-whisker plot of the distribution of the ratings.', '130': 'As we can see sentences with high similarity landmarks are perceived as more similar to the reference sentence.', '131': 'A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).', '132': 'We also measured how well humans agree in their ratings.', '133': 'We employed leave-one-out resampling (Weiss and Kulikowski, 1991), by correlating the data obtained from each participant with the ratings obtained from all other participants.', '134': 'We used Spearman’s ρ, a non parametric correlation coefficient, to avoid making any assumptions about the distribution of the similarity ratings.', '135': 'The average inter-subject agreement5 was ρ = 0.40.', '136': 'We believe that this level of agreement is satisfactory given that naive subjects are asked to provide judgments on fine-grained semantic distinctions (see Table 1).', '137': 'More evidence that this is not an easy task comes from Figure 2 where we observe some overlap in the ratings for High and Low similarity items.', '138': 'Model Parameters Irrespectively of their form, all composition models discussed here are based on a semantic space for representing the meanings of individual words.', '139': 'The semantic space we used in our experiments was built on a lemmatised version of the BNC.', '140': 'Following previous work (Bullinaria and Levy, 2007), we optimized its parameters on a word-based semantic similarity task.', '141': 'The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.', '142': 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).', '143': 'We used WordSim353, a benchmark dataset (Finkelstein et al., 2002), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs.', '144': 'We obtained best results with a model using a context window of five words on either side of the target word, the cosine measure, and 2,000 vector components.', '145': 'The latter were the most common context words (excluding a stop list of function words).', '146': 'These components were set to the ratio of the probability of the context word given the target word to the probability of the context word overall.', '147': 'This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.', '148': 'In addition, Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).', '149': 'Our composition models have no additional parameters beyond the semantic space just described, with three exceptions.', '150': 'First, the additive model in (7) weighs differentially the contribution of the two constituents.', '151': 'In our case, these are the subject noun and the intransitive verb.', '152': 'To this end, we optimized the weights on a small held-out set.', '153': 'Specifically, we considered eleven models, varying in their weightings, in steps of 10%, from 100% noun through 50% of both verb and noun to 100% verb.', '154': 'For the best performing model the weight for the verb was 80% and for the noun 20%.', '155': 'Secondly, we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.', '156': 'This yielded a weighted sum consisting of 95% verb, 0% noun and 5% of their multiplicative combination.', '157': 'Finally, Kintsch’s (2001) additive model has two extra parameters.', '158': 'The m neighbors most similar to the predicate, and the k of m neighbors closest to its argument.', '159': 'In our experiments we selected parameters that Kintsch reports as optimal.', '160': 'Specifically, m was set to 20 and m to 1.', '161': 'Evaluation Methodology We evaluated the proposed composition models in two ways.', '162': 'First, we used the models to estimate the cosine similarity between the reference sentence and its landmarks.', '163': 'We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).', '164': 'A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.', '165': 'Again, better models should correlate better with the experimental data.', '166': 'We assume that the inter-subject agreement can serve as an upper bound for comparing the fit of our models against the human judgments.', '167': 'Our experiments assessed the performance of seven composition models.', '168': 'These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch’s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).', '169': 'As a baseline we simply estimated the similarity between the reference verb and its landmarks without taking the subject noun into account (equation (8), NonComp).', '170': 'Table 2 shows the average model ratings for High and Low similarity items.', '171': 'For comparison, we also show the human ratings for these items (UpperBound).', '172': 'Here, we are interested in relative differences, since the two types of ratings correspond to different scales.', '173': 'Model similarities have been estimated using cosine which ranges from 0 to 1, whereas our subjects rated the sentences on a scale from 1 to 7.', '174': 'The simple additive model fails to distinguish between High and Low Similarity items.', '175': 'We observe a similar pattern for the non compositional baseline model, the weighted additive model and Kintsch (2001).', '176': 'The multiplicative and combined models yield means closer to the human ratings.', '177': 'The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).', '178': 'Figure 3 shows the distribution of estimated similarities under the multiplicative model.', '179': 'The results of our correlation analysis are also given in Table 2.', '180': 'As can be seen, all models are significantly correlated with the human ratings.', '181': 'In order to establish which ones fit our data better, we examined whether the correlation coefficients achieved differ significantly using a t-test (Cohen and Cohen, 1983).', '182': 'The lowest correlation (p = 0.04) is observed for the simple additive model which is not significantly different from the non-compositional baseline model.', '183': 'The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).', '184': 'Given that the basis of Kintsch’s model is the summation of the verb, a neighbor close to the verb and the noun, it is not surprising that it produces results similar to a summation which weights the verb more heavily than the noun.', '185': 'The multiplicative model yields a better fit with the experimental data, ρ = 0.17.', '186': 'The combined model is best overall with ρ = 0.19.', '187': 'However, the difference between the two models is not statistically significant.', '188': 'Also note that in contrast to the combined model, the multiplicative model does not have any free parameters and hence does not require optimization for this particular task.', '189': 'In this paper we presented a general framework for vector-based semantic composition.', '190': 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.', '191': 'Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.', '192': 'We conjecture that the additive models are not sensitive to the fine-grained meaning distinctions involved in our materials.', '193': 'Previous applications of vector addition to document indexing (Deerwester et al., 1990) or essay grading (Landauer et al., 1997) were more concerned with modeling the gist of a document rather than the meaning of its sentences.', '194': 'Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.', '195': 'The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.', '196': 'Further research is needed to gain a deeper understanding of vector composition, both in terms of modeling a wider range of structures (e.g., adjectivenoun, noun-noun) and also in terms of exploring the space of models more fully.', '197': 'We anticipate that more substantial correlations can be achieved by implementing more sophisticated models from within the framework outlined here.', '198': 'In particular, the general class of multiplicative models (see equation (4)) appears to be a fruitful area to explore.', '199': 'Future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets.', '200': 'The applications of the framework discussed here are many and varied both for cognitive science and NLP.', '201': 'We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al., 1988) and inductive inference (Heit and Rubinstein, 1994).', '202': 'NLP tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling (Coccaro and Jurafsky, 1998).'}","['P08-1028_swastika', 'P08-1028_aakansha', 'P08-1028_sweta']","['../data/summaries/P08-1028_swastika.txt', '../data/summaries/P08-1028_aakansha.txt', '../data/summaries/P08-1028_sweta.txt']","['../data/tba/P08-1028_swastika.json', '../data/tba/P08-1028_aakansha.json', '../data/tba/P08-1028_sweta.json']"
P08-1043,A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing,../data/papers/P08-1043.xml,"{'0': 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', '1': 'Morphological processes in Semitic languages deliver space-delimited words which introduce multiple, distinct, syntactic units into the structure of the input sentence.', '2': 'These words are in turn highly ambiguous, breaking the assumption underlying most parsers that the yield of a tree for a given sentence is known in advance.', '3': 'Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.', '4': 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.', '5': 'Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, ‘tokens’) that constitute the unanalyzed surface forms (utterances).', '6': 'In Semitic languages the situation is very different.', '7': 'In Modern Hebrew (Hebrew), a Semitic language with very rich morphology, particles marking conjunctions, prepositions, complementizers and relativizers are bound elements prefixed to the word (Glinert, 1989).', '8': ""The Hebrew token ‘bcl’1, for example, stands for the complete prepositional phrase 'We adopt here the transliteration of (Sima’an et al., 2001)."", '9': '“in the shadow”.', '10': 'This token may further embed into a larger utterance, e.g., ‘bcl hneim’ (literally “in-the-shadow the-pleasant”, meaning roughly “in the pleasant shadow”) in which the dominated Noun is modified by a proceeding space-delimited adjective.', '11': 'It should be clear from the onset that the particle b (“in”) in ‘bcl’ may then attach higher than the bare noun cl (“shadow”).', '12': 'This leads to word- and constituent-boundaries discrepancy, which breaks the assumptions underlying current state-of-the-art statistical parsers.', '13': 'One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done, to the best of our knowledge, in all parsing related work on Arabic and its dialects (Chiang et al., 2006)).', '14': 'The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).', '15': 'The aforementioned surface form bcl, for example, may also stand for the lexical item “onion”, a Noun.', '16': 'The implication of this ambiguity for a parser is that the yield of syntactic trees no longer consists of spacedelimited tokens, and the expected number of leaves in the syntactic analysis in not known in advance.', '17': 'Tsarfaty (2006) argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task.', '18': 'Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.', '19': 'Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', '20': 'We claim that no particular morphological segmentation is a-priory more likely for surface forms before exploring the compositional nature of syntactic structures, including manifestations of various long-distance dependencies.', '21': 'Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2', '22': 'Segmental morphology Hebrew consists of seven particles m(“from”) f(“when”/“who”/“that”) h(“the”) w(“and”) k(“like”) l(“to”) and b(“in”). which may never appear in isolation and must always attach as prefixes to the following open-class category item we refer to as stem.', '23': 'Several such particles may be prefixed onto a single stem, in which case the affixation is subject to strict linear precedence constraints.', '24': 'Co-occurrences among the particles themselves are subject to further syntactic and lexical constraints relative to the stem.', '25': 'While the linear precedence of segmental morphemes within a token is subject to constraints, the dominance relations among their mother and sister constituents is rather free.', '26': 'The relativizer f(“that”) for example, may attach to an arbitrarily long relative clause that goes beyond token boundaries.', '27': 'The attachment in such cases encompasses a long distance dependency that cannot be captured by Markovian processes that are typically used for morphological disambiguation.', '28': 'The same argument holds for resolving PP attachment of a prefixed preposition or marking conjunction of elements of any kind.', '29': 'A less canonical representation of segmental morphology is triggered by a morpho-phonological process of omitting the definite article h when occurring after the particles b or l. This process triggers ambiguity as for the definiteness status of Nouns following these particles.We refer to such cases in which the concatenation of elements does not strictly correspond to the original surface form as super-segmental morphology.', '30': 'An additional case of super-segmental morphology is the case of Pronominal Clitics.', '31': 'Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.', '32': 'The additional morphological material in such cases appears after the stem and realizes the extended meaning.', '33': 'The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).', '34': 'Lexical and Morphological Ambiguity The rich morphological processes for deriving Hebrew stems give rise to a high degree of ambiguity for Hebrew space-delimited tokens.', '35': 'The form fmnh, for example, can be understood as the verb “lubricated”, the possessed noun “her oil”, the adjective “fat” or the verb “got fat”.', '36': 'Furthermore, the systematic way in which particles are prefixed to one another and onto an open-class category gives rise to a distinct sort of morphological ambiguity: space-delimited tokens may be ambiguous between several different segmentation possibilities.', '37': 'The same form fmnh can be segmented as f-mnh, f (“that”) functioning as a reletivizer with the form mnh.', '38': 'The form mnh itself can be read as at least three different verbs (“counted”, “appointed”, “was appointed”), a noun (“a portion”), and a possessed noun (“her kind”).', '39': 'Such ambiguities cause discrepancies between token boundaries (indexed as white spaces) and constituent boundaries (imposed by syntactic categories) with respect to a surface form.', '40': 'Such discrepancies can be aligned via an intermediate level of PoS tags.', '41': 'PoS tags impose a unique morphological segmentation on surface tokens and present a unique valid yield for syntactic trees.', '42': 'The correct ambiguity resolution of the syntactic level therefore helps to resolve the morphological one, and vice versa.', '43': 'Morphological analyzers for Hebrew that analyze a surface form in isolation have been proposed by Segal (2000), Yona and Wintner (2005), and recently by the knowledge center for processing Hebrew (Itai et al., 2006).', '44': 'Such analyzers propose multiple segmentation possibilities and their corresponding analyses for a token in isolation but have no means to determine the most likely ones.', '45': 'Morphological disambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim et al. (2005), Adler and Elhadad (2006), Shacham and Wintner (2007), and achieved good results (the best segmentation result so far is around 98%).', '46': 'The development of the very first Hebrew Treebank (Sima’an et al., 2001) called for the exploration of general statistical parsing methods, but the application was at first limited.', '47': 'Sima’an et al. (2001) presented parsing results for a DOP tree-gram model using a small data set (500 sentences) and semiautomatic morphological disambiguation.', '48': 'Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.', '49': 'Tsarfaty and Sima’an (2007) have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation.', '50': 'The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty, 2006; Tsarfaty and Sima’an, 2004) and empirically explored in (Tsarfaty, 2006).', '51': 'Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.', '52': 'Cohen and Smith (2007) later on based a system for joint inference on factored, independent, morphological and syntactic components of which scores are combined to cater for the joint inference task.', '53': 'Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.', '54': 'A Hebrew surface token may have several readings, each of which corresponding to a sequence of segments and their corresponding PoS tags.', '55': 'We refer to different readings as different analyses whereby the segments are deterministic given the sequence of PoS tags.', '56': 'We refer to a segment and its assigned PoS tag as a lexeme, and so analyses are in fact sequences of lexemes.', '57': 'For brevity we omit the segments from the analysis, and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.', '58': 'Such tag sequences are often treated as “complex tags” (e.g.', '59': 'REL+VB) (cf.', '60': '(Bar-Haim et al., 2007; Habash and Rambow, 2005)) and probabilities are assigned to different analyses in accordance with the likelihood of their tags (e.g., “fmnh is 30% likely to be tagged NN and 70% likely to be tagged REL+VB”).', '61': 'Here we do not submit to this view.', '62': 'When a token fmnh is to be interpreted as the lexeme sequence f/REL mnh/VB, the analysis introduces two distinct entities, the relativizer f (“that”) and the verb mnh (“counted”), and not as the complex entity “that counted”.', '63': 'When the same token is to be interpreted as a single lexeme fmnh, it may function as a single adjective “fat”.', '64': 'There is no relation between these two interpretations other then the fact that their surface forms coincide, and we argue that the only reason to prefer one analysis over the other is compositional.', '65': 'A possible probabilistic model for assigning probabilities to complex analyses of a surface form may be and indeed recent sequential disambiguation models for Hebrew (Adler and Elhadad, 2006) and Arabic (Smith et al., 2005) present similar models.', '66': 'We suggest that in unlexicalized PCFGs the syntactic context may be explicitly modeled in the derivation probabilities.', '67': 'Hence, we take the probability of the event fmnh analyzed as REL VB to be This means that we generate f and mnh independently depending on their corresponding PoS tags, and the context (as well as the syntactic relation between the two) is modeled via the derivation resulting in a sequence REL VB spanning the form fmnh. based on linear context.', '68': 'In our model, however, all lattice paths are taken to be a-priori equally likely.', '69': 'We represent all morphological analyses of a given utterance using a lattice structure.', '70': 'Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.', '71': 'This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances (Adler, 2001; Bar-Haim et al., 2005; Smith et al., 2005; Cohen and Smith, 2007; Adler, 2007).', '72': 'Figure 1 depicts the lattice for a 2-words sentence bclm hneim.', '73': 'We use double-circles to indicate the space-delimited token boundaries.', '74': 'Note that in our construction arcs can never cross token boundaries.', '75': 'Every token is independent of the others, and the sentence lattice is in fact a concatenation of smaller lattices, one for each token.', '76': 'Furthermore, some of the arcs represent lexemes not present in the input tokens (e.g. h/DT, fl/POS), however these are parts of valid analyses of the token (cf. super-segmental morphology section 2).', '77': 'Segments with the same surface form but different PoS tags are treated as different lexemes, and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).', '78': 'A similar structure is used in speech recognition.', '79': 'There, a lattice is used to represent the possible sentences resulting from an interpretation of an acoustic model.', '80': 'In speech recognition the arcs of the lattice are typically weighted in order to indicate the probability of specific transitions.', '81': 'Given that weights on all outgoing arcs sum up to one, weights induce a probability distribution on the lattice paths.', '82': 'In sequential tagging models such as (Adler and Elhadad, 2006; Bar-Haim et al., 2007; Smith et al., 2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1, ... , wn of space-delimited tokens.', '83': 'Each token may admit multiple analyses, each of which a sequence of one or more lexemes (we use li to denote a lexeme) belonging a presupposed Hebrew lexicon LEX.', '84': 'The entries in such a lexicon may be thought of as meaningful surface segments paired up with their PoS tags li = (si, pi), but note that a surface segment s need not be a space-delimited token.', '85': 'The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.', '86': 'A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', '87': 'We define the lattice L to be the concatenation of the lattices Li corresponding to the input words wi (s.t.', '88': 'M(wi) = Li).', '89': 'Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer, we look for the most probable parse tree π s.t.', '90': 'Since the lattice L for a given sentence W is determined by the morphological analyzer M we have which is precisely the formula corresponding to the so-called lattice parsing familiar from speech recognition.', '91': 'Every parse π selects a specific morphological segmentation (l1...lk) (a path through the lattice).', '92': 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', '93': '(Charniak et al., 1996).', '94': 'Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.', '95': 'A compatible view is presented by Charniak et al. (1996) who consider the kind of probabilities a generative parser should get from a PoS tagger, and concludes that these should be P(w|t) “and nothing fancier”.3 In our setting, therefore, the Lattice is not used to induce a probability distribution on a linear context, but rather, it is used as a common-denominator of state-indexation of all segmentations possibilities of a surface form.', '96': 'This is a unique object for which we are able to define a proper probability model.', '97': 'Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.', '98': 'The Grammar Our parser looks for the most likely tree spanning a single path through the lattice of which the yield is a sequence of lexemes.', '99': 'This is done using a simple PCFG which is lexemebased.', '100': 'This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).', '101': 'The possible analyses of a surface token pose constraints on the analyses of specific segments.', '102': 'In order to pass these constraints onto the parser, the lexical rules in the grammar are of the form pi —* (si, pi) Parameter Estimation The grammar probabilities are estimated from the corpus using simple relative frequency estimates.', '103': 'Lexical rules are estimated in a similar manner.', '104': 'We smooth Prf(p —* (s, p)) for rare and OOV segments (s E l, l E L, s unseen) using a “per-tag” probability distribution over rare segments which we estimate using relative frequency estimates for once-occurring segments.', '105': '3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.', '106': 'Handling Unknown tokens When handling unknown tokens in a language such as Hebrew various important aspects have to be borne in mind.', '107': 'Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.', '108': 'Secondly, some segments in a proposed segment sequence may in fact be seen lexical events, i.e., for some p tag Prf(p —* (s, p)) > 0, while other segments have never been observed as a lexical event before.', '109': 'The latter arcs correspond to OOV words in English.', '110': 'Finally, the assignments of PoS tags to OOV segments is subject to language specific constraints relative to the token it was originated from.', '111': 'Our smoothing procedure takes into account all the aforementioned aspects and works as follows.', '112': 'We first make use of our morphological analyzer to find all segmentation possibilities by chopping off all prefix sequence possibilities (including the empty prefix) and construct a lattice off of them.', '113': 'The remaining arcs are marked OOV.', '114': 'At this stage the lattice path corresponds to segments only, with no PoS assigned to them.', '115': 'In turn we use two sorts of heuristics, orthogonal to one another, to prune segmentation possibilities based on lexical and grammatical constraints.', '116': 'We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.', '117': 'This heuristics is used to prune all segmentation possibilities involving “lexically improper” segments.', '118': 'For the remaining arcs, if the segment is in fact a known lexeme it is tagged as usual, but for the OOV arcs which are valid Hebrew entries lacking tags assignment, we assign all possible tags and then simulate a grammatical constraint.', '119': 'Here, all tokeninternal collocations of tags unseen in our training data are pruned away.', '120': 'From now on all lattice arcs are tagged segments and the assignment of probability P(p —* (s, p)) to lattice arcs proceeds as usual.4 A rather pathological case is when our lexical heuristics prune away all segmentation possibilities and we remain with an empty lattice.', '121': 'In such cases we use the non-pruned lattice including all (possibly ungrammatical) segmentation, and let the statistics (including OOV) decide.', '122': 'We empirically control for the effect of our heuristics to make sure our pruning does not undermine the objectives of our joint task.', '123': 'Previous work on morphological and syntactic disambiguation in Hebrew used different sets of data, different splits, differing annotation schemes, and different evaluation measures.', '124': 'Our experimental setup therefore is designed to serve two goals.', '125': 'Our primary goal is to exploit the resources that are most appropriate for the task at hand, and our secondary goal is to allow for comparison of our models’ performance against previously reported results.', '126': 'When a comparison against previous results requires additional pre-processing, we state it explicitly to allow for the reader to replicate the reported results.', '127': 'Data We use the Hebrew Treebank, (Sima’an et al., 2001), provided by the knowledge center for processing Hebrew, in which sentences from the daily newspaper “Ha’aretz” are morphologically segmented and syntactically annotated.', '128': 'The treebank has two versions, v1.0 and v2.0, containing 5001 and 6501 sentences respectively.', '129': 'We use v1.0 mainly because previous studies on joint inference reported results w.r.t. v1.0 only.5 We expect that using the same setup on v2.0 will allow a crosstreebank comparison.6 We used the first 500 sentences as our dev set and the rest 4500 for training and report our main results on this split.', '130': 'To facilitate the comparison of our results to those reported by (Cohen and Smith, 2007) we use their data set in which 177 empty and “malformed”7 were removed.', '131': 'The first 3770 trees of the resulting set then were used for training, and the last 418 are used testing.', '132': '(we ignored the 419 trees in their development set.)', '133': 'Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', '134': 'Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).', '135': 'We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.', '136': 'Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5.', '137': 'We use the HSPELL9 (Har’el and Kenigsberg, 2004) wordlist as a lexeme-based lexicon for pruning segmentations involving invalid segments.', '138': 'Models that employ this strategy are denoted hsp.', '139': 'To control for the effect of the HSPELL-based pruning, we also experimented with a morphological analyzer that does not perform this pruning.', '140': 'For these models we limit the options provided for OOV words by not considering the entire token as a valid segmentation in case at least some prefix segmentation exists.', '141': 'This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.', '142': 'We experimented with increasingly rich grammars read off of the treebank.', '143': 'Our first model is GTplain, a PCFG learned from the treebank after removing all functional features from the syntactic categories.', '144': 'In our second model GTvpi we also distinguished finite and non-finite verbs and VPs as 10Lattice parsing can be performed by special initialization of the chart in a CKY parser (Chappelier et al., 1999).', '145': 'We currently simulate this by crafting a WCFG and feeding it to BitPar.', '146': 'Given a PCFG grammar G and a lattice L with nodes n1 ... nk, we construct the weighted grammar GL as follows: for every arc (lexeme) l E L from node ni to node nj, we add to GL the rule [l --+ tni, tni+1, ... , tnj_1] with a probability of 1 (this indicates the lexeme l spans from node ni to node nj).', '147': 'GL is then used to parse the string tn1 ... tnk_1, where tni is a terminal corresponding to the lattice span between node ni and ni+1.', '148': 'Removing the leaves from the resulting tree yields a parse for L under G, with the desired probabilities.', '149': 'We use a patched version of BitPar allowing for direct input of probabilities instead of counts.', '150': 'We thank Felix Hageloh (Hageloh, 2006) for providing us with this version. proposed in (Tsarfaty, 2006).', '151': 'In our third model GTppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad (2007).', '152': 'In our forth model GTnph we add the definiteness status of constituents following Tsarfaty and Sima’an (2007).', '153': 'Finally, model GTv = 2 includes parent annotation on top of the various state-splits, as is done also in (Tsarfaty and Sima’an, 2007; Cohen and Smith, 2007).', '154': 'For all grammars, we use fine-grained PoS tags indicating various morphological features annotated therein.', '155': 'Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.', '156': 'To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).', '157': 'SEGTok(noH) is the segmentation accuracy ignoring mistakes involving the implicit definite article h.11 To evaluate our performance on the tagging task we report CPOS and FPOS corresponding to coarse- and fine-grained PoS tagging results (F1) measure.', '158': 'Evaluating parsing results in our joint framework, as argued by Tsarfaty (2006), is not trivial under the joint disambiguation task, as the hypothesized yield need not coincide with the correct one.', '159': 'Our parsing performance measures (SY N) thus report the PARSEVAL extension proposed in Tsarfaty (2006).', '160': 'We further report SYNCS, the parsing metric of Cohen and Smith (2007), to facilitate the comparison.', '161': 'We report the F1 value of both measures.', '162': 'Finally, our U (unparsed) measure is used to report the number of sentences to which our system could not propose a joint analysis.', '163': 'The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.', '164': 'In addition we report for each model its performance on goldsegmented input (GS) to indicate the upper bound 11Overt definiteness errors may be seen as a wrong feature rather than as wrong constituent and it is by now an accepted standard to report accuracy with and without such errors. for the grammars’ performance on the parsing task.', '165': 'The table makes clear that enriching our grammar improves the syntactic performance as well as morphological disambiguation (segmentation and POS tagging) accuracy.', '166': 'This supports our main thesis that decisions taken by single, improved, grammar are beneficial for both tasks.', '167': 'When using the segmentation pruning (using HSPELL) for unseen tokens, performance improves for all tasks as well.', '168': 'Yet we note that the better grammars without pruning outperform the poorer grammars using this technique, indicating that the syntactic context aids, to some extent, the disambiguation of unknown tokens.', '169': 'Table 2 compares the performance of our system on the setup of Cohen and Smith (2007) to the best results reported by them for the same tasks.', '170': 'We first note that the accuracy results of our system are overall higher on their setup, on all measures, indicating that theirs may be an easier dataset.', '171': 'Secondly, for all our models we provide better fine- and coarse-grained POS-tagging accuracy, and all pruned models outperform the Oracle results reported by them.12 In terms of syntactic disambiguation, even the simplest grammar pruned with HSPELL outperforms their non-Oracle results.', '172': 'Without HSPELL-pruning, our simpler grammars are somewhat lagging behind, but as the grammars improve the gap is bridged.', '173': 'The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.', '174': 'This essentially means that their model does not result in a true joint inference, as executions for different tasks involve tuning a parameter separately.', '175': 'In our model there are no such hyper-parameters, and the performance is the result of truly joint disambiguation. sults.', '176': 'Furthermore, the combination of pruning and vertical markovization of the grammar outperforms the Oracle results reported by Cohen and Smith.', '177': 'This essentially means that a better grammar tunes the joint model for optimized syntactic disambiguation at least in as much as their hyper parameters do.', '178': 'An interesting observation is that while vertical markovization benefits all our models, its effect is less evident in Cohen and Smith.', '179': 'On the surface, our model may seem as a special case of Cohen and Smith in which α = 0.', '180': 'However, there is a crucial difference: the morphological probabilities in their model come from discriminative models based on linear context.', '181': 'Many morphological decisions are based on long distance dependencies, and when the global syntactic evidence disagrees with evidence based on local linear context, the two models compete with one another, despite the fact that the PCFG takes also local context into account.', '182': 'In addition, as the CRF and PCFG look at similar sorts of information from within two inherently different models, they are far from independent and optimizing their product is meaningless.', '183': 'Cohen and Smith approach this by introducing the α hyperparameter, which performs best when optimized independently for each sentence (cf.', '184': 'Oracle results).', '185': 'In contrast, our morphological probabilities are based on a unigram, lexeme-based model, and all other (local and non-local) contextual considerations are delegated to the PCFG.', '186': 'This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.', '187': 'Employing a PCFG-based generative framework to make both syntactic and morphological disambiguation decisions is not only theoretically clean and linguistically justified and but also probabilistically apropriate and empirically sound.', '188': 'The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.', '189': 'Better grammars are shown here to improve performance on both morphological and syntactic tasks, providing support for the advantage of a joint framework over pipelined or factorized ones.', '190': 'We conjecture that this trend may continue by incorporating additional information, e.g., three-dimensional models as proposed by Tsarfaty and Sima’an (2007).', '191': 'In the current work morphological analyses and lexical probabilities are derived from a small Treebank, which is by no means the best way to go.', '192': 'Using a wide-coverage morphological analyzer based on (Itai et al., 2006) should cater for a better coverage, and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.', '193': '(Levinger et al., 1995; Goldberg et al., ; Adler et al., 2008)) will make the parser more robust and suitable for use in more realistic scenarios.', '194': 'Acknowledgments We thank Meni Adler and Michael Elhadad (BGU) for helpful comments and discussion.', '195': 'We further thank Khalil Simaan (ILLCUvA) for his careful advise concerning the formal details of the proposal.', '196': 'The work of the first author was supported by the Lynn and William Frankel Center for Computer Sciences.', '197': 'The work of the second author as well as collaboration visits to Israel was financed by NWO, grant number 017.001.271.'}","['P08-1043_sweta', 'P08-1043_aakansha', 'P08-1043_swastika']","['../data/summaries/P08-1043_sweta.txt', '../data/summaries/P08-1043_aakansha.txt', '../data/summaries/P08-1043_swastika.txt']","['../data/tba/P08-1043_sweta.json', '../data/tba/P08-1043_aakansha.json', '../data/tba/P08-1043_swastika.json']"
P08-1102,A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging,../data/papers/P08-1102.xml,"{'0': 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging', '1': 'We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.', '2': 'With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.', '3': 'Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.', '4': 'On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.', '5': 'Word segmentation and part-of-speech (POS) tagging are important tasks in computer processing of Chinese and other Asian languages.', '6': 'Several models were introduced for these problems, for example, the Hidden Markov Model (HMM) (Rabiner, 1989), Maximum Entropy Model (ME) (Ratnaparkhi and Adwait, 1996), and Conditional Random Fields (CRFs) (Lafferty et al., 2001).', '7': 'CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks.', '8': 'Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron.', '9': 'To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&T).', '10': 'Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004).', '11': 'Compared to performing segmentation and POS tagging one at a time, Joint S&T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004).', '12': 'Besides the usual character-based features, additional features dependent on POS’s or words can also be employed to improve the performance.', '13': 'However, as such features are generated dynamically during the decoding procedure, two limitation arise: on the one hand, the amount of parameters increases rapidly, which is apt to overfit on training corpus; on the other hand, exact inference by dynamic programming is intractable because the current predication relies on the results of prior predications.', '14': 'As a result, many theoretically useful features such as higherorder word or POS n-grams are difficult to be incorporated in the model efficiently.', '15': 'To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.', '16': 'Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.', '17': 'We will describe it in detail in Section 4.', '18': 'In this architecture, knowledge sources that are intractable to incorporate into the perceptron, can be easily incorporated into the outside linear model.', '19': 'In addition, as these knowledge sources are regarded as separate features, we can train their corresponding models independently with each other.', '20': 'This is an interesting approach when the training corpus is large as it reduces the time and space consumption.', '21': 'Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.', '22': '2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.', '23': 'We can see that segmentation and POS tagging task is to divide a character sequence into several subsequences and label each of them a POS tag.', '24': 'It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.', '25': 'According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.', '26': 'In order to perform POS tagging at the same time, we expand boundary tags to include POS information by attaching a POS to the tail of a boundary tag as a postfix following Ng and Low (2004).', '27': 'As each tag is now composed of a boundary part and a POS part, the joint S&T problem is transformed to a uniform boundary-POS labelling problem.', '28': 'A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.', '29': 'The perceptron algorithm introduced into NLP by Collins (2002), is a simple but effective discriminative training method.', '30': 'It has comparable performance to CRFs, while with much faster training.', '31': 'The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.', '32': 'We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.', '33': 'In following subsections, we describe the feature templates and the perceptron training algorithm.', '34': 'The feature templates we adopted are selected from those of Ng and Low (2004).', '35': 'To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.', '36': 'All feature templates and their instances are shown in Table 1.', '37': 'C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).', '38': 'Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.', '39': 'We called them non-lexical-target because predications derived from them can predicate without considering the current character C0.', '40': 'Templates in the column below are expanded from the upper ones.', '41': 'We add a field C0 to each template in the upper column, so that it can carry out predication according to not only the context but also the current character itself.', '42': 'As predications generated from such templates depend on the current character, we name these templates lexical-target.', '43': 'Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.', '44': 'With the two kinds of predications, the perceptron model will do exact predicating to the best of its ability, and can back off to approximately predicating if exact predicating fails.', '45': 'We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.', '46': 'Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) ∈ X × Y to a feature vector 4)(x, y) ∈ Rd, and a parameter vector α� ∈ Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.', '47': 'For an input character sequence x, we aim to find an output F(x) satisfying: vector 4)(x, y) and the parameter vector a.', '48': 'We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.', '49': 'To alleviate overfitting on the training examples, we use the refinement strategy called “averaged parameters” (Collins, 2002) to the algorithm in Algorithm 1.', '50': 'In theory, any useful knowledge can be incorporated into the perceptron directly, besides the characterbased features already adopted.', '51': 'Additional features most widely used are related to word or POS ngrams.', '52': 'However, such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.', '53': 'Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.', '54': 'We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones, not to mention the higher-order grams such as trigrams or 4-grams.', '55': 'In addition, even though these higher grams were managed to be used, there still remains another problem: as the current predication relies on the results of prior ones, the decoding procedure has to resort to approximate inference by maintaining a list of N-best candidates at each predication position, which evokes a potential risk to depress the training.', '56': 'To alleviate the drawbacks, we propose a cascaded linear model.', '57': 'It has a two-layer architecture, with a perceptron as the core and another linear model as the outside-layer.', '58': 'Instead of incorporating all features into the perceptron directly, we first trained the perceptron using character-based features, and several other sub-models using additional ones such as word or POS n-grams, then trained the outside-layer linear model using the outputs of these sub-models, including the perceptron.', '59': 'Since the perceptron is fixed during the second training step, the whole training procedure need relative small time and memory cost.', '60': 'The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.', '61': 'In this layer, each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.', '62': 'Suppose we have n features gj (j = 1..n) coupled with n corresponding weights wj (j = 1..n), each feature gj gives a score gj(r) to a candidate r, then the total score of r is given by: The decoding procedure aims to find the candidate r* with the highest score: While the mission of the training procedure is to tune the weights wj(j = 1..n) to guarantee that the candidate r with the highest score happens to be the best result with a high probability.', '63': 'As all the sub-models, including the perceptron, are regarded as separate features of the outside-layer linear model, we can train them respectively with special algorithms.', '64': 'In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.', '65': 'As shown in Figure 1, the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.', '66': 'Besides the output of the perceptron, the outside-layer also receive the outputs of the word LM, the POS LM, the co-occurrence model and a word count penalty which is similar to the translation length penalty in SMT.', '67': 'Language model (LM) provides linguistic probabilities of a word sequence.', '68': 'It is an important measure of fluency of the translation in SMT.', '69': 'Formally, an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.', '70': 'Given a training corpus with POS tags, we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.', '71': 'Using W = w1:m to denote the word sequence, T = t1:m to denote the corresponding POS sequence, P (T |W) to denote the probability that W is labelled as T, and P(W|T) to denote the probability that T generates W, we can define the cooccurrence model as follows: λwt and λtw denote the corresponding weights of the two components.', '72': 'Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.', '73': 'For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.', '74': 'To facilitate tuning the weights, we use two components of the co-occurrence model Co(W,T) to represent the co-occurrence probability of W and T, rather than use Co(W, T) itself.', '75': 'In the rest of the paper, we will call them labelling model and generating model respectively.', '76': 'Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure.', '77': 'In Chinese Joint S&T, the mission of the decoder is to find the boundary-POS labelled sequence with the highest score.', '78': 'Given a Chinese character sequence C1:n, the decoding procedure can proceed in a left-right fashion with a dynamic programming approach.', '79': 'By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.', '80': 'At each position i, we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i, K) (K is assigned 20 in all our experiments) and ending at position i, then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i−l), and select for position i a N-best list of candidate results from all these candidates.', '81': 'When we derive a candidate result from a word-POS pair p and a candidate q at prior position of p, we calculate the scores of the word LM, the POS LM, the labelling probability and the generating probability, Algorithm 2 Decoding algorithm. as well as the score of the perceptron model.', '82': 'In addition, we add the score of the word count penalty as another feature to alleviate the tendency of LMs to favor shorter candidates.', '83': 'By equation 2, we can synthetically evaluate all these scores to perform more accurately comparing between candidates.', '84': 'Algorithm 2 shows the decoding algorithm.', '85': 'Lines 3 — 11 generate a N-best list for each character position i.', '86': 'Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).', '87': 'Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.', '88': 'Line 8 considers each candidate result in N-best list at prior position of the current word.', '89': 'Function D derives the candidate result from the word-POS pair p and the candidate q at prior position of p.', '90': 'We reported results from two set of experiments.', '91': 'The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).', '92': 'The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.', '93': 'In all experiments, we use the averaged parameters for the perceptrons, and F-measure as the accuracy measure.', '94': 'With precision P and recall R, the balance F-measure is defined as: F = 2PR/(P + R).', '95': 'For convenience of comparing with others, we focus only on the close test, which means that any extra resource is forbidden except the designated training corpus.', '96': 'In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.', '97': 'Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.', '98': 'We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration, and its learning curve reaches a tableland at iteration 7.', '99': 'Then we trained LEX on each of the four corpora for 7 iterations.', '100': 'Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS, CityU and MSR).', '101': 'On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007).', '102': 'However, the accuracy on PKU corpus is obvious lower than the best score SIGHAN reported, we need to conduct further research on this problem.', '103': 'We turned to experiments on CTB 5.0 to test the performance of the cascaded model.', '104': 'According to the usual practice in syntactic analysis, we choose chapters 1 − 260 (18074 sentences) as training set, chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.', '105': 'At the first step, we conducted a group of contrasting experiments on the core perceptron, the first concentrated on the segmentation regardless of the POS information and reported the F-measure on segmentation only, while the second performed Joint S&T using POS information and reported the F-measure both on segmentation and on Joint S&T.', '106': 'Note that the accuracy of Joint S&T means that a word-POS pair is recognized only if both the boundary tags and the POS’s are correctly labelled.', '107': 'The evaluation results are shown in Table 3.', '108': 'We find that Joint S&T can also improve the segmentation accuracy.', '109': 'However, the F-measure on Joint S&T is obvious lower, about a rate of 95% to the F-measure on segmentation.', '110': 'Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&T, a ratio of 96% to the F-measure 0.952 on segmentation.', '111': 'As the next step, a group of experiments were conducted to investigate how well the cascaded linear model performs.', '112': 'Here the core perceptron was just the POS+ model in experiments above.', '113': 'Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.', '114': 'We used SRI Language Modelling Toolkit (Stolcke and Andreas, 2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998), and a 4-gram POS LM with Witten-Bell smoothing, and we trained a word-POS co-occurrence model simply by MLE without smoothing.', '115': 'To obtain their corresponding weights, we adapted the minimum-error-rate training algorithm (Och, 2003) to train the outside-layer model.', '116': 'In order to inspect how much improvement each feature brings into the cascaded model, every time we removed a feature while retaining others, then retrained the model and tested its performance on the test set.', '117': 'Table 4 shows experiments results.', '118': 'We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&T, over the perceptron-only model POS+.', '119': 'We also find that the perceptron model functions as the kernel of the outside-layer linear model.', '120': 'Without the perceptron, the cascaded model (if we can still call it “cascaded”) performs poorly on both segmentation and Joint S&T.', '121': 'Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.', '122': 'Another important feature is the labelling model.', '123': 'Without it, the F-measure on segmentation and Joint S&T both suffer a decrement of 0.2 points.', '124': 'The generating model, which functions as that in HMM, brings an improvement of about 0.1 points to each test item.', '125': 'However unlike the three features, the word LM brings very tiny improvement.', '126': 'We suppose that the character-based features used in the perceptron play a similar role as the lowerorder word LM, and it would be helpful if we train a higher-order word LM on a larger scale corpus.', '127': 'Finally, the word count penalty gives improvement to the cascaded model, 0.13 points on segmentation and 0.16 points on Joint S&T.', '128': 'In summary, the cascaded model can utilize these knowledge sources effectively, without causing the feature space of the percptron becoming even larger.', '129': 'Experimental results show that, it achieves obvious improvement over the perceptron-only model, about from 0.973 to 0.978 on segmentation, and from 0.925 to 0.934 on Joint S&T, with error reductions of 18.5% and 12% respectively.', '130': 'We proposed a cascaded linear model for Chinese Joint S&T.', '131': 'Under this model, many knowledge sources that may be intractable to be incorporated into the perceptron directly, can be utilized effectively in the outside-layer linear model.', '132': 'This is a substitute method to use both local and non-local features, and it would be especially useful when the training corpus is very large.', '133': 'However, can the perceptron incorporate all the knowledge used in the outside-layer linear model?', '134': 'If this cascaded linear model were chosen, could more accurate generative models (LMs, word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of McClosky (2006)?', '135': 'In addition, all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus, whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low, 2004).', '136': 'How can we utilize these knowledge sources effectively?', '137': 'We will investigate these problems in the following work.', '138': 'This work was done while L. H. was visiting CAS/ICT.', '139': 'The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.', '140': '2006AA010108 (W. J., Q. L., and Y. L.), and by NSF ITR EIA-0205456 (L. H.).', '141': 'We would also like to Hwee-Tou Ng for sharing his code, and Yang Liu and Yun Huang for suggestions.'}","['P08-1102_aakansha', 'P08-1102_sweta', 'P08-1102_swastika']","['../data/summaries/P08-1102_aakansha.txt', '../data/summaries/P08-1102_sweta.txt', '../data/summaries/P08-1102_swastika.txt']","['../data/tba/P08-1102_aakansha.json', '../data/tba/P08-1102_sweta.json', '../data/tba/P08-1102_swastika.json']"
P11-1060,Learning Dependency-Based Compositional Semantics,../data/papers/P11-1060.xml,"{'0': 'Learning Dependency-Based Compositional Semantics', '1': 'Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', '2': 'In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.', '3': 'In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms.', '4': 'On two stansemantic parsing benchmarks our system obtains the highest published accuracies, despite requiring no annotated logical forms.', '5': 'What is the total population of the ten largest capitals in the US?', '6': 'Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).', '7': 'Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.', '8': 'On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.', '9': 'As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.', '10': 'However, we still model the logical form (now as a latent variable) to capture the complexities of language.', '11': 'Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.', '12': 'We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.', '13': 'We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.', '14': 'The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y.', '15': 'Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z.', '16': 'Which one should we use?', '17': 'The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.', '18': 'CCG is one instantiation (Steedman, 2000), which is used by many semantic parsers, e.g., Zettlemoyer and Collins (2005).', '19': 'However, the logical forms there can become quite complex, and in the context of program induction, this would lead to an unwieldy search space.', '20': 'At the same time, representations such as FunQL (Kate et al., 2005), which was used in Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.', '21': 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', '22': 'The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.', '23': 'We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).', '24': 'Our system outperforms all existing systems despite using no annotated logical forms.', '25': 'We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.', '26': 'We then introduce the full version (Section 2.2), which handles linguistic phenomena such as quantification, where syntactic and semantic scope diverge.', '27': 'We start with some definitions, using US geography as an example domain.', '28': 'Let V be the set of all values, which includes primitives (e.g., 3, CA ∈ V) as well as sets and tuples formed from other values (e.g., 3, {3, 4, 7}, (CA, {5}) ∈ V).', '29': 'Let P be a set of predicates (e.g., state, count ∈ P), which are just symbols.', '30': 'A world w is mapping from each predicate p ∈ P to a set of tuples; for example, w(state) = {(CA), (OR),... }.', '31': 'Conceptually, a world is a relational database where each predicate is a relation (possibly infinite).', '32': 'Define a special predicate ø with w(ø) = V. We represent functions by a set of inputoutput pairs, e.g., w(count) = {(S, n) : n = |S|}.', '33': 'As another example, w(average) = {(S, ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.', '34': 'Figure 2(a) shows an example of a DCS tree.', '35': 'Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.', '36': 'It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.', '37': 'The basic version of DCS restricts R to join and aggregate relations (see Table 1).', '38': 'Let us start by considering a DCS tree z with only join relations.', '39': 'Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.', '40': ""The CSP has two types of constraints: (i) x ∈ w(p) for each node x labeled with predicate p ∈ P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j ∈ R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints."", '41': 'We say a value v is consistent for a node x if there exists a solution that assigns v to x.', '42': 'The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).', '43': 'Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter, 2003).', '44': 'The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v ∈ w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) ∈ S} with domain S1 = {x : (x, y) ∈ S}.', '45': 'The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.', '46': ""Formally: Definition 1 (DCS trees) Let Z be the set of DCS trees, where each z ∈ Z consists of (i) a predicate for each child i, the ji-th component of v must equal the j'i-th component of some t in the child’s denotation (t ∈ JciKw)."", '47': 'This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.', '48': 'In addition, trees enable efficient computation, thereby establishing a new connection between dependency syntax and efficient semantic evaluation.', '49': 'Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures, but they cannot capture higherorder phenomena in language.', '50': 'For example, consider the phrase number of major cities, and suppose that number corresponds to the count predicate.', '51': 'It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent’s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.', '52': 'The denotation of the middle node is {s}, where s is all major cities.', '53': 'Having instantiated s as a value, everything above this node is an ordinary CSP: s constrains the count node, which in turns constrains the root node to |s|.', '54': 'A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.', '55': 'The tree structure still enables us to compute denotations efficiently based on (1) and (2).', '56': 'The basic version of DCS described thus far handles a core subset of language.', '57': 'But consider Figure 4: (a) is headed by borders, but states needs to be extracted; in (b), the quantifier no is syntactically dominated by the head verb borders but needs to take wider scope.', '58': 'We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.', '59': 'The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E, Q, or C).', '60': 'Then higher up in the tree, we invoke it with an execute relation Xi to create the desired semantic scope.2 This mark-execute construct acts non-locally, so to maintain compositionality, we must augment the denotation d = JzKw to include any information about the marked nodes in z that can be accessed by an execute relation later on.', '61': 'In the basic version, d was simply the consistent assignments to the root.', '62': 'Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes), as well as information stored about each marked node.', '63': 'Think of d as consisting of n columns, one for each active node according to a pre-order traversal of z.', '64': 'Column 1 always corresponds to the root node.', '65': 'Formally, a denotation is defined as follows (see Figure 5 for an example): Definition 2 (Denotations) Let D be the set of denotations, where each d E D consists of where each store a contains a mark relation a.r E {E, Q, C, 0}, a base denotation a.b E D U{0}, and a child denotation a.c E D U{0}.', '66': 'We write d as ((A; (ri, bi, ci); ... ; (rn, bn, cn))).', '67': 'We use d{ri = x} to mean d with d.ri = d.ai.r = x (similar definitions apply for d{ai = x}, d{bi = x}, and d{ci = x}).', '68': 'The denotation of a DCS tree can now be defined recursively: The base case is defined in (3): if z is a single node with predicate p, then the denotation of z has one column with the tuples w(p) and an empty store.', '69': 'The other six cases handle different edge relations.', '70': 'These definitions depend on several operations (mj,j0, E, Xi, M) which we will define shortly, but let us first get some intuition.', '71': 'Let z be a DCS tree.', '72': 'If the last child c of z’s root is a join (jj0), aggregate (E), or execute (Xi) relation ((4)–(6)), then we simply recurse on z with c removed and join it with some transformation (identity, E, or Xi) of c’s denotation.', '73': 'If the last (or first) child is connected via a mark relation E, C (or Q), then we strip off that child and put the appropriate information in the store by invoking M. We now define the operations mj,j0, E, Xi, M. Some helpful notation: For a sequence v = (v1,... , vn) and indices i = (i1, ... , ik), let vi = (vi1, ... , vik) be the projection of v onto i; we write v−i to mean v�1 i.', '74': 'Extending this notation to denotations, let (hA; αii[i] = hh{ai : a ∈ A}; αiii.', '75': 'Let d[−ø] = d[−i], where i are the columns with empty stores.', '76': 'For example, for d in Figure 5, d[1] keeps column 1, d[−ø] keeps column 2, and d[2, −2] swaps the two columns.', '77': ""Join The join of two denotations d and d' with respect to components j and j' (∗ means all components) is formed by concatenating all arrays a of d with all compatible arrays a' of d', where compatibility means a1j = a'1j0."", '78': ""The stores are also concatenated (α + α')."", '79': 'Non-initial columns with empty stores are projected away by applying ·[1,−ø].', '80': 'The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations, which handles the divergence between syntactic and semantic scope.', '81': 'In some sense, this is the technical core of DCS.', '82': 'Marking is simple: When a node (e.g., size in Figure 5) is marked (e.g., with relation C), we simply put the relation r, current denotation d and child c’s denotation into the store of column 1: The execute operation Xi(d) processes columns i in reverse order.', '83': 'It suffices to define Xi(d) for a single column i.', '84': 'There are three cases: Extraction (d.ri = E) In the basic version, the denotation of a tree was always the set of consistent values of the root node.', '85': 'Extraction allows us to return the set of consistent values of a marked non-root node.', '86': 'Formally, extraction simply moves the i-th column to the front: Xi(d) = d[i, −(i, ø)]{α1 = ø}.', '87': 'For example, in Figure 4(a), before execution, the denotation of the DCS tree is hh{[(CA, OR), (OR)],... }; ø; (E, Qhstatei�w, ø)ii; after applying X1, we have hh{[(OR)], ... }; øii.', '88': 'Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.', '89': 'For example, In a DCS tree, the quantifier appears as the child of a Q relation, and the restrictor is the parent (see Figure 4(b) for an example).', '90': 'This information is retrieved from the store when the quantifier in column i is executed.', '91': 'In particu- using the exact same machinery as superlatives.', '92': 'Figlar, the restrictor is A = E (d.bi) and the nu- ure 4(g) shows that we can naturally account for clear scope is B = E (d[i, −(i, 0)]).', '93': 'We then superlative ambiguity based on where the scopeapply d.ci to these two sets (technically, denota- determining execute relation is placed. tions) and project away the first column: Xi(d) = 3 Semantic Parsing ((d.ci ./1,1 A) ./2,1 B) [−1].', '94': 'We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.', '95': 'Our first question is: given notation of the DCS tree before execution is an utterance x, what trees z ∈ Z are permissible?', '96': 'To California cities), and it also allows us to underspecify L. In particular, our L will not include verbs or prepositions; rather, we rely on the predicates corresponding to those words to be triggered by traces.', '97': 'The augmentation function A takes a set of trees and optionally attaches E and Xi relations to the root (e.g., A(hcityi) = {hcityi , hcity; E:øi}).', '98': 'The filtering function F rules out improperly-typed trees such as hcity; 00:hstateii.', '99': 'To further reduce the search space, F imposes a few additional constraints, e.g., limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.', '100': 'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.', '101': 'Formally, pθ(z  |x) ∝ eφ(x,z)Tθ, where θ and φ(x, z) are parameter and feature vectors, respectively.', '102': 'As a running example, consider x = city that is in California and z = hcity; 11:hloc; 21:hCAiii, where city triggers city and California triggers CA.', '103': 'To define the features, we technically need to augment each tree z ∈ ZL(x) with alignment information—namely, for each predicate in z, the span in x (if any) that triggered it.', '104': 'This extra information is already generated from the recursive definition in (13).', '105': 'The feature vector φ(x, z) is defined by sums of five simple indicator feature templates: (F1) a word triggers a predicate (e.g., [city, city]); (F2) a word is under a relation (e.g., [that, 11]); (F3) a word is under a trace predicate (e.g., [in, loc]); (F4) two predicates are linked via a relation in the left or right direction (e.g., [city,11, loc, RIGHT]); and (F5) a predicate has a child relation (e.g., [city, 11]).', '106': 'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.', '107': 'Our model is arc-factored, so we can sum over all DCS trees in ZL(x) using dynamic programming.', '108': 'However, in order to learn, we need to sum over {z ∈ ZL(x) : JzKw = y}, and unfortunately, the additional constraint JzKw = y does not factorize.', '109': 'We therefore resort to beam search.', '110': 'Specifically, we truncate each Ci,j to a maximum of K candidates sorted by decreasing score based on parameters θ.', '111': 'Let ˜ZL,θ(x) be this approximation of ZL(x).', '112': 'Our learning algorithm alternates between (i) using the current parameters θ to generate the K-best set ˜ZL,θ(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.', '113': ""Formally, let ˜O(θ, θ') be the objective function O(θ) with ZL(x) ˜ZL,θI(x)."", '114': ""We optimize ˜O(θ,θ') by setting θ(0) = 0~ and iteratively solving θ(t+1) = argmaxθ ˜O(θ, θ(t)) using L-BFGS until t = T. In all experiments, we set λ = 0.01, T = 5, and K = 100."", '115': 'After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy pθ(T)(y  |x, z ∈ ˜ZL,θ(T)).', '116': 'We tested our system on two standard datasets, GEO and JOBS.', '117': 'In each dataset, each sentence x is annotated with a Prolog logical form, which we use only to evaluate and get an answer y.', '118': 'This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).', '119': 'There are three types of predicates in P: generic (e.g., argmax), data (e.g., city), and value (e.g., CA).', '120': 'GEO has 48 non-value predicates and JOBS has 26.', '121': 'For GEO, w is the standard US geography database that comes with the dataset.', '122': 'For JOBS, if we use the standard Jobs database, close to half the y’s are empty, which makes it uninteresting.', '123': 'We therefore generated a random Jobs database instead as follows: we created 100 job IDs.', '124': 'For each data predicate p (e.g., language), we add each possible tuple (e.g., (job37, Java)) to w(p) independently with probability 0.8.', '125': 'We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).', '126': 'During development, we further held out a random 30% of the training sets for validation.', '127': 'Our lexical triggers L include the following: (i) predicates for a small set of ≈ 20 function words (e.g., (most, argmax)), (ii) (x, x) for each value predicate x in w (e.g., (Boston, Boston)), and (iii) predicates for each POS tag in {JJ, NN, NNS} (e.g., (JJ, size), (JJ, area), etc.', '128': ').3 Predicates corresponding to verbs and prepositions (e.g., traverse) are not included as overt lexical triggers, but rather in the trace predicates L(E).', '129': 'We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g., (large, size)), which cancels the predicates triggered by x’s POS tag.', '130': 'For GEO, there are 22 prototype words; for JOBS, there are 5.', '131': 'Specifying these triggers requires minimal domain-specific supervision.', '132': 'Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.', '133': 'Table 2 shows that our system using lexical triggers L (henceforth, DCS) outperforms SEMRESP (78.9% over 73.2%).', '134': 'In fact, although neither DCS nor SEMRESP uses logical forms, DCS uses even less supervision than SEMRESP.', '135': 'SEMRESP requires a lexicon of 1.42 words per non-value predicate, WordNet features, and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features.', '136': 'In fact, DCS performs comparably to even the version of SEMRESP trained using logical forms.', '137': 'If we add prototype triggers (use L+), the resulting system (DCS+) outperforms both versions of SEMRESP by a significant margin (87.2% over 73.2% and 80.4%).', '138': 'Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).', '139': 'All other systems require logical forms as training data, whereas ours does not.', '140': 'Table 3 shows that even DCS, which does not use prototypes, is comparable to the best previous system (Kwiatkowski et al., 2010), and by adding a few prototypes, DCS+ offers a decisive edge (91.1% over 88.9% on GEO).', '141': 'Rather than using lexical triggers, several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.', '142': 'This option is not available to us since we do not have annotated logical forms, so we must instead rely on lexical triggers to define the search space.', '143': 'Note that having lexical triggers is a much weaker requirement than having a CCG lexicon, and far easier to obtain than logical forms.', '144': 'Intuitions How is our system learning?', '145': 'Initially, the weights are zero, so the beam search is essentially unguided.', '146': 'We find that only for a small fraction of training examples do the K-best sets contain any trees yielding the correct answer (29% for DCS on GEO).', '147': 'However, training on just these examples is enough to improve the parameters, and this 29% increases to 66% and then to 95% over the next few iterations.', '148': 'This bootstrapping behavior occurs naturally: The “easy” examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.', '149': 'Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language, words and predicates.', '150': 'For example, area (by virtue which results in programs (DCS trees) which are of being a noun) triggers many predicates: city, much simpler than the logically-equivalent lambda state, area, etc.', '151': 'Inspecting the final parameters calculus formulae.', '152': '(DCS on GEO), we find that the feature [area, area] The idea of using CSPs to represent semantics is has a much higher weight than [area, city].', '153': 'Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents.', '154': 'The restriction to present (for example, [in, loc] has high weight). trees is similar to economical DRT (Bos, 2009).', '155': 'The errors that the system makes stem from mul- The other major focus of this work is program tiple sources, including errors in the POS tags (e.g., induction—inferring logical forms from their denostates is sometimes tagged as a verb, which triggers tations.', '156': 'There has been a fair amount of past work on no predicates), confusion of Washington state with this topic: Liang et al. (2010) induces combinatory Washington D.C., learning the wrong lexical asso- logic programs in a non-linguistic setting.', '157': 'Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.', '158': '5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation, DCS, which offers a new perspective lexical semantics.', '159': 'The closest work to ours is Clarke on compositional semantics.', '160': 'To contrast, consider et al. (2010), which we discussed earlier.', '161': 'CCG (Steedman, 2000), in which semantic pars- The integration of natural language with denotaing is driven from the lexicon.', '162': 'The lexicon en- tions computed against a world (grounding) is becodes information about how each word can used in coming increasingly popular.', '163': 'Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).', '164': 'Past work has also fofor the second.', '165': 'These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.', '166': 'Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al., 2010). compositional semantics.', '167': 'In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.', '168': 'A trigger for We built a system that interprets natural language borders specifies only that border can be used, but utterances much more accurately than existing sysnot how.', '169': 'The combination rules are encoded in the tems, despite using no annotated logical forms.', '170': 'Our features as soft preferences.', '171': 'This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.', '172': 'Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.', '173': 'In DCS, the mark-execute and Tom Kwiatkowski for providing us with data construct provides a flexible framework for dealing and answering questions.', '174': '598'}","['P11-1060_swastika', 'P11-1060_aakansha', 'P11-1060_sweta']","['../data/summaries/P11-1060_swastika.txt', '../data/summaries/P11-1060_aakansha.txt', '../data/summaries/P11-1060_sweta.txt']","['../data/tba/P11-1060_swastika.json', '../data/tba/P11-1060_aakansha.json', '../data/tba/P11-1060_sweta.json']"
P11-1061,Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections,../data/papers/P11-1061.xml,"{'0': 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', '1': 'We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.', '2': 'Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.', '3': 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).', '4': 'Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.', '5': 'Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems.', '6': 'Supervised part-of-speech (POS) taggers, for example, approach the level of inter-annotator agreement (Shen et al., 2007, 97.3% accuracy for English).', '7': 'However, supervised methods rely on labeled training data, which is time-consuming and expensive to generate.', '8': 'Unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for training models.', '9': 'Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.', '10': 'To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.', '11': 'This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al., 2000; Xi and Hwa, 2005; Ganchev et al., 2009).', '12': 'Naseem et al. (2009) and Snyder et al.', '13': '(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.', '14': 'Our work is closest to that of Yarowsky and Ngai (2001), but differs in two important ways.', '15': 'First, we use a novel graph-based framework for projecting syntactic information across language boundaries.', '16': 'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).', '17': 'Second, we treat the projected labels as features in an unsupervised model (§5), rather than using them directly for supervised training.', '18': 'To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).', '19': 'Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', '20': 'Because there might be some controversy about the exact definitions of such universals, this set of coarse-grained POS categories is defined operationally, by collapsing language (or treebank) specific distinctions to a set of categories that exists across all languages.', '21': 'These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.', '22': 'We evaluate our approach on eight European languages (§6), and show that both our contributions provide consistent and statistically significant improvements.', '23': 'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).', '24': 'The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.', '25': 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', '26': 'As discussed in more detail in §3, we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types, while the vertices on the English side are individual word types.', '27': 'Graph construction does not require any labeled data, but makes use of two similarity functions.', '28': 'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function, designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).', '29': 'To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.', '30': 'To initialize the graph we tag the English side of the parallel text using a supervised model.', '31': 'By aggregating the POS labels of the English tokens to types, we can generate label distributions for the English vertices.', '32': 'Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i.e. the ones adjacent to the English vertices) first, and then among all of the foreign vertices (§4).', '33': 'The POS distributions over the foreign trigram types are used as features to learn a better unsupervised POS tagger (§5).', '34': 'The following three sections elaborate these different stages is more detail.', '35': 'In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples, and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al., 2003).', '36': 'Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context necessary for disambiguation; on the other hand, it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.', '37': 'Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.', '38': 'More recently, Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.', '39': 'They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types, and edge weights based on distributional similarity, to improve a supervised conditional random field tagger.', '40': 'We extend Subramanya et al.’s intuitions to our bilingual setup.', '41': 'Because the information flow in our graph is asymmetric (from English to the foreign language), we use different types of vertices for each language.', '42': 'The foreign language vertices (denoted by Vf) correspond to foreign trigram types, exactly as in Subramanya et al. (2010).', '43': 'On the English side, however, the vertices (denoted by Ve) correspond to word types.', '44': 'Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.', '45': 'Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices.4 The graph vertices are extracted from the different sides of a parallel corpus (De, Df) and an additional unlabeled monolingual foreign corpus Ff, which will be used later for training.', '46': 'We use two different similarity functions to define the edge weights among the foreign vertices and between vertices from different languages.', '47': 'Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).', '48': 'We briefly review it here for completeness.', '49': 'We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.', '50': 'Each feature concept is akin to a random variable and its occurrence in the text corresponds to a particular instantiation of that random variable.', '51': 'For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.', '52': 'This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.', '53': 'Finally, note that while most feature concepts are lexicalized, others, such as the suffix concept, are not.', '54': 'Given this similarity function, we define a nearest neighbor graph, where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.', '55': 'We use N(u) to denote the neighborhood of vertex u, and fixed n = 5 in our experiments.', '56': 'To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.', '57': 'Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g., when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall, and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.', '58': 'Based on these high-confidence alignments we can extract tuples of the form [u H v], where u is a foreign trigram type, whose middle word aligns to an English word type v. Our bilingual similarity function then sets the edge weights in proportion to these tuple counts.', '59': 'So far the graph has been completely unlabeled.', '60': 'To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.', '61': 'These tag distributions are used to initialize the label distributions over the English vertices in the graph.', '62': 'Note that since all English vertices were extracted from the parallel text, we will have an initial label distribution for all vertices in Ve.', '63': 'A very small excerpt from an Italian-English graph is shown in Figure 1.', '64': 'As one can see, only the trigrams [suo incarceramento ,], [suo iter ,] and [suo carattere ,] are connected to English words.', '65': 'In this particular case, all English vertices are labeled as nouns by the supervised tagger.', '66': 'In general, the neighborhoods can be more diverse and we allow a soft label distribution over the vertices.', '67': 'It is worth noting that the middle words of the Italian trigrams are nouns too, which exhibits the fact that the similarity metric connects types having the same syntactic category.', '68': 'In the label propagation stage, we propagate the automatic English tags to the aligned Italian trigram types, followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.', '69': 'Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram.', '70': 'Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.', '71': 'We use label propagation in two stages to generate soft labels on all the vertices in the graph.', '72': 'In the first stage, we run a single step of label propagation, which transfers the label distributions from the English vertices to the connected foreign language vertices (say, Vf�) at the periphery of the graph.', '73': 'Note that because we extracted only high-confidence alignments, many foreign vertices will not be connected to any English vertices.', '74': 'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.', '75': 'We use a squared loss to penalize neighboring vertices that have different label distributions: kqi − qjk2 = Ey(qi(y) − qj(y))2, and additionally regularize the label distributions towards the uniform distribution U over all possible labels Y.', '76': 'It can be shown that this objective is convex in q.', '77': 'The first term in the objective function is the graph smoothness regularizer which encourages the distributions of similar vertices (large wij) to be similar.', '78': 'The second term is a regularizer and encourages all type marginals to be uniform to the extent that is allowed by the first two terms (cf. maximum entropy principle).', '79': 'If an unlabeled vertex does not have a path to any labeled vertex, this term ensures that the converged marginal for this vertex will be uniform over all tags, allowing the middle word of such an unlabeled vertex to take on any of the possible tags.', '80': 'While it is possible to derive a closed form solution for this convex objective function, it would require the inversion of a matrix of order |Vf|.', '81': 'Instead, we resort to an iterative update based method.', '82': 'We formulate the update as follows: where ∀ui ∈ Vf \\ Vfl, γi(y) and κi are defined as: We ran this procedure for 10 iterations.', '83': 'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.', '84': 'This vector tx is constructed for every word in the foreign vocabulary and will be used to provide features for the unsupervised foreign language POS tagger.', '85': 'We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).', '86': 'For a sentence x and a state sequence z, a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.', '87': 'This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.', '88': 'In our experiments, we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model, the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.', '89': 'The feature-based model replaces the emission distribution with a log-linear model, such that: on the word identity x, features checking whether x contains digits or hyphens, whether the first letter of x is upper case, and suffix features up to length 3.', '90': 'All features were conjoined with the state z.', '91': 'We trained this model by optimizing the following objective function: Note that this involves marginalizing out all possible state configurations z for a sentence x, resulting in a non-convex objective.', '92': 'To optimize this function, we used L-BFGS, a quasi-Newton method (Liu and Nocedal, 1989).', '93': 'For English POS tagging, BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977).8 Moreover, this route of optimization outperformed a vanilla HMM trained with EM by 12%.', '94': 'We adopted this state-of-the-art model because it makes it easy to experiment with various ways of incorporating our novel constraint feature into the log-linear emission model.', '95': 'This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.', '96': 'The function A : F —* C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in §6.2: Note that when tx(y) = 1 the feature value is 0 and has no effect on the model, while its value is −oc when tx(y) = 0 and constrains the HMM’s state space.', '97': 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', '98': '7).', '99': 'It would have therefore also been possible to use the integer programming (IP) based approach of Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side.', '100': 'However, we do not explore this possibility in the current work.', '101': 'Before presenting our results, we describe the datasets that we used, as well as two baselines.', '102': 'We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', '103': 'The availability of these resources guided our selection of foreign languages.', '104': 'For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007).', '105': 'The parallel data came from the Europarl corpus (Koehn, 2005) and the ODS United Nations dataset (UN, 2006).', '106': 'Taking the intersection of languages in these resources, and selecting languages with large amounts of parallel data, yields the following set of eight Indo-European languages: Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish.', '107': 'Of course, we are primarily interested in applying our techniques to languages for which no labeled resources are available.', '108': 'However, we needed to restrict ourselves to these languages in order to be able to evaluate the performance of our approach.', '109': 'We paid particular attention to minimize the number of free parameters, and used the same hyperparameters for all language pairs, rather than attempting language-specific tuning.', '110': 'We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.', '111': 'We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).', '112': 'While there might be some controversy about the exact definition of such a tagset, these 12 categories cover the most frequent part-of-speech and exist in one form or another in all of the languages that we studied.', '113': 'For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.', '114': 'The supervised POS tagging accuracies (on this tagset) are shown in the last row of Table 2.', '115': 'The taggers were trained on datasets labeled with the universal tags.', '116': 'The number of latent HMM states for each language in our experiments was set to the number of fine tags in the language’s treebank.', '117': 'In other words, the set of hidden states F was chosen to be the fine set of treebank tags.', '118': 'Therefore, the number of fine tags varied across languages for our experiments; however, one could as well have fixed the set of HMM states to be a constant across languages, and created one mapping to the universal POS tagset.', '119': 'To provide a thorough analysis, we evaluated three baselines and two oracles in addition to two variants of our graph-based approach.', '120': 'We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.', '121': 'For unaligned words, we set the tag to the most frequent tag in the corresponding treebank.', '122': 'For each language, we took the same number of sentences from the bitext as there are in its treebank, and trained a supervised feature-HMM.', '123': 'This can be seen as a rough approximation of Yarowsky and Ngai (2001).', '124': 'We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.', '125': '1).', '126': 'Because many foreign word types are not aligned to an English word (see Table 3), and we do not run label propagation on the foreign side, we expect the projected information to have less coverage.', '127': 'Furthermore we expect the label distributions on the foreign to be fairly noisy, because the graph constraints have not been taken into account yet.', '128': 'Our oracles took advantage of the labeled treebanks: While we tried to minimize the number of free parameters in our model, there are a few hyperparameters that need to be set.', '129': 'Fortunately, performance was stable across various values, and we were able to use the same hyperparameters for all languages.', '130': 'We used C = 1.0 as the L2 regularization constant in (Eq.', '131': '10) and trained both EM and L-BFGS for 1000 iterations.', '132': 'When extracting the vector t, used to compute the constraint feature from the graph, we tried three threshold values for r (see Eq.', '133': '7).', '134': 'Because we don’t have a separate development set, we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.', '135': 'For seven out of eight languages a threshold of 0.2 gave the best results for our final model, which indicates that for languages without any validation set, r = 0.2 can be used.', '136': 'For graph propagation, the hyperparameter v was set to 2 x 10−6 and was not tuned.', '137': 'The graph was constructed using 2 million trigrams; we chose these by truncating the parallel datasets up to the number of sentence pairs that contained 2 million trigrams.', '138': 'Table 2 shows our complete set of results.', '139': 'As expected, the vanilla HMM trained with EM performs the worst.', '140': 'The feature-HMM model works better for all languages, generalizing the results achieved for English by Berg-Kirkpatrick et al. (2010).', '141': 'Our “Projection” baseline is able to benefit from the bilingual information and greatly improves upon the monolingual baselines, but falls short of the “No LP” model by 2.5% on an average.', '142': 'The “No LP” model does not outperform direct projection for German and Greek, but performs better for six out of eight languages.', '143': 'Overall, it gives improvements ranging from 1.1% for German to 14.7% for Italian, for an average improvement of 8.3% over the unsupervised feature-HMM model.', '144': 'For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.', '145': 'Our full model (“With LP”) outperforms the unsupervised baselines and the “No LP” setting for all languages.', '146': 'It falls short of the “Projection” baseline for German, but is statistically indistinguishable in terms of accuracy.', '147': 'As indicated by bolding, for seven out of eight languages the improvements of the “With LP” setting are statistically significant with respect to the other models, including the “No LP” setting.11 Overall, it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages.', '148': 'Our full model outperforms the “No LP” setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features.', '149': 'We tabulate this increase in Table 3.', '150': 'For all languages, the vocabulary sizes increase by several thousand words.', '151': 'Although the tag distributions of the foreign words (Eq.', '152': '6) are noisy, the results confirm that label propagation within the foreign language part of the graph adds significant quality for every language.', '153': 'Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.', '154': 'While the first three models get three to four tags wrong, our best model gets only one word wrong and is the most accurate among the four models for this example.', '155': 'Examining the word fidanzato for the “No LP” and “With LP” models is particularly instructive.', '156': 'As Figure 1 shows, this word has no high-confidence alignment in the Italian-English bitext.', '157': 'As a result, its POS tag needs to be induced in the “No LP” case, while the 11A word level paired-t-test is significant at p < 0.01 for Danish, Greek, Italian, Portuguese, Spanish and Swedish, and p < 0.05 for Dutch. correct tag is available as a constraint feature in the “With LP” case.', '158': 'We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.', '159': 'Because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.', '160': 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language.', '161': 'Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.', '162': 'We would like to thank Ryan McDonald for numerous discussions on this topic.', '163': 'We would also like to thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data.', '164': 'Finally, we thank Kuzman Ganchev and the three anonymous reviewers for helpful suggestions and comments on earlier drafts of this paper.'}","['P11-1061_swastika', 'P11-1061_sweta', 'P11-1061_aakansha']","['../data/summaries/P11-1061_swastika.txt', '../data/summaries/P11-1061_sweta.txt', '../data/summaries/P11-1061_aakansha.txt']","['../data/tba/P11-1061_swastika.json', '../data/tba/P11-1061_sweta.json', '../data/tba/P11-1061_aakansha.json']"
P87-1015,CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*,../data/papers/P87-1015.xml,"{'0': 'CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*', '1': 'We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.', '2': 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', '3': 'Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.', '4': 'Little attention, however, has been paid to the structural descriptions that these formalisms can assign to strings, i.e. their strong generative capacity.', '5': 'This aspect of the formalism is both linguistically and computationally important.', '6': ""For example, Gazdar (1985) discusses the applicability of Indexed Grammars (IG's) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar (LFG) and Government and Bindings grammars (GB)."", '7': ""The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's."", '8': ""We consider properties of the tree sets generated by CFG's, Tree Adjoining Grammars (TAG's), Head Grammars (HG's), Categorial Grammars (CG's), and IG's."", '9': 'We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.', '10': 'These two properties of the tree sets are not only linguistically relevant, but also have computational importance.', '11': ""By considering derivation trees, and thus abstracting away from the details of the composition operation and the structures being manipulated, we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER, MCS82-07294 and DCR-84-10413, ARO grant DAA 29-84-9-0027, and DARPA grant N00014-85-K0018."", '12': 'We are very grateful to Tony Kroc.h, Michael Pails, Sunil Shende, and Mark Steedman for valuable discussions. formalisms.', '13': 'It is striking that from this point of view many formalisms can be grouped together as having identically structured derivation tree sets.', '14': ""This suggests that by generalizing the notion of context-freeness in CFG's, we can define a class of grammatical formalisms that manipulate more complex structures."", '15': ""In this paper, we outline how such family of formalisms can be defined, and show that like CFG's, each member possesses a number of desirable linguistic and computational properties: in particular, the constant growth property and polynomial recognizability."", '16': ""From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."", '17': 'We define the path set of a tree 1 as the set of strings that label a path from the root to frontier of 7.', '18': 'The path set of a tree set is the union of the path sets of trees in that tree set.', '19': ""It can be easily shown from Thatcher's result that the path set of every local set is a regular set."", '20': ""As a result, CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path."", '21': ""For example, CFG's cannot produce trees of the form shown in Figure 1 in which there are nested dependencies between S and NP nodes appearing on the spine of the tree."", '22': 'Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.', '23': 'He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.', '24': ""Head Grammars (HG's), introduced by Pollard (1984), is a formalism that manipulates headed strings: i.e., strings, one of whose symbols is distinguished as the head."", '25': 'Not only is concatenation of these strings possible, but head wrapping can be used to split a string and wrap it around another string.', '26': ""The productions of HG's are very similar to those of CFG's except that the operation used must be made explicit."", '27': ""Thus, the tree sets generated by HG's are similar to those of CFG's, with each node annotated by the operation (concatenation or wrapping) used to combine the headed strings derived by the daughters of Tree Adjoining Grammars, a tree rewriting formalism, was introduced by Joshi, Levy and Takahashi (1975) and Joshi (1983/85)."", '28': 'A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.', '29': 'Trees are composed using an operation called adjoining, which is defined as follows.', '30': 'Let n be some node labeled X in a tree -y (see Figure 3).', '31': 'Let 71 be a tree with root and foot labeled by X.', '32': ""When 7' is adjoined at ?I in the tree 7 we obtain a tree v&quot;."", '33': ""The subtree under,; is excised from 7, the tree 7' is inserted in its place and the excised subtree is inserted below the foot of y'."", '34': 'It can be shown that the path set of the tree set generated by a TAG G is a context-free language.', '35': ""TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers."", '36': ""From the definition of TAG's, it follows that the choice of adjunction is not dependent on the history of the derivation."", '37': ""Like CFG's, the choice is predetermined by a finite number of rules encapsulated in the grammar."", '38': ""Thus, the derivation trees for TAG's have the same structure as local sets."", '39': ""As with HG's derivation structures are annotated; in the case of TAG's, by the trees used for adjunction and addresses of nodes of the elementary tree where adjunctions occurred."", '40': 'We can define derivation trees inductively on the length of the derivation of a tree 1.', '41': 'If 7 is an elementary tree, the derivation tree consists of a single node labeled 7.', '42': ""Suppose -y results from the adjunction of 71, ,-y, at the k distinct tree addresses 141, , nk in some elementary tree 7', respectively."", '43': ""The tree denoting this derivation of 7 is rooted with a node labeled 7' having k subtrees for the derivations of 71, ... ,7a."", '44': 'The edge from the root to the subtree for the derivation of 7i is labeled by the address ni.', '45': 'To show that the derivation tree set of a TAG is a local set, nodes are labeled by pairs consisting of the name of an elementary tree and the address at which it was adjoined, instead of labelling edges with addresses.', '46': ""The following rule corresponds to the above derivation, where 71, , 7k are derived from the auxiliary trees , , fik, respectively. for all addresses n in some elementary tree at which 7' can be adjoined."", '47': ""If 7' is an initial tree we do not include an address on the left-hand side."", '48': ""There has been recent interest in the application of Indexed Grammars (IG's) to natural languages."", '49': ""Gazdar (1985) considers a number of linguistic analyses which IG's (but not CFG's) can make, for example, the Norwedish example shown in Figure 1."", '50': ""The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages."", '51': ""Trees derived by IG's exhibit a property that is not exhibited by the trees sets derived by TAG's or CFG's."", '52': 'Informally, two or more paths can be dependent on each other: for example, they could be required to be of equal length as in the trees in Figure 4. generates such a tree set.', '53': ""We focus on this difference between the tree sets of CFG's and IG's, and formalize the notion of dependence between paths in a tree set in Section 3."", '54': 'An IG can be viewed as a CFG in which each nonterminal is associated with a stack.', '55': 'Each production can push or pop symbols on the stack as can be seen in the following productions that generate tree of the form shown in Figure 4b.', '56': 'Gazdar (1985) argues that sharing of stacks can be used to give analyses for coordination.', '57': ""Analogous to the sharing of stacks in IC's, Lexical-Functional Grammar's (LFG's) use the unification of unbounded hierarchical structures."", '58': ""Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5."", '59': 'Bresnan, Kaplan, Peters, and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', '60': ""Gazdar (1985) considers a restriction of IG's in which no more than one nonterminal on the right-hand-side of a production can inherit the stack from the left-hand-side."", '61': 'Unbounded dependencies between branches are not possible in such a system.', '62': ""TAG's can be shown to be equivalent to this restricted system."", '63': ""Thus, TAG's can not give analyses in which dependencies between arbitrarily large branches exist."", '64': 'Steedman (1986) considers Categorial Grammars in which both the operations of function application and composition may be used, and in which function can specify whether they take their arguments from their right or left.', '65': ""While the generative power of CG's is greater that of CFG's, it appears to be highly constrained."", '66': ""Hence, their relationship to formalisms such as HG's and TAG's is of interest."", '67': 'On the one hand, the definition of composition in Steedman (1985), which technically permits composition of functions with unbounded number of arguments, generates tree sets with dependent paths such as those shown in Figure 6.', '68': 'This kind of dependency arises from the use of the composition operation to compose two arbitrarily large categories.', '69': 'This allows an unbounded amount of information about two separate paths (e.g. an encoding of their length) to be combined and used to influence the later derivation.', '70': ""A consequence of the ability to generate tree sets with this property is that CC's under this definition can generate the following language which can not be generated by either TAG's or HG's."", '71': ""0n0'i'i0'2&quot;bin242bn I n = 711 + n2 } On the other hand, no linguistic use is made of this general form of composition and Steedman (personal communication) and Steedman (1986) argues that a more limited definition of composition is more natural."", '72': 'With this restriction the resulting tree sets will have independent paths.', '73': ""The equivalence of CC's with this restriction to TAG's and HG's is, however, still an open problem."", '74': 'An extension of the TAG system was introduced by Joshi et al. (1975) and later redefined by Joshi (1987) in which the adjunction operation is defined on sets of elementary trees rather than single trees.', '75': 'A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', '76': 'We must adjoin all trees in an auxiliary tree set together as a single step in the derivation.', '77': 'The adjunction operation with respect to tree sets (multicomponent adjunction) is defined as follows.', '78': 'Each member of a set of trees can be adjoined into distinct nodes of trees in a single elementary tree set, i.e, derivations always involve the adjunction of a derived auxiliary tree set into an elementary tree set.', '79': ""Lilo CFG's, TAG's, and HG's the derivation tree set of a MCTAG will be a local set."", '80': 'The derivation trees of a MCTAG are similar to those of a TAG.', '81': 'Instead of the names of elementary trees of a TAG, the nodes are labeled by a sequence of names of trees in an elementary tree set.', '82': 'Since trees in a tree set are adjoined together, the addressing scheme uses a sequence of pairings of the address and name of the elementary tree adjoined at that address.', '83': 'The following context-free production captures the derivation step of the grammar shown in Figure 7, in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).', '84': '((fii, Q2, Pa) , —■ (01, i32, 03) , e),(02,e) Oa, en) The path complexity of the tee set generated by a MCTAG is not necessarily context-free.', '85': ""Like the string languages of MCTAG's, the complexity of the path set increases as the cardinality of the elementary tee sets increases, though both the string languages and path sets will always be semilinear."", '86': ""MCTAG's are able to generate tee sets having dependent paths."", '87': 'For example, the MCTAG shown in Figure 7 generates trees of the form shown in Figure 4b.', '88': 'The number of paths that can be dependent is bounded by the grammar (in fact the maximum cardinality of a tree set determines this bound).', '89': ""Hence, trees shown in Figure 8 can not be generated by any MCTAG (but can be generated by an IG) because the number of pairs of dependent paths grows with n. Since the derivation tees of TAG's, MCTAG's, and HG's are local sets, the choice of the structure used at each point in a derivation in these systems does not depend on the context at that point within the derivation."", '90': ""Thus, as in CFG's, at any point in the derivation, the set of structures that can be applied is determined only by a finite set of rules encapsulated by the grammar."", '91': 'We characterize a class of formalisms that have this property in Section 4.', '92': 'We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', '93': 'As is described in Section 4, the property of having a derivation tree set that is a local set appears to be useful in showing important properties of the languages generated by the formalisms.', '94': ""The semilinearity of Tree Adjoining Languages (TAL's), MCTAL's, and Head Languages (HL's) can be proved using this property, with suitable restrictions on the composition operations."", '95': 'Roughly speaking, we say that a tee set contains trees with dependent paths if there are two paths p., = vim., and g., = in each 7 E r such that v., is some, possibly empty, shared initial subpath; v., and wi are not bounded in length; and there is some &quot;dependence&quot; (such as equal length) between the set of all v., and w., for each 7 Er.', '96': 'A tree set may be said to have dependencies between paths if some &quot;appropriate&quot; subset can be shown to have dependent paths as defined above.', '97': 'We attempt to formalize this notion in terms of the tee pumping lemma which can be used to show that a tee set does not have dependent paths.', '98': 'Thatcher (1973) describes a tee pumping lemma for recognizable sets related to the string pumping lemma for regular sets.', '99': 'The tee in Figure 9a can be denoted by t1 i223 where tee substitution is used instead of concatenation.', '100': 'The tee pumping lemma states that if there is tree, t = 22 t2t3, generated by a CFG G, whose height is more than a predetermined bound k, then all trees of the form ti tP3 for each i > 0 will also generated by G (as shown in Figure 9b).', '101': ""The string pumping lemma for CFG's (uvwxy-theorem) can be seen as a corollary of this lemma. from this pumping lemma: a single path can be pumped independently."", '102': 'For example, let us consider a tree set containing trees of the form shown in Figure 4a.', '103': 'The tree t2 must be on one of the two branches.', '104': 'Pumping t2 will change only one branch and leave the other branch unaffected.', '105': 'Hence, the resulting trees will no longer have two branches of equal size.', '106': ""We can give a tree pumping lemma for TAG's by adapting the uvwxy-theorem for CFL's since the tree sets of TAG's have independent and context-free paths."", '107': 'This pumping lemma states that if there is tree, t = t2t3t4t5, generated by a TAG G, such that its height is more than a predetermined bound k, then all trees of the form ti it tstt ts for each i > 0 will also generated by G. Similarly, for tree sets with independent paths and more complex path sets, tree pumping lemmas can be given.', '108': 'We adapt the string pumping lemma for the class of languages corresponding to the complexity of the path set.', '109': 'A geometrical progression of language families defined by Weir (1987) involves tree sets with increasingly complex path sets.', '110': 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .', '111': '.', '112': '.t The path set of tree sets at level k +1 have the complexity of the string language of level k. The independence of paths in a tree set appears to be an important property.', '113': 'A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.', '114': 'For example, the formalisms in the hierarchy described above generate semilinear languages although their path sets become increasingly more complex as one moves up the hierarchy.', '115': 'From the point of view of recognition, independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently, which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.', '116': 'From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', '117': 'Our goal is to define a class of formal systems, and show that any member of this class will possess certain attractive properties.', '118': ""In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."", '119': ""In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs."", '120': 'To be a member of LCFRS a formalism must satisfy two restrictions.', '121': 'First, any grammar must involve a finite number of elementary structures, composed using a finite number of composition operations.', '122': 'These operations, as we see below, are restricted to be size preserving (as in the case of concatenation in CFG) which implies that they will be linear and non-erasing.', '123': 'A second restriction on the formalisms is that choices during the derivation are independent of the context in the derivation.', '124': ""As will be obvious later, their derivation tree sets will be local sets as are those of CFG's."", '125': 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', '126': 'These derivation trees show how the composition operations were used to derive the final structures from elementary structures.', '127': 'Nodes are annotated by the name of the composition operation used at that step in the derivation.', '128': ""As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures."", '129': 'Frontier nodes are annotated by zero arty functions corresponding to elementary structures.', '130': ""Each treelet (an internal node with all its children) represents the use of a rule that is encapsulated by the grammar The grammar encapsulates (either explicitly or implicitly) a finite number of rules that can be written as follows: n > 0 In the case of CFG's, for each production In the case of TAG's, a derivation step in which the derived trees RI, • • • , On are adjoined into fi at rhe addresses • • • • in. would involve the use of the following rule2."", '131': ""The composition operations in the case of CFG's are parameterized by the productions."", '132': ""In TAG's the elementary tree and addresses where adjunction takes place are used to instantiate the operation."", '133': 'To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.', '134': ""These systems are similar to those described by Pollard (1984) as Generalized Context-Free Grammars (GCFG's)."", '135': ""Unlike GCFG's, however, the composition operations of LCFRS's are restricted to be linear (do not duplicate unboundedly large structures) and nonerasing (do not erase unbounded structures, a restriction made in most modern transformational grammars)."", '136': ""These two restrictions impose the constraint that the result of composing any two structures should be a structure whose &quot;size&quot; is the sum of its constituents plus some constant For example, the operation 4, discussed in the case of CFG's (in Section 4.1) adds the constant equal to the sum of the length of the strings VI, un+r• Since we are considering formalisms with arbitrary structures it is difficult to precisely specify all of the restrictions on the composition operations that we believe would appropriately generalize the concatenation operation for the particular structures used by the formalism."", '137': ""In considering recognition of LCFRS's, we make further assumption concerning the contribution of each structure to the input string, and how the composition operations combine structures in this respect."", '138': ""We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments."", '139': 'Semilinearity and the closely related constant growth property (a consequence of semilinearity) have been discussed in the context of grammars for natural languages by Joshi (1983/85) and Berwick and Weinberg (1984).', '140': 'Roughly speaking, a language, L, has the property of semilinearity if the number of occurrences of each symbol in any string is a linear combination of the occurrences of these symbols in some fixed finite set of strings.', '141': 'Thus, the length of any string in L is a linear combination of the length of strings in some fixed finite subset of L, and thus L is said to have the constant growth property.', '142': 'Although this property is not structural, it depends on the structural property that sentences can be built from a finite set of clauses of bounded structure as noted by Joshi (1983/85).', '143': 'The property of semilinearity is concerned only with the occurrence of symbols in strings and not their order.', '144': 'Thus, any language that is letter equivalent to a semilinear language is also semilinear.', '145': 'Two strings are letter equivalent if they contain equal number of occurrences of each terminal symbol, and two languages are letter equivalent if every string in one language is letter equivalent to a string in the other language and vice-versa.', '146': ""Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing."", '147': 'Hence, the terminal symbols appearing in the structures that are composed are not lost (though a constant number of new symbols may be introduced).', '148': 'If 0(A) gives the number of occurrences of each terminal in the structure named by A, then, given the constraints imposed on the formalism, for each rule A --. fp(Ai, , An) we have the equality where c„ is some constant.', '149': 'We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A —* A1 Anup where tk (up) = cp.', '150': 'Thus, the language generated by a grammar of a LCFRS is semilinear.', '151': ""We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's)."", '152': ""As suggested at the end of Section 3, the restrictions that have been specified in the definition of LCFRS's suggest that they can be efficiently recognized."", '153': 'In this section for the purposes of showing that polynomial time recognition is possible, we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.', '154': 'Since each composition operation is linear and nonerasing, a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation, including each substring exactly once.', '155': ""CFG's, TAG's, MCTAG's and HG's are all members of this class since they satisfy these restrictions."", '156': ""Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings."", '157': ""For example, in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node), and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3)."", '158': 'We can represent any derived tree of a TAG by the two substrings that appear in its frontier, and then define how the adjunction operation concatenates the substrings.', '159': ""Similarly, for all the LCFRS's, discussed in Section 2, we can define the relationship between a structure and the sequence of substrings it spans, and the effect of the composition operations on sequences of substrings."", '160': ""A derived structure will be mapped onto a sequence zi of substrings (not necessarily contiguous in the input), and the composition operations will be mapped onto functions that can defined as follows3. f((zi,• • • , zni), (m.,• • • ,Yn3)) = (Z1, • • • , Zn3) where each z, is the concatenation of strings from z,'s and yk's."", '161': 'The linear and nonerasing assumptions about the operations discussed in Section 4.1 require that each z, and yk is used exactly once to define the strings zi, ,z1,3.', '162': 'Some of the operations will be constant functions, corresponding to elementary structures, and will be written as f () = zi), where each z, is a constant, the string of terminal symbols al an,,,.', '163': 'This representation of structures by substrings and the composition operation by its effect on substrings is related to the work of Rounds (1985).', '164': ""Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms."", '165': 'This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.', '166': 'With the additional assumptions, inspired by Rounds (1985), we can show that members of this class can be recognized in polynomial time.', '167': 'We use Alternating Turing Machines (Chandra, Kozen, and Stockmeyer, 1981) to show that polynomial time recognition is possible for the languages discussed in Section 4.3.', '168': 'An ATM has two types of states, existential and universal.', '169': 'In an existential state an ATM behaves like a nondeterministic TM, accepting if one of the applicable moves leads to acceptance; in an universal state the ATM accepts if all the applicable moves lead to acceptance.', '170': 'An ATM may be thought of as spawning independent processes for each applicable move.', '171': 'A k-tape ATM, M, has a read-only input tape and k read-write work tapes.', '172': 'A step of an ATM consists of reading a symbol from each tape and optionally moving each head to the left or right one tape cell.', '173': 'A configuration of M consists of a state of the finite control, the nonblank contents of the input tape and k work tapes, and the position of each head.', '174': 'The space of a configuration is the sum of the lengths of the nonblank tape contents of the k work tapes.', '175': 'M works in space S(n) if for every string that M accepts no configuration exceeds space S(n).', '176': 'It has been shown in (Chandra et al., 1981) that if M works in space log n then there is a deterministic TM which accepts the same language in polynomial time.', '177': 'In the next section, we show how an ATM can accept the strings generated by a grammar in a LCFRS formalism in logspace, and hence show that each family can be recognized in polynomial time.', '178': 'We define an ATM, M, recognizing a language generated by a grammar, G, having the properties discussed in Section 43.', '179': 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.', '180': 'The rewrite rules and the definition of the composition operations may be stored in the finite state control since G uses a finite number of them.', '181': 'Suppose M has to determine whether the k substrings ,.. .,ak can be derived from some symbol A.', '182': 'Since each zi is a contiguous substring of the input (say ai,), and no two substrings overlap, we can represent zi by the pair of integers (i2, i2).', '183': 'We assume that M is in an existential state qA, with integers i1 and i2 representing zi in the (2i — 1)th and 22th work tape, for 1 < i < k. For each rule p : A fp(B, C) such that fp is mapped onto the function fp defined by the following rule. jp((xi,.. • ,rnt), (1ii, • • • • Yn3))= (Zi , • • • , Zk) M breaks xi , zk into substrings xi, , xn, and yi,...,y&quot; conforming to the definition of fp.', '184': 'M spawns as many processes as there are ways of breaking up ri , .. • , zt, and rules with A on their left-hand-side.', '185': 'Each spawned process must check if xi , , xn, and , yn, can be derived from B and C, respectively.', '186': ""To do this, the x's and y's are stored in the next 2ni + 2n2 tapes, and M goes to a universal state."", '187': 'Two processes are spawned requiring B to derive z,.., and C to derive yi , , y,.', '188': 'Thus, for example, one successor process will be have M to be in the existential state qa with the indices encoding xi , , xn, in the first 2n i tapes.', '189': 'For rules p : A fpo such that fp is constant function, giving an elementary structure, fp is defined such that fp() = (Si ... xi() where each z is a constant string.', '190': 'M must enter a universal state and check that each of the k constant substrings are in the appropriate place (as determined by the contents of the first 2k work tapes) on the input tape.', '191': 'In addition to the tapes required to store the indices, M requires one work tape for splitting the substrings.', '192': 'Thus, the ATM has no more than 6km&quot; + 1 work tapes, where km&quot; is the maximum number of substrings spanned by a derived structure.', '193': 'Since the work tapes store integers (which can be written in binary) that never exceed the size of the input, no configuration has space exceeding 0(log n).', '194': 'Thus, M works in logspace and recognition can be done on a deterministic TM in polynomial tape.', '195': 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems, and classified these formalisms on the basis of two features: path complexity; and path independence.', '196': ""We contrasted formalisms such as CFG's, HG's, TAG's and MCTAG's, with formalisms such as IG's and unificational systems such as LFG's and FUG's."", '197': 'We address the question of whether or not a formalism can generate only structural descriptions with independent paths.', '198': 'This property reflects an important aspect of the underlying linguistic theory associated with the formalism.', '199': 'In a grammar which generates independent paths the derivations of sibling constituents can not share an unbounded amount of information.', '200': 'The importance of this property becomes clear in contrasting theories underlying GPSG (Gazdar, Klein, Pulluna, and Sag, 1985), and GB (as described by Berwick, 1984) with those underlying LFG and FUG.', '201': ""It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's."", '202': ""As illustrated by MCTAG's, it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's, HG's, and TAG's."", '203': 'In order to observe the similarity between these constrained systems, it is crucial to abstract away from the details of the structures and operations used by the system.', '204': ""The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets."", '205': 'Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.', '206': 'As suggested in Section 4.3.2, a derivation with independent paths can be divided into subcomputations with limited sharing of information.', '207': 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.', '208': ""This family represents an attempt to generalize the properties shared by CFG's, HG's, TAG's, and MCTAG's."", '209': ""Like HG's, TAG's, and MCTAG's, members of LCFRS can manipulate structures more complex than terminal strings and use composition operations that are more complex that concatenation."", '210': ""We place certain restrictions on the composition operations of LCFRS's, restrictions that are shared by the composition operations of the constrained grammatical systems that we have considered."", '211': 'The operations must be linear and nonerasing, i.e., they can not duplicate or erase structure from their arguments.', '212': ""Notice that even though IG's and LFG's involve CFG-like productions, they are (linguistically) fundamentally different from CFG's because the composition operations need not be linear."", '213': ""By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's), structures with unbounded dependencies between paths can be generated."", '214': ""LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85)."", '215': 'The results described in this paper suggest a characterization of mild context-sensitivity in terms of generalized context-freeness.', '216': ""Having defined LCFRS's, in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated."", '217': 'In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.', '218': 'We insisted that each structure dominates a bounded number of (not necessarily adjacent) substrings.', '219': 'The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.', '220': 'We showed that any system defined in this way can be recognized in polynomial time.', '221': 'Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).', '222': 'However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.', '223': ""It is known that CFG's, HG's, and TAG's can be recognized in polynomial time since polynomial time algorithms exist in for each of these formalisms."", '224': ""A corollary of the result of Section 4.3 is that polynomial time recognition of MCTAG's is possible."", '225': 'As discussed in Section 3, independent paths in tree sets, rather than the path complexity, may be crucial in characterizing semilinearity and polynomial time recognition.', '226': 'We would like to relax somewhat the constraint on the path complexity of formalisms in LCFRS.', '227': 'Formalisms such as the restricted indexed grammars (Gazdar, 1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths, but more complex path sets.', '228': 'Since these path sets are semilinear, the property of independent paths in their tree sets is sufficient to cause semilinearity of the languages generated by them.', '229': ""In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir)."", '230': ""LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class."", '231': ""In this paper, our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity."", '232': 'In considering this aspect of a formalism, we hope to better understand the relationship between the structural descriptions generated by the grammars of a formalism, and the properties of semilinearity and polynomial recognizability.'}","['P87-1015_swastika', 'P87-1015_sweta', 'P87-1015_vardha']","['../data/summaries/P87-1015_swastika.txt', '../data/summaries/P87-1015_sweta.txt', '../data/summaries/P87-1015_vardha.txt']","['../data/tba/P87-1015_swastika.json', '../data/tba/P87-1015_sweta.json', '../data/tba/P87-1015_vardha.json']"
P98-1081,Improving Data Driven Wordclass Tagging by System Combination,../data/papers/P98-1081.xml,"{'0': 'Improving Data Driven Wordclass Tagging by System Combination', '1': 'In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system.', '2': 'We do this by means of an experiment involving the task of morpho-syntactic wordclass tagging.', '3': 'Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.', '4': 'After comparison, their outputs are combined using several voting strategies and second stage classifiers.', '5': 'All combination taggers outperform their best component, with the best combination showing a 19.1% lower error rate than the best individual tagger.', '6': 'In all Natural Language Processing (NLP) systems, we find one or more language models which are used to predict, classify and/or interpret language related observations.', '7': 'Traditionally, these models were categorized as either rule-based/symbolic or corpus-based/probabilistic.', '8': 'Recent work (e.g. Brill 1992) has demonstrated clearly that this categorization is in fact a mix-up of two distinct Categorization systems: on the one hand there is the representation used for the language model (rules, Markov model, neural net, case base, etc.) and on the other hand the manner in which the model is constructed (hand crafted vs. data driven).', '9': 'Data driven methods appear to be the more popular.', '10': 'This can be explained by the fact that, in general, hand crafting an explicit model is rather difficult, especially since what is being modelled, natural language, is not (yet) well- understood.', '11': 'When a data driven method is used, a model is automatically learned from the implicit structure of an annotated training corpus.', '12': ""This is much easier and can quickly lead to a model which produces results with a 'reasonably' good quality."", '13': ""Obviously, 'reasonably good quality' is not the ultimate goal."", '14': 'Unfortunately, the quality that can be reached for a given task is limited, and not merely by the potential of the learning method used.', '15': 'Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material.', '16': 'Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system.', '17': 'However, the fact that any given system cannot go beyond this ceiling does not mean that machine learning as a whole is similarly limited.', '18': ""A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors."", '19': 'In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance.', '20': 'In the machine learning literature this approach is known as ensemble, stacked, or combined classifiers.', '21': 'It has been shown that, when the errors are uncorrelated to a sufficient degree, the resulting combined classifier will often perform better than all the individual systems (Ali and Pazzani 1996; Chan and Stolfo 1995; Tumer and Gosh 1996).', '22': 'The underlying assumption is twofold.', '23': ""First, the combined votes will make the system more robust to the quirks of each learner's particular bias."", '24': ""Also, the use of information about each individual method's behaviour in principle even admits the possibility to fix collective errors."", '25': 'We will execute our investigation by means of an experiment.', '26': 'The NLP task used in the experiment is morpho-syntactic wordclass tagging.', '27': 'The reasons for this choice are several.', '28': 'First of all, tagging is a widely researched and well-understood task (cf.', '29': 'van Halteren (ed.)', '30': '1998).', '31': ""Second, current performance levels on this task still leave room for improvement: 'state of the art' performance for data driven automatic wordclass taggers (tagging English text with single tags from a low detail tagset) is 9697% correctly tagged words."", '32': 'Finally, a number of rather different methods are available that generate a fully functional tagging system from annotated text.', '33': 'Component taggers In 1992, van Halteren combined a number of taggers by way of a straightforward majority vote (cf.', '34': 'van Halteren 1996).', '35': 'Since the component taggers all used n-gram statistics to model context probabilities and the knowledge representation was hence fundamentally the same in each component, the results were limited.', '36': 'Now there are more varied systems available, a variety which we hope will lead to better combination effects.', '37': 'For this experiment we have selected four systems, primarily on the basis of availability.', '38': 'Each of these uses different features of the text to be tagged, and each has a completely different representation of the language model.', '39': 'The first and oldest system uses a traditional trig-ram model (Steetskamp 1995; henceforth tagger T, for Trigrams), based on context statistics P(ti[ti-l,ti-2) and lexical statistics P(tilwi) directly estimated from relative corpus frequencies.', '40': 'The Viterbi algorithm is used to determine the most probable tag sequence.', '41': 'Since this model has no facilities for handling unknown words, a Memory-Based system (see below) is used to propose distributions of potential tags for words not in the lexicon.', '42': 'The second system is the Transformation Based Learning system as described by Brill (19941; henceforth tagger R, for Rules).', '43': ""This 1 Brill's system is available as a collection of C programs and Perl scripts at ftp ://ftp."", '44': 'cs.', '45': 'j hu.', '46': 'edu/pub/brill/Programs/ RULE_BASED_TAGGER_V.', '47': 'I. 14.', '48': 'tar.', '49': 'Z. system starts with a basic corpus annotation (each word is tagged with its most likely tag) and then searches through a space of transformation rules in order to reduce the discrepancy between its current annotation and the correct one (in our case 528 rules were learned).', '50': 'During tagging these rules are applied in sequence to new text.', '51': 'Of all the four systems, this one has access to the most information: contextual information (the words and tags in a window spanning three positions before and after the focus word) as well as lexical information (the existence of words formed by suffix/prefix addition/deletion).', '52': 'However, the actual use of this information is severely limited in that the individual information items can only be combined according to the patterns laid down in the rule templates.', '53': 'The third system uses Memory-Based Learning as described by Daelemans et al. (1996; henceforth tagger M, for Memory).', '54': 'During the training phase, cases containing information about the word, the context and the correct tag are stored in memory.', '55': 'During tagging, the case most similar to that of the focus word is retrieved from the memory, which is indexed on the basis of the Information Gain of each feature, and the accompanying tag is selected.', '56': 'The system used here has access to information about the focus word and the two positions before and after, at least for known words.', '57': 'For unknown words, the single position before and after, three suffix letters, and information about capitalization and presence of a hyphen or a digit are used.', '58': 'The fourth and final system is the MXPOST system as described by Ratnaparkhi (19962; henceforth tagger E, for Entropy).', '59': 'It uses a number of word and context features rather similar to system M, and trains a Maximum Entropy model that assigns a weighting parameter to each feature-value and combination of features that is relevant to the estimation of the probability P(tag[features).', '60': 'A beam search is then used to find the highest probability tag sequence.', '61': ""Both this system and Brill's system are used with the default settings that are suggested in their documentation."", '62': ""2Ratnaparkhi's Java implementation of this system is available at ftp://ftp.cis.upenn.edu/ pub/adwait/jmx/"", '63': 'The data we use for our experiment consists of the tagged LOB corpus (Johansson 1986).', '64': 'The corpus comprises about one million words, divided over 500 samples of 2000 words from 15 text types.', '65': 'Its tagging, which was manually checked and corrected, is generally accepted to be quite accurate.', '66': 'Here we use a slight adaptation of the tagset.', '67': 'The changes are mainly cosmetic, e.g. non-alphabetic characters such as ""$"" in tag names have been replaced.', '68': 'However, there has also been some retokenization: genitive markers have been split off and the negative marker ""n\'t"" has been reattached.', '69': 'An example sentence tagged with the resulting tagset is: The ATI singular or plural article Lord NPT singular titular noun Major NPT singular titular noun extended VBD past tense of verb an AT singular article invitation NN singular common noun to IN preposition all ABN pre-quantifier the ATI singular or plural article parliamentary JJ adjective candidates NNS plural common noun SPER period The tagset consists of 170 different tags (including ditto tags 3) and has an average ambiguity of 2.69 tags per wordform.', '70': 'The difficulty of the tagging task can be judged by the two baseline measurements in Table 2 below, representing a completely random choice from the potential tags for each token (Random) and selection of the lexically most likely tag (LexProb).', '71': 'For our experiment, we divide the corpus into three parts.', '72': 'The first part, called Train, consists of 80% of the data (931062 tokens), constructed 3Ditto tags are used for the components of multi- token units, e.g. if ""as well as"" is taken to be a coordination conjunction, it is tagged ""as_CC1 well_CC2 as_CC3"", using three related but different ditto tags.', '73': 'by taking the first eight utterances of every ten.', '74': 'This part is used to train the individual tag- gers.', '75': 'The second part, Tune, consists of 10% of the data (every ninth utterance, 114479 tokens) and is used to select the best tagger parameters where applicable and to develop the combination methods.', '76': 'The third and final part, Test, consists of the remaining 10% (.115101 tokens) and is used for the final performance measurements of all tuggers.', '77': 'Both Tune and Test contain around 2.5% new tokens (wrt Train) and a further 0.2% known tokens with new tags.', '78': 'The data in Train (for individual tuggers) and Tune (for combination tuggers) is to be the only information used in tagger construction: all components of all tuggers (lexicon, context statistics, etc.) are to be entirely data driven and no manual adjustments are to be done.', '79': 'The data in Test is never to be inspected in detail but only used as a benchmark tagging for quality measurement.', '80': 'In order to see whether combination of the component tuggers is likely to lead to improvements of tagging quality, we first examine the results of the individual taggers when applied to Tune.', '81': 'As far as we know this is also one of the first rigorous measurements of the relative quality of different tagger generators, using a single tagset and dataset and identical circumstances.', '82': 'The quality of the individual tuggers (cf.', '83': 'Table 2 below) certainly still leaves room for improvement, although tagger E surprises us with an accuracy well above any results reported so far and makes us less confident about the gain to be accomplished with combination.', '84': 'However, that there is room for improvement is not enough.', '85': 'As explained above, for combination to lead to improvement, the component taggers must differ in the errors that they make.', '86': 'That this is indeed the case can be seen in Table 1.', '87': 'It shows that for 99.22% of Tune, at least one tagger selects the correct tag.', '88': 'However, it is unlikely that we will be able to identify this 4This implies that it is impossible to note if errors counted against a tagger are in fact errors in the benchmark tagging.', '89': 'We accept that we are measuring quality in relation to a specific tagging rather than the linguistic truth (if such exists) and can only hope the tagged LOB corpus lives up to its reputation.', '90': 'All Taggers Correct 92.49 Majority Correct (31,211) 4.34 Correct Present, No Majority 1.37 (22,1111) Minority Correct (13,121) 1.01 All Taggers Wrong 0.78 Table 1: Tagger agreement on Tune.', '91': 'The patterns between the brackets give the distribution of correct/incorrect tags over the systems.', '92': 'tag in each case.', '93': 'We should rather aim for optimal selection in those cases where the correct tag is not outvoted, which would ideally lead to correct tagging of 98.21% of the words (in Tune).', '94': 'Simple Voting There are many ways in which the results of the component taggers can be combined, selecting a single tag from the set proposed by these taggers.', '95': 'In this and the following sections we examine a number of them.', '96': 'The accuracy measurements for all of them are listed in Table 2.', '97': '5 The most straightforward selection method is an n-way vote.', '98': 'Each tagger is allowed to vote for the tag of its choice and the tag with the highest number of votes is selected.', '99': '6 The question is how large a vote we allow each tagger.', '100': 'The most democratic option is to give each tagger one vote (Majority).', '101': 'However, it appears more useful to give more weight to taggers which have proved their quality.', '102': 'This can be general quality, e.g. each tagger votes its overall precision (TotPrecision), or quality in relation to the current situation, e.g. each tagger votes its precision on the suggested tag (Tag- Precision).', '103': ""The information about each tagger's quality is derived from an inspection of its results on Tune."", '104': '5For any tag X, precision measures which percentage of the tokens tagged X by the tagger are also tagged X in the benchmark and recall measures which percentage of the tokens tagged X in the benchmark are also tagged X by the tagger.', '105': 'When abstracting away from individual tags, precision and recall are equal and measure how many tokens are tagged correctly; in this case we also use the more generic term accuracy.', '106': '6In our experiment, a random selection from among the winning tags is made whenever there is a tie.', '107': 'Table 2: Accuracy of individual taggers and combination methods.', '108': 'But we have even more information on how well the taggers perform.', '109': 'We not only know whether we should believe what they propose (precision) but also know how often they fail to recognize the correct tag (recall).', '110': 'This information can be used by forcing each tagger also to add to the vote for tags suggested by the opposition, by an amount equal to 1 minus the recall on the opposing tag (Precision-Recall).', '111': 'As it turns out~ all voting systems outperform the best single tagger, E. 7 Also, the best voting system is the one in which the most specific information is used, Precision-Recall.', '112': 'However, specific information is not always superior, for TotPrecision scores higher than TagPrecision.', '113': 'This might be explained by the fact that recall information is missing (for overall performance this does not matter, since recall is equal to precision).', '114': ""7Even the worst combinator, Majority, is significantly better than E: using McNemar's chi-square, p--0."", '115': 'Pairwise Voting So far, we have only used information on the performance of individual taggers.', '116': 'A next step is to examine them in pairs.', '117': 'We can investigate all situations where one tagger suggests T1 and the other T2 and estimate the probability that in this situation the tag should actually be Tx.', '118': 'When combining the taggers, every tagger pair is taken in turn and allowed to vote (with the probability described above) for each possible tag, i.e. not just the ones suggested by the component taggers.', '119': 'If a tag pair T1T2 has never been observed in Tune, we fall back on information on the individual taggers, viz.', '120': 'the probability of each tag Tx given that the tagger suggested tag Ti.', '121': 'Note that with this method (and those in the next section) a tag suggested by a minority (or even none) of the taggers still has a chance to win.', '122': 'In principle, this could remove the restriction of gain only in 22 and 1111 cases.', '123': 'In practice, the chance to beat a majority is very slight indeed and we should not get our hopes up too high that this should happen very often.', '124': 'When used on Test, the pairwise voting strategy (TagPair) clearly outperforms the other voting strategies, 8 but does not yet approach the level where all tying majority votes are handled correctly (98.31%).', '125': 'Stacked classifiers From the measurements so far it appears that the use of more detailed information leads to a better accuracy improvement.', '126': 'It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely.', '127': 'The practice of feeding the outputs of a number of classifiers as features for a next learner sit is significantly better than the runner-up (Precision-Recall) with p=0.', '128': 'is usually called stacking (Wolpert 1992).', '129': 'The second stage can be provided with the first level outputs, and with additional information, e.g. about the original input pattern.', '130': 'The first choice for this is to use a Memory- Based second level learner.', '131': 'In the basic version (Tags), each case consists of the tags suggested by the component taggers and the correct tag.', '132': 'In the more advanced versions we also add information about the word in question (Tags+Word) and the tags suggested by all taggers for the previous and the next position (Tags+Context).', '133': 'For the first two the similarity metric used during tagging is a straightforward overlap count; for the third we need to use an Information Gain weighting (Daelemans ct al. 1997).', '134': 'Surprisingly, none of the Memory-Based based methods reaches the quality of TagPair.', '135': '9 The explanation for this can be found when we examine the differences within the Memory- Based general strategy: the more feature information is stored, the higher the accuracy on Tune, but the lower the accuracy on Test.', '136': 'This is most likely an overtraining effect: Tune is probably too small to collect case bases which can leverage the stacking effect convincingly, especially since only 7.51% of the second stage material shows disagreement between the featured tags.', '137': 'To examine if the overtraining effects are specific to this particular second level classifier, we also used the C5.0 system, a commercial version of the well-known program C4.5 (Quinlan 1993) for the induction of decision trees, on the same training material.', '138': '1° Because C5.0 prunes the decision tree, the overfitting of training material (Tune) is less than with Memory-Based learning, but the results on Test are also worse.', '139': 'We conjecture that pruning is not beneficial when the interesting cases are very rare.', '140': 'To realise the benefits of stacking, either more data is needed or a second stage classifier that is better suited to this type of problem.', '141': '9Tags (Memory-Based) scores significantly worse than TagPair (p=0.0274) and not significantly better than Precision-Recall (p=0.2766).', '142': '1°Tags+Word could not be handled by C5.0 due to the huge number of feature values.', '143': 'Test Increase vs % Reduc- Component tion Error Average Rate Best Component T 96.08 - R 96.46 M 96.95 MR 97.03 96.70+0.33 2.6 (M) RT 97.11 96.27+0.84 18.4 (R) MT 97.26 96.52+0.74 lO.2 (M) E 97.43 MRT 97.52 96.50+1.02 18.7 (M) ME 97.56 97.19+0.37 5.1 (E) ER 97.58 96.95+0.63 5.8 (E) ET 97.60 96.76+0.84 6.6 (E) MER 97.75 96.95+0.80 12.5 (E) ERT 97.79 96.66+1.13 14.0 (E) MET 97.86 96.82+1.04 16.7 (E) MERT 97.92 96.73+1.19 19.1 (E) Table 3: Correctness scores on Test for Pairwise Voting with all tagger combinations 7 The value of combination.', '144': 'The relation between the accuracy of combinations (using TagPair) and that of the individual taggers is shown in Table 3.', '145': 'The most important observation is that every combination (significantly) outperforms the combination of any strict subset of its components.', '146': 'Also of note is the improvement yielded by the best combination.', '147': 'The pairwise voting system, using all four individual taggers, scores 97.92% correct on Test, a 19.1% reduction in error rate over the best individual system, viz.', '148': 'the Maximum Entropy tagger (97.43%).', '149': 'A major factor in the quality of the combination results is obviously the quality of the best component: all combinations with E score higher than those without E (although M, R and T together are able to beat E alone11).', '150': 'After that, the decisive factor appears to be the difference in language model: T is generally a better combiner than M and R, 12 even though it has the lowest accuracy when operating alone.', '151': 'A possible criticism of the proposed combi11By a margin at the edge of significance: p=0.0608.', '152': '12Although not significantly better, e.g. the differences within the group ME/ER/ET are not significant.', '153': 'nation scheme is the fact that for the most successful combination schemes, one has to reserve a nontrivial portion (in the experiment 10% of the total material) of the annotated data to set the parameters for the combination.', '154': 'To see whether this is in fact a good way to spend the extra data, we also trained the two best individual systems (E and M, with exactly the same settings as in the first experiments) on a concatenation of Train and Tune, so that they had access to every piece of data that the combination had seen.', '155': 'It turns out that the increase in the individual taggers is quite limited when compared to combination.', '156': 'The more extensively trained E scored 97.51% correct on Test (3.1% error reduction) and M 97.07% (3.9% error reduction).', '157': 'Conclusion.', '158': 'Our experiment shows that, at least for the task at hand, combination of several different systems allows us to raise the performance ceiling for data driven systems.', '159': 'Obviously there is still room for a closer examination of the differences between the combination methods, e.g. the question whether Memory-Based combination would have performed better if we had provided more training data than just Tune, and of the remaining errors, e.g. the effects of inconsistency in the data (cf.', '160': 'Ratnaparkhi 1996 on such effects in the Penn Treebank corpus).', '161': 'Regardless of such closer investigation, we feel that our results are encouraging enough to extend our investigation of combination, starting with additional component taggers and selection strategies, and going on to shifts to other tagsets and/or languages.', '162': 'But the investigation need not be limited to wordclass tagging, for we expect that there are many other NLP tasks where combination could lead to worthwhile improvements.', '163': 'Our thanks go to the creators of the tagger generators used here for making their systems available.'}",['P98-1081'],['../data/summaries/P98-1081.txt'],['../data/tba/P98-1081.json']
P98-2143,Robust pronoun resolution with limited knowledge,../data/papers/P98-2143.xml,"{'0': 'Robust pronoun resolution with limited knowledge', '1': 'Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.', '2': 'One of the disadvantages of developing a knowledgeÂ\xad based system, however, is that it is a very labourÂ\xad intensive and time-consuming task.', '3': 'This paper presÂ\xad ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.', '4': 'Input is checked against agreement and for a number of antecedent indicators.', '5': 'Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.', '6': 'Evaluation reports a success rate of 89.7% which is better than the sucÂ\xad cess rates of the approaches selected for comparison and tested on the same data.', '7': 'In addition, preliminary experiments show that the approach can be successÂ\xad fully adapted for other languages with minimum modifications.', '8': 'For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).', '9': 'However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.', '10': 'While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ\xad mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ\xad cessing of growing language resources.', '11': 'Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ\xad guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).', '12': 'Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaphÂ\xad ora resolution.', '13': 'It is also an example of how anaphors in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing.', '14': 'Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).', '15': 'With a view to avoiding complex syntactic, semanÂ\xad tic and discourse analysis (which is vital for realÂ\xad world applications), we developed a robust, knowlÂ\xad edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.', '16': 'It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as ""antecedent indicators"").', '17': 'The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the reÂ\xad maining candidates (see next section).', '18': 'The noun phrase with the highest aggregate score is proposed as antecedent; in the rare event of a tie, priority is given to the candidate with the higher score for imÂ\xad mediate reference.', '19': 'If immediate reference has not been identified, then priority is given to the candi date with the best collocation pattern score.', '20': 'If this does not help, the candidate with the higher score for indicating verbs is preferred.', '21': 'If still no choice is possible, the most recent from the remaining candiÂ\xad dates is selected as the antecedent.', '22': '2.1 Antecedent indicators.', '23': 'Antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.', '24': 'Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the anteÂ\xad cedent.', '25': 'The antecedent indicators have been identiÂ\xad fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, ""nonÂ\xad prepositional"" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.', '26': 'Whilst some of the indicators are more genre-specific (term preferÂ\xad ence) and others are less genre-specific (""immediate reference""), the majority appear to be genreÂ\xad independent.', '27': 'In the following we shall outline some the indicators used and shall illustrate them by exÂ\xad amples.', '28': 'Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphors than indefinite ones (definite noun phrases score 0 and indefinite ones are penalised by -1).', '29': 'We regard a noun phrase as definite if the head noun is modified by a definite article, or by demonstrative or possesÂ\xad sive pronouns.', '30': ""This rule is ignored if there are no definite articles, possessive or demonstrative proÂ\xad nouns in the paragraph (this exception is taken into account because some English user's guides tend to omit articles)."", '31': 'Givenness Noun phrases in previous sentences representing the ""given information"" (theme) 1 are deemed good candidates for antecedents and score I (candidates not representing the theme score 0).', '32': 'In a coherent text (Firbas 1992), the given or known information, or theme, usually appears first, and thus forms a coÂ\xad referential link with the preceding text.', '33': 'The new information, or rheme, provides some information about the theme.', '34': '1We use the simple heuristics that the given information is the first noun phrase in a non-imperative sentence.', '35': 'Indicating verbs If a verb is a member of the Verb_set = {discuss, present, illustrate, identify, summarise, examine, describe, define, show, check, develop, review, reÂ\xad port, outline, consider, investigate, explore, assess, analyse, synthesise, study, survey, deal, cover}, we consider the first NP following it as the preferred anÂ\xad tecedent (scores 1 and 0).', '36': 'Empirical evidence sugÂ\xad gests that because of the salience of the noun phrases which follow them, the verbs listed above are particularly good indicators.', '37': 'Lexical reiteration Lexically reiterated items are likely candidates for antecedent (a NP scores 2 if is repeated within the same paragraph twice or more, 1 if repeated once and 0 if not).', '38': 'Lexically reiterated items include reÂ\xad peated synonymous noun phrases which may often be preceded by definite articles or demonstratives.', '39': 'Also, a sequence of noun phrases with the same head counts as lexical reiteration (e.g. ""toner bottle"", ""bottle of toner"", ""the bottle"").', '40': 'Section heading preference If a noun phrase occurs in the heading of the section, part of which is the current sentence, then we conÂ\xad sider it as the preferred candidate (1, 0).', '41': '""Non-prepositional"" noun phrases A ""pure"", ""non-prepositional"" noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase (0, -1 ).', '42': 'Example: Insert the cassettei into the VCR making sure iti is suitable for the length of recording.', '43': 'Here ""the VCR"" is penalised (-1) for being part of the prepositional phrase ""into the VCR"".', '44': 'This preference can be explained in terms of saliÂ\xad ence from the point of view of the centering theory.', '45': 'The latter proposes the ranking ""subject, direct obÂ\xad ject, indirect object"" (Brennan et al. 1987) and noun phrases which are parts of prepositional phrases are usually indirect objects.', '46': 'Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun (2,0).', '47': 'The collocation preference here is restricted to the patterns ""noun phrase (pronoun), verb"" and ""verb, noun phrase (pronoun)"".', '48': 'Owing to lack of syntactic information, this preference is somewhat weaker than the collocation preference described in (Dagan & ltai 1990).', '49': 'Example: Press the keyi down and turn the volume up...', '50': 'Press iti again.', '51': 'Immediate reference In technical manuals the ""immediate reference"" clue can often be useful in identifying the antecedent.', '52': 'The heuristics used is that in constructions of the form ""...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)"", where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun ""it"" immeÂ\xad diately following V2 and is therefore given preference (scores 2 and 0).', '53': 'This preference can be viewed as a modification of the collocation preference.', '54': 'It is also quite freÂ\xad quent with imperative constructions.', '55': 'Example: To print the paper, you can stand the printeri up or lay iti flat.', '56': 'To turn on the printer, press the Power buttoni and hold iti down for a moment.', '57': 'Unwrap the paperiâ\x80¢ form iti and align itiâ\x80¢ then load iti into the drawer.', '58': 'Referential distance In complex sentences, noun phrases in the previous clause2 are the best candidate for the antecedent of an anaphor in the subsequent clause, followed by noun phrases in the previous sentence, then by nouns situated 2 sentences further back and finally nouns 3 sentences further back (2, 1, 0, -1).', '59': 'For anaphors in simple sentences, noun phrases in the previous senÂ\xad tence are the best candidate for antecedent, followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back {1, 0, -1).', '60': 'Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms (score 1 if the NP is a term and 0 if not).', '61': '21dentification of clauses in complex sentences is do e heuristically.', '62': 'As already mentioned, each of the antecedent inÂ\xad dicators assigns a score with a value {-1, 0, 1, 2}.', '63': 'These scores have been determined experimentally on an empirical basis and are constantly being upÂ\xad dated.', '64': 'Top symptoms like ""lexical reiteration"" asÂ\xad sign score ""2"" whereas ""non-prepositional"" noun phrases are given a negative score of ""-1"".', '65': 'We should point out that the antecedent indicators are preferences and not absolute factors.', '66': 'There might be cases where one or more of the antecedent indicators do not ""point"" to the correct antecedent.', '67': 'For inÂ\xad stance, in the sentence ""Insert the cassette into the VCRi making sure iti is turned on"", the indicator ""non-prepositional noun phrases"" would penalise the correct antecedent.', '68': 'When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the ""non-prepositional noun phrases"" heuristics (penalty) would be overturned by the ""collocational preference"" heuristics.', '69': '2.2 Informal description of the algorithm.', '70': 'The algorithm for pronoun resolution can be deÂ\xad scribed informally as follows: 1.', '71': 'Examine the current sentence and the two preÂ\xad.', '72': 'ceding sentences (if available).', '73': 'Look for noun phrases3 only to the left of the anaphor4 2.', '74': 'Select from the noun phrases identified only.', '75': 'those which agree in gender and numberS with the pronominal anaphor and group them as a set of potential candidates', '76': 'tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric ""it"" occurring in constructions such as ""It is important"", ""It is necessary"" is eliminated by a ""referential filter"" 5Note that this restriction may not always apply in lanÂ\xad guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. ""government"", ""team"", ""parliament"" etc. can be referred to by ""they""; equally some plural nouns (e.g. ""data"") can be referred to by ""it"") and are exempted from the agreeÂ\xad ment test.', '77': 'For this purpose we have drawn up a compreÂ\xad hensive list of all such cases; to our knowledge, no other computational treatment of pronominal anaphora resoluÂ\xad tion has addressed the problem of ""agreement excepÂ\xad tions"".', '78': 'antecedent.', '79': 'If two candidates have an equal score, the candidate with the higher score for immediate reference is proposed as antecedent.', '80': 'If immediate reference does not hold, propose the candidate with higher score for collocational pattern.', '81': 'If collocational pattern suggests a tie or does not hold, select the candidate with higher score for indicating verbs.', '82': 'If this indicator does not hold again, go for the most recent candidate.', '83': '3.', '84': 'Evaluation.', '85': 'For practical reasons, the approach presented does not incorporate syntactic and semantic information (other than a list of domain terms) and it is not realÂ\xad istic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences.', '86': 'The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.', '87': 'Syntactic parallelism, useful in discrimiÂ\xad nating between identical pronouns on the basis of their syntactic function, also has to be forgone.', '88': 'Lack of semantic knowledge rules out the use of verb seÂ\xad mantics and semantic parallelism.', '89': 'Our evaluation, however, suggests that much less is lost than might be feared.', '90': 'In fact, our evaluation shows that the reÂ\xad sults are comparable to syntax-based methods (Lappin & Leass I994).', '91': 'We believe that the good success rate is due to the fact that a number of anteÂ\xad cedent indicators are taken into account and no facÂ\xad tor is given absolute preference.', '92': 'In particular, this strategy can often override incorrect decisions linked with strong centering preference (Mitkov & Belguith I998) or syntactic and semantic parallelism preferÂ\xad ences (see below).', '93': '3.1 Evaluation A. Our first evaluation exercise (Mitkov & Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).', '94': 'There were 71 pronouns in the 140 page technical manual; 7 of the pronouns were non-anaphoric and 16 exophoric.', '95': 'The resolution of anaphors was carried out with a sucÂ\xad cess rate of 95.8%.', '96': 'The approach being robust (an attempt is made to resolve each anaphor and a proÂ\xad posed antecedent is returned), this figure represents both ""precision"" and ""recall"" if we use the MUC terminology.', '97': 'To avoid any terminological confusion, we shall therefore use the more neutral term ""success rate"" while discussing the evaluation.', '98': 'In order to evaluate the effectiveness of the apÂ\xad proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ\xad dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.', '99': 'The success rate of the ""Baseline Subject"" was 29.2%, whereas the success rate of ""Baseline Most Recent NP"" was 62.5%.', '100': 'Given that our knowledgeÂ\xad poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ\xad tors, we see a dramatic improvement in performance (95.8%) when these preferences are called upon.', '101': 'Typically, our preference-based model proved superior to both baseline models when the anteceÂ\xad dent was neither the most recent subject nor the most recent noun phrase matching the anaphor in gender and number.', '102': 'Example: Identify the draweq by the lit paper port LED and add paper to itj.', '103': 'The aggregate score for ""the drawer"" is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate referÂ\xad ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (""the lit paper port LED"") is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reiteraÂ\xad tion 0 + section heading 0 + collocation 0 + referenÂ\xad tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).', '104': 'From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphor and theÂ· antecedent have not only different syntactic functions but also different semantic roles.', '105': 'Usually knowledge-based apÂ\xad proaches have difficulties in such a situation because they use preferences such as ""syntactic parallelism"" or ""semantic parallelism"".', '106': 'Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the synÂ\xad tactic function/semantic role of each individual word.', '107': 'As far as the typical failure cases are concerned, we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ\xad plex syntactic structure.', '108': 'This should not be surpris ing, given that the approach does not rely on any syntactic knowledge and in particular, it does not produce any parse tree.', '109': 'Indeed, the approach fails on the sentence: The paper through key can be used to feed [a blank sheet of paper]j through the copier out into the copy tray without making a copy on itj.', '110': 'where ""blank sheet of paper"" scores only 2 as opÂ\xad posed to the ""the paper through key"" which scores 6.', '111': ""3.2 Evaluation B. We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals (47-page Portable Style-Writer User's Guide (Stylewriter 1994)."", '112': 'Out of 223 proÂ\xad nouns in the text, 167 were non-anaphoric (deictic and non-anaphoric ""it"").', '113': 'The evaluation carried out was manual to ensure that no added error was genÂ\xad erated (e.g. due to possible wrong sentence/clause detection or POS tagging).', '114': ""Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin's method, which not being available to us, had to be hand-simulated (see 3.3)."", '115': 'The evaluation indicated 83.6% success rate.', '116': 'The ""Baseline subject"" model tested on the same data scored 33.9% recall and 67.9% precision, whereas ""Baseline most recent"" scored 66.7%.', '117': 'Note that ""Baseline subject"" can be assessed both in terms of recall and precision because this ""version"" is not robust: in the event of no subject being available, it is not able to propose an antecedent (the manual guide used as evaluation text contained many imÂ\xad perative zero-subject sentences).', '118': 'In the second experiment we evaluated the apÂ\xad proach from the point of view also of its ""critical success rate"".', '119': 'This measure (Mitkov 1998b) applies only to anaphors ""ambiguous"" from the point of view of number and gender (i.e. to those ""tough"" anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.', '120': 'Our evaluation estabÂ\xad lished the critical success rate as 82%.', '121': 'A case where the system failed was when the anaphor and the antecedent were in the same senÂ\xad tence and where preference was given to a candidate in the preceding sentence.', '122': 'This case and other cases suggest that it might be worthwhile reconsiderÂ\xad ing/refining the weights for the indicator ""referential distance"".', '123': 'Similarly to the first evaluation, we found that the robust approach was not very successful on senÂ\xad tences with too complicated syntax - a price we have to pay for the ""convenience"" of developing a knowlÂ\xad edge-poor system.', '124': 'The results from experiment 1 and experiment 2 can be summarised in the following (statistically) slightly more representative figures.', '125': 'R ob ust aQ pr oa ch B a s el i n e s u b je ct B as eli ne m os t re ce nt Su cc es s rat e (= Pr ec isi on / Re ca ll) 8 9.', '126': '7 % 31.', '127': '55 % I 48 .5 5 % 6 5 . 9 5 % The lower figure in ""Baseline subject"" corresponds to ""recall"" and the higher figure- to ""precision"".', '128': 'If we regard as ""discriminative power"" of each antecedent indicator the ratio ""number of successful antecedent identifications when this indicator was applied""/""number of applications of this indicator"" (for the non-prepositional noun phrase and definiteÂ\xad ness being penalising indicators, this figure is calcuÂ\xad lated as the ratio ""number of unsuccessful anteceÂ\xad dent identifications""/""number of applications""), the immediate reference emerges as the most discrimiÂ\xad native indicator (100%), followed by nonÂ\xad prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).', '129': 'The relaÂ\xad tively low figures for the majority of indicators should not be regarded as a surprise: firstly, we should bear in mind that in most cases a candidate was picked (or rejected) as an antecedent on the baÂ\xad sis of applying a number of different indicators and secondly, that most anaphors had a relatively high number of candidates for antecedent.', '130': 'In terms of frequency of use (""number of nonzero applications""/""number of anaphors""), the most freÂ\xad quently used indicator proved to be referential disÂ\xad tance used in 98.9% of the cases, followed by term preference (97.8%), givenness (83.3%), lexical reitÂ\xad eration (64.4%), definiteness (40%), section heading (37.8%), immediate reference (31.1%) and collocaÂ\xad tion (11.1%).', '131': 'As expected, the most frequent indicaÂ\xad tors were not the most discriminative ones.', '132': '3.3 Comparison to similar approaches: comparaÂ\xad.', '133': 'tive evaluation of Breck Baldwin\'s CogNIAC We felt appropriate to extend the evaluation of our approach by comparing it to Breck Baldwin\'s CogÂ\xad NIAC (Baldwin 1997) approach which features ""high precision coreference with limited knowledge and linguistics resources"".', '134': ""The reason is that both our approach and Breck Baldwin's approach share common principles (both are knowledge-poor and use a POS tagger to provide the input) and therefore a comparison would be appropriate."", '135': 'Given that our approach is robust and returns anÂ\xad tecedent for each pronoun, in order to make the comparison as fair as possible, we used CogNIAC\'s ""resolve all"" version by simulating it manually on the same training data used in evaluation B above.', '136': 'CogNIAC successfully resolved the pronouns in 75% of the cases.', '137': 'This result is comparable with the results described in (Baldwin 1997).', '138': 'For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).', '139': 'It would be fair to say that even though the results show superiority of our approach on the training data used (the genre of technical manuals), they cannot be generalised automatically for other genres or unrestricted texts and for a more accurate picture, further extensive tests are necessary.', '140': 'languages An attractive feature of any NLP approach would be its language ""universality"".', '141': 'While we acknowledge that most of the monolingual NLP approaches are not automatically transferable (with the same degree of efficiency) to other languages, it would be highly desirable if this could be done with minimal adaptaÂ\xad tion.', '142': 'We used the robust approach as a basis for develÂ\xad oping a genre-specific reference resolution approach in Polish.', '143': 'As expected, some of the preferences had to be modified in order to fit with specific features of Polish (Mitkov & Stys 1997).', '144': 'For the time being, we are using the same scores for Polish.', '145': 'The evaluation for Polish was based technical manuals available on the Internet (Internet Manual, 1994; Java Manual 1998).', '146': 'The sample texts conÂ\xad tained 180 pronouns among which were 120 inÂ\xad stances of exophoric reference (most being zero proÂ\xad nouns).', '147': 'The robust approach adapted for Polish demonstrated a high success rate of 93.3% in resolvÂ\xad ing anaphors (with critical success rate of 86.2%).', '148': 'Similarly to the evaluation for English, we comÂ\xad pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.', '149': 'Our preference-based approach showed clear suÂ\xad periority over both baseline models.', '150': 'The first BaseÂ\xad line Model (Baseline Subject) was successful in only 23.7% of the cases, whereas the second (Baseline Most Recent) had a success rate of 68.4%.', '151': 'ThereÂ\xad fore, the 93.3% success rate (see above) demonÂ\xad strates a dramatic increase in precision, which is due to the use of antecedent tracking preferences.', '152': 'We have recently adapted the approach for AraÂ\xad bic as well (Mitkov & Belguith 1998).', '153': 'Our evaluaÂ\xad tion, based on 63 examples (anaphors) from a techÂ\xad nical manual (Sony 1992), indicates a success rate of 95.2% (and critical success rate 89.3 %).', '154': 'We have described a robust, knowledge-poor apÂ\xad proach to pronoun resolution which operates on texts pre-processed by a part-of-speech tagger.', '155': 'Evaluation shows a success rate of 89.7% for the genre of techÂ\xad nical manuals and at least in this genre, the approach appears to be more successful than other similar methods.', '156': 'We have also adapted and evaluated the approach for Polish (93.3 % success rate) and for Arabic (95.2% success rate).'}",['P98-2143'],['../data/summaries/P98-2143.txt'],['../data/tba/P98-2143.json']
W03-0410,Semi-supervised Verb Class Discovery Using Noisy Features,../data/papers/W03-0410.xml,"{'0': 'Semi-supervised Verb Class Discovery Using Noisy Features', '1': 'We cluster verbs into lexical semantic classes, using a general set of noisy features that capture syntactic and semantic properties of the verbs.', '2': 'The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.', '3': 'In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task.', '4': 'We investigate various approaches to feature selection, using both unsupervised and semi-supervised methods, comparing the results to subsets of features manually chosen according to linguistic properties.', '5': 'We find that the unsupervised method we tried cannot be consistently applied to our data.', '6': 'However, the semi- supervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well.', '7': 'Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies.', '8': 'Learning the argument structure properties of verbsâ\x80\x94the semantic roles they assign and their mapping to syntactic positionsâ\x80\x94is both particularly important and difficult.', '9': 'A number of supervised learning approaches have extracted such information about verbs from corpora, including their argument roles (Gildea and Jurafsky, 2002), selectional preferences (Resnik, 1996), and lexical semantic classification (i.e., grouping verbs according to their argument structure properties) (Dorr and Jones, 1996; Lapata and Brew, 1999; Merlo and Stevenson, 2001; Joanis and Stevenson, 2003).', '10': 'Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).', '11': 'We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.', '12': 'Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).', '13': 'As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).', '14': 'However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.', '15': 'Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers.', '16': 'It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features.', '17': 'We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).', '18': 'In contrast to Merlo and Stevenson (2001), we confirmed that a set of general features can be successfully used, without the need for manually determining the relevant features for distinguishing particular classes (cf.', '19': 'Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).', '20': 'On the other hand, in contrast to Schulte im Walde and Brew (2002), we demonstrated that accurate subcategorization statistics are unnecessary (see also Sarkar and Tripasai, 2002).', '21': 'By avoiding the dependence on precise feature extraction, our approach should be more portable to new languages.', '22': 'However, a general feature space means that most features will be irrelevant to any given verb discrimination task.', '23': 'In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to â\x80\x9cthe curse of dimensionalityâ\x80\x9d?', '24': 'In supervised experiments, the learner uses class labels during the training stage to determine which features are relevant to the task at hand.', '25': 'In the unsupervised setting, the large number of potentially irrelevant features becomes a serious problem, since those features may mislead the learner.', '26': 'Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.', '27': 'In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).', '28': 'Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).', '29': 'To preview our results, we find that, overall, the semi-supervised method not only outperforms the entire feature space, but also the manually selected subset of features.', '30': 'The unsupervised feature selection method, on the other hand, was not usable for our data.', '31': 'In the remainder of the paper, we first briefly review our feature space and present our experimental classes and verbs.', '32': 'We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results.', '33': 'We conclude with a discussion of related work, our contributions, and future directions.', '34': 'Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).', '35': 'Levinâ\x80\x99s classes form a hierarchy of verb groupings with shared meaning and syntax.', '36': 'Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.', '37': 'It is important to emphasize, however, that our features are extracted from part-of-speech (POS) tagged and chunked text only: there are no semantic tags of any kind.', '38': 'Thus, the features serve as approximations to the underlying distinctions among classes.', '39': 'Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.', '40': 'Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.', '41': 'We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.', '42': 'In addition to approximating the syntactic frames themselves, we also want to capture regularities in the mapping of arguments to particular slots.', '43': 'For example, the location argument, the truck, is direct object in I loaded the truck with hay, and object of a preposition in I loaded hay onto the truck.', '44': 'These allowable alternations in the expressions of arguments vary according to the class of a verb.', '45': 'We measure this behaviour using features that encode the degree to which two slots contain the same entitiesâ\x80\x94that is, we calculate the overlap in noun (lemma) usage between pairs of syntactic slots.', '46': 'Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001).', '47': 'In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.', '48': 'The Animacy Features (76 features) Semantic properties of the arguments that fill certain roles, such as animacy or motion, are more challenging to detect automatically.', '49': 'Currently, our only such feature is an extension of the animacy feature of Merlo and Stevenson (2001).', '50': 'We approximate the animacy of each of the 76 syntactic slots by counting both pronouns and proper noun phrases (NPs) labelled as â\x80\x9cpersonâ\x80\x9d by our chunker (Abney, 1991).', '51': 'We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.', '52': 'Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.', '53': '3.1 The Verb Classes.', '54': 'Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.', '55': 'These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions.', '56': 'For example, some classes differ in both their semantic roles and frames, while others have the same roles in different frames, or different roles in the same frames.1 Here we summarize the argument structure distinctions between the classes; Table 1 below lists the classes with their Levin class numbers.', '57': 'Benefactive versus Recipient verbs.', '58': 'Mary baked... a cake for Joan/Joan a cake.', '59': 'Mary gave... a cake to Joan/Joan a cake.', '60': 'These dative alternation verbs differ in the preposition and the semantic role of its object.', '61': '1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs.', '62': 'Admire versus Amuse verbs.', '63': 'I admire Jane.', '64': 'Jane amuses me. These psychological state verbs differ in that the Experiencer argument is the subject of Admire verbs, and the object of Amuse verbs.', '65': 'Run versus Sound Emission verbs.', '66': 'The kids ran in the room./*The room ran with kids.', '67': 'The birds sang in the trees./The trees sang with birds.These activity verbs both have an Agent subject in the in transitive, but differ in the prepositional alternations they allow.', '68': 'Cheat versus Steal and Remove verbs.', '69': 'I cheated...', '70': 'Jane of her money/*the money from Jane.', '71': 'I stole...', '72': '*Jane of her money/the money from Jane.', '73': 'These classes also assign the same semantic arguments, but differ in their prepositional alternants.', '74': 'Wipe versus Steal and Remove verbs.', '75': 'Wipe... the dust/the dust from the table/the table.', '76': 'Steal... the money/the money from the bank/*the bank.', '77': 'These classes generally allow the same syntactic frames, but differ in the possible semantic role assignment.', '78': '(Location can be the direct object of Wipe verbs but not of Steal and Remove verbs, as shown.)', '79': 'Spray/Load versus Fill versus Other Verbs of Putting (several related Levin classes).', '80': 'I loaded... hay on the wagon/the wagon with hay.', '81': 'I filled...', '82': '*hay on the wagon/the wagon with hay.', '83': 'I put... hay on the wagon/*the wagon with hay.', '84': 'These three classes also assign the same semantic roles but differ in prepositional alternants.', '85': 'Note, however, that the options for Spray/Load verbs overlap with those of the other two types of verbs.', '86': 'Optionally Intransitive: Run versus Change of State versus â\x80\x9cObject Dropâ\x80\x9d.', '87': 'The horse raced./The jockey raced the horse.', '88': 'The butter melted./The cook melted the butter.', '89': 'The boy played./The boy played soccer.These three classes are all optionally intransitive but as sign different semantic roles to their arguments (Merlo and Stevenson, 2001).', '90': '(Note that the Object Drop verbs are a superset of the Benefactives above.)', '91': 'For many tasks, knowing exactly what PP arguments each verb takes may be sufficient to perform the classification (cf.', '92': 'Dorr and Jones, 1996).', '93': 'However, our features do not give us such perfect knowledge, since PP arguments and adjuncts cannot be distinguished with high accuracy.', '94': 'Using our simple extraction tools, for example, the PP argument in I admired Jane for her honesty is not distinguished from the PP adjunct in I amused Jane for the money.', '95': 'Furthermore, PP arguments differ in frequency, so that a highly distinguishing but rarely used alternant will likely not be useful.', '96': 'Indicators of PP usage are thus useful but not definitive.', '97': 'Ve rb Cl as s C la ss N u m b er # Ve rbs Be ne fa cti ve 26.', '98': '1, 26.', '99': '3 3 5 Re ci pi en t 13.', '100': '1, 13.', '101': '3 2 7 Ad mi re 31.', '102': '2 3 5 A m us e 31.', '103': '1 1 3 4 Ru n 51.', '104': '3.2 7 9 So un d E mi ssi on 43.', '105': '2 5 6 C he at 10.', '106': '6 2 9 St ea l an d Re m ov e 10.', '107': '5, 10.', '108': '1 4 5 Wi pe 10.', '109': '4.1 , 10.', '110': '4.2 3 5 Sp ra y/ Lo ad 9.7 3 6 Fi ll 9.8 6 3 Ot he r V. of Pu tti ng 9.1 â\x80\x936 4 8 C ha ng e of St at e 45.', '111': '1â\x80\x93 4 1 6 9 O bj ec t Dr op 26.', '112': '1, 26.', '113': '3, 26.', '114': '7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).', '115': '3.2 Verb Selection.', '116': 'Our experimental verbs were selected as follows.', '117': 'We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).', '118': 'Because we make the simplifying assumption of a single correct classification for each verb, we also removed any verb: that was deemed excessively polysemous; that belonged to another class under consideration in our study; or for which the class did not correspond to the main sense.', '119': 'Table 1 above shows the number of verbs in each class at the end of this process.', '120': 'Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).', '121': 'We began with this same set of 20 verbs per class for our current work.', '122': 'We then replaced 10 of the 260 verbs (4%) to enable us to have representative seed verbs for certain classes in our semi-supervised experiments (e.g., so that we could include wipe as a seed verb for the Wipe verbs, and fill for the Fill verbs).', '123': 'All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).', '124': '3.3 Feature Extraction.', '125': 'All features were estimated from counts over the British National Corpus (BNC), a 100M word corpus of text samples of recent British English ranging over a wide spectrum of domains.', '126': 'Since it is a general corpus, we do not expect any strong overall domain bias in verb usage.', '127': 'We used the chunker (partial parser) of Abney (1991) to preprocess the corpus, which (noisily) determines the NP subject and direct object of a verb, as well as the PPs potentially associated with it.', '128': 'Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.', '129': 'From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments.', '130': '4.1 Clustering Parameters.', '131': 'We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.', '132': 'In performing hierarchical clustering, both a vector distance measure and a cluster distance (â\x80\x9clinkageâ\x80\x9d) measure are specified.', '133': 'We used the simple Euclidean distance for the former, and Ward linkage for the latter.', '134': 'Ward linkage essentially minimizes the distances of all cluster points to the centroid, and thus is less sensitive to outliers than some other methods.', '135': 'We chose hierarchical clustering because it may be possible to find coherent subclusters of verbs even when there are not exactly good clusters, where is the number of classes.', '136': 'To explore this, we can induce any number of clusters by making a cut at a particular level in the clustering hierarchy.', '137': 'In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff.', '138': 'However, we did experiment with (as in Strehl et al., 2000), and found that performance was generally better (even on our measure, described below, that discounts oversplitting).', '139': 'This supports our intuition that the approach may enable us to find more consistent clusters at a finer grain, without too much fragmentation.', '140': '4.2 Evaluation Measures.', '141': 'We use three separate evaluation measures, that tap into very different properties of the clusterings.', '142': '4.2.1 Accuracy We can assign each cluster the class label of the majority of its members.', '143': 'Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed.', '144': 'Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.', '145': 'As we have defined it, necessarily generally increases as the number of clusters increases, with the extreme being at the #verbs correctly classified #verbs total thus provides a measure of the usefulness in practice of a clusteringâ\x80\x94that is, if one were to use the clustering as a classification, this measure tells how accurate overall the class assignments would be.', '146': 'The theoretical maximum is, of course, 1.', '147': 'To calculate a random baseline, we evaluated 10,000 random clusterings with the same number of verbs and classes as in each of our experimental tasks.', '148': 'Because the achieved depends on the precise size of clusters, we calculated mean over the best scenario (with equal-sized clusters), yielding a conservative estimate (i.e., an upper bound) of the baseline.', '149': 'These figures are reported with our results in Table 2 below.', '150': '4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good.', '151': 'Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.', '152': 'The formula is as follows (Hubert and Arabie, 1985): where is the entry in the contingency table between the classification and the clustering, counting the size of the intersection of class and cluster . Intuitively, measures the similarity of two partitions of data by considering agreements and disagreements between themâ\x80\x94 there is agreement, for example, if and from the same class are in the same cluster, and disagreement if they are not.', '153': 'It is scaled so that perfect agreement yields a value of 1, whereas random groupings (with the same number of groups in each) get a value around 0.', '154': 'It is therefore considered â\x80\x9ccorrected for chance,â\x80\x9d given a fixed number of clusters.3 In tests of the measure on some contrived cluster- ings, we found it quite conservative, and on our experimental clusterings it did not often attain values higher than .25.', '155': 'However, it is useful as a relative measure of good-.', '156': 'ness, in comparing clusterings arising from different feature sets.', '157': '4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.', '158': 'Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs.', '159': 'However, since we fix our number of clusters to the number of classes, the measure remains informative.', '160': '3 In our experiments for estimating the baseline, we in-.', '161': 'deed found a mean value of 0.00 for all random clusterings.', '162': '1.4 1.2 1 0.8 0.6 0.4 0.2 0.6 0.5 0.4 0.3 0.2 0.1 0 Ling: mean Sil = 0.33 Seed: meanS il = 0.89', '163': 'W e re p or t he re th e re su lt s of a n u m be r of cl us te ri n g ex - pe ri m en ts, us in g fe at ur e se ts as fo ll o w s: (1 ) th e fu ll fe at ur e sp ac e; (2 ) a m an ua ll y se le ct ed su bs et of fe at ur es ; (3 ) u n- su pe rv is ed se le ct io n of fe at ur es ; an d (4 ) se mi su p er vi se d se le ct io n, us in g a su pe rv is ed le ar ne r ap pl ie d to se ed ve rb s to se le ct th e fe at ur es . F or ea ch ty pe of fe at ur e se t, w e pe rf or m ed th e sa m e te n cl us te ri n g ta sk s, sh o w n in th e fir st co lu m n of Ta bl e 2.', '164': 'T he se ar e th e sa m e ta sk s pe rf or m ed in th e su pe rv is ed se t- ti n g of Jo an is an d St ev en so n (2 0 0 3) . T he 2- an d 3 w ay ta sk s, an d th ei r m ot iv at io n, w er e de sc ri be d in S ec ti o n 3.', '165': '1. T hr ee m ul ti w ay ta sk s ex pl or e pe rf or m an ce o ve r a la rg er n u m be r of cl as se s: T he 6 w ay ta sk in v ol v es th e C he at , St ea lâ\x80\x93 R e m ov e, W ip e, S pr ay /L o a d, Fi ll, an d â\x80\x9c O th er V er bs of P ut ti n g â\x80\x9d cl as se s, al l of w hi ch u n de rg o si m il ar lo ca ti v e Figure 1: The dendrograms and values for the 2-way Wipe/Stealâ\x80\x93Remove task, using the Ling and Seed sets.', '166': 'The higher (.89 vs. .33) reflects the better separation of the data.', '167': 'regard to the target classes.', '168': 'We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters.', '169': 'Silhouette values vary from +1 to -1, with +1 indicating that the point is near the centroid of its own cluster, and -1 indicating that the point is very close to another cluster (and therefore likely in the wrong cluster).', '170': 'A value of 0 suggests that a point is not clearly in a particular cluster.', '171': 'We calculate the mean silhouette of all points in a clustering to obtain an overall measure of how well the clusters are separated.', '172': 'Essentially, the measure numerically captures what we can intuitively grasp in the visual differences between the dendrograms of â\x80\x9cbetterâ\x80\x9d and â\x80\x9cworseâ\x80\x9d clusterings.', '173': '(A dendrogram is a tree diagram whose leaves are the data points, and whose branch lengths indicate similarity of subclusters; roughly, shorter vertical lines indicate closer clusters.)', '174': 'For example, Figure 1 shows two dendrograms using different feature sets (Ling and Seed, described in Section 5) for the same set of verbs from two classes.', '175': 'The Seed set has slightly lower values for and , but a much higher value (.89) for , indicating a better separation of the data.', '176': 'This captures what is reflected in the dendrogram, in that very short lines connect verbs low in the tree, and longer lines connect the two main clusters.', '177': 'The measure is independent of the true classification, and could be high when the other dependent measures are low, or vice versa.', '178': 'However, it gives important information about the quality of a clustering: The other measures being equal, a clustering with a higher value indicates tighter and more separated clusters, suggesting stronger inherent patterns in the data.', '179': 'alternations.', '180': 'To these 6, the 8-way task adds the Run and Sound Emission verbs, which also undergo locative alternations.', '181': 'The 13-way task includes all of our classes.', '182': 'The second column of Table 2 includes the accuracy of our supervised learner (the decision tree induction system, C5.0), on the same verb sets as in our clustering experiments.', '183': 'These are the results of a 10-fold cross- validation (with boosting) repeated 50 times.4 In our earlier work, we found that cross-validation performance averaged about .02, .04, and .11 higher than test performance on the 2-way, 3-way, and multiway tasks, respectively, and so should be taken as an upper bound on what can be achieved.', '184': 'The third column of Table 2 gives the baseline we calculated from random clusterings.', '185': 'Recall that this is an upper bound on random performance.', '186': 'We use this baseline in calculating reductions in error rate of . The remaining columns of the table give the , , and measures as described in Section 4.2, for each of the feature sets we explored in clustering, which we discuss in turn below.', '187': '5.1 Full Feature Set.', '188': 'The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).', '189': 'Although generally higher than the baseline, is well below that of the supervised learner, and and are generally low.', '190': '5.2 Manual Feature Selection.', '191': 'One approach to dimensionality reduction is to hand- select features that one believes to be relevant to a given task.', '192': 'Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2.', '193': 'Task C5.0 Base Full Ling Seed Full Ling Seed Full Ling Seed Benefactive/Recipient .74 .56 .60 .68 .58 .02 .10 .02 .22 .40 .81 Admire/Amuse .83 .56 .83 .80 .78 .41 .34 .29 .18 .49 .71 Run/Sound Emission .83 .56 .58 .50 .78 -.00 -.02 .29 .17 .44 .66 Cheat/Stealâ\x80\x93Remove .89 .56 .55 .53 .80 -.01 -.02 .34 .30 .29 .74 Wipe/Stealâ\x80\x93Remove .78 .56 .65 .73 .70 .07 .18 .15 .24 .33 .89 Mean of 2-way .81 .56 .64 .65 .73 .10 .12 .22 .22 .39 .76 Spray/Fill/Putting .80 .42 .53 .60 .47 .10 .16 .01 .12 .31 .48 Optionally Intrans.', '194': '.66 .42 .38 .38 .58 -.02 -.02 .25 .16 .27 .39 Mean of 3-way .73 .42 .46 .49 .53 .04 .07 .13 .14 .29 .44 8 Locative Classes .72 .24 .31 .38 .42 .10 .12 .12 .13 .23 .23.', '195': 'All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.', '196': 'C5.0 is supervised accuracy; Base is on random clusters.', '197': 'Full is full feature set; Ling is manually selected subset; Seed is seed-verb-selected set.', '198': 'See text for further description.', '199': 'indicated by the class description given in Levin.', '200': 'For each task, then, the linguistically-relevant subset is defined as the union of these subsets for all the classes in the task.', '201': 'The results for these feature sets in clustering are given in the second subcolumn (Ling) under each of the , , and measures in Table 2.', '202': 'On the 2-way tasks, the performance on average is very close to that of the full feature set for the and measures.', '203': 'On the 3-way and multiway tasks, there is a larger performance gain using the subset of features, with an increase in the reduction of the error rate (over Base ) of 67% over the full feature set.', '204': 'Overall, there is a small performance gain using the Ling subset of features (with an increase in error rate reduction from 13% to 17%).', '205': 'Moreover, the value for the manually selected features is almost always very much higher than that of the full feature set, indicating that the subset of features is more focused on the properties that lead to a better separation of the data.', '206': 'This performance comparison tentatively suggests that good feature selection can be helpful in our task.', '207': 'However, it is important to find a method that does not depend on having an existing classification, since we are interested in applying the approach when such a classification does not exist.', '208': 'In the next two sections, we present unsupervised and minimally supervised approaches to this problem.', '209': '5.3 Unsupervised Feature Selection.', '210': 'In order to deal with excessive dimensionality, Dash et al.', '211': '(1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise.', '212': 'Unfortunately, this promising method did not prove practical for our data.', '213': 'We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).', '214': 'Many feature sets performed very well, and some far outperformed our best results using other feature selection methods.', '215': 'However, across our 10 experimental tasks, there was no consistent range of feature ranks or feature set sizes that was correlated with good performance.', '216': 'While we could have selected a threshold that might work reasonably well with our data, we would have little confidence that it would work well in general, considering the inconsistent pattern of results.', '217': '5.4 Semi-Supervised Feature Selection.', '218': 'Unsupervised methods such as Dash et al.â\x80\x99s (1997) are appealing because they require no knowledge external to the data.', '219': 'However, in many aspects of computational linguistics, it has been found that a small amount of labelled data contains sufficient information to allow us to go beyond the limits of completely unsupervised approaches.', '220': 'In our domain in particular, verb class discovery â\x80\x9cin a vacuumâ\x80\x9d is not necessary.', '221': 'A plausible scenario is that researchers would have examples of verbs which they believe fall into different classes of interest, and they want to separate other verbs along the same lines.', '222': 'To model this kind of approach, we selected a sample of five seed verbs from each class.', '223': 'Each set of verbs was judged (by the authorsâ\x80\x99 intuition alone) to be â\x80\x9crepresentativeâ\x80\x9d of the class.', '224': 'We purposely did not carry out any linguistic analysis, although we did check that each verb was reasonably frequent (with log frequencies ranging from 2.6 to 5.1).', '225': 'For each experimental task, we ran our supervised Table 3: Feature counts for Ling and Seed feature sets.', '226': 'learner (C5.0) on the seed verbs for those classes, in a 5-fold cross-validation (without boosting).', '227': 'We extracted from the resulting decision trees the union of all features used, which formed the reduced feature set for that task.', '228': 'Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2.', '229': 'This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder.', '230': 'Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks.', '231': 'More importantly, the Seed set shows a mean overall reduction in error rate (over Base ) of 28%, compared to 17% for the Ling set.', '232': 'The increased reduction in error rate is particularly striking for the 2-way tasks, of 37% for the Seed set compared to 20% for the Ling set.', '233': 'Another striking result is the difference in values, which are very much higher than those for Ling (which are in turn much higher than for Full).', '234': 'Thus, not only do we see a sizeable increase in performance, we also obtain tighter and better separated clusters with our proposed feature selection approach.', '235': '5.5 Further Discussion.', '236': 'In our clustering experiments, we find that smaller subsets of features generally perform better than the full set of features.', '237': '(See Table 3 for the number of features in the Ling and Seed sets.)', '238': 'However, not just any small set of features is adequate.', '239': 'We ran 50 experiments using randomly selected sets of features of cardinality , where 5We also tried directly applying the mutual information (MI) measure used in decision-tree induction (Quinlan, 1986).', '240': 'We calculated the MI of each feature with respect to the classification of the seed verbs, and computed clusterings using the features above a certain MI threshold.', '241': 'This method did not work as well as running C5.0, which presumably captures important feature interactions that are ignored in the individual MI calculations.', '242': 'is the number of classes (a simple linear function roughly approximating the number of features in the Seed sets).', '243': 'Mean over these clusterings was much lower than for the Seed sets, and was extremely low (below .08 in all cases).', '244': 'Interestingly, was generally very high, indicating that there is structure in the data, but not what matches our classification.', '245': 'This confirms that appropriate feature selection, and not just a small number of features, is important for the task of verb class discovery.', '246': 'We also find that our semi-supervised method (Seed) is linguistically plausible, and performs as well as or better than features manually determined based on linguistic knowledge (Ling).', '247': 'We might also ask, would any subset of verbs do as well?', '248': 'To answer this, we ran experiments using 50 different randomly selected seed verb sets for each class.', '249': 'We found that the mean and values are the same as that of the Seed set reported above, but mean is a little lower.', '250': 'We tentatively conclude that, yes, any subset of verbs of the appropriate class may be sufficient as a seed set, although some sets are better than others.', '251': 'This is promising for our method, as it shows that the precise selection of a seed set of verbs is not crucial to the success of the semi-supervised approach.', '252': 'Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification.', '253': 'However, their study used a small set of five features manually devised for a set of three particular classes.', '254': 'Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.', '255': 'Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.', '256': 'The scores of Schulte im Walde (2003) range from .09 to .18, while ours range from .02 to .34, with a mean of .17 across all tasks.', '257': 'However, Schulte im Waldeâ\x80\x99s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).', '258': 'Performance differences may be due to the types of features (ours are noisier, but capture information beyond subcat), or due to the number or size of classes.', '259': 'While our results generally decrease with an increase in the number of classes, indicating that our tasks in general may be â\x80\x9ceasierâ\x80\x9d than her 40-way distinction, our classes also have many more members (20 versus an average of 4) that need to be grouped together.', '260': 'It is a question for future research to explore the effect of these variables in clustering performance.', '261': 'We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.', '262': 'We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task.', '263': 'An unsupervised method we tried (Dash et al., 1997) did not prove useful, because of the problem of having no consistent threshold for feature inclusion.', '264': 'We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering.', '265': 'We showed that this feature set outperformed both the full and the manually selected sets of features on all three of our clustering evaluation metrics.', '266': 'Furthermore, the method is relatively insensitive to the precise makeup of the selected seed set.', '267': 'As successful as our seed set of features is, it still does not achieve the accuracy of a supervised learner.', '268': 'More research is needed on the definition of the general feature space, as well as on the methods for selecting a more useful set of features for clustering.', '269': 'Furthermore, we might question the clustering approach itself, in the context of verb class discovery.', '270': 'Rather than trying to separate a set of new verbs into coherent clusters, we suggest that it may be useful to perform a nearest-neighbour type of classification using a seed set, asking for each new verb â\x80\x9cis it like these or not?â\x80\x9d In some ways our current clustering task is too easy, because all of the verbs are from one of the target classes.', '271': 'In other ways, however, it is too difficult: the learner has to distinguish multiple classes, rather than focus on the important properties of a single class.', '272': 'Our next step is to explore these issues, and investigate other methods appropriate to the practical problem of grouping verbs in a new language.', '273': 'We are indebted to Allan Jepson for helpful discussions and suggestions.', '274': 'We gratefully acknowledge the financial support of NSERC of Canada and Bell University Labs.'}",['W03-0410'],['../data/summaries/W03-0410.txt'],['../data/tba/W03-0410.json']
W04-0213,The Potsdam Commentary Corpus,../data/papers/W04-0213.xml,"{'0': 'The Potsdam Commentary Corpus', '1': 'A corpus of German newspaper commentaries has been assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure.', '2': 'The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation.', '3': 'A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees.', '4': 'Two aspects of the corpus have been presented in previous papers ((Re- itter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization).', '5': 'This paper, however, provides a comprehensive overview of the data collection effort and its current state.', '6': 'At present, the â\x80\x98Potsdam Commentary Corpusâ\x80\x99 (henceforth â\x80\x98PCCâ\x80\x99 for short) consists of 170 commentaries from MaÂ¨rkische Allgemeine Zeitung, a German regional daily.', '7': 'The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the key motivations for building up the corpus.', '8': 'Commentaries argue in favor of a specific point of view toward some political issue, often dicussing yet dismissing other points of view; therefore, they typically offer a more interesting rhetorical structure than, say, narrative text or other portions of newspapers.', '9': 'The choice of the particular newspaper was motivated by the fact that the language used in a regional daily is somewhat simpler than that of papers read nationwide.', '10': '(Again, the goal of also in structural features.', '11': 'As an indication, in our core corpus, we found an average sentence length of 15.8 words and 1.8 verbs per sentence, whereas a randomly taken sample of ten commentaries from the national papers SuÂ¨ddeutsche Zeitung and Frankfurter Allgemeine has 19.6 words and 2.1 verbs per sentence.', '12': 'The commentaries in PCC are all of roughly the same length, ranging from 8 to 10 sentences.', '13': 'For illustration, an English translation of one of the commentaries is given in Figure 1.', '14': 'The paper is organized as follows: Section 2 explains the different layers of annotation that have been produced or are being produced.', '15': 'Section 3 discusses the applications that have been completed with PCC, or are under way, or are planned for the future.', '16': 'Section 4 draws some conclusions from the present state of the effort.', '17': 'The corpus has been annotated with six different types of information, which are characterized in the following subsections.', '18': 'Not all the layers have been produced for all the texts yet.', '19': 'There is a â\x80\x98core corpusâ\x80\x99 of ten commentaries, for which the range of information (except for syntax) has been completed; the remaining data has been annotated to different degrees, as explained below.', '20': 'All annotations are done with specific tools and in XML; each layer has its own DTD.', '21': 'This offers the well-known advantages for inter- changability, but it raises the question of how to query the corpus across levels of annotation.', '22': 'We will briefly discuss this point in Section 3.1.', '23': '2.1 Part-of-speech tags.', '24': 'All commentaries have been tagged with part-of-speech information using Brantsâ\x80\x99 TnT1 tagger and the Stuttgart/TuÂ¨bingen Tag Set automatic analysis was responsible for this decision.)', '25': 'This is manifest in the lexical choices but 1 www.coli.unisb.de/â\x88¼thorsten/tnt/ Dagmar Ziegler is up to her neck in debt.', '26': 'Due to the dramatic fiscal situation in Brandenburg she now surprisingly withdrew legislation drafted more than a year ago, and suggested to decide on it not before 2003.', '27': 'Unexpectedly, because the ministries of treasury and education both had prepared the teacher plan together.', '28': 'This withdrawal by the treasury secretary is understandable, though.', '29': 'It is difficult to motivate these days why one ministry should be exempt from cutbacks â\x80\x94 at the expense of the others.', '30': 'Reicheâ\x80\x99s colleagues will make sure that the concept is waterproof.', '31': 'Indeed there are several open issues.', '32': 'For one thing, it is not clear who is to receive settlements or what should happen in case not enough teachers accept the offer of early retirement.', '33': 'Nonetheless there is no alternative to Reicheâ\x80\x99s plan.', '34': 'The state in future has not enough work for its many teachers.', '35': 'And time is short.', '36': 'The significant drop in number of pupils will begin in the fall of 2003.', '37': 'The government has to make a decision, and do it quickly.', '38': 'Either save money at any cost - or give priority to education.', '39': 'Figure 1: Translation of PCC sample commentary (STTS)2.', '40': '2.2 Syntactic structure.', '41': 'Annotation of syntactic structure for the core corpus has just begun.', '42': 'We follow the guidelines developed in the TIGER project (Brants et al. 2002) for syntactic annotation of German newspaper text, using the Annotate3 tool for interactive construction of tree structures.', '43': '2.3 Rhetorical structure.', '44': 'All commentaries have been annotated with rhetorical structure, using RSTTool4 and the definitions of discourse relations provided by Rhetorical Structure Theory (Mann, Thompson 1988).', '45': 'Two annotators received training with the RST definitions and started the process with a first set of 10 texts, the results of which were intensively discussed and revised.', '46': 'Then, the remaining texts were annotated and cross-validated, always with discussions among the annotators.', '47': 'Thus we opted not to take the step of creating more precise written annotation guidelines (as (Carlson, Marcu 2001) did for English), which would then allow for measuring inter-annotator agreement.', '48': 'The motivation for our more informal approach was the intuition that there are so many open problems in rhetorical analysis (and more so for German than for English; see below) that the main task is qualitative investigation, whereas rigorous quantitative analyses should be performed at a later stage.', '49': 'One conclusion drawn from this annotation effort was that for humans and machines alike, 2 www.sfs.nphil.unituebingen.de/Elwis/stts/ stts.html 3 www.coli.unisb.de/sfb378/negra-corpus/annotate.', '50': 'html 4 www.wagsoft.com/RSTTool assigning rhetorical relations is a process loaded with ambiguity and, possibly, subjectivity.', '51': 'We respond to this on the one hand with a format for its underspecification (see 2.4) and on the other hand with an additional level of annotation that attends only to connectives and their scopes (see 2.5), which is intended as an intermediate step on the long road towards a systematic and objective treatment of rhetorical structure.', '52': '2.4 Underspecified rhetorical structure.', '53': 'While RST (Mann, Thompson 1988) proposed that a single relation hold between adjacent text segments, SDRT (Asher, Lascarides 2003) maintains that multiple relations may hold simultaneously.', '54': 'Within the RST â\x80\x9cuser communityâ\x80\x9d there has also been discussion whether two levels of discourse structure should not be systematically distinguished (intentional versus informational).', '55': 'Some relations are signalled by subordinating conjunctions, which clearly demarcate the range of the text spans related (matrix clause, embedded clause).', '56': 'When the signal is a coordinating conjunction, the second span is usually the clause following the conjunction; the first span is often the clause preceding it, but sometimes stretches further back.', '57': 'When the connective is an adverbial, there is much less clarity as to the range of the spans.', '58': 'Assigning rhetorical relations thus poses questions that can often be answered only subjectively.', '59': 'Our annotators pointed out that very often they made almost random decisions as to what relation to choose, and where to locate the boundary of a span.', '60': '(Carlson, Marcu 2001) responded to this situation with relatively precise (and therefore long!)', '61': 'annotation guidelines that tell annotators what to do in case of doubt.', '62': 'Quite often, though, these directives fulfill the goal of increasing annotator agreement without in fact settling the theoretical question; i.e., the directives are clear but not always very well motivated.', '63': 'In (Reitter, Stede 2003) we went a different way and suggested URML5, an XML format for underspecifying rhetorical structure: a number of relations can be assigned instead of a single one, competing analyses can be represented with shared forests.', '64': 'The rhetorical structure annotations of PCC have all been converted to URML.', '65': 'There are still some open issues to be resolved with the format, but it represents a first step.', '66': 'What ought to be developed now is an annotation tool that can make use of the format, allow for underspecified annotations and visualize them accordingly.', '67': '2.5 Connectives with scopes.', '68': 'For the â\x80\x98coreâ\x80\x99 portion of PCC, we found that on average, 35% of the coherence relations in our RST annotations are explicitly signalled by a lexical connective.6 When adding the fact that connectives are often ambiguous, one has to conclude that prospects for an automatic analysis of rhetorical structure using shallow methods (i.e., relying largely on connectives) are not bright â\x80\x94 but see Sections 3.2 and 3.3 below.', '69': 'Still, for both human and automatic rhetorical analysis, connectives are the most important source of surface information.', '70': 'We thus decided to pay specific attention to them and introduce an annotation layer for connectives and their scopes.', '71': 'This was also inspired by the work on the Penn Discourse Tree Bank7 , which follows similar goals for English.', '72': 'For effectively annotating connectives/scopes, we found that existing annotation tools were not well-suited, for two reasons: â\x80¢ Some tools are dedicated to modes of annotation (e.g., tiers), which could only quite un-intuitively be used for connectives and scopes.', '73': 'â\x80¢ Some tools would allow for the desired annotation mode, but are so complicated (they can be used for many other purposes as well) that annotators take a long time getting used to them.', '74': '5 â\x80\x98Underspecified Rhetorical Markup Languageâ\x80\x99 6 This confirms the figure given by (Schauer, Hahn.', '75': 'Consequently, we implemented our own annotation tool ConAno in Java (Stede, Heintze 2004), which provides specifically the functionality needed for our purpose.', '76': 'It reads a file with a list of German connectives, and when a text is opened for annotation, it highlights all the words that show up in this list; these will be all the potential connectives.', '77': 'The annotator can then â\x80\x9cclick awayâ\x80\x9d those words that are here not used as connectives (such as the conjunction und (â\x80\x98andâ\x80\x99) used in lists, or many adverbials that are ambiguous between connective and discourse particle).', '78': 'Then, moving from connective to connective, ConAno sometimes offers suggestions for its scope (using heuristics like â\x80\x98for sub- junctor, mark all words up to the next comma as the first segmentâ\x80\x99), which the annotator can accept with a mouseclick or overwrite, marking instead the correct scope with the mouse.', '79': 'When finished, the whole material is written into an XML-structured annotation file.', '80': '2.6 Co-reference.', '81': 'We developed a first version of annotation guidelines for co-reference in PCC (Gross 2003), which served as basis for annotating the core corpus but have not been empirically evaluated for inter-annotator agreement yet.', '82': 'The tool we use is MMAX8, which has been specifically designed for marking co-reference.', '83': 'Upon identifying an anaphoric expression (currently restricted to: pronouns, prepositional adverbs, definite noun phrases), the an- notator first marks the antecedent expression (currently restricted to: various kinds of noun phrases, prepositional phrases, verb phrases, sentences) and then establishes the link between the two.', '84': 'Links can be of two different kinds: anaphoric or bridging (definite noun phrases picking up an antecedent via world-knowledge).', '85': 'â\x80¢ Anaphoric links: the annotator is asked to specify whether the anaphor is a repetition, partial repetition, pronoun, epithet (e.g., Andy Warhol â\x80\x93 the PopArt artist), or is-a (e.g., Andy Warhol was often hunted by photographers.', '86': 'This fact annoyed especially his dog...).', '87': 'â\x80¢ Bridging links: the annotator is asked to specify the type as part-whole, cause-effect (e.g., She had an accident.', '88': 'The wounds are still healing.), entity-attribute (e.g., She 2001), who determined that in their corpus of German computer tests, 38% of relations were lexically signalled.', '89': '7 www.cis.upenn.edu/â\x88¼pdtb/ 8 www.eml-research.de/english/Research/NLP/ Downloads had to buy a new car.', '90': 'The price shocked her.), or same-kind (e.g., Her health insurance paid for the hospital fees, but the automobile insurance did not cover the repair.).', '91': 'For displaying and querying the annoated text, we make use of the Annis Linguistic Database developed in our group for a large research effort (â\x80\x98Sonderforschungsbereichâ\x80\x99) revolving around 9 2.7 Information structure.', '92': 'information structure.', '93': 'The implementation is In a similar effort, (GÂ¨otze 2003) developed a proposal for the theory-neutral annotation of information structure (IS) â\x80\x94 a notoriously difficult area with plenty of conflicting and overlapping terminological conceptions.', '94': 'And indeed, converging on annotation guidelines is even more difficult than it is with co-reference.', '95': 'Like in the co-reference annotation, GÂ¨otzeâ\x80\x99s proposal has been applied by two annotators to the core corpus but it has not been systematically evaluated yet.', '96': 'We use MMAX for this annotation as well.', '97': 'Here, annotation proceeds in two phases: first, the domains and the units of IS are marked as such.', '98': 'The domains are the linguistic spans that are to receive an IS-partitioning, and the units are the (smaller) spans that can play a role as a constituent of such a partitioning.', '99': 'Among the IS-units, the referring expressions are marked as such and will in the second phase receive a label for cognitive status (active, accessible- text, accessible-situation, inferrable, inactive).', '100': 'They are also labelled for their topicality (yes / no), and this annotation is accompanied by a confidence value assigned by the annotator (since it is a more subjective matter).', '101': 'Finally, the focus/background partition is annotated, together with the focus question that elicits the corresponding answer.', '102': 'Asking the annotator to also formulate the question is a way of arriving at more reproducible decisions.', '103': 'For all these annotation taks, GÂ¨otze developed a series of questions (essentially a decision tree) designed to lead the annotator to the ap propriate judgement.', '104': 'Having explained the various layers of annotation in PCC, we now turn to the question what all this might be good for.', '105': 'This concerns on the one hand the basic question of retrieval, i.e. searching for information across the annotation layers (see 3.1).', '106': 'On the other hand, we are interested in the application of rhetorical analysis or â\x80\x98discourse parsingâ\x80\x99 (3.2 and 3.3), in text generation (3.4), and in exploiting the corpus for the development of improved models of discourse structure (3.5).', '107': 'basically complete, yet some improvements and extensions are still under way.', '108': 'The web-based Annis imports data in a variety of XML formats and tagsets and displays it in a tier-orientedway (optionally, trees can be drawn more ele gantly in a separate window).', '109': 'Figure 2 shows a screenshot (which is of somewhat limited value, though, as color plays a major role in signalling the different statuses of the information).', '110': 'In the small window on the left, search queries can be entered, here one for an NP that has been annotated on the co-reference layer as bridging.', '111': 'The portions of information in the large window can be individually clicked visible or invisible; here we have chosen to see (from top to bottom) â\x80¢ the full text, â\x80¢ the annotation values for the activated annotation set (co-reference), â\x80¢ the actual annotation tiers, and â\x80¢ the portion of text currently â\x80\x98in focusâ\x80\x99 (which also appears underlined in the full text).', '112': 'Different annotations of the same text are mapped into the same data structure, so that search queries can be formulated across annotation levels.', '113': 'Thus it is possible, for illustration, to look for a noun phrase (syntax tier) marked as topic (information structure tier) that is in a bridging relation (co-reference tier) to some other noun phrase.', '114': '3.2 Stochastic rhetorical analysis.', '115': 'In an experiment on automatic rhetorical parsing, the RST-annotations and PoS tags were used by (Reitter 2003) as a training corpus for statistical classification with Support Vector Machines.', '116': 'Since 170 annotated texts constitute a fairly small training set, Reitter found that an overall recognition accuracy of 39% could be achieved using his method.', '117': 'For the English RST-annotated corpus that is made available via LDC, his corresponding result is 62%.', '118': 'Future work along these lines will incorporate other layers of annotation, in particular the syntax information.', '119': '9 www.ling.unipotsdam.de/sfb/ Figure 2: Screenshot of Annis Linguistic Database 3.3 Symbolic and knowledge-based.', '120': 'rhetorical analysis We are experimenting with a hybrid statistical and knowledge-based system for discourse parsing and summarization (Stede 2003), (Hanneforth et al. 2003), again targeting the genre of commentaries.', '121': 'The idea is to have a pipeline of shallow-analysis modules (tagging, chunk- ing, discourse parsing based on connectives) and map the resulting underspecified rhetorical tree (see Section 2.4) into a knowledge base that may contain domain and world knowledge for enriching the representation, e.g., to resolve references that cannot be handled by shallow methods, or to hypothesize coherence relations.', '122': 'In the rhetorical tree, nuclearity information is then used to extract a â\x80\x9ckernel treeâ\x80\x9d that supposedly represents the key information from which the summary can be generated (which in turn may involve co-reference information, as we want to avoid dangling pronouns in a summary).', '123': 'Thus we are interested not in extraction, but actual generation from representations that may be developed to different degrees of granularity.', '124': 'In order to evaluate and advance this approach, it helps to feed into the knowledge base data that is already enriched with some of the desired information â\x80\x94 as in PCC.', '125': 'That is, we can use the discourse parser on PCC texts, emulating for instance a â\x80\x9cco-reference oracleâ\x80\x9d that adds the information from our co-reference annotations.', '126': 'The knowledge base then can be tested for its relation-inference capabilities on the basis of full-blown co-reference information.', '127': 'Conversely, we can use the full rhetorical tree from the annotations and tune the co-reference module.', '128': 'The general idea for the knowledge- based part is to have the system use as much information as it can find at its disposal to produce a target representation as specific as possible and as underspecified as necessary.', '129': 'For developing these mechanisms, the possibility to feed in hand-annotated information is very useful.', '130': '3.4 Salience-based text generation.', '131': 'Text generation, or at least the two phases of text planning and sentence planning, is a process driven partly by well-motivated choices (e.g., use this lexeme X rather than that more colloquial near-synonym Y ) and partly by con tation like that of PCC can be exploited to look for correlations in particular between syntactic structure, choice of referring expressions, and sentence-internal information structure.', '132': 'A different but supplementary perspective on discourse-based information structure is taken 11ventionalized patterns (e.g., order of informa by one of our partner projects, which is inter tion in news reports).', '133': 'And then there are decisions that systems typically hard-wire, because the linguistic motivation for making them is not well understood yet.', '134': 'Preferences for constituent order (especially in languages with relatively free word order) often belong to this group.', '135': 'Trying to integrate constituent ordering and choice of referring expressions, (Chiarcos 2003) developed a numerical model of salience propagation that captures various factors of authorâ\x80\x99s intentions and of information structure for ordering sentences as well as smaller constituents, and picking appropriate referring expressions.10 Chiarcos used the PCC annotations of co-reference and information structure to compute his numerical models for salience projection across the generated texts.', '136': '3.5 Improved models of discourse.', '137': 'structure Besides the applications just sketched, the over- arching goal of developing the PCC is to build up an empirical basis for investigating phenomena of discourse structure.', '138': 'One key issue here is to seek a discourse-based model of information structure.', '139': 'Since DaneË\x87sâ\x80\x99 proposals of â\x80\x98thematic development patternsâ\x80\x99, a few suggestions have been made as to the existence of a level of discourse structure that would predict the information structure of sentences within texts.', '140': '(Hartmann 1984), for example, used the term Reliefgebung to characterize the distibution of main and minor information in texts (similar to the notion of nuclearity in RST).', '141': '(Brandt 1996) extended these ideas toward a conception of kommunikative Gewichtung (â\x80\x98communicative-weight assignmentâ\x80\x99).', '142': 'A different notion of information structure, is used in work such as that of (?), who tried to characterize felicitous constituent ordering (theme choice, in particular) that leads to texts presenting information in a natural, â\x80\x9cflowingâ\x80\x9d way rather than with abrupt shifts of attention.', '143': 'â\x80\x94ested in correlations between prosody and dis course structure.', '144': 'A number of PCC commentaries will be read by professional news speakers and prosodic features be annotated, so that the various annotation layers can be set into correspondence with intonation patterns.', '145': 'In focus is in particular the correlation with rhetorical structure, i.e., the question whether specific rhetorical relations â\x80\x94 or groups of relations in particular configurations â\x80\x94 are signalled by speakers with prosodic means.', '146': 'Besides information structure, the second main goal is to enhance current models of rhetorical structure.', '147': 'As already pointed out in Section 2.4, current theories diverge not only on the number and definition of relations but also on apects of structure, i.e., whether a tree is sufficient as a representational device or general graphs are required (and if so, whether any restrictions can be placed on these graphâ\x80\x99s structures â\x80\x94 cf.', '148': '(Webber et al., 2003)).', '149': 'Again, the idea is that having a picture of syntax, co-reference, and sentence-internal information structure at oneâ\x80\x99s disposal should aid in finding models of discourse structure that are more explanatory and can be empirically supported.', '150': 'The PCC is not the result of a funded project.', '151': 'Instead, the designs of the various annotation layers and the actual annotation work are results of a series of diploma theses, of studentsâ\x80\x99 work in course projects, and to some extent of paid assistentships.', '152': 'This means that the PCC cannot grow particularly quickly.', '153': 'After the first step towards breadth had been taken with the PoS-tagging, RST annotation, and URML conversion of the entire corpus of 170 texts12 , emphasis shifted towards depth.', '154': 'Hence we decided to select ten commentaries to form a â\x80\x98core corpusâ\x80\x99, for which the entire range of annotation levels was realized, so that experiments with multi-level querying could commence.', '155': 'Cur In order to ground such approaches in linguistic observation and description, a multi-level anno 10 For an exposition of the idea as applied to the task of text planning, see (Chiarcos, Stede 2004).', '156': '11 www.ling.unipotsdam.de/sfb/projekt a3.php 12 This step was carried out in the course of the diploma thesis work of David Reitter (2003), which de serves special mention here.', '157': 'rently, some annotations (in particular the connectives and scopes) have already moved beyond the core corpus; the others will grow step by step.', '158': 'The kind of annotation work presented here would clearly benefit from the emergence of standard formats and tag sets, which could lead to sharable resources of larger size.', '159': 'Clearly this poses a number of research challenges, though, such as the applicability of tag sets across different languages.', '160': 'Nonetheless, the prospect of a network of annotated discourse resources seems particularly promising if not only a single annotation layer is used but a whole variety of them, so that a systematic search for correlations between them becomes possible, which in turn can lead to more explanatory models of discourse structure.'}",['W04-0213'],['../data/summaries/W04-0213.txt'],['../data/tba/W04-0213.json']
W06-2932,Multilingual Dependency Analysis with a Two-Stage Discriminative Parser,../data/papers/W06-2932.xml,"{'0': 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', '1': 'present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', '2': 'The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.', '3': 'The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.', '4': 'We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis.', '5': 'Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.', '6': 'With the availability of resources such as the Penn WSJ Treebank, much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure.', '7': ""However, recently their has been a revived interest in parsing models that produce dependency graph representations of sentences, which model words and their arguments through directed edges (Hudson, 1984; Mel'ˇcuk, 1988)."", '8': 'This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages.', '9': 'Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies.', '10': 'Dependency graphs also encode much of the deep syntactic information needed for further processing.', '11': 'This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005).', '12': 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', '13': 'We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al., 2006; Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and MartiAntonin, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003).', '14': 'The results are promising and show the language independence of our system under the assumption of a labeled dependency corpus in the target language.', '15': 'For the remainder of this paper, we denote by x = x1,... xn a sentence with n words and by y a corresponding dependency graph.', '16': 'A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.', '17': 'Each edge can be assigned a label l(ij) from a finite set L of predefined labels.', '18': 'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pages 216–220, New York City, June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use.', '19': 'The first stage of our system creates an unlabeled parse y for an input sentence x.', '20': 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', '21': 'That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.', '22': 'An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.', '23': 'That system uses MIRA, an online large-margin learning algorithm, to compute model parameters.', '24': 'Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.', '25': 'For instance, the system of McDonald et al. (2005a) incorporates features over the part of speech of words occurring between and around a possible head-dependent relation.', '26': 'These features are highly important to overall accuracy since they eliminate unlikely scenarios such as a preposition modifying a noun not directly to its left, or a noun modifying a verb with another verb occurring between them.', '27': 'We augmented this model to incorporate morphological features derived from each token.', '28': 'Consider a proposed dependency of a dependent xj on the head xi, each with morphological features Mj and Mi respectively.', '29': 'We then add to the representation of the edge: Mi as head features, Mj as dependent features, and also each conjunction of a feature from both sets.', '30': 'These features play the obvious role of explicitly modeling consistencies and commonalities between a head and its dependents in terms of attributes like gender, case, or number.', '31': 'Not all data sets in our experiments include morphological features, so we use them only when available.', '32': 'The second stage takes the output parse y for sentence x and classifies each edge (i, j) E y with a particular label l(i,j).', '33': 'Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.', '34': 'However, the parser is fundamentally limited by the scope of local factorizations that make inference tractable.', '35': 'In our case this means we are forced only to consider features over single edges or pairs of edges.', '36': 'However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', '37': 'The simplest labeler would be to take as input an edge (i, j) E y for sentence x and find the label with highest score, Doing this for each edge in the tree would produce the final output.', '38': 'Such a model could easily be trained using the provided training data for each language.', '39': 'However, it might be advantageous to know the labels of other nearby edges.', '40': 'For instance, if we consider a head xi with dependents xj1, ... , xjM, it is often the case that many of these dependencies will have correlated labels.', '41': 'To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.', '42': 'We attempted higher-order Markov factorizations but they did not improve performance uniformly across languages and training became significantly slower.', '43': 'For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi’s algorithm.', '44': 'We use the MIRA online learner to set the weights (Crammer and Singer, 2003; McDonald et al., 2005a) since we found it trained quickly and provide good performance.', '45': 'Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).', '46': 'Of course, we have to define a set of suitable features.', '47': 'We used the following: dependent have identical values?', '48': 'Is this the left/rightmost dependent for the head?', '49': 'Is this the first dependent to the left/right of the head?', '50': 'Various conjunctions of these were included based on performance on held-out data.', '51': 'Note that many of these features are beyond the scope of the edge based factorizations of the unlabeled parser.', '52': 'Thus a joint model of parsing and labeling could not easily include them without some form of re-ranking or approximate parameter estimation.', '53': 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al., 2006).', '54': 'Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.', '55': 'Furthermore, for Arabic and Spanish, we used lemmas instead of inflected word forms, again based on performance on held-out data1.', '56': 'Results on the test set are given in Table 1.', '57': 'Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.', '58': 'These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.', '59': 'Only Arabic, Turkish and Slovene have parsing accuracies significantly below 80%, and these languages have relatively small training sets and/or are highly inflected with little to no word order constraints.', '60': 'Furthermore, these results show that a twostage system can achieve a relatively high performance.', '61': 'In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).', '62': 'For the remainder of the paper we provide a general error analysis across a wide set of languages plus a detailed error analysis of Spanish and Arabic.', '63': 'Our system has several components, including the ability to produce non-projective edges, sequential Japanese, Portuguese, Slovene, Spanish, Swedish and Turkish.', '64': 'N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.', '65': 'The benefit of each of these is shown in Table 2.', '66': 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', '67': 'This allowed us to train new models quickly.', '68': 'Table 2 shows that each component of our system does not change performance significantly (rows 24 versus row 1).', '69': 'However, if we only allow projective parses, do not use morphological features and label edges with a simple atomic classifier, the overall drop in performance becomes significant (row 5 versus row 1).', '70': 'Allowing non-projective parses helped with freer word order languages like Dutch (78.8%/74.7% to 83.6%/79.2%, unlabeled/labeled accuracy).', '71': 'Including rich morphology features naturally helped with highly inflected languages, in particular Spanish, Arabic, Turkish, Slovene and to a lesser extent Dutch and Portuguese.', '72': 'Derived morphological features improved accuracy in all these languages by 1-3% absolute.', '73': 'Sequential classification of labels had very little effect on overall labeled accuracy (79.4% to 79.7%)2.', '74': 'The major contribution was in helping to distinguish subjects, objects and other dependents of main verbs, which is the most common labeling error.', '75': 'This is not surprising since these edge labels typically are the most correlated (i.e., if you already know which noun dependent is the subject, then it should be easy to find the object).', '76': 'For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.', '77': 'Similar improvements are common across all languages, though not as dramatic.', '78': 'Even with this improvement, the labeling of verb dependents remains the highest source of error.', '79': 'Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.', '80': 'These words form 17% of the test corpus.', '81': 'Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%), adverbs (82%) and subordinating conjunctions (80%), for a total of another 23% of the test corpus.', '82': 'These weaknesses are not surprising, since these decisions encode the more global aspects of sentence structure: arrangement of clauses and adverbial dependents in multi-clause sentences, and prepositional phrase attachment.', '83': 'In a preliminary test of this hypothesis, we looked at all of the sentences from a development set in which a main verb is incorrectly attached.', '84': 'We confirmed that the main clause is often misidentified in multi-clause sentences, or that one of several conjoined clauses is incorrectly taken as the main clause.', '85': 'To test this further, we added features to count the number of commas and conjunctions between a dependent verb and its candidate head.', '86': 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', '87': 'Unfortunately, accuracy for other word types decreases somewhat, resulting in no significant net accuracy change.', '88': 'Nevertheless, this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', '89': 'Another common verb attachment error is a switch between head and dependent verb in phrasal verb forms like dejan intrigar or qiero decir, possibly because the non-finite verb in these cases is often a main verb in training sentences.', '90': 'We need to look more carefully at verb features that may be useful here, in particular features that distinguish finite and non-finite forms.', '91': 'In doing this preliminary analysis, we noticed some inconsistencies in the reference dependency structures.', '92': 'For example, in the test sentence Lo que decia Mae West de si misma podriamos decirlo tambi´en los hombres:..., decia’s head is given as decirlo, although the main verbs of relative clauses are normally dependent on what the relative modifies, in this case the article Lo.', '93': 'A quick look at unlabeled attachment accuracies indicate that errors in Arabic parsing are the most common across all languages: prepositions (62%), conjunctions (69%) and to a lesser extent verbs (73%).', '94': 'Similarly, for labeled accuracy, the hardest edges to label are for dependents of verbs, i.e., subjects, objects and adverbials.', '95': 'Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions, conjunctions and verbs, and the latter makes mistakes on edges into nouns (subject/objects).', '96': 'Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%), but since there is very little overlap in the kinds of errors each makes, overall labeled accuracy drops to 67%.', '97': 'This drop is not nearly as significant for other languages.', '98': 'Another source of potential error is that the average sentence length of Arabic is much higher than other languages (around 37 words/sentence).', '99': 'However, if we only look at performance for sentences of length less than 30, the labeled accuracy is still only 71%.', '100': 'The fact that Arabic has only 1500 training instances might also be problematic.', '101': 'For example if we train on 200, 400, 800 and the full training set, labeled accuracies are 54%, 60%, 62% and 67%.', '102': 'Clearly adding more data is improving performance.', '103': 'However, when compared to the performance of Slovene (1500 training instances) and Spanish (3300 instances), it appears that Arabic parsing is lagging.', '104': 'We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.', '105': 'In the future we plan to extend these models in two ways.', '106': 'First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.', '107': 'It is our hypothesis that for languages with fine-grained label sets, joint parsing and labeling will improve performance.', '108': 'Second, we plan on integrating any available morphological features in a more principled manner.', '109': 'The current system simply includes all morphological bi-gram features.', '110': 'It is our hope that a better morphological feature set will help with both unlabeled parsing and labeling for highly inflected languages.'}","['W06-2932_swastika', 'W06-2932_sweta', 'W06-2932_vardha']","['../data/summaries/W06-2932_swastika.txt', '../data/summaries/W06-2932_sweta.txt', '../data/summaries/W06-2932_vardha.txt']","['../data/tba/W06-2932_swastika.json', '../data/tba/W06-2932_sweta.json', '../data/tba/W06-2932_vardha.json']"
W06-3114,Manual and Automatic Evaluation of Machine Translation between European Languages,../data/papers/W06-3114.xml,"{'0': 'Manual and Automatic Evaluation of Machine Translation between European Languages', '1': 'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', '2': '0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt', '3': 'was done by the participants.', '4': 'This revealed interesting clues about the properties of automatic and manual scoring.', '5': '• We evaluated translation from English, in addition to into English.', '6': 'English was again paired with German, French, and Spanish.', '7': 'We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.', '8': 'The evaluation framework for the shared task is similar to the one used in last year’s shared task.', '9': 'Training and testing is based on the Europarl corpus.', '10': 'Figure 1 provides some statistics about this corpus.', '11': 'To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.', '12': 'To summarize, we provided: The performance of the baseline system is similar to the best submissions in last year’s shared task.', '13': 'We are currently working on a complete open source implementation of a training and decoding system, which should become available over the summer. pus, from which also the in-domain test set is taken.', '14': 'There is twice as much language modelling data, since training data for the machine translation system is filtered against sentences of length larger than 40 words.', '15': 'Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.', '16': 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.', '17': 'Participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.', '18': 'In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.', '19': 'We aligned the texts at a sentence level across all four languages, resulting in 1064 sentence per language.', '20': 'For statistics on this test set, refer to Figure 1.', '21': 'The out-of-domain test set differs from the Europarl data in various ways.', '22': 'The text type are editorials instead of speech transcripts.', '23': 'The domain is general politics, economics and science.', '24': 'However, it is also mostly political content (even if not focused on the internal workings of the European Union) and opinion.', '25': 'We received submissions from 14 groups from 11 institutions, as listed in Figure 2.', '26': 'Most of these groups follow a phrase-based statistical approach to machine translation.', '27': 'Microsoft’s approach uses dependency trees, others use hierarchical phrase models.', '28': 'Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.', '29': 'About half of the participants of last year’s shared task participated again.', '30': 'The other half was replaced by other participants, so we ended up with roughly the same number.', '31': 'Compared to last year’s shared task, the participants represent more long-term research efforts.', '32': 'This may be the sign of a maturing research environment.', '33': 'While building a machine translation system is a serious undertaking, in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.', '34': 'For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.', '35': 'For the automatic evaluation, we used BLEU, since it is the most established metric in the field.', '36': 'The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.', '37': 'It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence.', '38': 'The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).', '39': 'However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.', '40': 'They demonstrated this with the comparison of statistical systems against (a) manually post-edited MT output, and (b) a rule-based commercial system.', '41': 'The development of automatic scoring methods is an open field of research.', '42': 'It was our hope that this competition, which included the manual and automatic evaluation of statistical systems and one rulebased commercial system, will give further insight into the relation between automatic and manual evaluation.', '43': 'At the very least, we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.', '44': 'We computed BLEU scores for each submission with a single reference translation.', '45': 'For each sentence, we counted how many n-grams in the system output also occurred in the reference translation.', '46': 'By taking the ratio of matching n-grams to the total number of n-grams in the system output, we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output, which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.', '47': 'Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.', '48': 'Confidence Interval: Since BLEU scores are not computed on the sentence level, traditional methods to compute statistical significance and confidence intervals do not apply.', '49': 'Hence, we use the bootstrap resampling method described by Koehn (2004).', '50': 'Following this method, we repeatedly — say, 1000 times — sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.', '51': 'When dropping the top and bottom 2.5% the remaining BLEU scores define the range of the confidence interval.', '52': 'Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.', '53': 'If two systems’ scores are close, this may simply be a random effect in the test data.', '54': 'To check for this, we do pairwise bootstrap resampling: Again, we repeatedly sample sets of sentences, this time from both systems, and compare their BLEU scores on these sets.', '55': 'If one system is better in 95% of the sample sets, we conclude that its higher BLEU score is statistically significantly better.', '56': 'The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005), as being too optimistic in deciding for statistical significant difference between systems.', '57': 'We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation.', '58': 'We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.', '59': 'The sign test checks, how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.', '60': 'Let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse?', '61': 'We check, how likely only up to k = 20 better scores out of n = 100 would have been generated by two equal systems, using the binomial distribution: If p(0..k; n, p) < 0.05, or p(0..k; n, p) > 0.95 then we have a statistically significant difference between the systems.', '62': 'While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.', '63': 'Many human evaluation metrics have been proposed.', '64': 'Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.', '65': 'The main disadvantage of manual evaluation is that it is time-consuming and thus too expensive to do frequently.', '66': 'In this shared task, we were also confronted with this problem, and since we had no funding for paying human judgements, we asked participants in the evaluation to share the burden.', '67': 'Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.', '68': 'We asked participants to each judge 200–300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.', '69': 'We settled on contrastive evaluations of 5 system outputs for a single test sentence.', '70': 'See Figure 3 for a screenshot of the evaluation tool.', '71': 'Presenting the output of several system allows the human judge to make more informed judgements, contrasting the quality of the different systems.', '72': 'The judgements tend to be done more in form of a ranking of the different systems.', '73': 'We assumed that such a contrastive assessment would be beneficial for an evaluation that essentially pits different systems against each other.', '74': 'While we had up to 11 submissions for a translation direction, we did decide against presenting all 11 system outputs to the human judge.', '75': 'Our initial experimentation with the evaluation tool showed that this is often too overwhelming.', '76': 'Making the ten judgements (2 types for 5 systems) takes on average 2 minutes.', '77': 'Typically, judges initially spent about 3 minutes per sentence, but then accelerate with experience.', '78': 'Judges where excluded from assessing the quality of MT systems that were submitted by their institution.', '79': 'Sentences and systems were randomly selected and randomly shuffled for presentation.', '80': 'We collected around 300–400 judgements per judgement type (adequacy or fluency), per system, per language pair.', '81': 'This is less than the 694 judgements 2004 DARPA/NIST evaluation, or the 532 judgements in the 2005 DARPA/NIST evaluation.', '82': 'This decreases the statistical significance of our results compared to those studies.', '83': 'The number of judgements is additionally fragmented by our breakup of sentences into in-domain and out-of-domain.', '84': 'The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:', '85': 'Judges varied in the average score they handed out.', '86': 'The average fluency judgement per judge ranged from 2.33 to 3.67, the average adequacy judgement ranged from 2.56 to 4.13.', '87': 'Since different judges judged different systems (recall that judges were excluded to judge system output from their own institution), we normalized the scores.', '88': 'The normalized judgement per judge is the raw judgement plus (3 minus average raw judgement for this judge).', '89': 'In words, the judgements are normalized, so that the average normalized judgement per judge is 3.', '90': 'Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.', '91': 'In fact, it is very difficult to maintain consistent standards, on what (say) an adequacy judgement of 3 means even for a specific language pair.', '92': 'The way judgements are collected, human judges tend to use the scores to rank systems against each other.', '93': 'If one system is perfect, another has slight flaws and the third more flaws, a judge is inclined to hand out judgements of 5, 4, and 3.', '94': 'On the other hand, when all systems produce muddled output, but one is better, and one is worse, but not completely wrong, a judge is inclined to hand out judgements of 4, 3, and 2.', '95': 'The judgement of 4 in the first case will go to a vastly better system output than in the second case.', '96': 'We therefore also normalized judgements on a per-sentence basis.', '97': 'The normalized judgement per sentence is the raw judgement plus (0 minus average raw judgement for this judge on this sentence).', '98': 'Systems that generally do better than others will receive a positive average normalizedjudgement per sentence.', '99': 'Systems that generally do worse than others will receive a negative one.', '100': 'One may argue with these efforts on normalization, and ultimately their value should be assessed by assessing their impact on inter-annotator agreement.', '101': 'Given the limited number of judgements we received, we did not try to evaluate this.', '102': 'Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.', '103': 'Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', '104': 'Unfortunately, we have much less data to work with than with the automatic scores.', '105': 'The way we cant distinction between system performance.', '106': 'Automatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300–400 sentences). collected manual judgements, we do not necessarily have the same sentence judged for both systems (judges evaluate 5 systems out of the 8–10 participating systems).', '107': 'Still, for about good number of sentences, we do have this direct comparison, which allows us to apply the sign test, as described in Section 2.2.', '108': 'The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.', '109': 'The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks), and then in graphical form in Figures 11–16.', '110': 'In the graphs, system scores are indicated by a point, the confidence intervals by shaded areas around the point.', '111': 'In all figures, we present the per-sentence normalized judgements.', '112': 'The normalization on a per-judge basis gave very similar ranking, only slightly less consistent with the ranking from the pairwise comparisons.', '113': 'The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.', '114': 'Pairwise comparison is done using the sign test.', '115': 'Often, two systems can not be distinguished with a confidence of over 95%, so there are ranked the same.', '116': 'This actually happens quite frequently (more below), so that the rankings are broad estimates.', '117': 'For instance: if 10 systems participate, and one system does better than 3 others, worse then 2, and is not significant different from the remaining 4, its rank is in the interval 3–7.', '118': 'At first glance, we quickly recognize that many systems are scored very similar, both in terms of manual judgement and BLEU.', '119': 'There may be occasionally a system clearly at the top or at the bottom, but most systems are so close that it is hard to distinguish them.', '120': 'In Figure 4, we displayed the number of system comparisons, for which we concluded statistical significance.', '121': 'For the automatic scoring method BLEU, we can distinguish three quarters of the systems.', '122': 'While the Bootstrap method is slightly more sensitive, it is very much in line with the sign test on text blocks.', '123': 'For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.', '124': 'More judgements would have enabled us to make better distinctions, but it is not clear what the upper limit is.', '125': 'We can check, what the consequences of less manual annotation of results would have been: With half the number of manual judgements, we can distinguish about 40% of the systems, 10% less.', '126': 'The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.', '127': 'Since the inclusion of out-ofdomain test data was a very late decision, the participants were not informed of this.', '128': 'So, this was a surprise element due to practical reasons, not malice.', '129': 'All systems (except for Systran, which was not tuned to Europarl) did considerably worse on outof-domain training data.', '130': 'This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.', '131': 'The manual scores are averages over the raw unnormalized scores.', '132': 'It is well know that language pairs such as EnglishGerman pose more challenges to machine translation systems than language pairs such as FrenchEnglish.', '133': 'Different sentence structure and rich target language morphology are two reasons for this.', '134': 'Again, we can compute average scores for all systems for the different language pairs (Figure 6).', '135': 'The differences in difficulty are better reflected in the BLEU scores than in the raw un-normalized manual judgements.', '136': 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', '137': 'This is because different judges focused on different language pairs.', '138': 'Hence, the different averages of manual scores for the different language pairs reflect the behaviour of the judges, not the quality of the systems on different language pairs.', '139': 'Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.', '140': 'We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.', '141': 'In-domain Systran scores on this metric are lower than all statistical systems, even the ones that have much worse human scores.', '142': 'Surprisingly, this effect is much less obvious for out-of-domain test data.', '143': 'For instance, for out-ofdomain English-French, Systran has the best BLEU and manual scores.', '144': 'Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.', '145': 'This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.', '146': 'This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.', '147': 'So, who won the competition?', '148': 'The best answer to this is: many research labs have very competitive systems whose performance is hard to tell apart.', '149': 'This is not completely surprising, since all systems use very similar technology.', '150': 'For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French), at least as measured by BLEU.', '151': 'The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.', '152': 'The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.', '153': 'This is the first time that we organized a large-scale manual evaluation.', '154': 'While we used the standard metrics of the community, the we way presented translations and prompted for assessment differed from other evaluation campaigns.', '155': 'For instance, in the recent IWSLT evaluation, first fluency annotations were solicited (while withholding the source sentence), and then adequacy annotations.', '156': 'Almost all annotators reported difficulties in maintaining a consistent standard for fluency and adequacy judgements, but nevertheless most did not explicitly move towards a ranking-based evaluation.', '157': 'Almost all annotators expressed their preference to move to a ranking-based evaluation in the future.', '158': 'A few pointed out that adequacy should be broken up into two criteria: (a) are all source words covered?', '159': '(b) does the translation have the same meaning, including connotations?', '160': 'Annotators suggested that long sentences are almost impossible to judge.', '161': 'Since all long sentence translation are somewhat muddled, even a contrastive evaluation between systems was difficult.', '162': 'A few annotators suggested to break up long sentences into clauses and evaluate these separately.', '163': 'Not every annotator was fluent in both the source and the target language.', '164': 'While it is essential to be fluent in the target language, it is not strictly necessary to know the source language, if a reference translation was given.', '165': 'However, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate — due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence).', '166': 'Lack of correct reference translations was pointed out as a short-coming of our evaluation.', '167': 'One annotator suggested that this was the case for as much as 10% of our test sentences.', '168': 'Annotators argued for the importance of having correct and even multiple references.', '169': 'It was also proposed to allow annotators to skip sentences that they are unable to judge.', '170': 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.', '171': 'While many systems had similar performance, the results offer interesting insights, especially about the relative performance of statistical and rule-based systems.', '172': 'Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.', '173': 'The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.', '174': 'The manual evaluation of scoring translation on a graded scale from 1–5 seems to be very hard to perform.', '175': 'Replacing this with an ranked evaluation seems to be more suitable.', '176': 'Human judges also pointed out difficulties with the evaluation of long sentences.', '177': 'This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.', '178': 'HR0011-06-C-0022.'}","['W06-3114_swastika', 'W06-3114_sweta', 'W06-3114_aakansha']","['../data/summaries/W06-3114_swastika.txt', '../data/summaries/W06-3114_sweta.txt', '../data/summaries/W06-3114_aakansha.txt']","['../data/tba/W06-3114_swastika.json', '../data/tba/W06-3114_sweta.json', '../data/tba/W06-3114_aakansha.json']"
W08-2222,Wide-Coverage Semantic Analysis with Boxer,../data/papers/W08-2222.xml,"{'0': 'Wide-Coverage Semantic Analysis with Boxer', '1': 'Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT).', '2': 'Used together with the C&C tools, Boxer reaches more than 95% coverage on newswire texts.', '3': 'The semantic representations produced by Boxer, known as Discourse Representation Structures (DRSs), incorporate a neoDavidsonian representations for events, using the VerbNet inventory of thematic roles.', '4': 'The resulting DRSs can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic.', '5': 'Boxerâ\x80\x99s performance on the shared task for comparing semantic represtations was promising.', '6': 'It was able to produce complete DRSs for all seven texts.', '7': 'Manually inspecting the output revealed that: (a) the computed predicate argument structure was generally of high quality, in particular dealing with hard constructions involving control or coordination; (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall correctly computed; (c) some measure and time expressions are correctly analysed, others arenâ\x80\x99t; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases.', '8': 'Boxer is distributed with the C&C tools and freely available for research purposes.', '9': '277', '10': 'Boxer is an open-domain tool for computing and reasoning with semantic representations.', '11': 'Based on Discourse Representation Theory (Kamp and Reyle, 1993), Boxer is able to construct Discourse Representation Structures (DRSs for short, informally called â\x80\x9cboxesâ\x80\x9d because of the way they are graphically displayed) for English sentences and texts.', '12': 'There is a translation from DRSs to first-order formulas, which opens the way to perform inference by including automated reasoning tools such as theorem provers and model builders (Blackburn and Bos, 2005).', '13': '2.1 Combinatory Categorial Grammar.', '14': 'As a preliminary to semantics, we need syntax.', '15': 'Boxer implements a syntax-semantics interface based on Combinatory Categorial Grammar, CCG (Steedman, 2001).', '16': 'CCG lends itself extremely well for this task because it is lexically driven and has only few â\x80\x9cgrammarâ\x80\x9d rules, and not less because of its type-transparency principle, which says that each syntactic type (a CCG category) corresponds to a unique semantic type (a lambda-expression).', '17': 'Because the syntax-semantics is clearly defined, the choice of logical form can be independent of the categorial framework underlying it.', '18': 'Steedman uses simple predicate argument structures expressed via the untyped lambda calculus to illustrate the construction of logical forms in CCG (Steedman, 2001).', '19': 'We instead opt for Discourse Representation Theory, a widely accepted sophisticated formal theory of natural language meaning dealing with a large variety of semantic phenomena.', '20': '2.2 Discourse Representation Theory.', '21': 'DRT is a formal semantic theory originally designed by Kamp to cope with anaphoric pronouns and temporal relations (Kamp, 1981).', '22': 'DRT uses an explicit intermediate semantic representation, called DRS (Discourse Representation Structure), for dealing with anaphoric or other contextually sensitive linguistic phenomena such as ellipsis and presupposition.', '23': 'We choose DRT because it has established itself as a well- documented formal theory of meaning, covering a number of semantic phenomena ranging from pronouns, abstract anaphora, presupposition, tense and aspect, propositional attitudes, to plurals (Kamp and Reyle, 1993; Asher, 1993; Van der Sandt, 1992).In terms of expressive power, three different kinds of representations are distin guished in Boxer: 1.', '24': 'Discourse Representation Structures (DRSs).', '25': '2.', '26': 'Underspecified DRSs (DRSs + merge + alfa).', '27': '3.', '28': 'Î»-DRSs (UDRSs + lambda + application) DRSs are the representations corresponding to natural language sentences or texts.', '29': 'This is the core DRT language compatible with first-order logic.', '30': 'The DRS language employed by Boxer is a subset of the one found in Kamp and Reyle (1993).', '31': 'We define the syntax of DRSs below with the help of BackusNaur form, where non-terminal symbols are enclosed in angle brackets.', '32': 'The non-terminal <ref> denotes a discourse referent, and <symn> an n-place predicate symbol.', '33': '<expe > ::= <ref> <expt > ::= <drs> <ref>â\x88\x97 <drs> ::= <condition>â\x88\x97 <condition> ::= <basic> | <complex> <basic> ::= <sym1 >(<expe >) | <sym2 >(<expe >,<expe >) | <named>(<expe >,<nam>,<sort>) <complex> ::= <expt > | <expt >â\x87\x92<expt > | <expt >â\x88¨<expt > | <ref>:<expt > DRSs are structures comprising two parts: 1) a set of discourse referents; and 2) a set of conditions constraining the interpretation of the discourse referents.', '34': 'Conditions can be simple properties of discourse referents, express relations between them, or be complex, introducing (recursively) subordinated DRSs.', '35': 'The standard version of DRT formulated in Kamp & Reyle incorporates a Davidsonian event semantics (Kamp and Reyle, 1993), where discourse referents can also stand for events and be referred to by anaphoric expressions or constrained by temporal relations.', '36': 'The neoDavidsonian system, as implemented in Boxer, uses the inventory of roles proposed by VerbNet (Kipper et al., 2008), and has some attractive formal properties (Dowty, 1989).', '37': 'There is only one way to state that an individual is participating in an eventâ\x80\x94namely by relating it to the event using a binary relation expressing some thematic role.', '38': 'Furthermore, the approach clearly distinguishes the participants of an event by the semantic roles they bear.', '39': 'Finally, it also allows us to characterize the meaning of thematic roles independently of the meaning of the verb that describes the event.', '40': 'We wonâ\x80\x99t show the standard translation from DRS to FOL here (Blackburn et al., 2001; Bos, 2004; Kamp and Reyle, 1993).', '41': 'Intuitively, translating DRSs into first-order formulas proceeds as follows: each discourse referent is translated as a first-order quantifier, and all DRS-conditions are translated into a conjunctive formula of FOL.', '42': 'Discourse referents usually are translated to existential quantifiers, with the exception of those declared in antecedents of implicational DRS-conditions, that are translated as universal quantifiers.', '43': 'Obviously, negated DRSs are translated as negated formulas, disjunctive DRSs as disjunctive formulas, and implicational DRSs as formulas with material implication.', '44': 'Boxer outputs either resolved semantic representations (in other words, completely disambiguated DRSs), or underspecified representations, where some ambiguities are left unresolved in the semantic representation.', '45': 'This level of representation is referred to as underspecified DRS, or UDRS for short.', '46': 'It is a small extension of the DRS language given in the previous section and is defined as follows: <expt > ::= <udrs> <udrs> ::= <drs> | (<expt >;<expt >) | (<expt >Î±<expt >) Note here that expressions of type t are redefined as UDRSs.', '47': 'UDRSs are either ordinarly DRSs, DRSs conjoined by the merge (for which we use the semicolon), or NP/N: A N/N: record N: date Î»q.Î»p.( x ;q@x;p@x) Î»p.Î»x.( y record(y) nn(y,x) ;p@x) Î»x. date(x) [fa] N: record date y Î»x.( record(y) nn(y,x) ; ) date(x) . . .', '48': '[merge] y Î»x. record(y) nn(y,x) date(x) [fa] NP: A record date y Î»p.( x ; record(y) nn(y,x) date(x) ;p@x) . . .', '49': '[merge] x y Î»p. record(y) nn(y,x) date(x) ;p@x Figure 1: Derivation with Î»-DRSs, including Î²-conversion, for â\x80\x9cA record dateâ\x80\x9d.', '50': 'Combinatory rules are indicated by solid lines, semantic rules by dotted lines.', '51': 'DRS composed by the Î±-operator.', '52': 'The merge conjoins two DRSs into a larger DRS â\x80\x94 semantically the merge is interpretated as (dynamic) logical conjunction.', '53': 'Merge- reduction is the process of eliminating the merge operation by forming a new DRS resulting from the union of the domains and conditions of the argument DRSso of a merge, respectively (obeying certain constraints).', '54': 'Figure 1 illustrates the syntax- semantics interface (and merge-reduction) for a derivation of a simple noun phrase.', '55': 'Boxer adopts Van der Sandtâ\x80\x99s view as presupposition as anaphora (Van der Sandt, 1992), in which presuppositional expressions are either resolved to previously established discourse entities or accommodated on a suitable level of discourse.', '56': 'Van der Sandtâ\x80\x99s proposal is cast in DRT, and therefore relatively easy to integrate in Boxerâ\x80\x99s semantic formalism.', '57': 'The Î±-operator indicates information that has to be resolved in the context, and is lexically introduced by anaphoric or presuppositional expressions.', '58': 'A DRS constructed with Î± resembles the protoDRS of Van der Sandtâ\x80\x99s theory of presupposition (Van der Sandt, 1992) although they are syntactically defined in a slightly different way to overcome problems with free and bound variables, following Bos (2003).', '59': 'Note that the difference between anaphora and presupposition collapses in Van der Sandtâ\x80\x99s theory.', '60': 'The types are the ingredients of a typed lambda calculus that is employed to construct DRSs in a bottom-up fashion, compositional way.', '61': 'The language of lambda DRSs is an extension of the language of (U)DRS defined before: <expe > ::= <ref> | <vare > <expt > ::= <udrs> | <vart > <expÎ± > ::= (<exp(Î²,Î±)> @ <varÎ² >) | <varÎ± > <exp(Î±,Î²)> ::= Î»<varÎ± >.<expÎ² > | <var(Î±,Î²)> Hence we define discourse referents as expressions of type e, and DRSs as expressions of type t . We use @ to indicate function application, and the Î»-operator to bind free variables over which we wish to abstract.', '62': '3.1 Preprocessing.', '63': 'The input text needs to be tokenised with one sentence per line.', '64': 'In the context of this paper, Boxer was put into action after using a combined processing pipeline of the C&C tools consisting of POS-tagging, named entity recognition, and parsing (Curran et al., 2007).', '65': 'The POS tags are used to specify the lexical semantics for ambiguous CCG categories (see below); the named entity tags are transferred to the level of DRSs as well and added as sorts to named discourse referents.', '66': 'An example of a CCG derivation is shown in Figure 2.', '67': 'a virus --[lex] --[lex] by np:nb/n n ---------------------[lex] -----------[fa] Cervical cancer caused ((s:pss\\np)\\(s:pss\\np))/np np:nb ---[lex] --[lex] ---[lex] --------------------------------------[fa] n/n n is s:pss\\np (s:pss\\np)\\(s:pss\\np) ------------[fa] ----------------[lex] -----------------------------------------------[ba] n (s:dcl\\np)/(s:pss\\np) s:pss\\np ------------[tc] ---------------------------------------------------------------------[fa] np s:dcl\\np --------------------------------------------------------------------------------------[ba] s:dcl Figure 2: CCG derivation as generated by the C&C tools 3.2 Lexicon.', '68': 'In CCG, the syntactic lexicon comprises the set of lexical categories.', '69': 'CCGbank hosts more than a thousand different categories.', '70': 'The semantic lexicon defines a suitable mapping from categories to semantic representations.', '71': 'In the context of Boxer, these semantic representations are defined in the shape of lambda-DRSs.', '72': 'Boxer implements almost all categories employed by the C&C parser, which is a subset of the ones found in CCGbank, leaving out extremely rare cases for the sake of efficiency.', '73': 'Defining the lexical semantics cannot always be done solely on the basis of the category, for one lexical category could give rise to several different semantic interpretations.', '74': 'So we need to take other resources into account, such as the assigned part of speech (PoS), and sometimes the wordform or named entity type associated with the category.', '75': 'For the majority of categories, in particular those that correspond to open-class lexical items, we also need access to the morphological root of the word that triggered the lexical category.', '76': 'Although there is a one-to-one mapping between the CCG categories and semantic types â\x80\x94 and this must be the case to ensure the semantic composition process proceeds without type clashes â\x80\x94 the actual instantiations of a semantic type can differ even within the scope of a single CCG category.', '77': 'For example, the category n/n can correspond to an adjective, a cardinal expression, or even common nouns and proper names (in the compound expressions).', '78': 'In the latter two cases the lexical entry introduces a new discourse referent, in the former two it does not.', '79': 'To account for this difference we also need to look at the part of speech that is assigned to a token.', '80': '3.3 Resolution.', '81': 'Boxer implements various presupposition triggers introduced by noun phrases, including personal pronouns, possessive pronouns, reflexive pronouns, emphasising pronouns, demonstrative pronouns, proper names, other-anaphora, definite descriptions.', '82': 'In addition, some aspects of tense are implemented as presupposition triggers, too.', '83': 'Anaphora and presupposition resolution takes place in a separate stage after building up the representation, following the resolution algorithm outlined in Bos (2003).', '84': 'The current implementation of Boxer aims at high precision in resolution: personal pronouns are only attempted to be resolved to named entities, definite descriptions and proper names are only linked to previous discourse referents if there is overlap in the DRS-conditions of the antencedent DRS and alpha-DRS.', '85': 'If no suitable antecedent can be found, global accommodation of the anaphoric discourse referent and conditions will take palce.', '86': 'Because Boxer has the option to output unresolved DRSs too, it is possible to include external anaphora or coreference resolution components.', '87': '3.4 Example Analysis.', '88': 'We illustrate the capabilities of Boxer with the following example text shown below (aka as Text 2 of the shared task).1 The text consists of three sentences, the second being a coordinated sentence.', '89': 'It contains a passive construction, three pronouns, relative clauses, control verbs, and a presupposition trigger other.', '90': 'Text 2 Cervical cancer is caused by a virus.', '91': 'That has been known for some time and it has led to a vaccine that seems to prevent it.', '92': 'Researchers have been looking for other cancers that may be caused by viruses.', '93': 'The output of Boxer for this text is shown in Figure 3.', '94': 'Only the box format is shown here â\x80\x94 Boxer is also able to output the DRSs in Prolog or XML encodings.', '95': 'It was run without analysing tense and aspect and without discourse segmentation (both of these are possible in Boxer, but still undergo development, and are therefore disregarded here).', '96': 'As we can see from the example and Boxerâ\x80\x99s analysis various things go right and various things go wrong.', '97': 'Boxer deals fine with the passive construction (assigned the 1 This text was taken from the Economist Volume 387 Number 8582, page 92.', '98': 'The third sentence has been simplified.', '99': 'appropriate semantic role), the relative clauses, and the control construction (vaccine is the agent of the prevent event).', '100': 'It also handles the presupposition trigger anaphorically linking the mention of other cancers in the third sentence with the phrase cervical cancer in the first sentence, and asserting an inequality condition in the DRS.', '101': 'Boxer failed to resolve three pronouns correctly.', '102': 'These are all accommodated at the global level of DRS, which is the DRS on the left-hand side in Figure 3.', '103': 'All of the pronouns have textual antecedents: the abstract pronoun that in the second sentence refers to the fact declared in the first sentence.', '104': 'The first occurrence of it in the second sentence also seems to refer to this fact â\x80\x94 the second occurrence of it refers to cervical cancer mentioned in the first sentence.', '105': 'bin/boxer --input working/step/text2.ccg --semantics drs --box --resolve --roles verbnet --format no %%% %%% | x0 x1 x2 | | x3 x4 x5 | | x6 x7 | | x8 x9 x10 x11 | | x13 x14 x15 x16 x17 | %%% |------------| |--------------| |--------------| |------------------------| |---------------------| %%% (| thing(x0) |+(| cancer(x3) |+(| know(x6) |+(| lead(x8) |+| researcher(x13) |)))) %%% | neuter(x1) | | cervical(x3) | | time(x7) | | vaccine(x9) | | look(x14) | %%% | neuter(x2) | | cause(x4) | | event(x6) | | seem(x10) | | agent(x14,x13) | %%% | | | virus(x5) | | theme(x6,x0) | | proposition(x11) | | cancer(x15) | %%% | event(x4) | | for(x6,x7) | | event(x10) | | | %%% | theme(x4,x3) | | | | event(x8) | | | | | %%% | by(x4,x5) | | agent(x8,x1) | | |----------| | %%% | | | agent(x10,x9) | | | | x15 = x3 | | %%% | theme(x10,x11) | | | | | %%% | to(x8,x9) | | cause(x16) | %%% | | | virus(x17) | %%% | | x12 | | | event(x16) | %%% | x11:|---------------| | | theme(x16,x15) | %%% | | prevent(x12) | | | by(x16,x17) | %%% | | event(x12) | | | for(x14,x15) | %%% | | agent(x12,x9) | | | event(x14) | %%% | | theme(x12,x2) | | | | %%% | | | | %%% | | Attempted: 3.', '106': 'Completed: 3 (100.00%).', '107': 'Figure 3: Boxer output for Shared Task Text 2', '108': 'Here we discuss the output of Boxer on the Shared Task Texts (Bos, 2008).', '109': 'Boxer was able to produce semantic representation for all text without any further modifications to the software.', '110': 'For each text we briefly say what was good and bad about Boxerâ\x80\x99s analysis.', '111': '(We wonâ\x80\x99t comment on the performance on the second text, as this is the text proposed by ourselves and already discussed in the previous section.)', '112': 'Text 1: An object is thrown with a horizontal speed ...', '113': 'Good: The resulting predicate argument structure was fine overall, including a difficult control construction (â\x80\x9chow long does it take the object to fall ...â\x80\x9d).', '114': 'The definite description â\x80\x9cthe objectâ\x80\x9d was correctly resolved.', '115': 'The conditional got correctly anal- ysed.', '116': 'Bad: The measure phrase â\x80\x9c125 m highâ\x80\x9d got misinterpreted as noun-noun comn- pound.', '117': 'The definite description â\x80\x9cthe fallâ\x80\x9d was not linked to the falling event mentioned before.', '118': 'Comments: Because there were two questions in this text we parsed it using the C&C parser with the model trained on questions.', '119': 'Text 3: John went into a restaurant ...', '120': 'Good: The pronouns were correctly resolved to the proper name â\x80\x9cJohnâ\x80\x9d rather than â\x80\x9cthe waiterâ\x80\x9d, even though this is based on the simple strategy in Boxer to link third- person pronouns to named entities of type human.', '121': 'The coordination construction â\x80\x9cwarm and friendlyâ\x80\x9d got correctly analysed (distributively), and the control construction â\x80\x9cbegan to read his bookâ\x80\x9d received a proper predicate argument structure.', '122': 'Bad: Boxer doesnâ\x80\x99t deal with bridging references introduced by relational nouns, so expressions like â\x80\x9cthe cornerâ\x80\x9d were not linked to other discourse entities.', '123': 'Text 4: The first school for the training of leader dogs ...', '124': 'Good: The named entities were correctly recognised and classified (locations and proper names).', '125': 'The VP coordination in the first and later sentences was correctly analysed.', '126': 'The expression â\x80\x9cthis schoolâ\x80\x9d got correctly linked to the schhol mentioned earlier in the text.', '127': 'The time expression â\x80\x9c1999â\x80\x9d got the right interpretation.', '128': 'Bad: The adjectives/determiners â\x80\x9cfirstâ\x80\x9d and â\x80\x9cseveralâ\x80\x9d didnâ\x80\x99t receive a deep analysis.', '129': 'The complex NP â\x80\x9cJoao Pedro Fonseca and Marta Gomesâ\x80\x9d was distributively interpreted, rather than collective.', '130': 'The pronoun â\x80\x9ctheyâ\x80\x9d wasnâ\x80\x99t resolved.', '131': 'The preposition â\x80\x9cInâ\x80\x9d starting the second sentence was incorrectly analysed by the parser.', '132': 'Text 5: As the 3 guns of Turret 2 were being loaded ...', '133': 'Good: The discourse structures invoked by the sentence initial adverbs â\x80\x9cAsâ\x80\x9d and â\x80\x9cWhenâ\x80\x9d was correctly computed.', '134': 'Predicate argument structure overall good, including treatment of the relative clauses.', '135': 'The expression â\x80\x9cthe propellantâ\x80\x9d was correctly resolved.', '136': 'Time expressions in the one but last sentence got a correct analysis.', '137': 'Bad: The name â\x80\x9cTurret 2â\x80\x9d was incorrectly analysed (not as a compound).', '138': 'The adverbs â\x80\x9cyetâ\x80\x9d and â\x80\x9cthenâ\x80\x9d got a shallow analysis.', '139': 'The first-person pronoun â\x80\x9cIâ\x80\x9d was not resolved to the crewman.', '140': 'Comments: The quotes were removed in the tokenisation phase, because the C&C parser, being trained on a corpus without quotes, performs badly on texts containing quotes.', '141': 'Text 6: Amid the tightly packed row houses of North Philadelphia ...', '142': 'Good: The named entities were correctly recognised and classified as locations.', '143': 'The various cases of VP coordination all got properly analysed.', '144': 'The numerical and date expressions got correct representations.', '145': 'Bad: The occurrences of the third-person neuter pronouns were not resolved.', '146': 'The preposition â\x80\x9cAmidâ\x80\x9d was not correctly analysed.', '147': 'Text 7: Modern development of wind-energy technology and applications ...', '148': 'Good: Correct interpretation of time expressions â\x80\x9c1930sâ\x80\x9d and â\x80\x9c1970sâ\x80\x9d.', '149': 'Correct pred icate argument structure overall.', '150': 'Bad: â\x80\x9cModernâ\x80\x9d was recognised as a proper name.', '151': 'The noun phrase â\x80\x9cwind-energy technology and applicationsâ\x80\x9d was distributively analysed with â\x80\x9cwind-energyâ\x80\x9d only applying to â\x80\x9ctechnologyâ\x80\x9d.', '152': 'The sentence-initial adverb â\x80\x9cSinceâ\x80\x9d did not introduce proper discourse structure.', '153': 'The units of measurement in the last two sentences were not recognised as such.', '154': 'The tricky time expression â\x80\x9cmid-80â\x80\x99sâ\x80\x9d only got a shallow interpretation.', '155': 'Boxer is a wide-coverage system for semantic interpretation.', '156': 'It takes as input a CCG derivation of a natural language expression, and produces formally interpretable semantic representations: either in the form of DRSs, or as formulas of first-order logic.', '157': 'The existence of CCGbank (Hockenmaier, 2003) and robust parsers trained on it (Clark and Curran, 2004; Bos et al., 2004) make Boxer a state-of-the-art open- domain tool for deep semantic analysis.', '158': 'Boxerâ\x80\x99s performance on the shared task for comparing semantic represtations was promising.', '159': 'It was able to produce DRSs for all texts.', '160': 'We canâ\x80\x99t quantify the quality of Boxerâ\x80\x99s output, as we donâ\x80\x99t have gold standard representations at our disposal.', '161': 'Manually inspecting the output gives us the following impression: â\x80¢ computed predicate argument structure is generally of good quality, including hard constructions involving control or coordination; â\x80¢ discourse structure triggered by conditionals, negation or discourse adverbs is overall correctly computed; â\x80¢ some measure and time expressions are correctly analysed, others arenâ\x80\x99t; â\x80¢ several shallow analyses are given for lexical phrases that require deep analysis; â\x80¢ bridging references and pronouns are not resolved in most cases; but when they are, they are mostly correctly resolved (high precision at the cost of recall).', '162': 'Finally, a comment on availability of Boxer.', '163': 'All sources of Boxer are available for download and free of noncommercial use.', '164': 'It is distributed with the C&C tools for natural language processing (Curran et al., 2007), which are hosted on this site: http://svn.ask.it.usyd.edu.au/trac/candc/wiki/boxer'}",['W08-2222'],['../data/summaries/W08-2222.txt'],['../data/tba/W08-2222.json']
W11-2123,KenLM: Faster and Smaller Language Model Queries,../data/papers/W11-2123.xml,"{'0': 'KenLM: Faster and Smaller Language Model Queries', '1': 'We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', '2': 'The structure uses linear probing hash tables and is designed for speed.', '3': 'Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', '4': 'Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.', '5': 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', '6': 'Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.', '7': 'This paper presents methods to query N-gram language models, minimizing time and space costs.', '8': 'Queries take the form p(wn|wn−1 1 ) where wn1 is an n-gram.', '9': 'Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.', '10': 'The problem is to store these two values for a large and sparse set of n-grams in a way that makes queries efficient.', '11': 'Many packages perform language model queries.', '12': 'Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.', '13': 'IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption.', '14': 'MITLM 0.4 (Hsu and Glass, 2008) is mostly designed for accurate model estimation, but can also compute perplexity.', '15': 'RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures.', '16': 'BerkeleyLM revision 152 (Pauls and Klein, 2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization.', '17': 'Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques, but did not release code.', '18': 'TPT Germann et al. (2009) describe tries with better locality properties, but did not release code.', '19': 'These packages are further described in Section 3.', '20': 'We substantially outperform all of them on query speed and offer lower memory consumption than lossless alternatives.', '21': 'Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.', '22': 'Our open-source (LGPL) implementation is also available for download as a standalone package with minimal (POSIX and g++) dependencies.', '23': 'We implement two data structures: PROBING, designed for speed, and TRIE, optimized for memory.', '24': 'The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties.', '25': 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', '26': 'We use two common techniques, hash tables and sorted arrays, describing each before the model that uses the technique.', '27': 'Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.', '28': 'Keys to the table are hashed, using for example Austin Appleby’s MurmurHash2, to integers evenly distributed over a large range.', '29': 'This range is collapsed to a number of buckets, typically by taking the hash modulo the number of buckets.', '30': 'Entries landing in the same bucket are said to collide.', '31': 'Several methods exist to handle collisions; we use linear probing because it has less memory overhead when entries are small.', '32': 'Linear probing places at most one entry in each bucket.', '33': 'When a collision occurs, linear probing places the entry to be inserted in the next (higher index) empty bucket, wrapping around as necessary.', '34': 'Therefore, a populated probing hash table consists of an array of buckets that contain either one entry or are empty.', '35': 'Non-empty buckets contain an entry belonging to them or to a preceding bucket where a conflict occurred.', '36': 'Searching a probing hash table consists of hashing the key, indexing the corresponding bucket, and scanning buckets until a matching key is found or an empty bucket is encountered, in which case the key does not exist in the table.', '37': 'Linear probing hash tables must have more buckets than entries, or else an empty bucket will never be found.', '38': 'The ratio of buckets to entries is controlled by space multiplier m > 1.', '39': 'As the name implies, space is O(m) and linear in the number of entries.', '40': 'The fraction of buckets that are empty is m−1 m , so average lookup time is O( m 1) and, crucially, constant in the number of entries.', '41': 'When keys are longer than 64 bits, we conserve space by replacing the keys with their 64-bit hashes.', '42': 'With a good hash function, collisions of the full 64bit hash are exceedingly rare: one in 266 billion queries for our baseline model will falsely find a key not present.', '43': 'Collisions between two keys in the table can be identified at model building time.', '44': 'Further, the special hash 0 suffices to flag empty buckets.', '45': 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', '46': 'Unigram lookup is dense so we use an array of probability and backoff values.', '47': 'For 2 < n < N, we use a hash table mapping from the n-gram to the probability and backoff3.', '48': 'Vocabulary lookup is a hash table mapping from word to vocabulary index.', '49': 'In all cases, the key is collapsed to its 64-bit hash.', '50': 'Given counts cn1 where e.g. c1 is the vocabulary size, total memory consumption, in bits, is Our PROBING data structure places all n-grams of the same order into a single giant hash table.', '51': 'This differs from other implementations (Stolcke, 2002; Pauls and Klein, 2011) that use hash tables as nodes in a trie, as explained in the next section.', '52': 'Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.', '53': 'Sorted arrays store key-value pairs in an array sorted by key, incurring no space overhead.', '54': 'SRILM’s compact variant, IRSTLM, MITLM, and BerkeleyLM’s sorted variant are all based on this technique.', '55': 'Given a sorted array A, these other packages use binary search to find keys in O(log |A|) time.', '56': 'We reduce this to O(log log |A|) time by evenly distributing keys over their range then using interpolation search4 (Perl et al., 1978).', '57': 'Interpolation search formalizes the notion that one opens a dictionary near the end to find the word “zebra.” Initially, the algorithm knows the array begins at b +— 0 and ends at e +— |A|−1.', '58': 'Given a key k, it estimates the position If the estimate is exact (A[pivot] = k), then the algorithm terminates succesfully.', '59': 'If e < b then the key is not found.', '60': 'Otherwise, the scope of the search problem shrinks recursively: if A[pivot] < k then this becomes the new lower bound: l +— pivot; if A[pivot] > k then u +— pivot.', '61': 'Interpolation search is therefore a form of binary search with better estimates informed by the uniform key distribution.', '62': 'If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words), then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', '63': 'The improvement is due to the cost of bit-level reads and avoiding reads that may fall in different virtual memory pages.', '64': 'Vocabulary lookup is a sorted array of 64-bit word hashes.', '65': 'The index in this array is the vocabulary identifier.', '66': 'This has the effect of randomly permuting vocabulary identifiers, meeting the requirements of interpolation search when vocabulary identifiers are used as keys.', '67': 'While sorted arrays could be used to implement the same data structure as PROBING, effectively making m = 1, we abandoned this implementation because it is slower and larger than a trie implementation.', '68': 'The trie data structure is commonly used for language modeling.', '69': 'Our TRIE implements the popular reverse trie, in which the last word of an n-gram is looked up first, as do SRILM, IRSTLM’s inverted variant, and BerkeleyLM except for the scrolling variant.', '70': 'Figure 1 shows an example.', '71': 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', '72': 'We maintain a separate array for each length n containing all n-gram entries sorted in suffix order.', '73': 'Therefore, for n-gram wn1 , all leftward extensions wn0 are an adjacent block in the n + 1-gram array.', '74': 'The record for wn1 stores the offset at which its extensions begin.', '75': 'Reading the following record’s offset indicates where the block ends.', '76': 'This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM’s compressed option.', '77': 'SRILM inefficiently stores 64-bit pointers.', '78': 'Unigram records store probability, backoff, and an index in the bigram table.', '79': 'Entries for 2 < n < N store a vocabulary identifier, probability, backoff, and an index into the n + 1-gram table.', '80': 'The highestorder N-gram array omits backoff and the index, since these are not applicable.', '81': 'Values in the trie are minimally sized at the bit level, improving memory consumption over trie implementations in SRILM, IRSTLM, and BerkeleyLM.', '82': 'Given n-gram counts {cn}Nn=1, we use Flog2 c1] bits per vocabulary identifier and Flog2 cn] per index into the table of ngrams.', '83': 'When SRILM estimates a model, it sometimes removes n-grams but not n + 1-grams that extend it to the left.', '84': 'In a model we built with default settings, 1.2% of n + 1-grams were missing their ngram suffix.', '85': 'This causes a problem for reverse trie implementations, including SRILM itself, because it leaves n+1-grams without an n-gram node pointing to them.', '86': 'We resolve this problem by inserting an entry with probability set to an otherwise-invalid value (−oc).', '87': 'Queries detect the invalid probability, using the node only if it leads to a longer match.', '88': 'By contrast, BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.', '89': 'Floating point values may be stored in the trie exactly, using 31 bits for non-positive log probability and 32 bits for backoff5.', '90': 'To conserve memory at the expense of accuracy, values may be quantized using q bits per probability and r bits per backoff6.', '91': 'We allow any number of bits from 2 to 25, unlike IRSTLM (8 bits) and BerkeleyLM (17−20 bits).', '92': 'To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.', '93': 'The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.', '94': 'Unigrams also have 64-bit overhead for vocabulary lookup.', '95': 'Using cn to denote the number of n-grams, total memory consumption of TRIE, in bits, is plus quantization tables, if used.', '96': 'The size of TRIE is particularly sensitive to F1092 c11, so vocabulary filtering is quite effective at reducing model size.', '97': 'SRILM (Stolcke, 2002) is widely used within academia.', '98': ""It is generally considered to be fast (Pauls 29 − 1 probabilities and 2' − 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node."", '99': 'Each trie node is individually allocated and full 64-bit pointers are used to find them, wasting memory.', '100': 'The compact variant uses sorted arrays instead of hash tables within each node, saving some memory, but still stores full 64-bit pointers.', '101': 'With some minor API changes, namely returning the length of the n-gram matched, it could also be faster—though this would be at the expense of an optimization we explain in Section 4.1.', '102': 'The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.', '103': 'IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.', '104': 'The developers aimed to reduce memory consumption at the expense of time.', '105': 'Their default variant implements a forward trie, in which words are looked up in their natural left-to-right order.', '106': 'However, their inverted variant implements a reverse trie using less CPU and the same amount of memory7.', '107': 'Each trie node contains a sorted array of entries and they use binary search.', '108': 'Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.', '109': 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', '110': 'IRSTLM’s quantized variant is the inspiration for our quantized variant.', '111': 'Unfortunately, we were unable to correctly run the IRSTLM quantized variant.', '112': 'The developers suggested some changes, such as building the model from scratch with IRSTLM, but these did not resolve the problem.', '113': 'Our code has been publicly available and intergrated into Moses since October 2010.', '114': 'Later, BerkeleyLM (Pauls and Klein, 2011) described ideas similar to ours.', '115': 'Most similar is scrolling queries, wherein left-to-right queries that add one word at a time are optimized.', '116': 'Both implementations employ a state object, opaque to the application, that carries information from one query to the next; we discuss both further in Section 4.2.', '117': 'State is implemented in their scrolling variant, which is a trie annotated with forward and backward pointers.', '118': 'The hash variant is a reverse trie with hash tables, a more memory-efficient version of SRILM’s default.', '119': 'While the paper mentioned a sorted variant, code was never released.', '120': 'The compressed variant uses block compression and is rather slow as a result.', '121': 'A direct-mapped cache makes BerkeleyLM faster on repeated queries, but their fastest (scrolling) cached version is still slower than uncached PROBING, even on cache-friendly queries.', '122': 'For all variants, we found that BerkeleyLM always rounds the floating-point mantissa to 12 bits then stores indices to unique rounded floats.', '123': 'The 1-bit sign is almost always negative and the 8-bit exponent is not fully used on the range of values, so in practice this corresponds to quantization ranging from 17 to 20 total bits.', '124': 'Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple, 2010) offer better memory consumption at the expense of CPU and accuracy.', '125': 'These enable much larger models in memory, compensating for lost accuracy.', '126': 'Typical data structures are generalized Bloom filters that guarantee a customizable probability of returning the correct answer.', '127': 'Minimal perfect hashing is used to find the index at which a quantized probability and possibly backoff are stored.', '128': 'These models generally outperform our memory consumption but are much slower, even when cached.', '129': 'In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.', '130': 'Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.', '131': 'Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.', '132': 'We call these N − 1 words state.', '133': 'When two partial hypotheses have equal state (including that of other features), they can be recombined and thereafter efficiently handled as a single packed hypothesis.', '134': 'If there are too many distinct states, the decoder prunes low-scoring partial hypotheses, possibly leading to a search error.', '135': 'Therefore, we want state to encode the minimum amount of information necessary to properly compute language model scores, so that the decoder will be faster and make fewer search errors.', '136': 'We offer a state function s(wn1) = wn� where substring wn� is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.', '137': 'The state function is integrated into the query process so that, in lieu of the query p(wnjwn−1 1 ), the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).', '138': 'The returned state s(wn1) may then be used in a followon query p(wn+1js(wn1)) that extends the previous query by one word.', '139': 'These make left-to-right query patterns convenient, as the application need only provide a state and the word to append, then use the returned state to append another word, etc.', '140': 'We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.', '141': 'Syntactic decoders, such as cdec (Dyer et al., 2010), build state from null context then store it in the hypergraph node for later extension.', '142': 'Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore, when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf , it may return state s(wn1) = wnf since no longer context will be found.', '143': 'IRSTLM and BerkeleyLM use this state function (and a limit of N −1 words), but it is more strict than necessary, so decoders using these packages will miss some recombination opportunities.', '144': 'State will ultimately be used as context in a subsequent query.', '145': 'If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.', '146': 'If the log backoff of wnf is also zero (it may not be in filtered models), then wf should be omitted from the state.', '147': 'This logic applies recursively: if wnf+1 similarly does not extend and has zero log backoff, it too should be omitted, terminating with a possibly empty context.', '148': 'We indicate whether a context with zero log backoff will extend using the sign bit: +0.0 for contexts that extend and −0.0 for contexts that do not extend.', '149': 'RandLM and SRILM also remove context that will not extend, but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.', '150': 'Section 4.1 explained that state s is stored by applications with partial hypotheses to determine when they can be recombined.', '151': 'In this section, we extend state to optimize left-to-right queries.', '152': 'All language model queries issued by machine translation decoders follow a left-to-right pattern, starting with either the begin of sentence token or null context for mid-sentence fragments.', '153': 'Storing state therefore becomes a time-space tradeoff; for example, we store state with partial hypotheses in Moses but not with each phrase.', '154': 'To optimize left-to-right queries, we extend state to store backoff information: where m is the minimal context from Section 4.1 and b is the backoff penalty.', '155': 'Because b is a function, no additional hypothesis splitting happens.', '156': 'As noted in Section 1, our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf , it visits wnn, wnn−1, ... , wnf .', '157': 'Each visited entry wni stores backoff b(wni ).', '158': 'These are written to the state s(wn1) and returned so that they can be used for the following query.', '159': 'Saving state allows our code to walk the data structure exactly once per query.', '160': 'Other packages walk their respective data structures once to find wnf and again to find {b(wn−1 i )}f−1 i=1if necessary.', '161': 'In both cases, SRILM walks its trie an additional time to minimize context as mentioned in Section 4.1.', '162': 'BerkeleyLM uses states to optimistically search for longer n-gram matches first and must perform twice as many random accesses to retrieve backoff information.', '163': 'Further, it needs extra pointers in the trie, increasing model size by 40%.', '164': 'This makes memory usage comparable to our PROBING model.', '165': 'The PROBING model can perform optimistic searches by jumping to any n-gram without needing state and without any additional memory.', '166': 'However, this optimistic search would not visit the entries necessary to store backoff information in the outgoing state.', '167': 'Though we do not directly compare state implementations, performance metrics in Table 1 indicate our overall method is faster.', '168': 'Only IRSTLM does not support threading.', '169': 'In our case multi-threading is trivial because our data structures are read-only and uncached.', '170': 'Memory mapping also allows the same model to be shared across processes on the same machine.', '171': 'Along with IRSTLM and TPT, our binary format is memory mapped, meaning the file and in-memory representation are the same.', '172': 'This is especially effective at reducing load time, since raw bytes are read directly to memory—or, as happens with repeatedly used models, are already in the disk cache.', '173': 'Lazy mapping reduces memory requirements by loading pages from disk only as necessary.', '174': 'However, lazy mapping is generally slow because queries against uncached pages must wait for the disk.', '175': 'This is especially bad with PROBING because it is based on hashing and performs random lookups, but it is not intended to be used in low-memory scenarios.', '176': 'TRIE uses less memory and has better locality.', '177': 'However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.', '178': 'TPT has theoretically better locality because it stores ngrams near their suffixes, thereby placing reads for a single query in the same or adjacent pages.', '179': 'We do not experiment with models larger than physical memory in this paper because TPT is unreleased, factors such as disk speed are hard to replicate, and in such situations we recommend switching to a more compact representation, such as RandLM.', '180': 'In all of our experiments, the binary file (whether mapped or, in the case of most other packages, interpreted) is loaded into the disk cache in advance so that lazy mapping will never fault to disk.', '181': 'This is similar to using the Linux MAP POPULATE flag that is our default loading mechanism.', '182': 'This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.', '183': 'Our test machine has two Intel Xeon E5410 processors totaling eight cores, 32 GB RAM, and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', '184': 'Sparse lookup is a key subproblem of language model queries.', '185': 'We compare three hash tables: our probing implementation, GCC’s hash set, and Boost’s8 unordered.', '186': 'For sorted lookup, we compare interpolation search, standard C++ binary search, and standard C++ set based on red-black trees.', '187': 'The data structure was populated with 64-bit integers sampled uniformly without replacement.', '188': 'For queries, we uniformly sampled 10 million hits and 10 million misses.', '189': 'The same numbers were used for each data structure.', '190': 'Time includes all queries but excludes random number generation and data structure population.', '191': 'Figure 2 shows timing results.', '192': 'For the PROBING implementation, hash table sizes are in the millions, so the most relevant values are on the right size of the graph, where linear probing wins.', '193': 'It also uses less memory, with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', '194': 'Further, the probing hash table does only one random lookup per query, explaining why it is faster on large data.', '195': 'Interpolation search has a more expensive pivot but performs less pivoting and reads, so it is slow on small data and faster on large data.', '196': 'This suggests a strategy: run interpolation search until the range narrows to 4096 or fewer entries, then switch to binary search.', '197': 'However, reads in the TRIE data structure are more expensive due to bit-level packing, so we found that it is faster to use interpolation search the entire time.', '198': 'Memory usage is the same as with binary search and lower than with set.', '199': 'For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.', '200': 'The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.', '201': 'Unlike Germann et al. (2009), we chose a model size so that all benchmarks fit comfortably in main memory.', '202': 'Benchmarks use the package’s binary format; our code is also the fastest at building a binary file.', '203': 'As noted in Section 4.4, disk cache state is controlled by reading the entire binary file before each test begins.', '204': 'For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.', '205': 'We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).', '206': 'Tokens were converted to vocabulary identifiers in advance and state was carried from each query to the next.', '207': 'Table 1 shows results of the benchmark.', '208': 'Compared to decoding, this task is cache-unfriendly in that repeated queries happen only as they naturally occur in text.', '209': 'Therefore, performance is more closely tied to the underlying data structure than to the cache.', '210': 'In fact, we found that enabling IRSTLM’s cache made it slightly slower, so results in Table 1 use IRSTLM without caching.', '211': 'Moses sets the cache size parameter to 50 so we did as well; the resulting cache size is 2.82 GB.', '212': 'The results in Table 1 show PROBING is 81% faster than TRIE, which is in turn 31% faster than the fastest baseline.', '213': 'Memory usage in PROBING is high, though SRILM is even larger, so where memory is of concern we recommend using TRIE, if it fits in memory.', '214': 'For even larger models, we recommend RandLM; the memory consumption of the cache is not expected to grow with model size, and it has been reported to scale well.', '215': 'Another option is the closedsource data structures from Sheffield (Guthrie and Hepple, 2010).', '216': 'Though we are not able to calculate their memory usage on our model, results reported in their paper suggest lower memory consumption than TRIE on large-scale models, at the expense of CPU time.', '217': 'This task measures how well each package performs in machine translation.', '218': 'We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.', '219': 'Based on revision 4041, we modified Moses to print process statistics before terminating.', '220': 'Process statistics are already collected by the kernel (and printing them has no meaningful impact on performance).', '221': 'SRILM’s compact variant has an incredibly expensive destructor, dwarfing the time it takes to perform translation, and so we also modified Moses to avoiding the destructor by calling exit instead of returning normally.', '222': 'Since our destructor is an efficient call to munmap, bypassing the destructor favors only other packages.', '223': 'The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.', '224': 'Time starts when Moses is launched and therefore includes model loading time.', '225': 'These conaUses lossy compression. bThe 8-bit quantized variant returned incorrect probabilities as explained in Section 3.', '226': 'It did 402 queries/ms using 1.80 GB. cMemory use increased during scoring due to batch processing (MIT) or caching (Rand).', '227': 'The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.', '228': 'Timing is based on plentiful memory.', '229': 'Then we ran binary search to determine the least amount of memory with which it would run.', '230': 'The first value reports resident size after loading; the second is the gap between post-loading resident memory and peak virtual memory.', '231': 'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.', '232': 'Memory usage is likely much lower than ours. fThe original paper (Germann et al., 2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.', '233': 'The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times, such as in parameter tuning.', '234': 'Table 2 shows single-threaded results, mostly for comparison to IRSTLM, and Table 3 shows multi-threaded results.', '235': 'Part of the gap between resident and virtual memory is due to the time at which data was collected.', '236': 'Statistics are printed before Moses exits and after parts of the decoder have been destroyed.', '237': 'Moses keeps language models and many other resources in static variables, so these are still resident in memory.', '238': 'Further, we report current resident memory and peak virtual memory because these are the most applicable statistics provided by the kernel.', '239': 'Overall, language modeling significantly impacts decoder performance.', '240': 'In line with perplexity results from Table 1, the PROBING model is the fastest followed by TRIE, and subsequently other packages.', '241': 'We incur some additional memory cost due to storing state in each hypothesis, though this is minimal compared with the size of the model itself.', '242': 'The TRIE model continues to use the least memory of ing (-P) with MAP POPULATE, the default.', '243': 'IRST is not threadsafe.', '244': 'Time for Moses itself to load, including loading the language model and phrase table, is included.', '245': 'Along with locking and background kernel operations such as prefaulting, this explains why wall time is not one-eighth that of the single-threaded case. aLossy compression with the same weights. bLossy compression with retuned weights. the non-lossy options.', '246': 'For RandLM and IRSTLM, the effect of caching can be seen on speed and memory usage.', '247': 'This is most severe with RandLM in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.', '248': 'As noted for the perplexity task, we do not expect cache to grow substantially with model size, so RandLM remains a low-memory option.', '249': 'Caching for IRSTLM is smaller at 0.09 GB resident memory, though it supports only a single thread.', '250': 'The BerkeleyLM direct-mapped cache is in principle faster than caches implemented by RandLM and by IRSTLM, so we may write a C++ equivalent implementation as future work.', '251': 'RandLM’s stupid backoff variant stores counts instead of probabilities and backoffs.', '252': 'It also does not prune, so comparing to our pruned model would be unfair.', '253': 'Using RandLM and the documented settings (8-bit values and 1 256 false-positive probability), we built a stupid backoff model on the same data as in Section 5.2.', '254': 'We used this data to build an unpruned ARPA file with IRSTLM’s improved-kneser-ney option and the default three pieces.', '255': 'Table 4 shows the results.', '256': 'We elected run Moses single-threaded to minimize the impact of RandLM’s cache on memory use.', '257': 'RandLM is the clear winner in RAM utilization, but is also slower and lower quality.', '258': 'However, the point of RandLM is to scale to even larger data, compensating for this loss in quality.', '259': 'There any many techniques for improving language model speed and reducing memory consumption.', '260': 'For speed, we plan to implement the direct-mapped cache from BerkeleyLM.', '261': 'Much could be done to further reduce memory consumption.', '262': 'Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.', '263': 'Quantization can be improved by jointly encoding probability and backoff.', '264': 'For even larger models, storing counts (Talbot and Osborne, 2007; Pauls and Klein, 2011; Guthrie and Hepple, 2010) is a possibility.', '265': 'Beyond optimizing the memory size of TRIE, there are alternative data structures such as those in Guthrie and Hepple (2010).', '266': 'Finally, other packages implement language model estimation while we are currently dependent on them to generate an ARPA file.', '267': 'While we have minimized forward-looking state in Section 4.1, machine translation systems could also benefit by minimizing backward-looking state.', '268': 'For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.', '269': 'If they knew that the first four words in a hypergraph node would never extend to the left and form a 5-gram, then three or even fewer words could be kept in the backward state.', '270': 'This information is readily available in TRIE where adjacent records with equal pointers indicate no further extension of context is possible.', '271': 'Exposing this information to the decoder will lead to better hypothesis recombination.', '272': 'Generalizing state minimization, the model could also provide explicit bounds on probability for both backward and forward extension.', '273': 'This would result in better rest cost estimation and better pruning.10 In general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement.', '274': 'We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.', '275': 'The PROBING model is 2.4 times as fast as the fastest alternative, SRILM, and uses less memory too.', '276': 'The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM.', '277': 'These performance gains transfer to improved system runtime performance; though we focused on Moses, our code is the best lossless option with cdec and Joshua.', '278': 'We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.', '279': 'The code is opensource, has minimal dependencies, and offers both C++ and Java interfaces for integration.', '280': 'Alon Lavie advised on this work.', '281': 'Hieu Hoang named the code “KenLM” and assisted with Moses along with Barry Haddow.', '282': 'Adam Pauls provided a pre-release comparison to BerkeleyLM and an initial Java interface.', '283': 'Nicola Bertoldi and Marcello Federico assisted with IRSTLM.', '284': 'Chris Dyer integrated the code into cdec.', '285': 'Juri Ganitkevitch answered questions about Joshua.', '286': 'This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No.', '287': '0750271 and by the DARPA GALE program.'}","['W11-2123_swastika', 'W11-2123_aakansha', 'W11-2123_vardha']","['../data/summaries/W11-2123_swastika.txt', '../data/summaries/W11-2123_aakansha.txt', '../data/summaries/W11-2123_vardha.txt']","['../data/tba/W11-2123_swastika.json', '../data/tba/W11-2123_aakansha.json', '../data/tba/W11-2123_vardha.json']"
W95-0104,A Bayesian hybrid method for context-sensitive spelling correction,../data/papers/W95-0104.xml,"{'0': 'A Bayesian hybrid method for context-sensitive spelling correction', '1': 'Two classes of methods have been shown to be useful for resolving lexical ambiguity.', '2': 'The first relies on the presence of particular words within some distance of the ambiguous target word; the second uses the pattern of words and part-of-speech tags around the target word.', '3': 'These methods have complementary coverage: the former captures the lexical ""atmosphere"" (discourse topic, tense, etc.), while the latter captures local syntax.', '4': 'Yarowsky has exploited this complementarity by combining the two methods using decision lists.', '5': 'The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.', '6': ""This paper takes Yarowsky's work as a starting point, applying decision lists to the problem of context-sensitive spelling correction."", '7': 'Decision lists are found, by and large, to outperform either component method.', '8': 'However, it is found that further improvements can be obtained by taking into account not just the single strongest piece of evidence, but all the available evidence.', '9': 'A new hybrid method, based on Bayesian classifiers, is presented for doing this, and its performance improvements are demonstrated.', '10': 'Two classes of methods have been shown useful for resolving lexical ambiguity.', '11': 'The first tests for the presence of particular context words within a certain distance of the ambiguous target word.', '12': 'The second tests for collocations - patterns of words and part-of-speech tags around the target word.', '13': 'The context-word and collocation methods have complementary coverage: the former captures the lexical ""atmosphere"" (discourse topic, tense, etc.), while the latter captures local syntax.', '14': 'Yarowsky [1994] has exploited this complementarity by combining the two methods using decision lists.', '15': 'The idea is to pool the evidence provided by the component methods, and to then solve a target problem by applying the single strongest piece of evidence, whatever type it happens to be.', '16': 'Yarowsky applied his method to the task of restoring missing accents in Spanish and French, and found that it outperformed both the method based on context words, and one based on local syntax.', '17': ""This paper takes Yarowsky's method as a starting point, and hypothesizes that further improvements can be obtained by taking into account not only the single strongest piece of evidence, but all the available evidence."", '18': 'A method is presented for doing this, based on Bayesian classifiers.', '19': 'The work reported here was applied not to accent restoration, but to a related lexical disamÂ\xad biguation task: context-sensitive spelling correction.', '20': ""The task is to fix spelling errors that happen to result in valid words in the lexicon; for example: I'd like the chocolate cake for *desert."", '21': 'where dessert was misspelled as desert.', '22': 'This goes beyond the capabilities of conventional spell checkers, which can only detect errors that result in non-words.', '23': 'We start by applying a very simple method to the task, to serve as a baseline for comparison with the other methods.', '24': '\\Ve then apply each of the two component methods mentioned aboveÂ\xad context words and collocations.', '25': '\\Ve try two ways of combining these components: decision lists, and Bayesian classifiers.', '26': '\\Ve evaluate the above methods by comparing them with an alternative approach to spelling correction based on part-of-speech trigrams.', '27': 'The sections below discuss the task of context-sensitive spelling correction, the five methods we tried for the task (baseline, two component methods, and two hybrid methods), and the evaluation.', '28': 'The final section draws some conclusions.', '29': 'Context-sensitive spelling correction is the problem of correcting spelling errors that result in valid words in the lexicon.', '30': 'Such errors can arise for a variety of reasons, including typos (e.g., out for our), homonym confusions (there for their), and usage errors (between for among).', '31': 'These errors are not detected by conventional spell checkers, as they only notice errors resulting in non-words.', '32': '\\Ve treat context-sensitive spelling correction as a task of word disambiguation.', '33': 'The ambiguity among words is modelled by confusion sets.', '34': 'A confusion set C = { w 1, ...,wn} means that each word Wi in the set is ambiguous with each other word in the set.', '35': 'Thus if C = {deserÂ·t, desserÂ·t}, then when the spelling-correction program sees an occurrence of either desert or dessert in the target document, it takes it to be ambiguous between desert and dessert, and tries to infer from the context which of the two it should be.', '36': 'This treatment requires a collection of confusion sets to start with.', '37': 'There are several ways to obtain such a collection.', '38': 'One is based on finding words in the dictionary that are one typo away from each other [Mays et al., 1991).1 Another finds words that have the same or similar pronunciations.', '39': 'Since this was not the focus of the work reported here, we simply took (most of) our confusion sets from the list of ""\\Vords Commonly Confused"" in the back of the Random House unabridged dictionary [Fiexner, 1983].', '40': 'A final point concerns the two types of errors a spelling-correction program can make: false negatives (complaining about a correct word), and false positives (failing to notice an error).', '41': 'We will make the simplifying assumption that both kinds of errors are equally bad.', '42': 'In practice, however, false negatives are much worse, as users get irritated by programs that badger them with bogus complaints.', '43': 'However, given the probabilistic nature of the methods that will be presented below, it would not be hard to modify them to take this into account.', '44': ""We would merely set a confidence threshold, and report a suggested correction only if the probability of the suggested word exceeds the probability of the user's original spelling by at least the threshold amount."", '45': 'The reason this was not done in the work reported here is that setting this confidence threshold involves a certain subjective factor (which depends on the user\'s ""irritability threshold"").', '46': 'Our simplifying assumption allows us to measure performance objectively, by the single parameter of prediction accuracy.', '47': '1Constructing confusion sets in this way requires assigning each word in the lexicon its own confusion set.', '48': 'For instance, cat might have the confusion set {lwf,carÂ·, ...', '49': '}, hat might have {cat,had, ...', '50': '}, and so on.', '51': 'We cannot use the symmetric confusion sets that we have adopted - where every word in the set is confusable with every other one - because the ""confusable"" relation is no longer transitive.', '52': 'This section presents a. progression of five methods for context-sensitive spelling correction: Baseline An indicator of ""minimal competency"" for comparison with the other methods Context words Tests for particular words within Â±k words of the ambiguous target word Collocations Tests for syntactic patterns around the ambiguous target word Decision lists Combines context words and collocations via.', '53': 'decision lists Bayesian classifiers Combines context words and collocations via Bayesian classifiers.', '54': 'Each method will be described in terms of its operation on a single confusion set C = {Wt, ...', '55': ', wn}; that is, we will say how the method disambiguates occurrences of words w1 through Wn from the context.', '56': 'The methods handle multiple confusion sets by applying the same technique to each confusion set independently.', '57': 'Each method involves a training phase and a test phase.', '58': 'The performance figures given below are based on training each method on the 1-million-word Brown corpus [Kucera.', '59': 'and Francis, 1967] and testing it on a 3/4-million-word corpus of Wall Street Journal text [Marcus et al., 1993].', '60': '3.1 Baseline method.', '61': 'The baseline method disambiguates words w1 through Wn by simply ignoring the context, and always guessing that the word should be whichever Wi occurred most often in the training corpus.', '62': 'For instance, if C = {desert, rlessert}, and rlesert occurred more often than dessert in the training corpus, then the method will predict that every occurrence of desert or dessert in the test corpus should be changed to (oÂ·r left as) desert.', '63': 'Table 1 shows the performance of the baseline method for 18 confusion sets.', '64': 'This collection of confusion sets will be used for evaluating the methods throughout the paper.', '65': 'Each line of the table gives the results for one confusion set: the words in the confusion set; the number of instances of any word in the confusion set in the training corpus and in the test corpus; the word in the confusion set that occurred most often in the training corpus; and the prediction accuracy of the baseline method for the test corpus.', '66': 'Prediction accuracy is the number of times the correct word was predicted, divided by the total number of test cases.', '67': 'For example, the members of the confusion set {I, me} occurred 840 times in the test corpus, the breakdown being 744 I and 96 me. The baseline method predicted I every time, and thus was right 744 times, for a score of 744/840 = 0.886.', '68': 'Essentially the baseline method measures how accurately one can predict words using just their prior probabilities.', '69': 'This provides a lower bound on the performance we would expect from the other methods, which use more than just the priors.', '70': '3.2 Component method 1: Context words.', '71': 'One clue about the identity of an ambiguous target word comes from the words around it.', '72': 'For instance, if the target word is ambiguous between desert and dessert, and we see words like arid, sand, and sun nearby, this suggests that the target word should be desert.', '73': 'On the other hand, words such as chocolate and delicious in the context imply desserÂ·t. This observation is the basis for the method of context words.', '74': ""The idea is that each word Wi in the confusion set will have a characteristic distribution of words that occur in its context; thus to classify a.n ambiguous target word, we look at the set of words around it and see which w; 's distribution they most closely follow."", '75': 'C on fu si on se t No.', '76': 'of No.', '77': 'of t r a i n i n g t e s t c a s e s c a s e s M os t Baseline f r e q u e n t w o r d w h et h e r, w e at h er 3 3 1 2 4 5 I, m e 61 2.', '78': ""5 84 0 its , it' s 19 .5 1 3."", '79': ""57 5 p as t, pa ss ed 38 .5 39 7 th a n, th en 29 49 16 59 be in g, be gi n 72 7 44 9 ef fe ct, af fe ct 22 8 16 2 yo ur , yo u'r e 10 47 21 2 n u m be r, a m o u nt 58 8 42 9 co un cil , co un se l 82 8 3 ris e, rai se 13 9 30 1 be t w ee n, a m on g 10 03 73 0 le d, le ad 22 6 21 9 ex ce pt , ac ce pt 23 2 95 pe ac e, pi ec e 31 0 6 1 th er e, th ei r, th e y' re 50 26 21 87 pr in ci pl e, pr in ci pa l 18 4 69 si gh t, sit e, cit e 14 9 44 w h e t h e r 0 . 9 2 2 I 0 . 8 8 6 i t s 0 . 8 6 3 p a s t 0 . 8 6 1 t h a n 0 . 8 0 7 b e i n g 0 . 7 8 0 e f f e c t 0 . 7 4 1 y o u r 0 . 7 2 6 n u m b e r 0 . 6 2 7 c o u n c i l 0 . 6 1 4 n s e 0 . 5 7 5 b e t w e e n 0 . 5 3 8 l e d 0 . 5 3 0 e x c e p t 0 . 4 4 2 p e a c e 0 . 3 9 3 t h e r e 0 . 3 0 6 p r i n c i p l e 0 . 2 9 0 s i g h t 0 . 1 1 4 Table 1: Performance of the baseline method for 18 confusion sets."", '80': 'The ""Most frequent word"" column gives the word in the confusion set that occurred most frequently in the training corpus.', '81': '(In subsequent tables, confusion sets will be referred to by their most frequent word.)', '82': 'The ""Baseline"" column gives the prediction accuracy of the baseline system on the test corpus.', '83': 'Following previous work [Gale et al., 1994], we formulate the method in a Bayesian framework.', '84': 'The task is to pick the word Wi that is most probable, given the context words Cj observed within a Â±k-word window of the target word.', '85': ""The probability for each Wi is calculated using Bayes' rule: As it stands, the likelihood term, p( c_k."", '86': ', c_ 1, c1, ...', '87': ', cklwi), is difficult to estimate from training data - we would have to count situations in which the entire context was previously observed around word Wi, which raises a. severe sparse-data problem.', '88': 'Instead, therefore, we assume that the presence of one word in the context is independent of the presence of any other word.', '89': 'This lets us decompose the likelihood into a product: II jE-k,...,-l,l,...,k Gale et al. [1994] provide evidence that this is in fact a reasonable approximation.', '90': 'We still have the problem, however, of estimating the individual p(cilwi) probabilities from our training corpus.', '91': 'The straightforward way would be to use a. maximum likelihood estimate - we would count Af;, the total number of occurrences of w; in the training corpus, and m;, the number of such occurrences for which Cj occurred within Â±k words, and we would then take the ratio miflvf;.2 Unfortunately, we may not have enough training data to get an accurate estimate this way.', '92': 'Gale et al. [1994] address this problem by interpolating between two maximum-likelihood estimates: one of p(cjlw;), and one of p(cj).', '93': 'The former measures the desired quantity, but is subject to inaccuracy due to sparse data; the latter provides a robust estimate, but of a potentially irrelevant quantity.', '94': 'Gale et al. interpolate between the two so as to minimize the overall inaccuracy.', '95': 'We have pursued an alternative approach to the problem of estimating the likelihood terms.', '96': 'We start with the observation that there is no need to use every word in the Â±k-word window to discriminate among the words in the confusion set.', '97': 'If we do not have enough training data for a given word c to accurately estimate p(ciw;) for all w;, then we simply disregard c, and base our discrimination on other, more reliable evidence.', '98': 'We implement this by introducing a ""minimum occurrences"" threshold, Tmin.', '99': 'It is currently set to 10.', '100': 'We then ignore a context word c if: L m; < Tmin or L (Af;- m;) < Tmin l i n l5i n where m; and A{ are defined as above.', '101': 'In other words, c is ignored if it practically never occurs within the context of any w;, or if it practically always occurs within the context of every w;.', '102': 'In the former case, we have insufficient data to measure its presence; in the latter, its absence.', '103': 'Besides the reason of insufficient data, a second reason to ignore a context word is if it does not help discriminate among the words in the confusion set.', '104': 'For instance, if we are trying to decide between I and me, then the presence of the in the context probably does not help.', '105': 'By ignoring such words, we eliminate a source of noise in our discrimination procedure, as well as reducing storage requirements and run time.', '106': 'To determine whether a context word cis a useful discriminator, we run a chi-square test [Fleiss, 1981] to check for an association between the presence of c and the choice of word in the confusion set.', '107': 'If the observed association is not judged to be significant,3 then c is discarded.', '108': 'The significance level is currently set to 0.05.', '109': 'Figure 1 pulls together the points of the preceding discussion into an outline of the method of context words.', '110': 'In the training phase, it identifies a list of context words that are useful for discriminating among the words in the confusion set.', '111': 'At run time, it estimates the probability of each word in the confusion set.', '112': 'It starts with the prior probabilities, and multiplies them by the likelihood of each context word from its list that appears in the Â±k-word window of the target word.', '113': 'Finally, it selects the word in the confusion set with the greatest probability.', '114': 'The main parameter to tune for the method of context words is k, the half-width of the context window.', '115': 'Previous work [Yarowsky, 1994] shows that sma.ller values of k (3 or 4) work well for resolving local syntactic ambiguities, while larger values (20 to 50) are suitable for resolving semantic ambiguities.', '116': '\\Ve tried the values 3, 6, 12, and 24 on some practice confusion sets (not shown here), and found that k = 3 generally did best, indicating that most of the action, for our task and confusion sets, comes from local syntax.', '117': 'In the rest of this paper, this value of k will be used.', '118': '2 We are interpreting the condition ""c. occurs within a Â±k-word window of w;"" as a binary feature - either it happens, or it does not.', '119': 'This allows us to handle context words in the same Bayesian framework as will be used later for other binary features (see Section 3.3).', '120': 'A more conventional interpretation is to take into account the number of occurrences of each Cj within the Â±k-word window, and to estimate p(cilw;) accordingly.', '121': 'However, either interpretation is valid, as long as it is applied consistently - that is, both when estimating the likelihoods from training data, and when classifying test cases.', '122': '3 An association is significant if the probability that it occurred by chance is low.', '123': 'This is not a statement about.', '124': 'the strength of the association.', '125': 'Even a wea.k association may be judged significant if there are enough da.ta to support it.', '126': 'Measures of the strength of association will be discussed in Section 3.4.', '127': 'Training phase (1) Propose all words a.s candidate context words.', '128': '(2) Count occurrences of each candidate context word in the training corpus.', '129': '(3) Prune context words that have insufficient data or are uninformative discriminators.', '130': '(4) Store the remaining context words (and their associated statistics) for use at run time.', '131': 'Run time (1) Initialize the probability for each word in the confusion set to its prior probability.', '132': '(2) Go through the list of context words that was saved during training.', '133': 'For each context word that appears in the context of the ambiguous target word, update the probabilities.', '134': '(3) Choose the word in the confusion set with the highest probability.', '135': 'Figure 1: Outline of the method of context words.', '136': 'Table 2 shows the effect of varying k for our usual collection of confusion sets.', '137': 'It can be seen that performance generally degrades as k increases.', '138': 'The reason is that the method starts picking up spurious correlations in the training corpus.', '139': 'Table 4 gives some examples of the context words learned for the confusion set {peace, piece}, with k = 24.', '140': 'The context words co1Â·ps, united, nations, etc., all imply peace, and appear to be plausible (although united and nations are a counterexample to our earlier assumption of independence).', '141': 'On the other hand, consider the context word how, which allegedly also implies peace.', '142': 'If we look back at the training corpus for the supporting data for this word, we find excerpts such a.s: But oh, how I do sometimes need just a moment of rest, and peace ..', '143': 'No ma.tter how earnest is our quest for guaranteed peace ..', '144': 'How best to destroy your peace ? There does not seem to be a necessary connection here between how and peace; the correlation is probably spurious.', '145': 'Although we are using a chi-square test expressly to filter out such spurious correlations, we can only expect the test to catch 95% of them (given that the significance level was set to 0.05).', '146': 'As mentioned above, most of the legitimate context words show up for small k; thus as k gets large, the limited number of legitimate context words gets overwhelmed by the 5% of the spurious correlations that make it through our filter.', '147': '3.3 Component method 2: Collocations.', '148': 'The method of context words is good at capturing generalities that depend on the presence of nearby words, but not their order.', '149': '\\Vhen order matters, other more syntax-based methods, such as collocations and trigrams, are appropriate.', '150': 'In the work reported here, the method of collocations was used to capture order dependencies.', '151': 'A collocation expresses a pattern of syntactic elements around the target word.', '152': 'We allow two types of syntactic elements: words, and part-of-speech tags.', '153': 'Going back to the {desert, dessert} example, a collocation that would imply desert might be: PREP the C on fu si on se t B a s e l i n e C w or ds Cwords Cwords Cwords Â± 3 Â± 6 Â± 1 2 Â± 2 4 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e pr in ci pl e si gh t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . . 5 7 5 0 . 5 3 8 0 . 5 3 0 0 . 4 4 2 0 . 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.922 0.927 0.922 0 . 9 1 0 . 8 6 2 0.795 0.743 0.702 0 . 8 6 1 0.849 0.801 0.743 0 . 9 3 1 0.901 0.896 0.855 0 . 7 9 1 0.795 0.793 0.755 0 . 7 4 7 0.741 0.759 0.716 0 . 8 1 6 0.783 0.774 0.736 0 . 6 4 6 0.622 0.636 0.639 0 . 6 3 9 0.614 0.602 0.614 0 ..', '154': '5 7 5 0.575 0.585 0.498 0 . 7 5 9 0.697 0.671 0.586 0 . 5 3 0 0.530 0.521 0.557 0 . 6 9 5 0.526 0.516 0.558 0 . 7 5 4 0.705 0..574 0.574 0 . 7 2 6 0.623 0.557 0.466 0 . 2 9 0 0.290 0.290 0.435 0 . 4 5 5 0.2.50 0.364 0.318 A vg no.', '155': 'of contex t words 2 7 . 9 36.9 55.9 92.9 Table 2: Performance of the method of context words as a function of k, the half-width of the context window.', '156': 'The bottom line of the table shows the number of context words learned, averaged over all confusion sets, also as a function of k. This collocation would match the sentences: Travelers entering from the desert were confounded ...', '157': 'along with some guerrilla fighting in the desert.', '158': 'two ladies who lay pinkly nude beside him in the desert Matching part-of-speech tags (here, PREP) against the sentence is done by first tagging each word in the sentence with its set of possible part-of-speech tags, obtained from a dictionary.', '159': ""For instance, walk has the ta.g set {Ns, v}, corresponding to its use as a singular noun and as a verb.4 For a tag to match a word, the ta.g must be a member of the word's tag set."", '160': 'The reason we use tag sets, instead of running a tagger on the sentence to produce unique tags, is that taggers need to look at all words in the sentence, which is impossible when the target word is taken to be ambiguous (but see the trigram method in Section 4 ).', '161': 'The method of collocations was implemented in much the same way as the method of context words.', '162': 'The idea is to discriminate among the words Wi in the confusion set by identifying the collocations that tend to occur around each w;.', '163': 'An ambiguous target word is then classified by finding all collocations that match its context.', '164': ""Each collocation provides some degree of evidence 4 0ur tag inventory contains 40 tags, and includes the usual categories for determiners, nouns, verbs, modals, etc., a few specialized tags (for be, have, and do), and a dozen compound tags (such as V+PRO for let's)."", '165': '45 for each word in the confusion set.', '166': ""This evidence is combined using Bayes' rule."", '167': 'In the end, the Wi with the highest probability, given the evidence, is selected.', '168': 'A new complication arises for collocations, however, in that collocations, unlike context words, cannot be assumed independent.', '169': 'Consider, for example, the following collocations for desert: PREP the in the the These collocations are highly interdependent- we will say they conflict.', '170': 'To deal with this problem, we invoke our earlier observation that there is no need to use all the evidence.', '171': 'If two pieces of evidence conflict, we simply eliminate one of them, and base our decision on the rest of the evidence.', '172': 'We identify conflicts by the heuristic that two collocations conflict iff they overlap.', '173': 'The overlapping portion is the factor they have in common, and thus represents their lack of independence.', '174': 'This is only a heuristic because we could imagine collocations that do not overlap, but still conflict.', '175': 'Note, incidentally, that there can be at most two non-conflicting collocations for any decision - one matching on the left-hand side of the target word, and one on the right.', '176': 'Having said that we resolve conflicts between two collocations by eliminating one of them, we still need to specify which one.', '177': 'Our approach is to assign each one a strength, just as Yarowsky [1994] does in his hybrid method, and to eliminate the one with the lower strength.', '178': 'This preserves the strongest non-conflicting evidence as the basis for our answer.', '179': 'The strength of a collocation reflects its reliability for decision-making; a further discussion of strength is deferred to Section 3.4.', '180': 'Figure 2 ties together the preceding discussion into an outline of the method of collocations.', '181': 'The method is described in terms of ""features"" rather than ""collocations"" to reflect its full generality; the features could be context words a.s well as collocations.', '182': 'In fact, the method subsumes the method of context words -it does everything that method does, and resolves conflicts among its features as well.', '183': 'To facilitate the conflict resolution, it sorts the features by decreasing strength.', '184': 'Like the method of context words, the method of collocations has one main parameter to tune: f, the maximum number of syntactic elements in a collocation.', '185': 'Since the number of collocations grows exponentially with e, it was only practical to vary f from 1 to 3.', '186': 'We tried this on some practice confusion sets, and found that a.ll values ofÂ£ gave roughly comparable performance.', '187': 'We selected f = 2 to use from here on, as a compromise between reducing the expressive power of collocations (with e = 1) and incurring a high computational cost (with e = 3).', '188': 'Table 3 shows the results of varyingÂ£ for the usual confusion sets.', '189': 'There is no clear winner; each value ofÂ£ did best for certain confusion sets.', '190': 'Table 5 gives examples of the collocations learned for {peace, piece} withÂ£= 2.', '191': 'A good deal of redundancy can be seen among the collocations.', '192': 'There is also some redundancy between the collocations and the context words of the previous section (e.g., for corps).', '193': 'Many of the collocations a.t the end of the list appear to be overgeneral and irrelevant.', '194': '3.4 Hybrid method 1: Decision lists.', '195': 'Yarowsky [1994] pointed out the complementarity between context words and collocations: context words pick up those generalities that are best expressed in an order-independent way, while colloÂ\xad cations capture drder-dependent generalities.', '196': 'Ya.rowsky proposed decision lists as a way to get the best of both methods.', '197': 'The idea is to make one big list of all features - in this case, context words and collocations.', '198': 'The features are sorted in order of decreasing strength, where the strength of a feature reflects its reliability for decision-making.', '199': 'An ambiguous target word is then classified by running down the list and matching each feature against the target context.', '200': 'The first feature that 46 Training phase (1) (2) (3) (3.5) (4) Propose all possible features as candidat e features.', '201': 'Count occurren ces of each candidat e feature in the training corpus.', '202': 'P r u n e f e a t u r e s t h a t h a v e i n s u f f i c i e n t d a t a . o r a r e u n i n f o r m a t i v e d i s c r i m i n a t o r s . S o r t t h e r e m a i n i n g f e a t u r e s i n o r d e r o f d e c r e a s i n g s t r e n g t h . Store the list of features (and their associat ed statistics ) for use at run time.', '203': 'Run time (1) Initialize the probability for each word in the confusion set to its prior probability.', '204': '(2) Go through the sorted list of features that was saved during training.', '205': 'For each feature that matches the context of the ambiguous target word, and does not conflict with a feature accepted previously, update the probabilities.', '206': '(3) Choose the word in the confusion set with the highest probability.', '207': 'Figure 2: Outline of the method of collocations.', '208': 'Differences from the method of context words are highlighted in boldface.', '209': 'The method is described in terms of ""features"" rather than ""collocations"" to reflect its full generality.', '210': 'matches is used to classify the target word.', '211': 'Yarowsky [1994] describes further refinements, such as detecting and pruning features that make a zero or negative contribution to overall performance.', '212': 'The method of decision lists, as just described, is almost the same as the method for collocations in Figure 2, where we take ""features"" in that figure to include both context words and collocations.', '213': 'The main difference is that during evidence gathering (step (2) at run time), decision lists terminate after matching the first feature.', '214': 'This obviates the need for resolving conflicts between features.', '215': 'Given that decision lists base their answer for a problem on the single strongest feature, their performance rests heavily on how the strength of a feature is defined.', '216': 'Yarowsky [1994] used the following metric to calculate the strength of a feature f: reliability(!)', '217': '= abs (log( ::: D) This is for the case of a confusion set of two words, w1 and w2.', '218': ""It can be shown that this metric produces the identical ranking of features as the following somewhat simpler metric, provided p( w;IJ) > 0 for all i:5 reliability'(!)"", '219': ""= max: p( w;if) ' As an example of using the metric, suppose f is the context word arid, and suppose that arid coÂ\xad occurs 10 times with desert and 1 time with dessert in the training corpus."", '220': ""Then reliability'(!)"", '221': '== max(10/11, 1/11) = 10/11 = 0.909.', '222': 'This value measures the extent to which the presence of the feature is unambiguously correlated with one particular w;.', '223': ""It can be thought of as the feature's reliability a.t picking out that w; from the others in the confusion set."", '224': '5Jn fact, we guarantee that this inequalit.y holds by performing smoothing before calculating strength.', '225': 'We smooth the data by adding 1 to the count.', '226': 'of how many times each feature was observed for each w;.', '227': '47 C o nf us io n se t B a s e l i n e C o H oe s CoHoes CoHoes : : : ; 1 : S 2 : S 3 w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l r i s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 . 5 0 . 5 3 8 0 . . 5 3 0 0 . 4 4 2 0 . : 3 9 3 0 . 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 3 9 0.931 0.931 0 . 9 7 9 0.981 0.980 0 . 9 4 3 0.945 0.950 0 . 9 1 9 0.909 0.909 0 . 9 6 6 0.965 0.966 0 . 8 5 3 0.853 0.842 0 . 8 2 1 0.821 0.821 0 . 8 7 7 0.887 0.887 0 . 6 4 6 0.646 0.681 0 . 6 6 3 0.639 0.639 0 . 8 0 7 0.807 0.807 0 . 6 9 9 0.730 0.733 0 . 8 4 9 0.840 0.863 0 . 8 0 0 0.789 0.789 0 . 8 6 9 0.869 0.852 0 . 9 1 1 0.932 0.932 0 . 8 4 1 0.812 0.812 0 . 3 4 1 0.318 0.318 A vg no.', '228': 'of colloc ations 3 3 . 9 263.1 98.5.4 Table 3: Performance of the method of collocations as a function of f, the maximum length of a collocation.', '229': 'The bottom line of the table shows the number of collocations learned, averaged over all confusion sets, also as a function of e. One peculiar property of the reliability metric is that it ignores the prior probabilities of the words in the confusion set.', '230': ""For instance, in the arid example, it would award the same high score even if the total number of occurrences of desert and dessert in the training corpus were 50 and 5, respectively - in which case arid's performance of 10/11 would be exactly what one would expect by chance, and therefore hardly impressive."", '231': 'Besides the reliability metric, therefore, we also considered an alternative metric: the uncertainty coefficient of x, denoted U(xiy) [Press et al., 1988, p..501].', '232': 'U(xiy) measures how much additional information we get about the presence of the feature by knowing the choice of word in the confusion set.6 U(xiy) is calculated as follows: U(xiy) H(x) H(xiy) H ( x ) H ( x i y ) H ( x )-p(f)lnp(f)- p( .J)lnp(-.J) - Lp(w;) (p(flw;)lnp(flw;) + p( .Jiw;)lnp(â\x80¢flw;)) The probabilities are ca.lculated for the population consisting of all occurrences in the training corpus of any w;.', '233': 'For instance, p(f) is the probability of feature f being present within this 6 This definition may seem backwards, but.', '234': 'is appropriate for use on the right-hand side of Bayes\' rule, where the choice of word in the confusion set is t.he ""given"".', '235': '48 C on te xt w or d pe ac e pzece co rp s p e a c e u n i t e d n a t i o n s o u r h e a r t j u s t i c e s t a t e a m e r i c a n a i d i n t e r n a t i o n a l w o m e n w a r w o r l d p i e c e o v e r m u s t g r e a t u n d e r h o w 4 9 1 4 1 1 2 0 0 1 5 0 2 7 1 1 2 0 1 2 0 1 2 0 1 1 0 1 1 0 1 1 0 1 0 0 2 0 1 4 0 3 1 1 . 5 1 1 4 1 1 1 1 1 1 1 0 1 1 0 1 t w o f o r a b o u t e v e r y l i t t l e l o n g o n e t h e so 5 1 2 8 3 3 8 4 9 4 9 5 1 0 6 1 1 1 4 2 3 1 7 9 113 9 1 4 1 6 2 2 To tal oc cu rr en ce s 1 8 4 126 Table 4: Excerpts from the list of 43 context words learned for {peace, piece} with k = 24.', '236': 'Each line gives a context word, and the numÂ\xad ber of peace and piece occurrences for which that context word occurred within Â±k words.', '237': 'The last line of the table gives the total numÂ\xad ber of occurrences of peace and piece in the training corpus.', '238': 'Table 5: Excerpts from the sorted list of 98 collocations learned for {peace, piece} with Â£ = 2.', '239': 'Each line gives a collocation, and the number of peace and piece occurrences it matched.', '240': 'The last line of the table gives the total number of occurrences of peace and piece in the training corpus.', '241': '49 population.', '242': 'Applying the U(xjy) metric to the arid example, the value returned now depends on the number of occurrences of deserÂ·t and dessert in the training corpus.', '243': 'If these numbers are 50 and 5, then U(xly) = 0.0, reflecting the uninformativeness of the arid feature in this situation.', '244': ""If instead the numbers are 50 and 500, then U(:rjy) = 0.402, indicating arid's better-than-chance ability to pick out desrrt (10 out of 50 occurrences) over dessert (1 out of 500 occurrences)."", '245': 'To compare the two strength metrics, we tried both on some practice confusion sets.', '246': 'Sometimes one metric did substantially better, sometimes the other.', '247': 'In the balance, the reliability metric seemed to give higher performance.', '248': 'This metric is therefore the one that will be used from here on.', '249': 'It was also used for all experiments involving the method of collocations.', '250': 'Table 6 shows the performance of decision lists with each metric for the usual confusion sets.', '251': 'As with the practice confusion sets, we see sometimes dramatic performance differences between the two metrics, and no clear winner.', '252': 'For instance, for {I, me}, the reliability metric did better than U(xjy) (0.980 versus 0.808); whereas for {between, among}, it did worse (0.659 versus 0.800).', '253': 'Further research is needed to understand the circumstances under which each metric performs best.', '254': 'Focusing for now on the reliability metric, Table 6 shows that the method of decision lists does, by and large, accomplish what it set out to do - namely, outperform either component method alone.', '255': 'There are, however, a. few cases where it falls short; for instance, for {between, among}, decision lists score only 0.6.59, compared with 0.759 for context words and 0.730 for collocations.7 We believe that the problem lies in the strength metric: because decision lists make their judgements based on a single piece of evidence, their performance is very sensitive to the metric used to select that piece of evidence.', '256': 'But as the reliability and U(xjy) metrics indicate, it is not completely clear how the metric should be defined.', '257': 'This problem is addressed in the next section.', '258': '3.5 Hybrid method 2: Bayesian classifiers.', '259': 'The previous section confirmed that decision lists are effective at combining two complementary methods- context words and collocations.', '260': 'In doing the combination, however, decision lists look only at the single strongest piece of evidence for a given problem.', '261': '\\Ve hypothesize that even better performance can be obtained by taking into account all available evidence.', '262': 'This section presents a method of doing this based on Bayesian classifiers.', '263': 'Like decision lists, the Bayesian method starts with a list of all features, sorted by decreasing strength.', '264': 'It classifies an ambiguous target word by matching each feature in the list in turn against the target context.', '265': 'Instead of stopping at the first matching feature, however, it traverses the entire list, combining evidence from all matching features, and resolving conflicts where necessary.', '266': 'This method is essentially the same as the one for collocations (see Figure 2), except that it uses context words as well as collocations for the features.', '267': 'The only new wrinkle is in checking for conflicts between features (in step (2) at run time), as there are now two kinds of features to consider.', '268': 'If both features are context words, we say the features never conflict (as in the method of context words).', '269': 'If both features are collocations, we say they conflict iff they overlap (as in the method of collocations).', '270': 'The new case is if one feature is a context word, and the other is a collocation.', '271': ""Consider, for example, the context word walk, and the following collocations: (1) (2) (3) CONJ walk v PREP 7 1Â£ we use the U (xiy) metric instead, then d cision lists fall down on different examples; e.g., {its, it's}."", '272': '50 C on fu si on se t Ba sel in e Cwords Collocs Â± 3 2 Dl ist Dlist R el y U(xiy) w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0.902 0.931 0 . 8 8 6 0.914.', '273': '0.981 0 . 8 6 3 0.862 0.945 0 . 8 6 1 0.861 0.909 0 . 8 0 7 0.931 0.965 0 . 7 8 0 0.791 0.853 0 . 7 4 1 0.747 0.821 0 . 7 2 6 0.816 0.887 0 . 6 2 7 0.646 0.646 0 . 6 1 4 0.639 0.639 0 . 5 7 . 5 0..575 0.807 0 . 5 3 8 0.759 0.730 0 . 5 3 0 0.530 0.840 0 . 4 4 2 0.695 0.789 0 . 3 9 3 0.754 0.869 0 . 3 0 6 0.726 0.932 0 . 2 9 0 0.290 0.812 0 . 1 1 4 0.455 0.318 0.', '274': '93 5 0.829 0.', '275': '98 0 0.808 0.', '276': '93 1 0.805 0.', '277': '93 2 0.892 0.', '278': '96 7 0.961 0.', '279': '84 2 0.933 0.', '280': '82 1 0.654 0.', '281': '86 8 0.896 0.', '282': '62 9 0.667 0.', '283': '62 7 0.651 0.', '284': '80', '285': '0.', '286': '65 9 0.800 0.', '287': '84 0 0.840 0.', '288': '78 9 0.726 0.', '289': '85 2 0.836 0.', '290': '91 4 0.906 0.', '291': '81 2 0.841 0.', '292': '43 2 0.568 Table 6: Performance of decision lists with the reliability and U(xiy) strength metrics.', '293': 'To some extent, all of these collocations conflict with walk.', '294': 'Collocation (1) is the most blatant case; if it matches the target context, this logically implies that the context word walk will match.', '295': 'If collocation (2) matches, this guarantees that one of the possible tags of walk will be present nearby the target word, thereby elevating the probability that walk will match within Â±k words.', '296': 'If collocation (3) matches, this guarantees that there are two positions nearby the target word that are incompatible with walk, thereby reducing the probability that walk will match.', '297': 'If we were to treat all of these cases as conflicts, we would end up losing a great deal of (potentially useful) evidence.', '298': 'Instead, we adopt the more relaxed policy of only flagging the most egregious conflicts - here, the one between collocation (1) and walk.', '299': 'In general, we will say that a collocation and a context word conflict iff the collocation contains an explicit test for the context word.', '300': 'Table 7 compares all methods covered so far - baseline, two component methods, and two hybrid methods.', '301': '(A sixth method, trigrams, is included as well-it will be discussed in Section 4.)', '302': 'The table shows that the Bayesian hybrid method does at least as well as the previous four methods for almost every confusion set.', '303': 'Occasionally it scores slightly less than collocations; this appears to be due to some averaging effect where noisy context words are dragging it down.', '304': 'Occasionally too it scores less than decision lists, but never by much; on the whole, it yields a modest but consistent improvement, and in the case of {between, among}, a sizable improvement.', '305': 'We believe the improvement is due to considering all of the evidence, rather than just the single strongest piece, which makes the method more robust to inaccurate judgements about which piece of evidence is ""strongest"".', '306': '51 C on fu si on se t B as eli 11 e C w or ds Collocs Dlist Bayes Â± 3 : : ; 2 R e l y R e l y Trigra ms w he th er I i t s p a s t t h a n b e i n g e f f e c t y o u r n u m b e r c o u n c i l n s e b e t w e e n l e d e x c e p t p e a c e t h e r e p r i n c i p l e s i g h t 0 . 9 2 2 0 . 8 8 6 0 . 8 6 3 0 . 8 6 1 0 . 8 0 7 0 . 7 8 0 0 . 7 4 1 0 . 7 2 6 0 . 6 2 7 0 . 6 1 4 0 . 5 7 5 0 ..', '307': '5 3 8 0 ..', '308': '5 3 0 0 . 4 4 2 0 . 3 9 3 0 . : 3 0 6 0 . 2 9 0 0 . 1 1 4 0 . 9 0 2 0.931 0.93.5 0.935 0 . 9 1 4 0.981 0.980 0.985 0 . 8 6 2 0.945 0.931 0.942 0 . 8 6 1 0.909 0.932 0.924 0 . 9 3 1 0.96.5 0.967 0.973 0 . 7 9 1 0.853 0.842 0.869 0 . 7 4 7 0.821 0.821 0.827 0 . 8 1 6 0.887 0.868 0.901 0 . 6 4 6 0.646 0.629 0.662 0 . 6 : 3 9 0.639 0.627 0.639 0 ..', '309': '5 7 . 0 . 7 . 5 9 0.730 0.6.59 0.786 0 ..', '310': '5 3 0 0.840 0.840 0.840 O . G 9.', '311': '0 . 7 . 5 4 0.869 0.8.52 0.8.52 0 . 7 2 6 0.932 0.914 0.916 0 . 2 9 0 0.812 0.812 0.812 0 . 4 . 5 . 0.8 73 0.9 85 0.9 65 0.9 55 0.7 80 0.9 78 0.9 75 0.9 58 0.6 36 0.6 51 0.5 74 0..', '312': '538 0.9 09 0.6 9.5 0.3 93 0.9 61 0.6 09 0.2 50 Table 7: Performance of six methods for context-sensitive spelling correction.', '313': '4 Evaluation.', '314': 'While the previous section demonstrated that the Bayesian hybrid method does better than its components, we would still like to know how it compares with alternative methods.', '315': 'We looked at a method based on part-of-speech trigrams, developed and implemented by Schabes [1995].', '316': ""Schabes's method can be viewed as performing an abductive inference: given a sentence conÂ\xad taining an ambiguous word, it asks which choice w; for that word would best explain the observed sequence of words in the sentence."", '317': 'It answers this question by substituting each w; in turn into the sentence.', '318': 'The w; that produces the highest-probability sentence is selected.', '319': 'Sentence probabilities are calculated using a part-of-speech trigram model.', '320': ""We tried Schabes's method on the usual confusion sets; the results are in the last column of Table 7."", '321': 'It can be seen that trigrams and the Bayesian hybrid method each have their better moments.', '322': 'Trigrams are at their worst when the words in the confusion set have the same part of speech.', '323': 'In this case, trigrams can distinguish between the words only by their prior probabilitiesÂ\xad this follows from the way the method calculates sentence probabilities.', '324': 'Thus, for {between, among}, for example, where both words are prepositions, trigrams score the same as the baseline method.', '325': 'In such cases, the Bayesian hybrid method is clearly better.', '326': ""On the other hand, when the words in the confusion set have different parts of speech- as in, for example, {there, their, they're}Â\xad trigrams are often better than the Bayesian method."", '327': 'We believe this is because trigrams look not just at a few words on either side of the target word, but at the part-of-speech sequence of the whole sentence.', '328': 'This analysis indicates a complementarity between trigrams and Bayes, and suggests a 52 combination in which trigrams would be applied first, but if trigrams determine that the words in the confusion set have the same part of speech for the sentence at issue, then the sentence would be passed to the Bayesian method.', '329': 'This is a research direction we plan to pursue.', '330': ""The work reported here builds on Yarowsky's use of decision lists to combine two component methods- context words and collocations."", '331': 'Decision lists pool the evidence from the two methods, and solve a target problem by applying the single strongest piece of evidence, whichever type that happens to be.', '332': 'This paper investigated the hypothesis that even better performance can be obtained by basing decisions on not just the single strongest piece of evidence, but on all available evidence.', '333': 'A method for doing this, based on Bayesian classifiers, was presented.', '334': 'It was applied to the task of context-sensitive spelling correction, and was found to outperform the component methods as well as decision lists.', '335': ""A comparison of the Bayesian hybrid method with Schabes's trigram-based method suggested a further combination in which trigrams would be used when the words in the confusion set had different parts of speech, and the Bayesian method would be used otherwise."", '336': 'This is a direction we plan to pursue in future research.', '337': 'We would like to thank Bill Freeman, Yves Schabes, Emmanuel Roche, and Jacki Golding for helpful and enjoyable discussions on the work reported here.'}",['W95-0104'],['../data/summaries/W95-0104.txt'],['../data/tba/W95-0104.json']
W99-0613,Unsupervised Models for Named Entity Classification Collins,../data/papers/W99-0613.xml,"{'0': 'Unsupervised Models for Named Entity Classification Collins', '1': 'This paper discusses the use of unlabeled examples for the problem of named entity classification.', '2': 'A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &quot;seed&quot; rules.', '3': 'The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.', '4': 'We present two algorithms.', '5': 'The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98).', '6': 'The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98).', '7': 'Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.', '8': 'Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.', '9': 'This paper discusses the use of unlabeled examples for the problem of named entity classification.', '10': 'The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.', '11': 'For example, a good classifier would identify Mrs. Frank as a person, Steptoe & Johnson as a company, and Honduras as a location.', '12': 'The approach uses both spelling and contextual rules.', '13': 'A spelling rule might be a simple look-up for the string (e.g., a rule that Honduras is a location) or a rule that looks at words within a string (e.g., a rule that any string containing Mr. is a person).', '14': 'A contextual rule considers words surrounding the string in the sentence in which it appears (e.g., a rule that any proper name modified by an appositive whose head is president is a person).', '15': 'The task can be considered to be one component of the MUC (MUC-6, 1995) named entity task (the other task is that of segmentation, i.e., pulling possible people, places and locations from text before sending them to the classifier).', '16': 'Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).', '17': 'At first glance, the problem seems quite complex: a large number of rules is needed to cover the domain, suggesting that a large number of labeled examples is required to train an accurate classifier.', '18': 'But we will show that the use of unlabeled data can drastically reduce the need for supervision.', '19': 'Given around 90,000 unlabeled examples, the methods described in this paper classify names with over 91% accuracy.', '20': 'The only supervision is in the form of 7 seed rules (namely, that New York, California and U.S. are locations; that any name containing Mr is a person; that any name containing Incorporated is an organization; and that I.B.M. and Microsoft are organizations).', '21': 'The key to the methods we describe is redundancy in the unlabeled data.', '22': 'In many cases, inspection of either the spelling or context alone is sufficient to classify an example.', '23': 'For example, in .., says Mr. Cooper, a vice president of.. both a spelling feature (that the string contains Mr.) and a contextual feature (that president modifies the string) are strong indications that Mr. Cooper is of type Person.', '24': 'Even if an example like this is not labeled, it can be interpreted as a &quot;hint&quot; that Mr and president imply the same category.', '25': 'The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label, and these hints turn out to be surprisingly useful when building a classifier.', '26': 'We present two algorithms.', '27': 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', '28': '(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.', '29': ""Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function."", '30': '(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of unlabeled examples can help classification, and suggest an objective function when training with unlabeled examples.', '31': ""Our first algorithm is similar to Yarowsky's, but with some important modifications motivated by (Blum and Mitchell 98)."", '32': 'The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.', '33': 'The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).', '34': 'The AdaBoost algorithm was developed for supervised learning.', '35': 'AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.', '36': 'Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.', '37': 'The algorithm builds two classifiers iteratively: each iteration involves minimization of a continuously differential function which bounds the number of examples on which the two classifiers disagree.', '38': 'There has been additional recent work on inducing lexicons or other knowledge sources from large corpora.', '39': '(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.', '40': '(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g., &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.', '41': '(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).', '42': '(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &quot;vehicle&quot; or &quot;weapon&quot; categories).', '43': 'The approach builds from an initial seed set for a category, and is quite similar to the decision list approach described in (Yarowsky 95).', '44': 'More recently, (Riloff and Jones 99) describe a method they term &quot;mutual bootstrapping&quot; for simultaneously constructing a lexicon and contextual extraction patterns.', '45': 'The method shares some characteristics of the decision list algorithm presented in this paper.', '46': '(Riloff and Jones 99) was brought to our attention as we were preparing the final version of this paper.', '47': '971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).', '48': 'For example, take ..., says Maury Cooper, a vice president at S.&P.', '49': 'In this case, Maury Cooper is extracted.', '50': 'It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).', '51': '2.', '52': 'The NP is a complement to a preposition, which is the head of a PP.', '53': 'This PP modifies another NP, whose head is a singular noun.', '54': 'For example, ... fraud related to work on a federally funded sewage plant in Georgia In this case, Georgia is extracted: the NP containing it is a complement to the preposition in; the PP headed by in modifies the NP a federally funded sewage plant, whose head is the singular noun plant.', '55': 'In addition to the named-entity string (Maury Cooper or Georgia), a contextual predictor was also extracted.', '56': 'In the appositive case, the contextual predictor was the head of the modifying appositive (president in the Maury Cooper example); in the second case, the contextual predictor was the preposition together with the noun it modifies (plant_in in the Georgia example).', '57': 'From here on we will refer to the named-entity string itself as the spelling of the entity, and the contextual predicate as the context.', '58': 'Having found (spelling, context) pairs in the parsed data, a number of features are extracted.', '59': 'The features are used to represent each example for the learning algorithm.', '60': 'In principle a feature could be an arbitrary predicate of the (spelling, context) pair; for reasons that will become clear, features are limited to querying either the spelling or context alone.', '61': 'The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.', '62': '(e.g., N.Y. would contribute this feature, IBM would not). nonalpha=x Appears if the spelling contains any characters other than upper or lower case letters.', '63': 'In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g., for Thomas E. Petry nonalpha= .', '64': ', for A. T.&T. nonalpha.. .', '65': '.', '66': '). context=x The context for the entity.', '67': 'The', '68': 'The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).', '69': 'Before describing the unsupervised case we first describe the supervised version of the algorithm: Input to the learning algorithm: n labeled examples of the form (xi, y„). y, is the label of the ith example (given that there are k possible labels, y, is a member of y = {1 ... 0). xi is a set of mi features {x,1, Xi2 .', '70': '.', '71': '.', '72': 'Xim, } associated with the ith example.', '73': 'Each xii is a member of X, where X is a set of possible features.', '74': 'Output of the learning algorithm: a function h:Xxy [0, 1] where h(x, y) is an estimate of the conditional probability p(y1x) of seeing label y given that feature x is present.', '75': 'Alternatively, h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x, y).', '76': 'The label for a test example with features x is then defined as In this paper we define h(x, y) as the following function of counts seen in training data: Count(x,y) is the number of times feature x is seen with label y in training data, Count(x) = EyEy Count(x, y). a is a smoothing parameter, and k is the number of possible labels.', '77': 'In this paper k = 3 (the three labels are person, organization, location), and we set a = 0.1.', '78': 'Equation 2 is an estimate of the conditional probability of the label given the feature, P(yjx).', '79': '2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).', '80': 'The 2(Yarowsky 95) describes the use of more sophisticated smoothing methods.', '81': ""It's not clear how to apply these methods in the unsupervised case, as they required cross-validation techniques: for this reason we use the simpler smoothing method shown here. input to the unsupervised algorithm is an initial, &quot;seed&quot; set of rules."", '82': 'In the named entity domain these rules were Each of these rules was given a strength of 0.9999.', '83': ""The following algorithm was then used to induce new rules: Let Count' (x) be the number of times feature x is seen with some known label in the training data."", '84': ""For each label (Per s on, organization and Location), take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin."", '85': '(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.', '86': 'Thus at each iteration the method induces at most n x k rules, where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.', '87': 'Otherwise, label the training data with the combined spelling/contextual decision list, then induce a final decision list from the labeled examples where all rules (regardless of strength) are added to the decision list.', '88': 'We can now compare this algorithm to that of (Yarowsky 95).', '89': ""The core of Yarowsky's algorithm is as follows: where h is defined by the formula in equation 2, with counts restricted to training data examples that have been labeled in step 2."", '90': 'Set the decision list to include all rules whose (smoothed) strength is above some threshold Pmin.', '91': 'There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.', '92': 'Thus an explicit assumption about the redundancy of the features — that either the spelling or context alone should be sufficient to build a classifier — has been built into the algorithm.', '93': 'To measure the contribution of each modification, a third, intermediate algorithm, Yarowsky-cautious was also tested.', '94': 'Yarowsky-cautious does not separate the spelling and contextual features, but does have a limit on the number of rules added at each stage.', '95': '(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)', '96': 'The first modification — cautiousness — is a relatively minor change.', '97': 'It was motivated by the observation that the (Yarowsky 95) algorithm added a very large number of rules in the first few iterations.', '98': 'Taking only the highest frequency rules is much &quot;safer&quot;, as they tend to be very accurate.', '99': 'This intuition is born out by the experimental results.', '100': 'The second modification is more important, and is discussed in the next section.', '101': 'An important reason for separating the two types of features is that this opens up the possibility of theoretical analysis of the use of unlabeled examples.', '102': '(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.', '103': 'In the named entity task, X1 might be the instance space for the spelling features, X2 might be the instance space for the contextual features.', '104': 'By this assumption, each element x E X can also be represented as (xi, x2) E X1 x X2.', '105': 'Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.', '106': 'Now assume we have n pairs (xi,, x2,i) drawn from X1 X X2, where the first m pairs have labels whereas for i = m+ 1...n the pairs are unlabeled.', '107': 'In a fully supervised setting, the task is to learn a function f such that for all i = 1...m, f (xi,i, 12,i) = yz.', '108': 'In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.', '109': 'The key point is that the second constraint can be remarkably powerful in reducing the complexity of the learning problem.', '110': '(Blum and Mitchell 98) give an example that illustrates just how powerful the second constraint can be.', '111': 'Consider the case where IX].', '112': 'I = 1X21 N and N is a &quot;medium&quot; sized number so that it is feasible to collect 0(N) unlabeled examples.', '113': 'Assume that the two classifiers are &quot;rote learners&quot;: that is, 1.1 and 12 are defined through look-up tables that list a label for each member of X1 or X2.', '114': 'The problem is a binary classification problem.', '115': 'The problem can be represented as a graph with 2N vertices corresponding to the members of X1 and X2.', '116': 'Each unlabeled pair (x1,i, x2,i) is represented as an edge between nodes corresponding to x1,i and X2,i in the graph.', '117': 'An edge indicates that the two features must have the same label.', '118': 'Given a sufficient number of randomly drawn unlabeled examples (i.e., edges), we will induce two completely connected components that together span the entire graph.', '119': 'Each vertex within a connected component must have the same label — in the binary classification case, we need a single labeled example to identify which component should get which label.', '120': '(Blum and Mitchell 98) go on to give PAC results for learning in the cotraining case.', '121': 'They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).', '122': 'The method halves the error rate in comparison to a method using the labeled examples alone.', '123': 'Limitations of (Blum and Mitchell 98): While the assumptions of (Blum and Mitchell 98) are useful in developing both theoretical results and an intuition for the problem, the assumptions are quite limited.', '124': 'In particular, it may not be possible to learn functions fi (x f2(x2,t) for i = m + 1...n: either because there is some noise in the data, or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.', '125': 'It may be more realistic to replace the second criteria with a softer one, for example (Blum and Mitchell 98) suggest the alternative Alternatively, if Ii and 12 are probabilistic learners, it might make sense to encode the second constraint as one of minimizing some measure of the distance between the distributions given by the two learners.', '126': ""The question of what soft function to pick, and how to design' algorithms which optimize it, is an open question, but appears to be a promising way of looking at the problem."", '127': 'The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.', '128': 'At each iteration the algorithm increases the number of rules, while maintaining a high level of agreement between the spelling and contextual decision lists.', '129': 'Inspection of the data shows that at n = 2500, the two classifiers both give labels on 44,281 (49.2%) of the unlabeled examples, and give the same label on 99.25% of these cases.', '130': 'So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.', '131': 'In the next section we present an alternative approach that builds two classifiers while attempting to satisfy the above constraints as much as possible.', '132': 'The algorithm, called CoBoost, has the advantage of being more general than the decision-list learning alInput: (xi , yi), , (xim, ) ; x, E 2x, yi = +1 Initialize Di (i) = 1/m.', '133': 'Fort= 1,...,T:', '134': 'This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.', '135': 'We first give a brief overview of boosting algorithms.', '136': 'We then discuss how we adapt and generalize a boosting algorithm, AdaBoost, to the problem of named entity classification.', '137': 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', '138': ""(We would like to note though that unlike previous boosting algorithms, the CoBoost algorithm presented here is not a boosting algorithm under Valiant's (Valiant 84) Probably Approximately Correct (PAC) model.)"", '139': 'This section describes AdaBoost, which is the basis for the CoBoost algorithm.', '140': 'AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.', '141': 'For a description of the application of AdaBoost to various NLP problems see the paper by Abney, Schapire, and Singer in this volume.', '142': 'The input to AdaBoost is a set of training examples ((xi , yi), , (x„.„ yrn)).', '143': 'Each xt E 2x is the set of features constituting the ith example.', '144': 'For the moment we will assume that there are only two possible labels: each y, is in { —1, +1}.', '145': 'AdaBoost is given access to a weak learning algorithm, which accepts as input the training examples, along with a distribution over the instances.', '146': 'The distribution specifies the relative weight, or importance, of each example — typically, the weak learner will attempt to minimize the weighted error on the training set, where the distribution specifies the weights.', '147': 'The weak learner for two-class problems computes a weak hypothesis h from the input space into the reals (h : 2x -4 R), where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction, and numbers close to zero indicate low confidence.', '148': 'The weak hypothesis can abstain from predicting the label of an instance x by setting h(x) = 0.', '149': 'The final strong hypothesis, denoted 1(x), is then the sign of a weighted sum of the weak hypotheses, 1(x) = sign (Vii atht(x)), where the weights at are determined during the run of the algorithm, as we describe below.', '150': 'Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.', '151': 'Note that Zt is a normalization constant that ensures the distribution Dt+i sums to 1; it is a function of the weak hypothesis ht and the weight for that hypothesis at chosen at the tth round.', '152': 'The normalization factor plays an important role in the AdaBoost algorithm.', '153': 'Schapire and Singer show that the training error is bounded above by Thus, in order to greedily minimize an upper bound on training error, on each iteration we should search for the weak hypothesis ht and the weight at that minimize Z.', '154': 'In our implementation, we make perhaps the simplest choice of weak hypothesis.', '155': 'Each ht is a function that predicts a label (+1 or —1) on examples containing a particular feature xt, while abstaining on other examples: The prediction of the strong hypothesis can then be written as We now briefly describe how to choose ht and at at each iteration.', '156': 'Our derivation is slightly different from the one presented in (Schapire and Singer 98) as we restrict at to be positive.', '157': 'Zt can be written as follows Following the derivation of Schapire and Singer, providing that W+ > W_, Equ.', '158': '(4) is minimized by setting Since a feature may be present in only a few examples, W_ can be in practice very small or even 0, leading to extreme confidence values.', '159': 'To prevent this we &quot;smooth&quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.', '160': '(5) and ht into Equ.', '161': '(4) gives In order to minimize Zt, at each iteration the final algorithm should choose the weak hypothesis (i.e., a feature xt) which has values for W+ and W_ that minimize Equ.', '162': '(6), with W+ > W_.', '163': 'We now describe the CoBoost algorithm for the named entity problem.', '164': 'Following the convention presented in earlier sections, we assume that each example is an instance pair of the from (xi ,i, x2,) where xj,, E 2x3 , j E 2}.', '165': 'In the namedentity problem each example is a (spelling,context) pair.', '166': 'The first m pairs have labels yi, whereas for i = m + 1, , n the pairs are unlabeled.', '167': 'We make the assumption that for each example, both xi,. and x2,2 alone are sufficient to determine the label yi.', '168': 'The learning task is to find two classifiers : 2x1 { —1, +1} 12 : 2x2 { —1, +1} such that (x1,) = f2(x2,t) = yt for examples i = 1, , m, and f1 (x1,) = f2 (x2,t) as often as possible on examples i = m + 1, ,n. To achieve this goal we extend the auxiliary function that bounds the training error (see Equ.', '169': '(3)) to be defined over unlabeled as well as labeled instances.', '170': 'Denote by g3(x) = Et crithl(x) , j E {1,2} the unthresholded strong-hypothesis (i.e., f3 (x) = sign(gi (x))).', '171': 'We define the following function: If Zco is small, then it follows that the two classifiers must have a low error rate on the labeled examples, and that they also must give the same label on a large number of unlabeled instances.', '172': 'To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.', '173': '(3)), with one term for each classifier.', '174': 'The two new terms force the two classifiers to agree, as much as possible, on the unlabeled examples.', '175': 'Put another way, the minimum of Equ.', '176': '(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.', '177': 'Formally, let el (62) be the number of classification errors of the first (second) learner on the training data, and let Eco be the number of unlabeled examples on which the two classifiers disagree.', '178': 'Then, it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.', '179': 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.', '180': 'As in boosting, the algorithm works in rounds.', '181': 'Each round is composed of two stages; each stage updates one of the classifiers while keeping the other classifier fixed.', '182': 'Denote the unthresholded classifiers after t — 1 rounds by git—1 and assume that it is the turn for the first classifier to be updated while the second one is kept fixed.', '183': 'We first define &quot;pseudo-labels&quot;,-yt, as follows: = Yi t sign(g 0\\ 2— kx2,m < i < n Thus the first m labels are simply copied from the labeled examples, while the remaining (n — m) examples are taken as the current output of the second classifier.', '184': 'We can now add a new weak hypothesis 14 based on a feature in X1 with a confidence value al hl and atl are chosen to minimize the function We now define, for 1 <i <n, the following virtual distribution, As before, Ztl is a normalization constant.', '185': 'Equ.', '186': '(8) can now be rewritten5 as which is of the same form as the function Zt used in AdaBoost.', '187': 'Using the virtual distribution Di (i) and pseudo-labels&quot;y.,„ values for Wo, W± and W_ can be calculated for each possible weak hypothesis (i.e., for each feature x E Xi); the weak hypothesis with minimal value for Wo + 2/WW _ can be chosen as before; and the weight for this weak hypothesis at = ln ww+411:) can be calculated.', '188': 'This procedure is repeated for T rounds while alternating between the two classifiers.', '189': 'The pseudo-code describing the algorithm is given in Fig.', '190': '2.', '191': 'The CoBoost algorithm described above divides the function Zco into two parts: Zco = 40 + 40.', '192': 'On each step CoBoost searches for a feature and a weight so as to minimize either 40 or 40.', '193': 'In Input: {(x1,i, Initialize: Vi, j : e(xi) = 0.', '194': ""For t = 1, T and for j = 1, 2: where 4 = exp(-jg'(xj,i)). practice, this greedy approach almost always results in an overall decrease in the value of Zco."", '195': 'Note, however, that there might be situations in which Zco in fact increases.', '196': 'One implementation issue deserves some elaboration.', '197': 'Note that in our formalism a weakhypothesis can abstain.', '198': 'In fact, during the first rounds many of the predictions of Th., g2 are zero.', '199': 'Thus corresponding pseudo-labels for instances on which gj abstain are set to zero and these instances do not contribute to the objective function.', '200': 'Each learner is free to pick the labels for these instances.', '201': 'This allow the learners to &quot;bootstrap&quot; each other by filling the labels of the instances on which the other side has abstained so far.', '202': 'The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.', '203': 'Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).', '204': 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.', '205': 'AdaBoost.MH maintains a distribution over instances and labels; in addition, each weak-hypothesis outputs a confidence vector with one confidence value for each possible label.', '206': 'We again adopt an approach where we alternate between two classifiers: one classifier is modified while the other remains fixed.', '207': 'Pseudo-labels are formed by taking seed labels on the labeled examples, and the output of the fixed classifier on the unlabeled examples.', '208': 'AdaBoost.MH can be applied to the problem using these pseudolabels in place of supervised examples.', '209': 'For the experiments in this paper we made a couple of additional modifications to the CoBoost algorithm.', '210': 'The algorithm in Fig.', '211': '(2) was extended to have an additional, innermost loop over the (3) possible labels.', '212': 'The weak hypothesis chosen was then restricted to be a predictor in favor of this label.', '213': 'Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.', '214': 'This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating — this deserves more theoretical investigation.', '215': 'We also removed the context-type feature type when using the CoBoost approach.', '216': 'This &quot;default&quot; feature type has 100% coverage (it is seen on every example) but a low, baseline precision.', '217': 'When this feature type was included, CoBoost chose this default feature at an early iteration, thereby giving non-abstaining pseudo-labels for all examples, with eventual convergence to the two classifiers agreeing by assigning the same label to almost all examples.', '218': 'Again, this deserves further investigation.', '219': 'Finally, we would like to note that it is possible to devise similar algorithms based with other objective functions than the one given in Equ.', '220': '(7), such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).', '221': 'We are currently exploring such algorithms.', '222': 'The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.', '223': 'A generative model was applied (similar to naive Bayes) with the three labels as hidden vanables on unlabeled examples, and observed variables on (seed) labeled examples.', '224': 'The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).', '225': 'We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}, and the last (n — m) examples are unlabeled.', '226': 'For the purposes of EM, the &quot;observed&quot; data is {(xi, Ya• • • (xrn, Yrn), xfil, and the hidden data is {ym+i y}.', '227': 'The likelihood of the observed data under the model is where P(yi, xi) is defined as in (9).', '228': 'Training under this model involves estimation of parameter values for P(y), P(m) and P(x I y).', '229': 'The maximum likelihood estimates (i.e., parameter values which maximize 10) can not be found analytically, but the EM algorithm can be used to hill-climb to a local maximum of the likelihood function from some initial parameter settings.', '230': 'In our experiments we set the parameter values randomly, and then ran EM to convergence.', '231': 'Given parameter estimates, the label for a test example x is defined as We should note that the model in equation 9 is deficient, in that it assigns greater than zero probability to some feature combinations that are impossible.', '232': 'For example, the independence assumptions mean that the model fails to capture the dependence between specific and more general features (for example the fact that the feature full.-string=New_York is always seen with the features contains (New) and The baseline method tags all entities as the most frequent class type (organization). contains (York) and is never seen with a feature such as contains (Group) ).', '233': 'Unfortunately, modifying the model to account for these kind of dependencies is not at all straightforward.', '234': '88,962 (spelling,context) pairs were extracted as training data.', '235': '1,000 of these were picked at random, and labeled by hand to produce a test set.', '236': 'We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.', '237': 'The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.', '238': '123 examples fell into the noise category.', '239': 'Of these cases, 38 were temporal expressions (either a day of the week or month of the year).', '240': 'We excluded these from the evaluation as they can be easily identified with a list of days/months.', '241': 'This left 962 examples, of which 85 were noise.', '242': 'Taking /V, to be the number of examples an algorithm classified correctly (where all gold standard items labeled noise were counted as being incorrect), we calculated two measures of accuracy: See Tab.', '243': '2 for the accuracy of the different methods.', '244': 'Note that on some examples (around 2% of the test set) CoBoost abstained altogether; in these cases we labeled the test example with the baseline, organization, label.', '245': 'Fig.', '246': '(3) shows learning curves for CoBoost.', '247': 'N, portion of examples on which both classifiers give a label rather than abstaining), and the proportion of these examples on which the two classifiers agree.', '248': 'With each iteration more examples are assigned labels by both classifiers, while a high level of agreement (> 94%) is maintained between them.', '249': 'The test accuracy more or less asymptotes.', '250': 'Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.', '251': 'In addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98).', '252': 'The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.', '253': 'We are currently exploring other methods that employ similar ideas and their formal properties.', '254': 'Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them.', '255': 'The contextual rules are restricted and may not be applicable to every example, but the spelling rules are generally applicable and should have good coverage.', '256': 'The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.'}","['W99-0613_vardha', 'W99-0613_aakansha', 'W99-0613_swastika', 'W99-0613_sweta']","['../data/summaries/W99-0613_vardha.txt', '../data/summaries/W99-0613_aakansha.txt', '../data/summaries/W99-0613_swastika.txt', '../data/summaries/W99-0613_sweta.txt']","['../data/tba/W99-0613_vardha.json', '../data/tba/W99-0613_aakansha.json', '../data/tba/W99-0613_swastika.json', '../data/tba/W99-0613_sweta.json']"
W99-0623,Exploiting Diversity in Natural Language Processing: Combining Parsers,../data/papers/W99-0623.xml,"{'0': 'Exploiting Diversity in Natural Language Processing: Combining Parsers', '1': 'Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', '2': 'Two general approaches are presented and two combination techniques are described for each approach.', '3': 'Both parametric and non-parametric models are explored.', '4': 'The resulting parsers surpass the best previously published performance results for the Penn Treebank.', '5': 'The natural language processing community is in the strong position of having many available approaches to solving some of its most fundamental problems.', '6': 'The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992; Heath et al., 1996).', '7': 'Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers.', '8': 'The theory has also been validated empirically.', '9': 'Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998).', '10': 'In both cases the investigators were able to achieve significant improvements over the previous best tagging results.', '11': 'Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).', '12': 'The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997).', '13': 'These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).', '14': 'We used these three parsers to explore parser combination techniques.', '15': 'We are interested in combining the substructures of the input parses to produce a better parse.', '16': 'We call this approach parse hybridization.', '17': 'The substructures that are unanimously hypothesized by the parsers should be preserved after combination, and the combination technique should not foolishly create substructures for which there is no supporting evidence.', '18': 'These two principles guide experimentation in this framework, and together with the evaluation measures help us decide which specific type of substructure to combine.', '19': 'The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.', '20': 'Since our goal is to perform well under these measures we will similarly treat constituents as the minimal substructures for combination.', '21': ""One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set."", '22': 'If enough parsers suggest that a particular constituent belongs in the parse, we include it.', '23': 'We call this technique constituent voting.', '24': 'We include a constituent in our hypothesized parse if it appears in the output of a majority of the parsers.', '25': 'In our particular case the majority requires the agreement of only two parsers because we have only three.', '26': 'This technique has the advantage of requiring no training, but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.', '27': 'Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.', '28': 'The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.', '29': 'Our original hope in combining these parsers is that their errors are independently distributed.', '30': 'This is equivalent to the assumption used in probability estimation for naïve Bayes classifiers, namely that the attribute values are conditionally independent when the target value is given.', '31': 'For this reason, naïve Bayes classifiers are well-matched to this problem.', '32': 'In Equations 1 through 3 we develop the model for constructing our parse using naïve Bayes classification.', '33': 'C is the union of the sets of constituents suggested by the parsers. r(c) is a binary function returning t (for true) precisely when the constituent c E C should be included in the hypothesis.', '34': 'Mi(c) is a binary function returning t when parser i (from among the k parsers) suggests constituent c should be in the parse.', '35': 'The hypothesized parse is then the set of constituents that are likely (P > 0.5) to be in the parse according to this model.', '36': 'The estimation of the probabilities in the model is carried out as shown in Equation 4.', '37': 'Here NO counts the number of hypothesized constituents in the development set that match the binary predicate specified as an argument.', '38': 'Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.', '39': 'There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.', '40': 'Lemma: If the number of votes required by constituent voting is greater than half of the parsers under consideration the resulting structure has no crossing constituents.', '41': 'IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.', '42': 'Call the crossing constituents A and B.', '43': 'A receives a votes, and B receives b votes.', '44': 'Each of the constituents must have received at least 1 votes from the k parsers, so a > I1 and 2 — 2k±-1 b > ri-5-111.', '45': 'Let s = a + b.', '46': 'None of the parsers produce parses with crossing brackets, so none of them votes for both of the assumed constituents.', '47': 'Hence, s < k. But by addition of the votes on the two parses, s > 2N-11> k, a contradiction.', '48': '• Similarly, when the naïve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.', '49': 'In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.', '50': 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', '51': 'One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.', '52': 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.', '53': 'For example, we may have semantic information (e.g. database query operations) associated with the productions in a grammar.', '54': 'If the parse contains productions from outside our grammar the machine has no direct method for handling them (e.g. the resulting database query may be syntactically malformed).', '55': 'We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.', '56': 'The combining algorithm is presented with the candidate parses and asked to choose which one is best.', '57': 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', '58': 'We call this approach parser switching.', '59': 'Once again we present both a non-parametric and a parametric technique for this task.', '60': 'First we present the non-parametric version of parser switching, similarity switching: The intuition for this technique is that we can measure a similarity between parses by counting the constituents they have in common.', '61': 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', '62': 'This is the parse that is closest to the centroid of the observed parses under the similarity metric.', '63': 'The probabilistic version of this procedure is straightforward: We once again assume independence among our various member parsers.', '64': 'Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.', '65': 'We model each parse as the decisions made to create it, and model those decisions as independent events.', '66': 'Each decision determines the inclusion or exclusion of a candidate constituent.', '67': 'The set of candidate constituents comes from the union of all the constituents suggested by the member parsers.', '68': 'This is summarized in Equation 5.', '69': 'The computation of Pfr1(c)1Mi M k (C)) has been sketched before in Equations 1 through 4.', '70': ""In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers."", '71': 'It is chosen such that the decisions it made in including or excluding constituents are most probable under the models for all of the parsers.', '72': 'The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.', '73': 'We used section 23 as the development set for our combining techniques, and section 22 only for final testing.', '74': 'The development set contained 44088 constituents in 2416 sentences and the test set contained 30691 constituents in 1699 sentences.', '75': ""A sentence was withheld from section 22 because its extreme length was troublesome for a couple of the parsers.'"", '76': 'The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.', '77': 'Each parse is converted into a set of constituents represented as a tuples: (label, start, end).', '78': 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', '79': 'Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.', '80': 'For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.', '81': 'F-measure is the harmonic mean of precision and recall, 2PR/(P + R).', '82': 'It is closer to the smaller value of precision and recall when there is a large skew in their values.', '83': 'We performed three experiments to evaluate our techniques.', '84': 'The first shows how constituent features and context do not help in deciding which parser to trust.', '85': 'We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.', '86': 'Finally we show the combining techniques degrade very little when a poor parser is added to the set.', '87': 'It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.', '88': 'For example, one parser could be more accurate at predicting noun phrases than the other parsers.', '89': 'None of the models we have presented utilize features associated with a particular constituent (i.e. the label, span, parent label, etc.) to influence parser preference.', '90': 'This is not an oversight.', '91': 'Features and context were initially introduced into the models, but they refused to offer any gains in performance.', '92': 'While we cannot prove there are no such useful features on which one should condition trust, we can give some insight into why the features we explored offered no gain.', '93': 'Because we are working with only three parsers, the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.', '94': 'This is the only important case, because otherwise the simple majority combining technique would pick the correct constituent.', '95': 'One side of the decision making process is when we choose to believe a constituent should be in the parse, even though only one parser suggests it.', '96': 'We call such a constituent an isolated constituent.', '97': 'If we were working with more than three parsers we could investigate minority constituents, those constituents that are suggested by at least one parser, but which the majority of the parsers do not suggest.', '98': 'Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.', '99': 'Consider for a set of constituents the isolated constituent precision parser metric, the portion of isolated constituents that are correctly hypothesized.', '100': ""When this metric is less than 0.5, we expect to incur more errors' than we will remove by adding those constituents to the parse."", '101': 'We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.', '102': 'In Table 1 we see with very few exceptions that the isolated constituent precision is less than 0.5 when we use the constituent label as a feature.', '103': 'The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.', '104': 'In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.', '105': 'Similarly Figures 1 and 2 show how the isolated constituent precision varies by sentence length and the size of the span of the hypothesized constituent.', '106': 'In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.', '107': 'Again we notice that the isolated constituent precision is larger than 0.5 only in those partitions that contain very few samples.', '108': 'From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.', '109': 'The results in Table 2 were achieved on the development set.', '110': 'The first two rows of the table are baselines.', '111': 'The first row represents the average accuracy of the three parsers we combine.', '112': ""The second row is the accuracy of the best of the three parsers.'"", '113': 'The next two rows are results of oracle experiments.', '114': 'The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.', '115': 'It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.', '116': 'The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.', '117': 'Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.', '118': 'The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.', '119': 'We do not show the numbers for the Bayes models in Table 2 because the parameters involved were established using this set.', '120': 'The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.', '121': 'Table 3 contains the results for evaluating our systems on the test set (section 22).', '122': 'All of these systems were run on data that was not seen during their development.', '123': 'The difference in precision between similarity and Bayes switching techniques is significant, but the difference in recall is not.', '124': 'This is the first set that gives us a fair evaluation of the Bayes models, and the Bayes switching model performs significantly better than its non-parametric counterpart.', '125': 'The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', '126': 'Table 4 shows how much the Bayes switching technique uses each of the parsers on the test set.', '127': 'Parser 3, the most accurate parser, was chosen 71% of the time, and Parser 1, the least accurate parser was chosen 16% of the time.', '128': 'Ties are rare in Bayes switching because the models are fine-grained — many estimated probabilities are involved in each decision.', '129': 'In the interest of testing the robustness of these combining techniques, we added a fourth, simple nonlexicalized PCFG parser.', '130': 'The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', '131': 'It was then tested on section 22 of the Treebank in conjunction with the other parsers.', '132': 'The results of this experiment can be seen in Table 5.', '133': 'The entries in this table can be compared with those of Table 3 to see how the performance of the combining techniques degrades in the presence of an inferior parser.', '134': 'As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.', '135': 'The average individual parser accuracy was reduced by more than 5% when we added this new parser, but the precision of the constituent voting technique was the only result that decreased significantly.', '136': 'The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.', '137': 'We see from these results that the behavior of the parametric techniques are robust in the presence of a poor parser.', '138': 'Surprisingly, the non-parametric switching technique also exhibited robust behaviour in this situation.', '139': 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', '140': 'For each experiment we gave an nonparametric and a parametric technique for combining parsers.', '141': 'All four of the techniques studied result in parsing systems that perform better than any previously reported.', '142': 'Both of the switching techniques, as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.', '143': 'Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.', '144': 'Combining multiple highly-accurate independent parsers yields promising results.', '145': 'We plan to explore more powerful techniques for exploiting the diversity of parsing methods.', '146': 'We would like to thank Eugene Charniak, Michael Collins, and Adwait Ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments.', '147': 'This work was funded by NSF grant IRI-9502312.', '148': 'Both authors are members of the Center for Language and Speech Processing at Johns Hopkins University.'}","['W99-0623_sweta', 'W99-0623_swastika', 'W99-0623_vardha']","['../data/summaries/W99-0623_sweta.txt', '../data/summaries/W99-0623_swastika.txt', '../data/summaries/W99-0623_vardha.txt']","['../data/tba/W99-0623_sweta.json', '../data/tba/W99-0623_swastika.json', '../data/tba/W99-0623_vardha.json']"
X96-1048,OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION,../data/papers/X96-1048.xml,"{'0': 'OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION', '1': 'Test abstract', '2': 'The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference (MUC6) in November.', '3': 'Participants were invited to enter their systems in as many as four different task-oriented evaluations.', '4': 'The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.', '5': 'The other two tasks, Template Element and Scenario Template, were information extraction tasks that followed on from the MUC evaluations conducted in previous years.', '6': 'The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume.', '7': 'All except the Scenario Template task are defined independently of any particular domain.', '8': 'This paper surveys the results of the evaluation on each task and, to a more limited extent, across tasks.', '9': 'Discussion of the results for each task is organized generally under the following topics: \x95 Results on task as whole; \x95 Results on some aspects of task; \x95 Performance on ""walkthrough article.""', '10': 'The walkthrough article is an article selected from the test set.', '11': ""Participants were asked to analyze their system's performance on that article and comment on it in their presentations and papers."", '12': 'EVALUATION TASKS A basic characterization of the challenge presented by each evaluation task is as follows: \x95 Named Entity (NE) --Insert SGML tags into the text to mark each string that represents a person, organization, or location name, or a date or time stamp, or a currency or percentage figure.', '13': '\x95 Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.', '14': '\x95 Template Element (TE) --Extract basic information related to organization and person entities, drawing evidence from anywhere in the text.', '15': '\x95 Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event.', '16': 'The two SGML-based tasks required innovations to tie system-internal data structures to the original text so that the annotations could be inserted by the system without altering the original text in any other way.', '17': 'This capability has other useful applications as well, e.g., it enables text highlighting in a browser.', '18': 'It also facilitates information extraction, since some of the information in the extraction templates is in the form of literal text strings, which some systems have in the past had difficulty reproducing in their output.', '19': 'The inclusion of four different tasks in the evaluation implicitly encouraged sites to design general-purpose architectures that allow the production of a variety of types of output from a single internal representation in order to allow use of the full range of analysis techniques for all tasks.', '20': 'Even the simplest of the tasks, Named Entity, occasionally requires in-depth processing, e.g., to determine whether ""60 pounds"" is an expression of weight or of monetary value.', '21': 'Nearly half the sites chose to participate in all four tasks, and all but one site participated in at least one SGML task and one extraction task.', '22': 'The variety of tasks designed for MUC6 reflects the interests of both participants and sponsors in assessing and furthering research that can satisfy some urgent text processing needs in the very near term and can lead to solutions to more challenging text understanding problems in the longer term.', '23': 'Identification of certain common types of names, which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task, has proven to be largely a solved problem.', '24': 'Recognition of alternative ways of identifying an entity constitutes a large portion of the Coreference task and another critical portion of the Template Element task and has been shown to represent only a modest challenge when the referents are names or pronouns.', '25': 'The mix of challenges that the Scenario Template task represents has been shown to yield levels of performance that are smilar to those achieved in previous MUCs, but this time with a much shorter time required for porting.', '26': 'Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [1].', '27': 'CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.', '28': 'The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.', '29': 'This period comprised the ""evaluation epoch.""', '30': 'As a condition for participation in the evaluation, the sites agreed not to seek out and exploit Wall Street Journal articles from that epoch once the training phase of the evaluation had begun, i.e., once the scenario for the Scenario Template task had been disclosed to the participants.', '31': 'The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query.', '32': 'It can also be used to do unranked, Boolean retrievals.', '33': 'The Boolean retrieval method was used in the initial probing of the corpus to identify candidates for the Scenario Template task, because the Boolean retrieval is relatively fast, and the unranked results are easy to scan to get a feel for the variety of nonrelevant as well as relevant documents that match all or some of the query terms.', '34': 'Once the scenario had been identified, the ranked retrieval method was used, and the ranked list was sampled at different points to collect approximately 200 relevant and 200 nonrelevant articles, representing a variety of article types (feature articles, brief notices, editorials, etc.).', '35': 'From those candidate articles, the training and test sets were selected blindly, with later checks and corrections for imbalances in the relevant/nonrelevant categories and in article types.', '36': 'From the 100 test articles, a subset of 30 articles (some relevant to the Scenario Template task, others not) was selected for use as the test set for the Named Entity and Coreference tasks.', '37': 'The selection was again done blindly, with later checks to ensure that the set was fairly representative in terms of article length and type.', '38': 'Note that although Named Entity, Coreference and Template Element are defined as domain-independent tasks, the articles that were used for MUC6 testing were selected using domain-dependent criteria pertinent to the Scenario Template task.', '39': 'The manually filled templates were created with the aid of Tabula Rasa, a software tool developed for the Tipster Text Program by New Mexico State University Computing Research Laboratory.', '40': 'NAMED ENTITY The Named Entity (NE) task requires insertion of SGML tags into the text stream.', '41': 'The tag elements are ENAMEX (for entity names, comprising organizations, persons, and locations), TIMEX (for temporal expressions, namely direct mentions of dates and times), and NUMEX (for number expressions, consisting only of direct mentions of currency values and percentages).', '42': 'A TYPE attribute accompanies each tag element and identifies the subtype of each tagged string: for ENAMEX, the TYPE value can be ORGANIZATION, PERSON, or LOCATION; for TIMEX, the TYPE value can be DATE or TIME; and for NUMEX, the TYPE value can be MONEY or PERCENT.', '43': 'Text strings that are to be annotated are termed markables.', '44': 'As indicated above, markables include names of organizations, persons, and locations, and direct mentions of dates, times, currency values and percentages.', '45': 'Non-markables include names of products and other miscellaneous names (""Macintosh,"" ""Wall Street Journal"" (in reference to the periodical as a physical object), ""Dow Jones Industrial Average""); names of groups of people and miscellaneous usages of person names (""Republicans,"" ""GrammRudman,"" ""Alzheimer[\'s]""); addresses and adjectival forms of location names (""53140 Gatchell Rd.,"" ""American""); indirect and vague mentions of dates and times (""a few minutes after the hour,"" ""thirty days before the end of the year""); and miscellaneous uses of numbers, including some that are similar to currency or percentage expressions (""[Fees] 1 3/4,"" ""12 points,"" ""1.5 times"").', '46': 'The evaluation metrics used for NE are essentially the same as those used for the two template-filling tasks, Template Element and Scenario Template.', '47': 'The following breakdowns of overall scores on NE are computed: \x95 by slot, i.e., for performance across tag elements, across TYPE attributes, and across tag strings; \x95 by subcategorization, i.e., for performance on each TYPE attribute separately; \x95 by document section, i.e., for performance on distinct subparts of the article, as identified by the SGML tags contained in the original text: <HL> (""headline""), <DD> (""document date""), <DATELINE>, and <TXT> (the body of the article).', '48': 'NE Results Overall Fifteen sites participated in the NE evaluation, including two that submitted two system configurations for testing and one that submitted four, for a total of 20 systems.', '49': 'As shown in table 1, performance on the NE task overall was over 90% on the F-measure for half of the systems tested, which includes systems from seven different sites.', '50': 'On the basis of the results of the dry run, in which two of the nine systems scored over 90%, we were not surprised to find official scores that were similarly high, but it was not expected that so many systems would enter the formal evaluation and perform so well.', '51': 'It was also unexpected that one of the systems would match human performance on the task.', '52': 'Human performance was measured by comparing the 30 draft answer keys produced by the annotator at NRaD with those produced by the annotator at SAIC.', '53': 'This test measures the amount of variability between the annotators.', '54': 'When the outputs are scored in ""key-to-response"" mode, as though one annotator\'s output represented the ""key"" and the other the ""response,"" the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%.', '55': 'The top-scoring system, the baseline configuration of the SRA system, achieved an F-measure of 96.42 and a corresponding error score of 5%.', '56': 'In considering the significance of these results from a general standpoint, the following facts about the test set need to be remembered: 96.42 95.66 94.92 94.00 93.65 93.33 92.88 92.74 92.61 91.20 90.84 89.06 88.19 85.82 85.73 84.95 5 7 8 10 10 11 10 12 12 13 14 18 19 20 23 22 96 95 93 92 94 92 94 92 89 91 91 84 86 85 80 82 97 96 96 96 93 95 92 93 96 91 91 94 90 87 92 89 Table 1.', '57': 'Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA ""fast"" configuration 95.66, SRA ""fastest"" configuration 92.61, SRA ""nonames"" configuration 94.92, SRI 94.0, Sterling Software 92.74..', '58': '\x95 It represents just one style of writing ""the Chrysler division"" (currently, only ""Chrysler"" (journalistic) and has a basic basic toward financial news and a specific bias toward the topic of the Scenario Template task.', '59': '\x95 It was very small (only 30 articles).', '60': 'There were no markable time expressions in the test set, and there were only a few markable percentage expressions.', '61': 'The results should also be qualified by saying that they reflect performance on data that makes accurate usage of upper and lower case distinctions.', '62': ""What would performance be on data where case provided no (reliable) clues and for languages where case doesn't distinguish names?"", '63': 'SRA ran an experiment on an uppercase version of the test set that showed 85% recall and 89% precision overall, with identification of organization names presenting the greatest problem.', '64': 'That result represents nearly a 10-point decrease on the F-measure from their official baseline.', '65': ""The case-insensitive results would be slightly better if the task guidelines themselves didn't depend on case distinctions in certain situations, as when identifying the right boundary for the organization name span in a string such as would be tagged)."", '66': 'NE Results on Some Aspects of Task Figures 1 and 2 show the sample size for the various tag elements and TYPE values.', '67': 'Note that nearly 80% of the tags were ENAMEX and that almost half of those were subcategofized as organization names.', '68': 'As indicated in table 2, all systems performed better on identifying person names than on identifying organization or location names, and all but a few systems performed better on location names than on organization names.', '69': 'Organization names are varied in their form, consisting of proper nouns, general vocabulary, or a mixture of the two.', '70': 'They can also be quite long and complex and can even have internal punctuation such as a commas or an ampersand.', '71': 'Sometimes it is difficult to distinguish them from names of other types, especially from person names.', '72': 'Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..', '73': 'The difference that recourse to lists can make in performance is seen by comparing two runs made by SRA.', '74': 'The experimental configuration resulted in a three point decrease in recall and one point decrease in precision, compared to the performance of the baseline system configuration.', '75': 'The changes occurred only in performance on identifying organizations.', '76': 'BBN conducted a comparative test in which the experimental configuration used a larger lexicon than the baseline configuration, but the exact nature of the difference is not known and the performance differences are very small.', '77': 'As with the SRA experiment, the only differences in performance between the two BBN configurations are with the organization type.', '78': ""The University of Durham reported that they had intended to use gazetteer and company name lists, but didn't, because they found that the lists did not have much effect on their system's performance."", '79': 'The error scores for persons, dates, and monetary expressions was less than or equal to 10% for the large majority of systems.', '80': 'Several systems posted scores under 10% error for locations, but none was able to do so for oganizations.', '81': 'For percentages, about half the systems had 0% error, which reflects the simplicity of that particular subtask.', '82': 'Note that the number of instances of percentages in the test set is so small that a single mistake could result in an error of 6%.', '83': 'Slot-level performance on ENAMEX follows a different pattern for most systems from slot-level performance on NUMEX and TIMEX.', '84': 'The general pattern is for systems to have done better on the TEXT slot than on the TYPE slot for ENAMEX tags and for systems to have done better on the TYPE slot than on the TEXT slot for NUMEX and TIMEX tags.', '85': 'Errors on the TEXT slot are errors in finding the right span for the tagged string, and this can be a problem for all three subcategories of tag.', '86': 'The TYPE slot, however, is a more difficult slot for ENAMEX than for the other subcategories.', '87': 'It involves a three-way distinction for ENAMEX and only a two-way distinction for NUMEX and TIMEX, and it offers the possibility of confusing names of one type with names of another, especially the possibility of confusing organization names with person names.', '88': 'Looking at the document section scores in table 3, we see that the error score on the body of the text was much lower than on the headline for all but a few systems.', '89': 'There was just one system that posted a higher error score on the body than on the headline, the baseline NMSU CRL configuration, and the difference in scores is largely due to the fact that the system overgenerated to a greater extent on the body than on the headline.', '90': 'Its basic strategy for 96.42 0 95.66 0 0 7 7 94.92 0 0 8 8 94.00 0 0 20 9 93.65 0 2 16 10 93.33 0 4 38 9 92.88 0 0 18 10 92.74 0 0 22 11 92.61 100 0 18 9 91.20 0 0 30 13 90.84 3 11 19 14 89.06 3 4 28 18 88.19 0 0 22 20 85.82 0 6 18 21 85.73 0 44 53 21 84.95 0 0 50 21 Table 3.', '91': 'NE document subsection scores (ERR metric), in order of decreasing overall F-measure (P&R) headlines was a conservative one: tag a string in the headline as a name only if the system had found it in the body of the text or if the system had predicted the name based on truncation of names found in the body of the text.', '92': 'Most, if not all, the systems that were evaluated on the NE task adopted the basic strategy of processing the headline after processing the body of the text.', '93': 'The interannotator variability test provides reference points indicating human performance on the different aspects of the NE task.', '94': 'The document section results show 0% error on Document Date and Dateline, 7% error on Headline, and 6% error on Text.', '95': 'The subcategory error scores were 6% on Organization, 1% on Person, and 4% on Location, 8% on Date, and 0% on Money and Percent.', '96': 'These results show that human variability on this task patterns in a way that is similar to the performance of most of the systems in all respects except perhaps one: the greatest source of difficulty for the humans was on identifying dates.', '97': 'Analysis of the results shows that some Date errors were a result of simple oversight (e.g., ""fiscal 1994"") and others were a consequence of forgetting or misinterpreting the task guidelines with respect to determining the maximal span of the date expression (e.g., tagging ""fiscal 1993\'s second quarter"" and ""Aug. 1"" separately, rather than tagging ""fiscal 1993\'s second quarter, ended Aug. 1"" as a single expression in accordance with the task guidelines).', '98': 'NE Results on ""Walkthrough Article"" In the answer key for the walkthrough article there are 69 ENAMEX tags (including a few optional ones), six TIMEX tags and six NUMEX tags.', '99': 'Interannotator scoring showed that one annotator missed tagging one instance of ""Coke"" as an (optional) organization, and the other annotator missed one date expression (""September"").', '100': 'Common mistakes made by the systems included missing the date expression, ""the 21st century,"" and spuriously identifying ""60 pounds"" (which appeared in the context, ""Mr. Dooner, who recently lost 60 pounds over three-and-a-half months ....', '101': '"") as a monetary value rather than ignoring it as a weight.', '102': 'In addition, a number of errors identifying entity names were made; some of those errors also showed up as errors on the Template Element task and are described in a later section of this paper.', '103': 'COREFERENCE The task as defined for MUC6 was restricted to noun phrases (NPs) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe.', '104': 'The variety of high-frequency phenomena covered by the task is partially represented in the following hypothetical example, where all bracketed text segments are considered coreferential: 428 [Motor Vehicles International Corp.] announced a major management shakeup ....', '105': '[MVI] said the chief executive officer has resigned ....', '106': '[The Big 10 auto maker] is attempting to regain market share ....', '107': '[It] will announce significant losses for the fourth quarter ....', '108': 'A [company] spokesman said [they] are moving [their] operations to Mexico in a cost-saving effort ....', '109': '[MVI, [the first company to announce such a move since the passage of the new international trade agreement],] is facing increasing demands from unionized workers ....', '110': '[Motor Vehicles International] is [the biggest American auto exporter to Latin America].', '111': 'The example passage covers a broad spectrum of the phenomena included in the task.', '112': 'At one end of the spectrum are the proper names and aliases, which are inherently definite and whose referent may appear anywhere in the text.', '113': 'In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.', '114': 'On the periphery of the central phenomena are markables whose status as coreferring expressions is determined by syntax, such as predicate nominals (""Motor Vehicles International is the biggest American auto exporter to Latin America"") and 100 90 80 70 60 50 40 30 20 10 0 0 10 20 30 appositives (""MVI, the first company to announce such a move since the passage of the new international trade agreement"").', '115': 'At the far end of the spectrum are bare common nouns, such as the prenominal ""company"" in the example, whose status as a referring expression may be questionable.', '116': 'An algorithm developed by the MITRE Corporation for MUC6 was implemented by SAIC and used for scoring the task.', '117': 'The algorithm compares the equivalence classes defined by the coreference links in the manually-generated answer key and the system-generated response.', '118': 'The equivalence classes are the models of the identity equivalence coreference relation.', '119': 'Using a simple counting scheme, the algorithm obtains recall and precision scores by determining the minimal perturbations required to align the equivalence classes in the key and response.', '120': 'No metrics other than recall and precision were defined for this task, and no statistical significance testing was performed on the scores.', '121': 'CO Results Overall In all, seven sites participated in the MUC6 coreference evaluation.', '122': 'Most systems achieved approximately the same levels of performance: five of the seven systems were in the 51%-63% recall O0 \x95 40 50 60 70 80 90 100 Recall Figure 3.', '123': 'Overall recall and precision on the CO task 2 2 Key to recall and precision scores: UDurham 36R/44P, UManitoba 63R/63P, UMass 44R/51P, NYU 53R/62P, UPenn 55R/63P, USheffield 51R/71P, SRI 59R/72P..', '124': 'range and 62%-72% precision range.', '125': 'About half the systems focused only on individual coreference, which has direct relevance to the other MUC6 evaluation tasks.', '126': 'A few of the evaluation sites reported that good name/alias recognition alone would buy a system a lot of recall and precision points on this task, perhaps about 30% recall (since proper names constituted a large minority of the annotations) and 90% precision.', '127': 'The precision figure is supported by evidence from the NE evaluation.', '128': 'In that evaluation, a number of systems scored over 90% on the named entity recall and precision metrics, providing a sound basis for good performance on the coreference task for individual entities.', '129': 'In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted.', '130': 'The two versions of the independently prepared, manual annotations of 17 articles were scored against each other using the scoring program in the normal ""key to response"" scoring mode.', '131': 'The amount of agreement between the two annotators was found to be 80% recall and 82% precision.', '132': 'There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc..', '133': 'Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.', '134': 'CO Results on Some Aspects of Task and on ""Walkthrough Article"" To keep the annotation of the evaluation data fairly simple, the MUC6 planning committee decided not to design the notation to subcategorize linkages and markables in any way.', '135': 'Two useful attributes for the equivalence class as a whole would be one to distinguish individual coreference from type coreference and one to identify the general semantic type of the class (organization, person, location, time, currency, etc.).', '136': 'For each NP in the equivalence class, it would be useful to identify its grammatical type (proper noun phrase, definite common noun phrase, bare singular common noun phrase, personal pronoun, etc.).', '137': 'The decision to minimize the annotation effort makes it difficult to do detailed quantitative analysis of the results.', '138': ""An analysis by the participating sites of their system's performance on the walkthrough article provides some insight into performance on aspects of the coreference task that were dominant in that article."", '139': 'The article contains about 1000 words and approximately 130 coreference links, of which all but about a dozen are references to individual persons or individual organizations.', '140': 'Approximately 50 of the anaphors are personal pronouns, including reflexives and possessives, and 58 of the markables (anaphors and antecedents) are proper names, including aliases.', '141': 'The percentage of personal pronouns is relatively high (38%), compared to the test set overall (24%), as is the percentage of proper names (40% on this text versus an estimate of 30% overall).', '142': 'Performance on this particular article for some systems was higher than performance on the test set overall, reaching as high as 77% recall and 79% precision.', '143': 'These scores indicate that pronoun resolution techniques as well as proper noun matching techniques are good, compared to the techniques required to determine references involving common noun phrases.', '144': 'For common noun phrases, the systems were not required to include the entire NP in the response; the response could minimally contain only the head noun.', '145': 'Despite this flexibility in the expected contents of the response, the systems nonetheless had to implicitly recognize the full NP, since to be considered coreferential, the head and its modifiers all had to be consistent with another markable.', '146': 'TEMPLATE ELEMENT The Template Element (TE) task requires extraction of certain general types of information about entities and merging of the information about any given entity before presentation in the form of a template (or ""object"").', '147': 'For MUC6 the entities that were to be extracted were limited to organizations and persons) The ORGANIZATION object contains attributes (""slots"") for the string representing the organization name (ORG NAME), for strings representing any abbreviated versions of the name (ORG_ALIAS), for a string that describes the particular organization (ORG_DESCRIPTOR), for a subcategory of the type of organization (ORG_TYPE, whose permissible values are GOVERNMENT, COMPANY, and OTHER), and for canonical forms of the specific and general location of the organization (ORG LOCALE and ORG_COUNTRY).', '148': 'The PERSON object contains 3The task documentation includes definition of an ""artifact"" entity, but that entity type was not used in MUC6 for either the dry run or the formal run.', '149': 'The entity types that were involved in the evaluation are the same as those required for the Scenario Template task.', '150': 'slots only for the string representing the person name (PER_NAME), for strings representing any abbreviated versions of the name (PERALIAS), and for strings representing a very limited range of titles (PER_TITLE).', '151': 'The task places heavy emphasis on recognizing proper noun phrases, as in the NE task, since all slots except ORG_DESCRIPTOR and PERTITLE expect proper names as slot fillers (in string or canonical form, depending on the slot.', '152': 'However, the organization portion of the TE task is not limited to recognizing the referential identity between full and shortened names; it requires the use of text analysis techniques at all levels of text structure to associate the descriptive and locative information with the appropriate entity.', '153': 'Analysis of complex NP structures, such as appositional structures and postposed modifier adjuncts, is needed in order to relate the locale and descriptor to the name in ""Creative Artists Agency, the big Hollywood talent agency"" and in ""Creative Artists Agency, a big talent agency based in Hollywood.""', '154': 'Analysis of sentence structures to identify grammatical relations such as predicate nominals is needed in order to relate those same pieces of information in ""Creative Artists Agency is a big talent agency based in Hollywood.""', '155': 'Analysis of discourse structure is needed in order to identify long-distance relationships.', '156': 'The answer key for the TE task contains one object for each specific organization and person mentioned in the text.', '157': 'For generation of a PERSON object, the text must provide the name of the person (full name or part of a name).', '158': 'For generation of an ORGANIZATION object, the text must provide either the name (full or part) or a descriptor of the organization.', '159': 'Since the generation of these objects is independent of the relevance criteria imposed by the Scenario Template (ST) task, there are many more ORGANIZATION and PERSON objects in the TE key than in the ST key.', '160': 'For the formal evaluation, there were 606 ORGANIZATION and 496 PERSON objects in the TE key, versus 120 ORGANIZATION and 137 PERSON objects in the ST key.', '161': 'The same set of articles was used for TE as for ST; therefore, the content of the articles is oriented toward the terms and subject matter covered by the ST task, which concerns changes in corporate management.', '162': '4 One effect of this bias is simply the number of entities mentioned in the articles: for the 4 The method used for selecting the articles for the test set is described at the beginning of this article..', '163': 'test set used for the MUC6 dry run, which was based on a scenario concerning labor union contract negotiations, there were only about half as many organizations and persons mentioned as there were in the test set used for the formal run.', '164': 'TE Results Overall Twelve systems -- from eleven sites, including one that submitted two system configurations for testing--were tested on the TE task.', '165': 'All but two of the systems posted F-measure scores in the 7080% range, and four of the systems were able to achieve recall in the 7080% range while maintaining precision in the 8090% range, as shown in the figure 4.', '166': 'Human performance was measured in terms of variability between the outputs produced by the two NRaD and SAIC evaluators for 30 of the articles in the test set (the same 30 articles that were used for NE and CO testing).', '167': 'Using the scoring method in which one annotator\'s draft key serves as the ""key"" and the other annotator\'s draft key serves as the ""response,"" the overall consistency score was 93.14 on the F-measure, with 93% recall and 93% precision.', '168': 'TE Results on Some Aspects of Task Given the more varied extraction requirements for the ORGANIZATION object, it is not surprising that performance on that portion of the TE task was not as good as on the PERSON object 5, as is clear in figure 5.', '169': 'Figure 6 indicates the relative amount of error contributed by each of the slots in the ORGANIZATION object.', '170': 'It is evident that the more linguistic processing necessary to fill a slot, the harder the slot is to fill correctly.', '171': 'The ORG_COUNTRY slot is a special case in a way, since it is required to be filled when the ORG_LOCALE slot is filled.', '172': '(The reverse is not the case, i.e., ORG_COUNTRY may be filled even if ORG_LOCALE is not, but this situation is relatively rare.)', '173': 'Since a missing or spurious ORG_LOCALE is likely to incur the same error in ORG_COUNTRY, the error scores for the two slots are understandably similar.', '174': '5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision..', '175': '((XI 90 \x95 ,,v 80 \x95 \x95 04 ~) 5O 4O 20 10 0 ..', '176': '0 10 220 30 41) 50 60 7(1 80 91) Recall I(X) Figure 4.', '177': 'Overall recall and precision on the TE task 6 10090 80 ° ""7"" o qb l \x95 70 60 50 40 30 20 10 0 0 10 20 30 40 50 60 70 80 90 100 Recall Figure 5.', '178': 'Organization and Person object recall and precision on the TE task 6Key to recall and precision scores: BBN 66R/79P, UDurham 49R/60P, Lockheed-Martin 76R/77P, UManitoba 71R/78P, UMass 53R/72P, MITRE 71R/85P, NYU 62R/83P, USheffield 66R/74P, SRA baseline configuration 75R/86P, SRA ""noref"" configuration 74R/87P, SRI 74R/76P, Sterling Software 72R/83P.', '179': '432 8o I ,o l II ii 40 3o 2o ,~ 10- type name alias country locale descriptor ORGANIZATION Slot= Figure 6.', '180': 'Best and average error per response fill Organization object slot scores for TE task With respect to performance on ORG_DESCRIPTOR, note that there may be multiple descriptors (or none) in the text.', '181': 'However, the task does not require the system to extract all descriptors of an entity that are contained in the text; it requires only that the system extract one (or none).', '182': 'Frequently, at least one can be found in close proximity to an organization\'s name, e.g., as an appositive (""Creative Artists Agency, the big Hollywood talent agency"").', '183': 'Nonetheless, performance is much lower on this slot than on others.', '184': 'Leaving aside the fact that descriptors are common noun phrases, which makes them less obvious candidates for extraction than proper noun phrases would be, what reasons can we find to account for the relatively low performance on the ORG_DESCRIPTOR slot?', '185': 'One reason for low performance is that an organization may be identified in a text solely by a descriptor, i.e., without a fill for the ORG_NAME slot and therefore without the usual local clues that the NP is in fact a relevant descriptor.', '186': 'It is, of course, also possible that a text may identify an organization solely by name.', '187': 'Both possibilities present increased opportunities for systems to undergenerate or overgenerate.', '188': 'Also, the descriptor is not always close to the name, and some discourse processing may be requ~ed in order to identify it --this is likely to increase the opportunity for systems to miss the information.', '189': 'A third significant reason is that the response fill had to match the key fill exactly in order to be counted correct; there was no allowance made in the scoring software for assigning full or partial credit if the response fill only partially matched the key fill.', '190': 'It should be noted that human performance on this task was also relatively low, but it is unclear whether the degree of disagreement can be accounted for primarily by the reasons given above or whether the disagreement is attributable to the fact that the guidelines for that slot had not been finalized at the time when the annotators created their version of the keys.', '191': 'TE Results on ""Walkthrough Article"" TE performance of all systems on the walkthrough article was not as good as performance on the test set as a whole, but the difference is small for about half the systems.', '192': 'Viewed from the perspective of the TE task, the walkthrough article presents a number of interesting examples of entity type confusions that can result from insufficient processing.', '193': 'There are cases of organization names misidentified as person names, there is a case of a location name misidentified as an organization name, and there are cases of nonrelevant entity types (publications, products, indefinite references, etc.) misidentified as organizations.', '194': 'Errors of these kinds result in a penalty at the object level, since the extracted information is contained in the wrong type of object.', '195': 'Examples of each of these types of error appear below, along with the number of systems that committed the error.', '196': '(An experimental configuration of the SRA system produced the same output as the baseline configuration and has been disregarded in the tallies; thus, the total number of systems tallied is eleven.)', '197': '1.', '198': 'Miscategorizations of entities as person (PER_NAME or PER_ALIAS) instead of organization (ORG_NAME or ORG_ALIAS).', '199': '433 \x95 Six systems: McCannErickson (also extracted with the name of ""McCann,"" ""One McCann,"" ""While McCann""; organization category is indicated clearly by context in which full name appears, ""John Dooner Will Succeed James At Helm of McCannErickson"" in headline and ""Robert L. James, chairman and chief executive officer of McCannErickson, and John J. Dooner Jr., the agency\'s president and chief operating officer"" in the body of the article) eSix systems: J. Walter Thompson (also extracted with the name of ""Walter Thompson""; organization category is indicated by context, ""Peter Kim was hired from WPP Group\'s J. Walter Thompson last September..."") eFour systems: Fallon McElligott (organization category is indicated by context, ""...other ad agencies, such as Fallon McElligott"") eOne system: Ammirati & Puris (the presence of the ampersand is a clue, as is the context, ""...president and chief executive officer of Ammirati & Puris""; but note that the article also mentions the name of one of the company\'s founders, Martin Puris)', '200': 'organization (ORG NAME) instead of location (ORG_LOCALE) eTwo systems: Hollywood (location category is indicated by context, ""Creative Artists Agency, the big Hollywood talent agency"") . Miscategorization of nonrelevant entities as organization name, alias or descriptor (ORG NAME, ORG_ALIAS, ORG_DESCRIPTOR).', '201': 'oSix systems: New York Times (publication name in phrase, ""a framed page from the New York Times""; without sufficient context, the name can be ambiguous in its reference to a physical object versus an organization) eThree systems: Coca-Cola Classic (product name deriving from ""Coca-Cola,"" which appears separately in several places in the article and is occasionally ambiguous even in context between product name and organization name) eOne system: Not Butter (part of product name, ""I Can\'t Believe It\'s Not Butter"") eOne system: Taster (part of product name, ""Taster\'s Choice"") \x95 One system: Choice (part of product name, ""Taster\'s Choice"") eFive systems: a hot agency (nonspecific use of indefinite in phrase ""...is interested in acquiring a hot agency"") Given the variety of contextual clues that must be taken into account in order to analyze the above entities correctly, it is understandable that just about any given system would commit at least one of them.', '202': 'But the problems are certainly tractable; none of the fifteen TE entities in the key (ten ORGANIZATION entities and five PERSON entities) was miscategofized by all of the systems.', '203': 'In addition to miscategorization errors, the walkthrough text provides other interesting examples of system errors at the object level and the slot level, plus a number of examples of system successes.', '204': 'One success for the systems as a group is that each of the six smaller ORGANIZATION objects and four smaller PERSON objects (those with just one or two filled slots in the key) was matched perfectly by at least one system; in addition, one larger ORGANIZATION object and two larger PERSON objects were perfectly matched by at least one system.', '205': 'Thus, each of the five PERSON objects in the key and seven of the ten ORGANIZATION objects in the key were matched perfectly by at least one system.', '206': 'The three larger ORGANIZATION objects that none of the systems got perfectly correct are for the McCannErickson, Creative Artists Agency, and Coca-Cola companies.', '207': ""Common errors in these three ORGANIZATION objects included missing the locale/country or failing to organization's alias with its name."", '208': 'descriptor identify or the SCENARIO TEMPLATE A Scenario Template (ST) task captures domain-and task-specific information.', '209': 'Three scenarios were defined in the course of MUC6: (1) a scenario concerning the event of organizations placing orders to buy aircraft with aircraft manufacturers (the ""aircraft order"" scenario); (2) a scenario concerning the event of contract negotiations between labor unions and companies (the ""labor negotiations"" scenario); (3) a scenario concerning changes in corporate managers occupying executive posts (the ""management succession"" scenario).', '210': 'The first scenario was used as an example of the general design of the ST task, the second was used for the MUC6 dry run evaluation, and the third was used for the formal evauation.', '211': 'One of the innovations of MUC6 was to formalize the general structure of event templates, and all three scenarios defined in the course of MUC6 conformed to that general structure.', '212': 'In this article, the management succession scenario will be used as the basis for discussion.', '213': 'The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.', '214': 'At the top level is the TEMPLATE object, of which there is one instantiated for every document.', '215': 'This object points down to one or more SUCCESSION_EVENT objects if the document meets the event relevance criteria given in the task documentation.', '216': 'Each event object captures the changes occurring within a company with respect to one management post.', '217': 'The SUCCESSION_EVENT object points down to the Ib~AND_OUT object, which in turn points down to PERSON Template Element objects that represent the persons involved in the succession event.', '218': 'The IN_AND_OUT object contains ST-specific information that relates the event with the persons.', '219': 'The ORGANIZATION Template Element objects are present at the lowest level along with the PERSON objects, and they are pointed to not only by the IN_AND_OUT object but also by the SUCCESSION_EVENT object.', '220': 'The organization pointed to by the event object is the organization where the relevant management post exists; the organization pointed to by the relational object is the organization that the person who is moving in or out of the post is coming from or going to.', '221': 'The scenario is designed around the management post rather than around the succession act itself.', '222': 'Although the management post and information associated with it are represented in the SUCCESSION_EVENT object, that object does not actually represent an event, but rather a state, i.e., the vacancy of some management post.', '223': 'The relational-level Iih~AND_OUT objects represent the personnel changes pertaining to that state.', '224': 'ST Results Overall Nine sites submitted a total of eleven systems for evaluation on the ST task.', '225': 'All the participating sites also submitted systems for evaluation on the TE and NE tasks.', '226': 'All but one of the development teams (UDurham) had members who were veterans of MUC5.', '227': 'Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.', '228': 'Marginally relevant event objects are marked in the answer key as being optional, which means that a system is not penalized if it does not produce such an event object.', '229': 'The approximate 5050 split between relevant and nonrelevant texts was Template Level (Doc_Nr) JCCESSION_EVE/~ (Post, Vacancy_Reason) In_and_Out r IN_AND_OUT "" Succession Org (New_Status, On_the_Job, Rel Other_Org) j IO Template Element Level PERSON ORGANIZATION 1ame, Per_Alias, (Org_Name, Org_Alias, Org_Descriptor, Per_Title) ~Q0rg_Type, Org_Locale, Org_Country) Figure 7.', '230': 'Management Succession Template Structure intentional and is comparable to the richness of the MUC3 ""TST2"" test set and the MUC4 ""TST4"" test set.', '231': '(The test sets used for MUC5 had a much higher proportion of relevant texts.)', '232': 'Systems are measured for their performance on distinguishing relevant from nonrelevant texts via the text filtering metric, which uses the classic information retrieval definitions of recall and precision.', '233': 'For MUC6, text filtering scores were as high as 98% recall (with precision in the 80th percentile) or 96% precision (with recall in the 80th percentile).', '234': 'Similar tradeoffs and upper bounds on performance can be seen in the TST2 and TST4 results (see score reports in sections 2 and 4 of appendix G in [2]).', '235': 'However, performance of the systems as a group is better on the MUC6 test set.', '236': 'The text filtering results for MUC6, MUC4 (TST4) and MUC3 (TST2) are shown in figure 8.', '237': ""Whereas the Text Filter row in the score report shows the system's ability to do text filtering (document detection), the All Objects row and the individual Slot rows show the system's ability to do information extraction."", '238': 'The measures used for information extraction include two overall ones, the F-measure and error per response fill, and several other, more diagnostic ones (recall, precision, undergeneration, overgeneration, and substitution).', '239': 'The text filtering definition of precision is different from the information extraction definition of precision; the latter definition includes an element in the formula that accounts for the number of spurious template fills generated.', '240': 'The All Objects recall and precision scores are shown in figure 9.', '241': 'The highest ST F-measure score was 56.40 (47% recall, 70% precision).', '242': 'Statistically, large differences of up to 15 points may not be reflected as a difference in the ranking of the systems.', '243': 'Most of the systems fall into the same rank at the high end, and the evaluation does not clearly distinguish more than two ranks (see the paper on statistical significance testing by Chinchor in [1]).', '244': 'Human performance was measured in terms of interannotator variability on only 30 texts in the test set and showed agreement to be approximately 83%, when one annotator\'s templates were treated as the ""key"" and the other annotator\'s templates were treated as the ""response"".', '245': 'No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.', '246': 'The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.', '247': 'Nonetheless, as one rough measure of progress in the area of information extraction as a whole, we can consider the F-measures of the top-scoring systems from the MUC5 and MUC6 evaluations.', '248': 'MUC6 56.40 MUC5 EJV 52.75 MUC5 JJV 60.07 MUC5 EME 49.18 MUC5 JME 56.31 Table 4.', '249': 'Highest P&R F-Measure scores posted for MUC6 and MUC5 ST tasks Note that table 4 shows four top scores for MUC5, one for each language-domain pair: English Joint Ventures (EJV), Japanese Joint Ventures (JJV), English Microelectronics (EME), and Japanese Microelectronics (JME).', '250': 'From this table, it may be reasonable to conclude that progress has been made, since the MUC6 performance level is at least as high as for three of the four MUC5 tasks and since that performance level was reached after a much shorter time.', '251': 'ST Results on Some Aspects of Task and on ""Walkthrough Article"" Three succession events are reported in the walkthrough article.', '252': 'Successful interpretation of three sentences from the walkthrough article is necessary for high performance on these events.', '253': 'The tipoff on the first two events comes at the end of the second paragraph: Yesterday, McCann made official what had been widely anticipated: Mr. James, 57 years old, is stepping down as chief executive officer on July 1 and will retire as chairman at the end of the year.', '254': 'He will be succeeded by Mr. Dooner, 45.', '255': ""The basis of the third event comes halfway through the two-page article: In addition, Peter Kim was hired from WPP Group's J. Walter Thompson last September as vice chairman, chief strategy officer, worldwide."", '256': '7Key to recall and precision scores: BBN 50R/59P, UDurham 33R/34P, Lockheed-Martin 43R/64P, UManitoba 39R/62P, UMass 36R/46P, NYU 47R/70P, USheffield 37R/73P, SRA baseline configuration 47R/62P, SRA ""precision"" configuration 32R/66P, SRA ""recall"" configuration 58R/46P, SRI 44R/61P.', '257': '437 Answer Key System Output Event James out, Dooner in as CEO of McCannJames out, Dooner in as CEO of McCann- #1 Erickson as a result of James departing the Erickson as a result of a reassignment of James; workforce; James is still on the job as CEO; James is no__!', '258': 'on the job as CEO any more, Dooner is not on the job as CEO yet, and his old his new job is at the same as his old job; Dooner job was with the same org as his new job.', '259': 'may or may not be on the job as CEO yet, and his old job was with the same org as his new job.', '260': '(SRA satie_base system) Event James out, Dooner in as chairman of James out, Dooner in as chairman of #2 McCannErickson as a result of James departing McCannErickson as a result of James departing the workforce; James is still on the job as the workforce; James is no_4 on the job as chairman; Dooner is not on the job as chairman chairman any more; Dooner is already on the job yet, and his old job was with the same org as his as chairman, and his old job was with Ammirati new job.', '261': '& Puris.', '262': '(NYU system) Event Kim in as ""vice chairman, chief strategy Kim in as vice chairman of WPP Group, #3 officer, worldwide"" of McCannErickson, where where the vacancy existed for other/unknown the vacancy existed for other/unknown reasons; reasons; he may or may not be on the job in that he is already on the job in the post, and his old post yet, and the article doesn\'t say where his old job was with J. Walter Thompson job was.', '263': '(BBN system) Table 5.', '264': 'Paraphrased summary of ST outputs for walkthrough article The article was relatively straightforward for the annotators who prepared the answer key, and there were no substantive differences in the output produced by each of the two annotators.', '265': 'Table 5 contains a paraphrased summary of the output that was to be generated for each of these events, along with a summary of the output that was actually generated by systems evaluated for MUC6.', '266': 'The system-generated outputs are from three different systems, since no one system did better than all other systems on all three events.', '267': 'The substantive differences between the system-generated output and the answer key are indicated by underlining in the system output.', '268': ""Recurring problems in the system outputs include the information about whether the person is currently on the job or not and the information on where the outgoing person's next job would be and where the incoming person's previous job was."", '269': 'Note also that even the best system on the third event was unable to determine that the succession event was occurring at McCannEfickson; in addition, it only partially captured the full title of the post.', '270': 'To its credit, however, it did recognize that the event was relevant; only two systems produced output that is recognizable as pertaining to this event.', '271': 'One common problem was the simple failure to recognize ""hire"" as an indicator of a succession.', '272': 'Two systems never filled the OTHER_ORG slot or its dependent slot, REL OTHER_ORG, despite the fact that data to fill those slots was often present; over half the IN_AND_OUT objects in the answer key contain data for those two slots.', '273': 'Almost without exception, systems did more poorly on those two slots than on any others in the SUCCESSION_EVENT and IN_AND_OUT objects; the best scores posted were 70% error on OTHER_ORG (median score of 79%) and 72% error on REL_OTHER ORG (median of 86%).', '274': 'Performance on the VACANCY_REASON and ON_THE JOB slots was better for nearly all systems.', '275': ""The lowest error scores were 56% on VACANCY_REASON (median of 70%) and 62% on ONZI'HE_JOB (median of 71%)."", '276': 'The slot that most systems performed best on is NEWSTATUS; the lowest error score posted on that slot is 47% (median of 55%).', '277': 'This slot has a limited number of fill options, and the right answer is almost always either IN or OUT, depending on whether the person involved is assuming a post (IN) or vacating a post (OUT).', '278': 'Performance on the POST slot was not quite as good; the lowest error was 52% (median of 65%).', '279': 'The POST slot requires a text string as fill, and there is no finite list of possible fills for the slot.', '280': 'As seen in the third event of the walkthrough article, the fill can be an extended title such as ""vice chairman, chief strategy officer, worldwide.""', '281': 'For most events, however, the fill is one of a large handful of possibilities, including ""chairman,"" ""president,"" ""chief executive [officer],"" ""CEO,"" ""chief operating officer,"" ""chief financial officer,"" etc. 438 DISCUSSION: CRITIQUE OF TASKS Named Entity The primary subject for review in the NE evaluation is its limited scope.', '282': 'A variety of proper name types were excluded, e.g. product names.', '283': 'The range of numerical and temporal expressions covered by the task was also limited; one notable example is the restriction of temporal expressions to exclude ""relative"" time expressions such as ""last week"".', '284': 'Restriction of the corpus to Wall Street Journal articles resulted in a limited variety of markables and in reliance on capitalization to identify candidates for annotation.', '285': 'Some work on expanding the scope of the NE task has been carried out in the context of a foreign- language NE evaluation conducted in the spring of 1996.', '286': 'This evaluation is called the MET (Multilingual Named Entity) and, like MUC6, was carried out under the auspices of the Tipster Text program.', '287': 'The experience gained from that evaluation will serve as critical input to revising the Engish version of the task.', '288': 'Coreference Many aspects of the CO task are in definite need of review for reasons of either theory or practice.', '289': 'One set of issues concerns the range of syntactically governed correference phenomena that are considered markable.', '290': 'For example, apposition as a markable phenomenon was restrictively defined to exclude constructs that could rather be analyzed as left modification, such as ""chief executive Scott McNealy,"" which lacks the comma punctuation that would clearly identify ""executive"" as the head of an appositive construction.', '291': 'Another set of issues is semantic in nature and includes fimdamental questions such as the validity of including type coreferrence in the task and the legitimacy of the implied definition of coteference versus reference.', '292': 'If an antecedent expression is nonreferential, can it nonetheless be considered coreferential with subsequent anaphoric expressions?', '293': 'Or can only referring expressions corefer?', '294': 'Finally, the current notation presents a set of issues, such as its inability to represent multiple antecedents, as in conjoined NPs, or alternate antecedents, as in the case of referential ambiguity.', '295': 'In short, the preliminary nature of the task design is reflected in the somewhat unmotivated boundaries between markables and nonmarkables and in weaknesses in the notation.', '296': 'One indication of immaturity of the task definition (as well as an indication of the amount of genuine textual ambiguity) is the fact that over ten percent of the linkages in the answer key were marked as ""optional.""', '297': '(Systems were not penalized if they failed to include such linkages in their output.)', '298': 'The task definition is now under review by a discourse working group formed in 1996 with representatives from both inside and outside the MUC commuity, including representatives from the spoken-language community.', '299': 'Template Element There are miscellaneous outstanding problems with the TE task.', '300': 'With respect to the ORGANIZATION and PERSON objects, there are issues such as rather fuzzy distinctions among the three organization subtypes and between the organization name and alias, the extremely limited scope of the person title slot, and the lack of a person descriptor slot.', '301': 'The ARTIFACT object, which was not used for either the dry run or the formal evaluation, needs to be reviewed with respect to its general utility, since its definition reflects primarily the requirements of the MUC5 microelectronics task domain.', '302': 'There is a task-neutral DATE slot that is defined as a template element; it was used in the MUC6 dry run as part of the labor negotiation scenario, but as currently defined, it fails to capture meaningfully some of the recurring kinds of date information.', '303': 'In particular, problems remain with normalizing various types of date expressions, including ones that are vague and/or require extensive use of calendar information.', '304': 'Scenario Template The issues with respect to the ST task relate primarily to the ambitiousness of the scenario templates defined for MUC6.', '305': 'Although the management scenario contained only five domain- specific slots (disregarding slots containing pointers to other objects), it nonetheless reflected an interest in capturing as complete a representation of the basic event as possible.', '306': 'As a result, a few ""peripheral"" facts about the event were included that were difficult to define in the task documentation and/or were not reported clearly in many of the articles.', '307': 'Two of the slots, VACANCY_REASON and ON_THE_JOB, had to be filled on the basis of inference from subtle linguistic cues in many cases.', '308': 'An entire appendix to the scenario definition is devoted to heuristics for filling the ON_THE JOB slot.', '309': 'These two slots caused problems for the annotators as well as for the systems.', '310': ""The annotators' problems with VACANCY_REASON may have had more to do with understanding what the scenario definition was saying than with understanding what the news articles were saying."", '311': ""The annotators' problems with ONZI'HE_JOB were probably more substantive, since the heuristics documented in the appendix were complex and sometimes hard to map onto the expressions found in the news articles."", '312': 'A third slot, REL_OTHER_ORG, required special inferencing on the basis of both linguistics and world knowledge in order to determine the corporate relationship between the organization a manager is leaving and the one the manager is going to.', '313': 'There may, in fact, be just one organization involved --the person could be leaving a post at a company in order to take a different (or an additional) post at the same company.', '314': 'Defining a generalized template structure and using Template Element objects as one layer in the structure reduced the amount of effort required for participants to move their system from one scenario to another.', '315': 'Further simplification may be advisable in order to focus on core information elements and exclude somewhat idiosyncratic ones such as the three slots described above.', '316': 'In the case of the management succession scenario, a proposal was made to eliminate the three slots discussed above and more, including the relational object itself, and to put the personnel information in the event object.', '317': 'Much less information about the event would be captured, but there would be a much stronger focus on the most essential information elements.', '318': 'This would possibly lead to significant improvements in performance on the basic event-related elements and to development of good end-user tools for incorporating some of the domain-specific patterns into a generic extraction system.', '319': 'CONCLUSIONS The results of the evaluation give clear evidence of the challenges that have been overcome and the ones that remain along dimensions of both breadth and depth in automated text analysis.', '320': 'The NE evaluation results serve mainly to document in the MUC context what was already strongly suspected: 1.', '321': 'Automated identification is extremely accurate when identification of lexical pattern types depends only on ""shallow"" information, such as the form of the string that satisfies the pattern and/or immediate context; 2.', '322': 'Automated identification is significantly less accurate when identification is clouded by uncertainty or ambiguity (as when case distinctions are not made, when organizations are named after persons, etc.) and must depend on one or more ""deep"" pieces of information (such as world knowledge, pragmatics, or inferences drawn from structural analysis at the sentential and suprasentential levels).', '323': 'The vast majority of cases are simple ones; thus, some systems score extremely well --well enough, in fact, to compete overall with human performance.', '324': 'Commercial systems are available already that include identification of those defined for this MUC6 task, and since a number of systems performed very well for MUC6, it is evident that high performance is probably within reach of any development site that devotes enough effort to the task.', '325': 'Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one.', '326': 'The TE evaluation task makes explicit one aspect of extraction that is fundamental to a very broad range of higher-level extraction tasks.', '327': 'The identification of a name as that of an organization (hence, instantiation of an ORGANIZATION object) or as a person (PERSON object) is a named entity identification task.', '328': 'The association of shortened forms of the name with the full name depends on techniques that could be used for NE and CO as well as for TE.', '329': 'The real challenge of TE comes from associating other bits of information with the entity.', '330': 'For PERSON objects, this challenge is small, since the only additional bit of information required is the person\'s title (""Mr.,"" ""Ms.,"" ""Dr.,"" etc.), which appears immediately before the name/alias in the text.', '331': 'For ORGANIZATION objects, the challenge is greater, requiring extraction of location, description, and identification of the type of organization.', '332': 'Performance on TE overall is as high as 80% on the F-measure, with performance on ORGANIZATION objects significantly lower (70th percentile) than on PERSON objects (90th percentile).', '333': 'Top performance on PERSON objects came close to human performance, while performance on ORGANIZATION objects fell significantly short of human performance, with the caveat that human performance was measured on only a portion of the test set.', '334': 'Some of the shortfall in performance on the ORGANIZATION object is due to inadequate discourse processing, which is needed in order to get some of the non-local instances of the ORG_DESCRIPTOR, ORG LOCALE and ORG_COUNTRY slot fills.', '335': 'In the case of ORG_DESCRIPTOR, the results of the CO evaluation seem to provide further evidence for the relative inadequacy of current techniques for relating entity descriptions with entity names.', '336': 'Systems scored approximately 1525 points lower (F-measure) on ST than on TE.', '337': 'As defined for MUC6, the ST task presents a significant challenge in terms of system portability, in that the test procedure requ~ed that all domain-specific development be done in a period of one month.', '338': 'For past MUC evaluations, the formal run had been conducted using the same scenario as the dry run, and the task definition was released well before the dry run.', '339': 'Since the development time for the MUC6 task was extremely short, it could be expected that the test would result in only modest performance levels.', '340': 'However, there were at least three factors that might lead one to expect higher levels of performance than seen in previous MUC evaluations: 1.', '341': 'The standardized template structure minimizes the amount of idiosyncratic programming required to produce the expected types of objects, links, and slot fills.', '342': '2.', '343': 'The fact that the domain-neutral Template Element evaluation was being conducted led to increased focus on getting the low- level information correct, which would carry over to the ST task, since approximately 25% of the expected information in the ST test set was contained in the low-level objects.', '344': '3.', '345': 'Many of the veteran participating sites had gotten to the point in their ongoing development where they had fast and efficient methods for updating their systems and monitoring their progress.', '346': 'It appears that there is a wide variety of sources of error that impose limits on system effectiveness, whatever the techniques employed by the system.', '347': 'In addition, the short time frame allocated for domain-specific development naturally makes it very difficult for developers to do sufficient development to fill complex slots that either are not always expected to be filled or are not crucial elements in the template structure.', '348': 'Sites have developed architectures that are at least as general-purpose techniques as ever, perhaps as a result of having to produce outputs for as many as four different tasks.', '349': 'Many of the sites have emphasized their pattern-matching techniques in discussing the strengths of their MUC6 systems.', '350': 'However, we still have full-sentence parsing (e.g. USheffield, UDurham, UManitoba); we sometimes have expectations of ""deep understanding"" (cf.', '351': ""UDurham's use of a world model) and sometimes not (cf."", '352': ""UManitoba's production of ST output directly from dependency trees, with no semantic representation per se)."", '353': 'Some systems completed all stages of analysis before producing outputs for any of the tasks, including NE.', '354': 'Six of the seven sites that participated in the coreference evaluation also participated in the MUC6 information extraction evaluation, and five of the six made use of the results of the processing that produced their coreference output in the processing that produced their information extraction output.', '355': 'The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations.', '356': 'Other sources of excitement are the spinoff efforts that the NE and CO tasks have inspired that bring these tasks and their potential applications to the attention of new research groups and new customer groups.', '357': 'In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure.', '358': 'Finally, a change in administration of the MUC evaluations is occurring that will bring fresh ideas.', '359': 'The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington, D.C. Ms. Marsh has many years of experience in computational linguistics to offer, along with extensive familiarity with the MUC evaluations, and will undoubtedly lead the work exceptionally well.', '360': 'The definition and implementation of the evaluations reported on at the Message Understanding Conference was once again a ""community"" effort, requiring active involvement on the part of the evaluation participants as well as the organizers and sponsors.', '361': 'Individual thanks go to Ralph Grishman of NYU for serving as program co- chair, to Nancy Chinchor for her critical efforts on virtually all aspects of MUC6, and to the other members of the program committee, which included Chinatsu Aone of SRA Corp., Lois Childs of Lockheed Martin Corp., Jerry Hobbs of SRI International, Boyan Onyshkevych of the U.S. Dept. of Defense, Marc Vilain of The MITRE Corp., Takahiro Wakao of the Univ. of Sheffield, and Ralph Weischedel of BBN Systems and Technologies.', '362': 'The author would also like to acknowledge the critical behind-the-scenes computer support rendered at NRaD by Tim Wadsworth, who passed away suddenly in August 1995, leaving a lasting empty spot in my work and my heart.'}",['X96-1048'],['../data/summaries/X96-1048.txt'],['../data/tba/X96-1048.json']
